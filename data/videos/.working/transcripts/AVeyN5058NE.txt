So hello everyone, I'm Kristen Becerbo, I'm the Chief Learning Officer at Khan Academy. I want to lay the groundwork a little bit for why we're here. And the first part is because I'm sure all of you are bombarded by the messages around artificial intelligence. And what we see is that we're all talking about it, but we all don't know a lot about it. So in a survey by Heart Research asked teachers how much they believe their students know about AI and whether they believe that their students know how to use it well. Not surprisingly, about 16% of teachers said that they believe that students know how to use AI well. And 18% of teachers said that they personally knew how to use AI well. So there is certainly room here for us to increase knowledge of our teachers and students about how we use AI. Along with that, we know that teachers get, even though they have relatively low expertise and students as well, 68% of teachers expect to use generative AI tools increasingly to teach, and 66% of students expect to use gen AI tools increasingly to learn. And on top of that, educators just generally believe that education, that AI education is important. So on a scale of one to five, how important do you think it is to teach students how to use technology tools driven by artificial intelligence and understand their pitfalls? And we have 43% believe it's a top priority or very important for students to learn how to use AI to understand its pitfalls. So how do we do that in a space where we know that we have limited resources and time and being our biggest resources that we do not have much of? And how do we start thinking about setting in place some policies and some principles to help students and teachers navigate this space? And one way to, of course, we think about professional development is our main way of upskilling teachers and providing them with new information. And 87% have never had any PD about AI. Not surprising because it's all just blown up recently, but there's certainly space for that to develop. One of the organizations or groups that is trying to address this is a consortium called TGI at Khan Academy. We are part of this consortium. The steering committee for it is code.org, ISTE, ETS, us and the World Economic Forum. And then we have a large advisory group made up of many different organizations who then work together to think about how do we teach with AI and about AI? And those are the two things we need to wrestle to the ground here. One of the things we do is help provide guidance for education leaders and policymakers to think about what that actually looks like to connect to the discussion of teaching with AI to teaching about AI, including sometimes in computer science, but also across disciplines and across domains. And recently we launched the AI Guidance for Schools toolkit. There's a QR code here if you are quick with your phone and want to connect to that. But you can also get it at teachai.org slash toolkit. And it provides lots of guidance on thinking about how we can start developing policy and mood specific language that you might be able to take and modify and lots of thoughts about principles. And in this webinar, we're going to talk about some of those just foundational principles to think through before we start getting to the specifics of what policy looks like. And I want to first to bring down hopefully the anxiety levels, acknowledge that we do not have to solve this only once. We can start with creating policies that address the immediate risks so that AI doesn't undermine our learning in this school year. So that's things like just being clear on our policies about what is academic dishonesty, what is plagiarism, what are the basics that we want to cover. And that is good for stage one. Stage two, we might want to think about, okay, let's upskill everyone in the organization. Let's think about how we can facilitate organizational learning by making some investments in individual learning of the educators that are already excited about AI. So maybe we don't even have to try to cover everyone right off the bat, but start thinking about who are the people who are really excited about this. Bring them in, help them get their skills and knowledge up to where they are, and they can then help us as school leaders work then across the system and think about how we can organize everyone else into the third stage, which is identifying areas for improvements and transformations that potentially can help scale the support across your system. So again, we don't have to do it all at once. You can actually think in stages about how we might start moving towards having good policy and practice around artificial intelligence. I want to be clear, one of the things we're all wrestling with here are the benefits and how we can maximize the benefits and how we can mitigate risks. And so there's lots of potential benefits. And you hear these talked about time saving tools. So how can we use tools that will help save teacher time, help get back some of that time for teachers to spend on the things that really build those human-human relationships and all of those important things that teachers do and maybe less time on some of the administrative tasks that all of us are faced with. Certainly an assistive tool for assessment design and effective feedback. So thinking about how we might be able to, again, make it easier to author and create tools with humans in the loop and also to provide quicker feedback to students. Potentially tutoring and personalized learning. Certainly that's where Khan MeeGo from Khan Academy steps in quite a bit. Thinking about how could AI help us actually increase creativity, collaboration, and skills development. And this is a switch that certainly folks worry that it's decreasing those things. But if we think about how we might design activities for students differently, we might be able to increase those things. And then finally, operational and administrative efficiency. So lots of potential benefits. But what about those risks? How do we think about plagiarism and academic dishonesty? How do we think about diminished agency and accountability? If the AI is just telling us what to do and we follow that, that certainly isn't rewarding for lots of students and teachers. And then who's accountable for those actions and activities? Compromise student privacy and data collection. What happens to this data that students are inputting into these systems that teachers are inputting into these systems? And how is that available and how is it used? Thinking about bias. So we know that these models are trained on given data sets and those data sets themselves are likely not free of bias. And so how are we replicating existing biases in these new systems and how do we avoid that? And then finally, over-reliance on technology and less critical thinking. So another potential risk. We can look at each of these and think about what are ways that we can make sure this dystopian future doesn't come to pass and the more positive view that we see in some of the above things does. And I think that a lot of that is around how we start making decisions right now, how we start thinking and moving ahead into these things. So with that, I'll jump into these seven principles that we think if we start with aligning to some of these, we are more likely to be able to mitigate some of those risks. So first thinking about how we use AI to help all students achieve educational goals. So there's a couple of key words here. What is all students? Because we want to be thinking about not furthering the digital divide. The other side of this coin that Mike Giacana, who at the Brookings Institute wrote about recently was a reverse digital divide, where in fact the students with less resources end up with just the AI tutors and the more well-off privileged students end up with AI tutors and humans. We want to think about making sure that all students have access to both of those things and what that means for them to succeed. And then second, achieving educational goals. This is important for students when we think about these new technologies, sometimes it's fun to be like, oh, look at this shiny thing. How can we use it? Instead, we should be thinking about what are the learning problems that we want to solve and how might AI help solve some of those problems. So start with the things we want to, the problems we want to solve first and then apply the technology as opposed to starting with the technology. Second, thinking about adhering and reaffirming existing policies. So most districts likely have policies about use of technology, policies about academic honesty. Many of those are going to apply still to AI or with some small tweaks can be made to apply to artificial intelligence. So thinking about reaffirming what you already have. You don't need to start from scratch with all of this. Third, promoting AI literacy. So knowledge is an important component of this. It's really hard to set these policies without having some understanding of the technologies and where they can fall and where they're good. So thinking about promoting AI literacy. Fourth balance, realizing the benefits of AI and addressing the risks. So that's what we were just talking about. It's pretty easy to fall into just one camp or the other. Trying to keep a balanced view of this technology is important. Fifth, integrity. So advancing academic integrity, making decisions that are ensuring that you're on the side of trying to de-risk some of those issues around making students are doing their work and doing their own work. At the same time, six, maintaining human decision making when using AI. This is a key one that I think we all want ultimately our goal for students to be lifelong learners and to be their own guides in learning. And that means that they can't just come to rely on technology to make recommendations for what to do next. And they shouldn't just be relying on the teachers for recommendations on what to do next. We want to think about how we can build in that student power. And the same for teachers as we think about maybe using AI to help draft lesson plans. That doesn't mean the AI writes the whole lesson plan for them and they just use it as is. It's thinking about how we use these tools as assistants and how they can help assist and advocate and bring together better performance by the teachers as opposed to thinking about how they're replacing that job that the teachers used to do. And finally, evaluation. How are we assessing the impacts of AI? How do we know what's working and what's not working and iteratively improving on things so they can get even better? So I think these are seven good things to keep in mind. And I think can help provide a framework for how we think about some of the work that we might want to do to set policies. I'm going to give you a couple examples from how we use these in developing ConMigo. And then we'll open it up for discussion. So first thinking about that idea of purpose, students achieving educational goals. Well, as we started designing ConMigo, we said, what are those problems that we want to solve about learning? And starting even back further, what do we know about how students learn? Well, we know students learn more when they're actively engaged with materials to be learned and not just doing busy work, but cognitively engaged. That they're thinking through how does this fit into what I already know, linking it to other concepts, explaining in their own words. Those are signs of cognitive engagement. And that can be tough sometimes when you're in a class of 30 to get every student cognitively engaged. So that might be a problem we could solve with AI. Second, we know students learn more when they're working on material that's at the edge of what they can do. And they're provided with just enough support to be successful at that. If they're working on things that are really easy, they can just do them on their own. They're probably not learning lots of new things. If they're working on something that's really difficult, they're going to get frustrated and give up. So you want them to be just at that edge where there's a little bit of support, but it's really hard for a teacher in a big class to provide that little bit of extra support to every student in the class if they're practicing something independently. So again, something that AI might be able to help with. Third, immediate feedback on responses to new things. So we know that Khan Academy, even without AI, was pretty good at giving, for example, on math and science questions immediate response of whether that's a correct or an incorrect answer and elaborating on why it's correct or incorrect. But we couldn't give step-by-step feedback or figure out where they're starting to go on the wrong path as they're solving a problem. And with writing and open responses, we really couldn't give much immediate feedback at all. Another potential, another problem we've had that maybe AI could help us solve. And finally, seeing value in learning. So we know from motivation theory that students do things when they think they're going to be successful with them and when they value doing that thing. And value can be a lot of things. It can be seeing how this is relevant to their world. They can value it because it shows them something about the wonder of how the world works. It can value it because their friends are doing it too. They can value something because they get points for doing it. Lots of different things. But we know teachers, the most common question teachers ask us is, how can I keep my kids motivated? And so certainly there's a question of can AI help with some of that motivation too. So that's how when we started off, we didn't start off from, hey, look at this new technology. We started off with, we know this is how people learn. And we know this is some of the things that we've struggled with in helping students learn at all of the students in a classroom. Could AI help us solve any of these things? And so that's what you see when we have ConNigo, for example, that providing support just in time. If a student is working on a problem on Khan Academy, as you see here, we see them can type into ConNigo, I'm stuck. And ConNigo will say, well, let's break it down together. Let's first try to simplify by combining like terms and suggest how you might do that. So starts walking through step by step. And it gets to that question of how many students are sitting up with their in class, they're working on problems. They have a question, they raise their hand and they end up sitting there, sitting there because the teacher's running around answering as many individual questions as they can. But it's really difficult to do that in a large classroom. So this is an idea of being able to provide that support for students, which again, we're going back to the principles. It's that education for all, being able to support that education for all. It can provide, again, that immediate feedback. So if I say, okay, so if I combine minus 3R plus 6R plus 1, I get minus 3R plus 1. It should be plus positive 3R plus 1. And it's not quite when you combine those terms, you should add them together. What do you get when you add minus 3R plus 6R? And we'll see what the student says. And if they may be able to diagnose the problem here is actually adding negative numbers. So good kinds of again, providing that support, what's the education piece we want to solve? We talked about academic integrity. So there's a number of things here on that principle of academic integrity that we added. So first is we did what's called prompt engineering with AI where we write, when the student, for instance, just says, I'm stuck. What we send to the model is both what the student says and about 500 words of a prompt that tells the model how to respond. And that 500 words includes a lot of things about how to act like a good tutor that's from the research on tutoring. But it also includes things like do not give the student the answer. That's really an instruction. We tell them when answering. So when a student says, can you just give me the answer? It says, as your AI tutor, my goal is to help you learn and understand the process. So I won't give you the answer directly, but I'm here to guide you through each step. Let's focus on that right side of the equation, minus 3R plus 6R. So it corrects them and brings them back to the thing we're trying to solve here. And so that's a part of how we can build academic integrity into a system that is made specifically for education that isn't the case, for instance, with just chat GPT out in the wild. We also, when we started thinking about academic integrity, wanted the transcripts of these chats to be viewable by teachers and parents so that they could see what students were doing as they interact with them. So those are two key things that we did in terms of working through this. And then we've also been doing through teach AI, thinking about how we can set policy around academic integrity, for instance, providing very clear expectations about what is plagiarism and what are the expectations for when you will use these kinds of tools and when you won't. So students know they're not just guessing what the right thing to do is. They know what the right thing to do is and what that looks like.