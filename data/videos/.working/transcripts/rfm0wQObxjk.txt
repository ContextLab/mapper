I've got a transformation T that's a mapping from Rn to Rn. It can be represented by the matrix A. So the transformation of x is equal to A times x. We saw in the last video, it's interesting to find the vectors that only get scaled up or down by the transformation. So we're interested in the vectors where if I take the transformation of some special vector v, it equals, of course, A times v. And we say it only gets scaled up by some factor lambda times v. And these are interesting because they make for interesting basis vectors. The transformation matrix and the alternate basis, this is one of the basis vectors might be easier to compute, might make for good coordinate systems. But they're in general interesting. And we call vectors v that satisfy this. We call them eigenvectors. And we call their scaling factors the eigenvalues. Eigenvalues associated with this transformation and that eigenvector. And hopefully from that last video, we have a little bit of appreciation of why they're useful. But now in this video, let's at least try to determine what some of them are. Based on what we know so far, if you show me an eigenvector, I can verify that it definitely is the case or an eigenvalue. I could verify the case. But I don't know how a systematic way of solving for either of them. So let's see if we can come up with something. So in general, we're looking for solutions to the equation. a times v is equal to lambda v. It's equal to lambda times the vector. Now one solution might immediately pop out at you. And that's just v is equal to the zero vector. And that definitely is a solution. Although it's not normally considered to be an eigenvector just because, one, it's not a useful basis vector. It doesn't add anything to a basis. It doesn't add really the amount of vectors that you can span when you throw the basis vector in there. And also, it's not clear what is your eigenvalue that's associated with it. Because if v is equal to zero, any eigenvalue will work for that. So normally when we're looking for eigenvectors, we start with the assumption that we're looking for non-zero vectors. So we're looking for vectors that are not equal to the zero vector. So given that, let's see if we can play around with this equation a little bit and see if we can at least come up with eigenvalues maybe in this video. So we subtract a v from both sides. We get the zero vector is equal to lambda v minus a times v. Now we can rewrite v as v is just the same thing as the identity matrix times v. v is a member of Rn. The identity matrix n by n, you just multiply. We're just going to get v again. So if I rewrite v this way, at least on this part of the expression, and let me swap sides. So then I'll get lambda times, instead of v, I'll write the identity matrix, the n by n identity matrix, times v minus a times v is equal to the zero vector. Now I have one matrix times v minus another matrix times v. Matrix vector products, they have the distributive property. So this is equivalent to the matrix lambda times the identity matrix minus a times the vector v, and that's going to be equal to zero. This is just some matrix right here. And the whole reason why I made this substitution is so that I could write this as a matrix vector product instead of just a scalar vector product. And that way I was able to essentially factor out the v and just write this whole equation as essentially some matrix vector product is equal to zero. Now, if we assume that this is the case, and we're assuming, remember, we're assuming that v does not equal to zero. So what does this mean? So we know that v is a member of the null space of this matrix right here. Let me write this down. v is a member of the null space of lambda i sub n minus a. I know that might look a little convoluted to you right now, but just imagine this is just some matrix B. It might make it simpler. This is just some matrix here, right? That's B. Let's make that substitution. Then this equation just becomes B v is equal to zero. Now, if we want to look at the null space of this, the null space of B is all of the vectors x that are a member of rn such that B times x is equal to zero. Well, v is clearly one of those guys, right? Because B times v is equal to zero. We're assuming v solves this equation. That gets us all the way to the assumption that v must solve this equation. And v is not equal to zero. So v is a member of the null space, and this is a non-trivial member of the null space. We already said that the zero vector is always going to be a member of the null space, and it would make this true. But we're assuming v is non-zero. We're only interested in non-zero eigenvectors, and that means that this guy's null space has to be non-trivial. So this means that the null space of lambda, i n minus a, is non-trivial. The zero vector is not the only member. And you might remember before that the only time we write this in general, if I have some matrix, I don't know if I used a and b, let's say I have some matrix D. D is columns. D's columns are linearly independent if and only if the null space of D only contains the zero vector. So if we have some matrix here whose null space does not only contain the zero vector, then it has linearly dependent columns. It has linearly dependent columns. So we know that, and I just wrote that there to kind of show you what we do know. And the fact that this one doesn't have a trivial null space tells us that we're dealing with linearly dependent columns. So lambda, i n minus a, it looks all fancy, but this is just a matrix. Must have linearly dependent columns. Or another way to say that is, if you have linearly dependent columns, you're not invertible, which also means that your determinant must be equal to zero. All of these are true. If your determinant is equal to zero, you're not going to be invertible. You're going to have linearly dependent columns. If your determinant is equal to zero, then that also means that you have non-trivial members in your null space. And so if your determinant is equal to zero, that means there's some lambdas for which this is true for non-zero vectors v. So if there are some solutions, if there are some non-zero vector v's that satisfy this equation, then this matrix right here must have a determinant of zero. And it goes the other way. If this guy has a determinant of zero, then there must be, or there's some lambdas that make this guy have a determinant of zero, then those lambdas are going to satisfy this equation. And you can go the other way. If there's some lambdas that satisfy this, then those lambdas are going to make this matrix have a zero determinant. So the determinant, let me write this. So if and only if, so let me write this. a v is equal to lambda v for non-zero v's if and only if the determinant of lambda i n minus a is equal to the zero vector. No, not the zero vector, sorry, it's just equal to zero. The determinant is just a scalar factor. So that's our big takeaway. I know what you're saying now. How is that useful for me, Sal? We did all of this manipulation. I talked a little bit about the null spaces. And my big takeaway is that in order for this to be true for some non-zero vectors v, then lambda has to be some value such that if I take the determinant of lambda times the identity matrix minus a, it has got to be equal to zero. And the reason why this is useful is that you can actually set this equation up for your matrices and then solve for your lambdas. And we're going to do that in the next video.