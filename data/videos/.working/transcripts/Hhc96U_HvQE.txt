We know that if we have some linear transformation, that's a transformation from x to y, and these are just sets of vectors, and t is a linear transformation from y to z, that we can construct a composition of s with t that is a linear transformation from x all the way to z. We saw this several videos ago. And the definition of our linear transformation, or the composition of our linear transformation. So the composition of s with t applied to some vector x in our set x, our domain, is equal to s of t of x. This was our definition. And then we went on and we said, look, if s of x can be represented as the matrix multiplication A x, the matrix vector product, and if t of x can be represented, or the transformation t can be represented as the product of the matrix B with x, we saw that this thing right here, which is, of course, if we just write it this way, this is equal to A times t times x, which is just Bx, we saw in multiple videos now, that this is equivalent to, by our definition of matrix products, this is equal to the matrix AB, right, when you take the product of two matrices, you just get another matrix, the product AB times x. So you take, essentially, the first linear transformation in your composition, its matrix, which was A, and then you take the product with the second one. Fair enough. All of this is reviewed so far. So let's take three linear transformations. Let's say that I have the linear transformation H, and when I apply that to a vector x, it's equivalent to multiplying my vector x by the matrix A. Let's say I have the linear transformation G. When I apply that to a vector x, it's equivalent to multiplying that vector, there should be a new concept called a vector x. It's equivalent to multiplying that vector times the matrix B, and then I have a final linear transformation, F, when it's applied to some vector x. It's equivalent to multiplying that vector x times the matrix C. Now, what I'm curious about is what happens when I take the composition of H with G, and then I take the composition of that with F, and then I take the composition of that with F, these are all linear transformations, and then I apply that to some vector x. And then I apply that to some vector x, which is necessarily going to be in the domain of this guy. I haven't actually drawn out their domain and codomain definitions, but I think you get the idea. So let's explore what this is a little bit. Well, by the definition of what a, let's go back, by this definition right here of what composition even means, we can just apply that to this right here. So we could just imagine this as being our S, if we imagine this was our S, and then this is our T right there, then what is this going to be equal to? If we just do a straight up pattern match right there, this is going to be equal to S, the transformation S applied to the transformation F applied to x. So S is h of g. So it is h, or I shouldn't say h of g, the composition of h with g, that is our S. And then I apply that to F applied to x, F is our T. I apply that to F applied to x, just like that. Now, what is this equal to? What is this equal to? Now we can imagine that this is our x, if we just pattern match according to this definition, that this is this guy right here, that this is our T, and that this is our S. And so if we just pattern match here, this is equal to what? This is just straight from the definition of a composition. So it's equal to S of, S is our h, so h of t, which in this case is g, g applied to x. But instead of an x, we have a whole, this vector here, which was the transformation F applied to x. So g of f of x. That's what this is equal to. The composition of h with g, or the composition of f with h the composition of h and g, all of that applied to x is equal to h of g of f of x. Now what is this equal to? What is this equal to? Well this is equal to, I'll do it right here, this is equal to h, the transformation h applied to, what is this term right here, I'll do it in pink. What is this? That is the composition of g and f applied to x. You can just replace s with g and f with t, and you'll get that right there. So this is just equal to the composition of g with f applied to x. That's all that is. Now what is this equal to right there? And it's probably confusing to see two parentheses in different colors, but I think you get the idea. What is this equal to? Well just go back to your definition of the composition. I just want to make it very clear what we're doing. If you imagine this being your t and then this being your s, this is just the composition of s with t applied to x. So this is just equal to, like this way, this is equal to, I shouldn't write s's, this is the composition of h with the composition of g and f. And then all of that applied to x. Now why did I do all of this? Well one, to show you that the composition is associative. And then I went all the way back. And essentially it doesn't matter where you put the parentheses. The composition of h with g with f is equivalent to the composition of h with the composition of g and f. That these two things are equivalent. And essentially you can just rewrite them. The parentheses are essentially unnecessary. You can write this as the composition of h with g with f, all of that applied to x. Now I took the time to say that each of these linear transformations I can represent as matrix multiplications. Why did I do that? Well we saw before that any composition, when you take the composition of s with t, the matrix version of this transformation, of this composition, is going to be equal to the product, by our definition of matrix-matrix products, the product of the s's transformation matrix, and t's transformation matrix. So what are these going to be equal to? So this one right here, if you think of this transformation right here, this statement right here, it's matrix version of it. Let me write that. The matrix version of the composition of h with g. And then the composition of that with f applied to x is going to be equal to, and we've seen this before, the product of these matrices. So this composition, its matrix is going to be ab. h and g, their matrices are a and b. So it's going to be ab, ab, and I'll do it in parentheses. And then you take that matrix, and you take the product, so this guy's matrix representation is ab, and this guy's matrix representation is c. So the matrix representation of this whole thing is this guy taking the product of ab, and then taking the product of that with c. So ab and then c. And then if you look at this guy right here, and of course all of that times a vector x, all of that times some vector x right there. That's the vector x. Now let's look at this one right here. If we take the composition of h with the composition of g and f, and apply all of that to some vector x, what is that equivalent to? Well, this composition right here, the matrix version of it, I guess we can say, is going to be the product bc, and we're going to apply that to x. So we're going to have the product bc, and then we're going to take the product of that with this guy's matrix representation, which is a. And we've shown this before. We never showed it with 3, but it extends. I mean, I kind of showed it extends. You can just keep applying the definition. You can keep applying this property right here, and so it'll just naturally extend. Because every time we're just taking the composition of two things, even though it looks like we're taking the composition of 3, we're taking the composition of two things first here, and then we get its matrix representation. And then we take the composition of that with this other thing. So the matrix representation of the entire composition is going to be this matrix times this matrix, which I did here. Similarly, here we take first the matrix composition of these two linear transformations, and their matrix representation will be that right there. And then we take the linear, we take the composition of that with that. So it's the entire matrix representation is going to be this guy's matrix times this guy's matrix. So a times bc. And of course, all of that applied to the vector x. Now, in this video, I've showed you that these two things are equivalent. If anything, the parentheses are completely unnecessary. And I showed you that there. They both essentially boil down to h of g of f of x. So these two things are equivalent. So we could say, essentially, that these two things over here are equivalent. Or that AB, the product AB, and then taking the product of that matrix with the matrix C, is equivalent to taking the product A with the matrix bc, which is just another product matrix. Or another way of saying it is that these parentheses don't matter, that all of these is just equal to abc. I mean, this is just a statement that matrix products exhibit the associative property. It doesn't matter where you put the parentheses. And sometimes it's confusing in the word associative. It just means it doesn't matter where you put the parentheses. Matrix products do not exhibit the commutative property. We saw that in the last video. In general, we cannot make the statement that ab is equal to ba. We cannot do that. And in fact, in the last video, I think it was the last video, I showed you that if ab is defined, sometimes ba is not even defined. Or if ba is defined, sometimes ab isn't defined. So it's not commutative. It is associative, though. In the next video, I'll see if matrix products are actually distributive.