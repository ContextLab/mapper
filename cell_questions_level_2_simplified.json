{
  "questions": [
    {
      "question": "Why do organisms with more complex nervous systems usually show greater learning ability and behavioral flexibility instead of just more fixed, innate actions?",
      "options": {
        "A": "Because a more complex nervous system simply increases the number of hardwired reflex circuits, so behavior becomes more pre-programmed.",
        "B": "Because complexity expands the neural architecture for experience-dependent plasticity, allowing the organism to form new associations and learn new strategies.",
        "C": "Because greater complexity removes variability in sensory inputs, causing responses to be uniform across different contexts.",
        "D": "Because a more complex nervous system ensures that all behaviors are strictly genetically predetermined and unchangeable."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was made more concise and directed to undergraduates; retained technical term 'experience-dependent plasticity' for precision; clarified contrast between learned flexibility and fixed innate actions.",
      "content_preserved": true,
      "source_article": "Behavior",
      "x": 1.3006635904312134,
      "y": 1.0175594091415405,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Innate versus learned behavior and the interaction of genetics and environment in shaping behavior.",
        "Concept 2: The relationship between nervous system complexity and learning capacity/behavioral flexibility.",
        "Concept 3: Behavior as an output to environmental stimuli, influenced by extrinsic and intrinsic motivation, within different theoretical frameworks (ethology vs. behaviorism)."
      ],
      "original_question_hash": "38a775e6"
    },
    {
      "question": "Why do nudges that set a beneficial option as the default usually increase uptake more reliably than campaigns that only provide information?",
      "options": {
        "A": "Because defaults reduce effort and exploit status‑quo inertia and loss aversion: people often stick with the preset option even when they know alternatives.",
        "B": "Because informational campaigns always increase conscious deliberation and therefore reliably overcome cognitive biases to produce better choices.",
        "C": "Because making an option the default removes any future decision-making entirely, permanently eliminating the cognitive load associated with that choice.",
        "D": "Because default nudges work by applying coercive incentives that override individual preferences and force people to comply in most situations."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened for clarity; jargon reduced but kept key terms (status‑quo inertia, loss aversion). Options were rewritten to be concise and plausible while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Behavioural sciences",
      "x": 1.3076823949813843,
      "y": 1.0211070775985718,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Brain–behaviour linkage — how neural structures, neurochemistry, and physiology underpin observable actions.",
        "Concept 2: Cognitive biases/heuristics and behavior change — how biases influence decisions and how nudges or interventions can modify behavior.",
        "Concept 3: Interdisciplinary integration and methods — the necessity of combining psychology, neuroscience, economics, and social sciences using correlational and longitudinal designs to understand causes and consequences of behaviour."
      ],
      "original_question_hash": "7859461f"
    },
    {
      "question": "Why does natural selection tend to increase the frequency of an advantageous trait in a population over successive generations?",
      "options": {
        "A": "Because individuals carrying the advantageous trait have higher survival and/or reproductive success, so they produce more offspring and transmit more copies of the trait to the next generation.",
        "B": "Because the environment repeatedly creates new copies of the trait allele within long-lived individuals independent of reproduction, raising its frequency.",
        "C": "Because completely random mating makes all alleles equally likely to increase, so beneficial alleles rise in frequency without differential reproduction.",
        "D": "Because new beneficial mutations are produced every generation and natural selection immediately fixes each one in the population."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the stem to be concise and clear for undergraduates; clarified option A to emphasize differential survival and reproduction; kept distractors plausible but incorrect; removed unnecessary historical detail.",
      "content_preserved": true,
      "source_article": "Evolutionary biology",
      "x": 1.8243932723999023,
      "y": 1.113900065422058,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Natural selection as differential survival and reproduction of variants, leading to the increasing frequency of advantageous traits over generations.",
        "Concept 2: Genetic variation as the substrate for evolution, arising from mutation and transmitted through inheritance, drift, and gene flow to shape adaptation.",
        "Concept 3: Evolutionary frameworks (modern synthesis and evo-devo) that integrate multiple disciplines to explain how development, genetics, ecology, and paleontology together account for evolutionary change."
      ],
      "original_question_hash": "be881166"
    },
    {
      "question": "Why does specifying a probability distribution over outcomes together with a loss (or valuation) function let us treat a situation as \"risk\" instead of mere \"uncertainty\"?",
      "options": {
        "A": "Because risk is characterised by a probability distribution over outcomes plus a loss function, which makes the situation measurable and lets us compute an expected loss (e.g. $E[L]=\\sum_i p_iL_i$); uncertainty lacks that full probabilistic and loss specification.",
        "B": "Because risk requires observing a random sample of realized outcomes, whereas uncertainty involves no probabilities or stochastic structure at all.",
        "C": "Because uncertainty always refers to past, observed data, while risk only applies to future events and forecasts.",
        "D": "Because risk assumes all outcomes are equally desirable and only uncertainty allows different losses to be assigned to different outcomes."
      },
      "correct_answer": "A",
      "simplification_notes": "Question reworded for clarity and concision; added explicit expected-loss formula $E[L]=\\sum_i p_iL_i$ to show measurability. Distractors rewritten to be plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Uncertainty",
      "x": 1.568312168121338,
      "y": 1.1321742534637451,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The relationship between uncertainty and risk, including how risk is defined and measured (probabilities and potential losses) and the view that risk may not be a true form of uncertainty.",
        "Concept 2: The measurement framework for uncertainty (using probability distributions and density functions) and the idea of second-order uncertainty as a distribution over first-order probabilities.",
        "Concept 3: The distinction between risk and variability (risk is tied to a probability distribution with potential losses; variability is about observed frequencies from data)."
      ],
      "original_question_hash": "b0025e4b"
    },
    {
      "question": "How does geographic isolation (allopatry) facilitate the formation of new species over geological time?",
      "options": {
        "A": "It increases gene flow among separated populations, causing their gene pools to become more similar.",
        "B": "It prevents genetic drift and natural selection from acting differently in each population, keeping them the same.",
        "C": "It reduces or blocks gene flow so populations experience different selection pressures and drift, allowing them to diverge and evolve reproductive barriers.",
        "D": "It causes identical mutations to arise in all isolated populations, ensuring they do not diverge."
      },
      "correct_answer": "C",
      "simplification_notes": "Question rephrased for undergraduate level; technical terms (allopatry, gene flow, drift, reproductive barriers) used explicitly. Distractors rewritten to be plausible misconceptions about isolation.",
      "content_preserved": true,
      "source_article": "Biogeography",
      "x": 1.6893751621246338,
      "y": 1.0465501546859741,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3"
      ],
      "original_question_hash": "36ba4f98"
    },
    {
      "question": "In a self-directed learning framework, why does giving learners control over what, how, and when they study improve their ability to apply knowledge to new situations?",
      "options": {
        "A": "It reduces reliance on external feedback, forcing learners to depend on self-assessment and thereby slowing their development.",
        "B": "It cultivates metacognitive regulation, goal-setting, and experimentation with learning strategies, strengthening adaptability and transfer to novel problems.",
        "C": "It guarantees faster mastery by allowing learners to skip difficult topics and concentrate only on easier material.",
        "D": "It produces a standardized pacing and curriculum so all learners progress identically."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified and shortened the question to focus on learner control and knowledge transfer; preserved technical terms (metacognition, transfer) and kept correct answer unchanged. Distractors were made concise and plausible.",
      "content_preserved": true,
      "source_article": "Autodidacticism",
      "x": 1.272708535194397,
      "y": 1.0016356706619263,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Learner autonomy and self-directed learning (heutagogy, self-determined learning) — how and why the learner should center and direct their own learning, choose methods, and adapt strategies.",
        "Concept 2: Relationship between autodidacticism and formal education — why autodidactic learning can complement or substitute formal schooling, and how this relationship affects outcomes and the learning process.",
        "Concept 3: Self-regulation mechanisms and learning environments — how self-study strategies (materials selection, pacing, critical thinking, learning spaces) enable effective autonomous learning and competency development."
      ],
      "original_question_hash": "64dbbb00"
    },
    {
      "question": "In the standard rational labour‑supply model, why can an increase in the wage rate $w$ lead to an ambiguous change in the number of hours an individual chooses to work?",
      "options": {
        "A": "Because a higher wage always raises the opportunity cost of leisure, so the substitution effect unambiguously increases hours worked.",
        "B": "Because higher wages always increase income and, if leisure is a normal good, individuals unambiguously choose more leisure and thus work fewer hours.",
        "C": "Because a wage increase generates two opposing effects: a substitution effect (work is relatively more attractive, tending to raise hours) and an income effect (higher income allows more leisure, tending to reduce hours); the net change depends on which effect dominates.",
        "D": "Because individual labour supply is unaffected by wages; hours are fixed and only firms' hiring decisions determine employment."
      },
      "correct_answer": "C",
      "simplification_notes": "Removed extraneous historical and descriptive material; restated the question succinctly using standard labour‑supply terminology (wage $w$, substitution and income effects); preserved the economic reasoning and made distractors plausible.",
      "content_preserved": true,
      "source_article": "Labour economics",
      "x": 1.2746483087539673,
      "y": 0.933634877204895,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Rational optimization in labor supply and demand (how wages, leisure, and profits drive decisions and shape market outcomes)",
        "Concept 2: Division of labor and human capital as drivers of productivity and economic growth (how specialization and education affect output and efficiency)",
        "Concept 3: Relationships among labor market statistics (labor force, labor force participation rate, unemployment rate) and what they reveal about labor market health and utilization"
      ],
      "original_question_hash": "a03dbf10"
    },
    {
      "question": "Why does an aggregate increase in households' demand for consumer goods typically lead to higher total output and employment in a market economy without a central planner?",
      "options": {
        "A": "Because higher consumer demand automatically lowers market prices, which induces more purchases and therefore causes firms to expand output.",
        "B": "Because higher demand raises firms' revenues and profits, sending a signal to expand production and hire more workers to meet stronger sales.",
        "C": "Because higher consumer demand directly increases the money supply, which creates jobs independently of firms' production decisions.",
        "D": "Because higher demand reduces the need for capital, causing firms to substitute labour for capital and thus hire more workers."
      },
      "correct_answer": "B",
      "simplification_notes": "Compressed wording; used 'aggregate increase' and 'market economy' for clarity; made the correct mechanism explicit (revenues/profits signal firms to expand); kept the other options as plausible misconceptions.",
      "content_preserved": true,
      "source_article": "Economics",
      "x": 1.2247207164764404,
      "y": 0.9428570866584778,
      "level": 2,
      "concepts_tested": [
        "Micro-to-macro link: how the interactions of individual agents (households, firms, buyers, sellers) in markets generate larger-scale outcomes (production, distribution, consumption, inflation, growth).",
        "Positive vs. normative economics: distinction between describing how the economy works and advocating how it should work, shaping analysis and policy implications.",
        "Factors of production and macro outcomes: the roles of labor, capital, land, and enterprise in affecting production, growth, inflation, and the impact of public policies."
      ],
      "original_question_hash": "74ada649"
    },
    {
      "question": "Why does combining quantitative spatial methods (e.g., cartography, GIS) with qualitative methods (e.g., interviews) give a better understanding of spatial phenomena?",
      "options": {
        "A": "Because it simply increases the amount of data, improving statistical significance.",
        "B": "Because it uses the strengths of both data types to reveal spatial patterns and the social meanings behind them, enabling triangulation and interpretation across scales.",
        "C": "Because qualitative methods eliminate measurement error in spatial data.",
        "D": "Because it removes the need for theoretical frameworks by providing more empirical data."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the question to plain academic language, gave concrete examples (cartography, GIS, interviews), shortened sentences, retained the idea of triangulation and multiscale interpretation.",
      "content_preserved": true,
      "source_article": "Geography",
      "x": 1.5406858921051025,
      "y": 1.0278006792068481,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Space, place, time, and scale as core lenses for analyzing geographic phenomena.",
        "Concept 2: Geography as an interdisciplinary bridge between natural science and social science, focusing on human–environment interactions.",
        "Concept 3: Methodological pluralism in geography (quantitative and qualitative approaches, including cartography, remote sensing, interviews, and surveying) and the use of mixed-methods to study spatial phenomena."
      ],
      "original_question_hash": "2a2d0fda"
    },
    {
      "question": "If an attacker has stolen a database of password hashes (an \"offline\" attack), why does adding a unique random salt to each user's password before hashing make rainbow-table attacks ineffective?",
      "options": {
        "A": "Because salts slightly increase the per-hash work, making construction or use of rainbow tables noticeably slower across all users.",
        "B": "Because salts cause the same plaintext password to produce different hashes for different users, so a single precomputed table can't be used against all the salted hashes.",
        "C": "Because salts encrypt the password before hashing, so attackers cannot recover the original password.",
        "D": "Because salts allow passwords to be stored in plaintext without exposing them to attackers."
      },
      "correct_answer": "B",
      "simplification_notes": "Made the scenario explicit (attacker has stolen hashes), tightened phrasing, kept the concept of a unique per-user random salt and its effect on rainbow tables; options rewritten to be plausible while preserving correct answer B.",
      "content_preserved": true,
      "source_article": "Computer security",
      "x": 1.4012386798858643,
      "y": 1.067022681236267,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3"
      ],
      "original_question_hash": "0cb8ad87"
    },
    {
      "question": "In systems theory, why can interactions among components produce system-level properties that no individual component exhibits on its own?",
      "options": {
        "A": "Because the interactions are linear and strictly additive, so the system’s behavior is just the sum of each component’s behavior.",
        "B": "Because nonlinear interactions and feedback loops can reorganize the system and generate emergent patterns and behaviors that are not predictable from any single component.",
        "C": "Because a single component always dominates and forces the entire system to adopt that component’s properties.",
        "D": "Because when components are isolated and do not interact, the system behaves merely as a simple collection of independent parts."
      },
      "correct_answer": "B",
      "simplification_notes": "Removed historical and domain examples, tightened wording to focus on core concepts (emergence, feedback, nonlinearity, dominance, isolation). Kept four plausible choices and preserved the original correct answer.",
      "content_preserved": true,
      "source_article": "Systems theory",
      "x": 1.506901502609253,
      "y": 1.1055961847305298,
      "level": 2,
      "concepts_tested": [
        "Emergence and synergy: A system is more than the sum of its parts when it exhibits emergent behavior or synergy; ask how interactions generate properties not present in individual components.",
        "Boundaries, context, and interdependence: Systems operate with causal boundaries and are influenced by context; changing one component can affect others or the whole system; ask how boundaries and context shape cause-and-effect within a system.",
        "Adaptation, learning, and equifinality: Growth and adaptation depend on engagement with the environment; multiple paths (equifinality) can lead to similar outcomes; ask how environment engagement drives adaptation and why multiple routes to the same end exist."
      ],
      "original_question_hash": "8d72d350"
    },
    {
      "question": "From a cognitive-processing perspective, why does an initial numerical anchor bias subsequent estimates?",
      "options": {
        "A": "The initial number provides a starting reference point that people adjust from; because those adjustments are usually incomplete—due to limited cognitive resources or a drive for coherent judgments—the final estimate remains biased toward the anchor.",
        "B": "The initial number changes the external situation or objective value of what is being judged, so later judgments track that altered reality rather than the true prior state.",
        "C": "The anchor creates a durable memory trace that completely overrides later evidence, causing people to rely on the anchor regardless of new information.",
        "D": "Anchoring only occurs when numbers are shown visually; if the same information is conveyed verbally or as probabilities, the anchoring effect vanishes."
      },
      "correct_answer": "A",
      "simplification_notes": "Question shortened and rephrased for clarity; retained cognitive-processing framing and reason (insufficient adjustment due to limited resources/coherence). Distractors rewritten to be brief but plausible.",
      "content_preserved": true,
      "source_article": "Bias",
      "x": 1.2889187335968018,
      "y": 1.020701289176941,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Bias as a systematic, processing-based deviation from accurate judgment, with potential adaptive roles and limitations.",
        "Concept 2: Anchoring as a mechanism where an initial reference point biases subsequent estimates or decisions.",
        "Concept 3: Pattern perception biases (apophenia) and related irrational beliefs (e.g., gambler's fallacy) as how people infer meaning from random data."
      ],
      "original_question_hash": "c7a5a7f7"
    },
    {
      "question": "How do larger cross-border flows of goods, services, capital, people, and information increase economic interdependence among countries?",
      "options": {
        "A": "They mainly reduce domestic production costs while leaving other countries' economies unaffected.",
        "B": "They cause global markets to become insulated from national policy changes, preventing cross-border effects.",
        "C": "They create transnational linkages—through supply chains, investment positions, migration, and information networks—that transmit shocks, demand shifts, and innovations across borders.",
        "D": "They always produce uniform policy harmonization that eliminates meaningful national economic differences."
      },
      "correct_answer": "C",
      "simplification_notes": "Reworded the stem for clarity and concision; highlighted the same mechanisms (supply chains, investment, migration, information) in option C; made all distractors plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Globalization",
      "x": 1.208848476409912,
      "y": 0.9481195211410522,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Interdependence and integration across borders as the defining outcome of globalization, driven by cross-border flows of goods, services, capital, people, and information.",
        "Concept 2: Technological and infrastructural enablers (transportation and communication technologies) that increase connectivity and thereby strengthen economic and cultural interdependence.",
        "Concept 3: Liberalization and removal of trade/capital barriers as mechanisms that facilitate global markets, integrated with the social, cultural, and diplomatic dimensions of globalization."
      ],
      "original_question_hash": "cb5180fa"
    },
    {
      "question": "In international political economy (IPE), economics is said to be \"embedded\" in politics. Why can negotiated trade agreements differ from the allocations predicted by pure economic-efficiency models?",
      "options": {
        "A": "Because negotiators adopt textbook assumptions of perfect markets and ignore domestic political pressures, producing terms that do not reflect political realities.",
        "B": "Because actors with different power, interests, and distributional stakes mobilize and bargain, so negotiated terms reflect political bargaining rather than only efficiency-maximizing allocations.",
        "C": "Because formal economic models always incorporate institutional and political constraints, so political dynamics cannot change the model’s efficiency outcome.",
        "D": "Because international agreements are decided only by global welfare maximization, making national-level political conflicts irrelevant to the final terms."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified the concept of \"embeddedness\" and reworded the stem to ask why negotiated trade terms diverge from pure-efficiency model predictions. Options were tightened to be concise and plausible while preserving the original distractors.",
      "content_preserved": true,
      "source_article": "International political economy",
      "x": 1.2271299362182617,
      "y": 0.9093001484870911,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Embeddedness of economics in politics – international economic phenomena do not exist independently of the actors and institutions that regulate them.",
        "Concept 2: Power, actors, and distributive consequences – states, IOs, and MNCs shape the international economic system, producing winners and losers and driving political contention.",
        "Concept 3: Role of institutions and political-rational analysis in understanding the global economy – IPE emphasizes institutions, politics, and power relations alongside formal economic theories to explain economic outcomes."
      ],
      "original_question_hash": "ec1b2fc1"
    },
    {
      "question": "Why does control of a maritime chokepoint often give a state outsized geopolitical influence, even if that state is not the largest or most populous?",
      "options": {
        "A": "Because it allows the state to project cultural or ideological influence through control of major trade and shipping routes.",
        "B": "Because the chokepoint holder can raise transit costs and create uncertainty for rivals’ shipments of goods and energy, producing dependency and bargaining leverage.",
        "C": "Because controlling a chokepoint effectively expands the country’s land area and access to natural resources.",
        "D": "Because control of the chokepoint prevents other states from developing their own naval power or establishing alternative trade routes."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem for clarity and concision aimed at undergraduates; removed historical names and extended context; preserved the strategic geography concept. Options were kept plausible distractors while maintaining the original correct answer.",
      "content_preserved": true,
      "source_article": "Geopolitics",
      "x": 0.6955108642578125,
      "y": 0.531193733215332,
      "level": 2,
      "concepts_tested": [
        "Geography-to-power link: How geography (territory, seas, climate, resources) shapes political power, strategic interests, and international relations.",
        "Geopolitics as a methodological approach: Using geographic variables to understand, explain, and predict foreign policy and state behavior.",
        "Critical geopolitics and power/ideology: How geopolitical theories themselves reflect political power and ideological aims, prompting deconstruction of traditional doctrines."
      ],
      "original_question_hash": "620342ca"
    },
    {
      "question": "How did technologies such as pottery and ovens causally expand the range of cooking techniques and the availability of ingredients?",
      "options": {
        "A": "They mainly increased flavor by changing chemical reactions in food, without altering the set of available techniques or ingredient access.",
        "B": "They created new constraints and affordances—for example, heat control, containment, and moisture management—that enabled techniques like boiling, steaming, and roasting, and facilitated use of traded or storable ingredients.",
        "C": "They affected only cooking speed (making some processes faster or slower) but did not change the variety of techniques or ingredient access.",
        "D": "They removed the need for specialized human skill, making cooking techniques uniform and independent of cook training."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording shortened and clarified causal mechanisms (heat control, containment, moisture) and explicit examples of techniques (boiling, steaming, roasting) and ingredient access (trade, storage). Kept the original causal claim that technology enabled new methods and ingredient use.",
      "content_preserved": true,
      "source_article": "Cooking",
      "x": 1.593475341796875,
      "y": 0.8251979351043701,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Heat-based transformation of food improves safety, digestibility, nutrition, and palatability.",
        "Concept 2: Technological tools (fire control, pottery, ovens) enable diverse cooking techniques and access to new ingredients, showing a causal link between technology and culinary practice.",
        "Concept 3: Cooking is intertwined with human evolution and cultural exchange (e.g., potential evolutionary impacts of cooking; diffusion of ingredients through trade like the Columbian Exchange)."
      ],
      "original_question_hash": "b60bd391"
    },
    {
      "question": "Why does the choice of financing (debt versus equity) matter when evaluating a capital project — i.e., why should a firm consider how a project will be funded when deciding whether to undertake it?",
      "options": {
        "A": "Because a project’s cash flows come only from its operations and are completely independent of how the project is financed.",
        "B": "Because debt creates a tax shield and changes the firm’s risk profile and its weighted average cost of capital ($WACC$), so the financing mix can change the project’s net present value and whether it is value-adding.",
        "C": "Because debt financing always reduces the project’s hurdle rate, making every project more attractive regardless of risk or other factors.",
        "D": "Because financing with equity removes financial risk entirely, so the funding choice does not affect project selection."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; retained technical terms (e.g. tax shield, risk, $WACC$). Distractors made plausible but incorrect (A: ignores financing effects; C and D contain absolute statements that are false).",
      "content_preserved": true,
      "source_article": "Corporate finance",
      "x": 1.3372235298156738,
      "y": 0.9089249968528748,
      "level": 2,
      "concepts_tested": [
        "The interdependence of capital budgeting and financing decisions: how choosing which projects to fund interacts with how those projects are financed (debt vs equity) to affect firm value.",
        "The overarching goal of shareholder value and how investment and financing choices are directed toward maximizing it.",
        "Working capital management as a mechanism linking day-to-day liquidity and operating efficiency to overall financial health and value creation."
      ],
      "original_question_hash": "36eec33e"
    },
    {
      "question": "In cost–benefit analysis, why do analysts convert all future costs and benefits into present values before comparing options?",
      "options": {
        "A": "Because money has a time value: a dollar today can be invested to earn returns, so future dollars are worth less in present terms; discounting places all flows on a common basis for comparison.",
        "B": "Because future benefits are inherently riskier, and discounting removes uncertainty by uniformly reducing future values.",
        "C": "Because inflation makes future dollars worthless, so discounting simply equalizes their purchasing power.",
        "D": "Because converting to present value makes calculations easier by allowing analysts to ignore the timing of cash flows."
      },
      "correct_answer": "A",
      "simplification_notes": "Question rephrased concisely for undergraduates; clarified 'present value' and 'discounting' and emphasized the time value of money. Distractors were rewritten as common misconceptions about risk, inflation, and convenience.",
      "content_preserved": true,
      "source_article": "Cost–benefit analysis",
      "x": 1.397453784942627,
      "y": 0.9800301790237427,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Monetization and time value of money — all benefits and costs are expressed in monetary terms and adjusted to a common present value for comparison.",
        "Concept 2: Decision rule and welfare comparison — CBA ranks alternatives by total expected costs vs. benefits (e.g., via cost–benefit ratio) to determine which option improves welfare, with reference to concepts like Pareto efficiency.",
        "Concept 3: Limitations and uncertainty — estimates are often flawed, and biases or incomplete information can influence outcomes and the reliability of conclusions."
      ],
      "original_question_hash": "e12e3d18"
    },
    {
      "question": "From a life-cycle perspective, why does investing in human capital (education, health, training) typically raise future productivity and earnings?",
      "options": {
        "A": "Because these investments increase the stock of productive capabilities people possess, raising the marginal product of labor and future output; foregoing consumption now is rational if the present value of higher future earnings exceeds the cost.",
        "B": "Because human-capital investments fully substitute for physical capital, eliminating the need for machines and factories and thereby reducing production costs across all sectors.",
        "C": "Because human capital raises earnings only when labor markets achieve perfect matching and complete transferability of skills; without such matching, investment produces no productivity gains.",
        "D": "Because health and education investments are immediately productive and have no opportunity cost, so they always and instantly increase current earnings."
      },
      "correct_answer": "A",
      "simplification_notes": "Language was tightened to undergraduate level and the life-cycle consumption–investment trade-off and production-function idea (stock of capabilities raising marginal product) were emphasized; distractors were reworded to be plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Human capital",
      "x": 1.291021466255188,
      "y": 0.9587127566337585,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Human capital as a production input or stock that influences outputs and earnings (the production-function viewpoint; akin to physical capital but involving human attributes).",
        "Concept 2: Investment in human capital (education, training, health) as a mechanism to raise productivity and future earnings, including the idea of life-cycle trade-offs between consumption and investment.",
        "Concept 3: Human capital as a driver of economic growth and development, with relevance in growth theories and macroeconomic welfare."
      ],
      "original_question_hash": "5cef1c0f"
    },
    {
      "question": "In X-ray crystallography, why are phase values needed to reconstruct the electron density from measured diffraction data, and conceptually how do phasing methods solve the \"missing phase\" problem?",
      "options": {
        "A": "The electron density $\\rho(\\mathbf{r})$ is the inverse Fourier transform of the complex structure factors $F(hkl)$, but diffraction experiments measure only the amplitudes $|F(hkl)|$ and not the phases $\\phi(hkl)$; without phases the inverse transform is non‑unique. Phasing methods provide estimated phases (from prior structural information, molecular replacement, isomorphous/anomalous scattering, or statistical phase relationships) so an interpretable electron‑density map can be calculated.",
        "B": "The amplitudes $|F(hkl)|$ by themselves already fix atomic positions, so phases are redundant; phasing procedures are only used later to refine which atom types occupy those positions.",
        "C": "Electron density can be obtained directly from measured intensities via a simple square‑root relationship, so phases are only needed to convert the map to absolute electron counts and do not affect the spatial distribution.",
        "D": "Diffraction intensities contain the full information needed for the density, so phases are unnecessary in principle; modern phasing algorithms merely accelerate calculations but do not change the final density outcome."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the original technical question into concise undergraduate language; explicitly stated the inverse Fourier relation using LaTeX ($\\rho(\\mathbf{r})$ and $F(hkl)$) and noted that experiments measure amplitudes $|F(hkl)|$ but not phases $\\phi(hkl)$. Kept all four options plausible and preserved the correct answer. Removed peripheral historical detail and focused on the core missing‑phase concept and typical phasing approaches.",
      "content_preserved": true,
      "source_article": "Crystallography",
      "x": 1.840088129043579,
      "y": 1.0620038509368896,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Diffraction data can be used to determine atomic positions and electron density, linking experimental measurements to structural models.",
        "Concept 2: The development and relationship of X-ray crystallography and electron diffraction, including how advances in instruments and phasing algorithms improve structure solving.",
        "Concept 3: The role of crystal symmetry and geometry (goniometers, Miller indices, stereographic nets) in determining and representing crystal structure."
      ],
      "original_question_hash": "fc54157f"
    },
    {
      "question": "The Erlangen program views geometry of a space $X$ as the study of properties invariant under a group $G$ of transformations. Why does identifying geometric properties with $G$-invariants capture the essence of geometric structure and emphasize symmetry?",
      "options": {
        "A": "Because geometric properties are precisely the features left unchanged by every symmetry in $G$; this makes geometric statements independent of a particular representation and directly links geometry to the symmetry group.",
        "B": "Because geometric properties are those that transform in a predictable way under some elements of $G$, so geometry must track how objects deform rather than what stays fixed.",
        "C": "Because the group $G$ encodes only algebraic relations; invariants under $G$ therefore correspond to algebraic constants and not to intrinsic geometric structure.",
        "D": "Because the geometry is determined first by a fixed metric or structure, and $G$ only reorders points; invariants under $G$ are incidental rather than foundational."
      },
      "correct_answer": "A",
      "simplification_notes": "Condensed and clarified the Erlangen-program question, introduced $G$ and $X$ as inline LaTeX, tightened option wording while keeping all distractors plausible; preserved the original correct choice.",
      "content_preserved": true,
      "source_article": "Group theory",
      "x": 1.6790293455123901,
      "y": 1.2072173357009888,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Groups as abstract symmetry objects that underpin diverse mathematical structures and physical models, with connections to representation theory.",
        "Concept 2: The Erlangen program as a principle that geometry can be understood through groups acting as organizing symmetry; geometry is described via associated group actions.",
        "Concept 3: The relationship between groups and other algebraic structures (rings, fields, vector spaces) as groups with additional operations, and the idea of finite simple groups as fundamental building blocks in classification efforts."
      ],
      "original_question_hash": "14d34c99"
    },
    {
      "question": "If an organism's genotype remains the same, how can environmental context change its phenotype, and by what molecular mechanism does this occur?",
      "options": {
        "A": "By triggering random mutations in the DNA each time the environment changes, producing new genetic variants that alter phenotype.",
        "B": "By extracellular or intracellular signals that activate transcription factors and signaling pathways, changing the transcriptional activity of genes that control growth and development.",
        "C": "By directly changing the DNA sequences of developmental genes in response to environmental stimuli, so the genotype is altered to produce a different phenotype.",
        "D": "By the environment acting only through post-translational modifications of proteins, with no effect on gene transcription or mRNA levels."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and focused on genotype constant → phenotype change; retained key mechanism language (transcription factors, signaling pathways). Options rewritten to be concise and equally plausible alternatives.",
      "content_preserved": true,
      "source_article": "Genetics",
      "x": 1.8988800048828125,
      "y": 1.128084421157837,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Genes as discrete units of inheritance that govern how traits are transmitted across generations (Mendelian inheritance).",
        "Concept 2: Environment and genetic information interact to influence development and phenotype, including gene transcription responding to intracellular/extracellular conditions.",
        "Concept 3: Genetics operates across multiple levels (molecular, cellular, organismal, population) and connects gene structure/function to variation, distribution, and evolutionary change."
      ],
      "original_question_hash": "be730fe2"
    },
    {
      "question": "How do climate observations and climate models complement each other to determine the current state of Earth's climate and to project future climate changes?",
      "options": {
        "A": "Long observational records by themselves determine future climate by averaging past variability; climate models are unnecessary.",
        "B": "Observations define the real climate state, its variability, and boundary conditions; climate models represent the governing physical processes so they can reproduce observed behavior and simulate responses to future forcings, with observations used to validate and constrain the models.",
        "C": "Climate models can predict future climate without observational input because the underlying physical laws are universal; observations only record past events.",
        "D": "Observations should be used only to calibrate models after predictions are produced, so model forecasts are not influenced by observed data."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the wording, clarified that observations provide the real state/variability and that models encode physical processes and are validated by observations; removed lengthy examples and maintained technical terms like 'forcings' and 'validate'.",
      "content_preserved": true,
      "source_article": "Climatology",
      "x": 1.866840124130249,
      "y": 0.9271267056465149,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Climate vs. weather and the idea that climate equals long-term averages (and that understanding weather relates to longer-term climate phenomena).",
        "Concept 2: Climate variability and mechanisms (physical processes and cycles such as ENSO, MJO, NAO, AO, PDO, IPO) that drive changes in climate.",
        "Concept 3: The methodological relationship between observations and modelling as the core tools to determine climate and predict future climate."
      ],
      "original_question_hash": "8a35e799"
    },
    {
      "question": "Why do ecologists study ecosystems as integrated wholes rather than as collections of independent species?",
      "options": {
        "A": "Because species' roles are entirely redundant and any species can substitute for any other.",
        "B": "Because emergent properties—such as resilience, nutrient cycling, and energy flow—arise from interactions and feedbacks among species and abiotic components, and cannot be inferred by studying species in isolation.",
        "C": "Because energy and matter fluxes can be predicted by simply summing the independent contributions of each species.",
        "D": "Because ecosystems are static assemblies in which species have fixed, non‑interacting roles."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; retained technical terms (emergent properties, nutrient cycling, energy flow) and emphasized interactions and feedbacks; distractors made plausible.",
      "content_preserved": true,
      "source_article": "Ecology",
      "x": 1.6652064323425293,
      "y": 1.026342511177063,
      "level": 2,
      "concepts_tested": [
        "Emergent properties and holistic study: ecosystems exhibit patterns that cannot be understood by looking at species in isolation; the whole-system approach is required to explain ecosystem behavior.",
        "Energy and material flux through ecosystems: processes such as primary production, nutrient cycling, and niche construction regulate the flow of energy and matter, with feedbacks between biotic and abiotic components.",
        "Interactions shaping structure and services: cooperation, competition, predation, and adaptations among organisms, in concert with environmental factors, determine biodiversity and ecosystem functioning, including ecosystem services."
      ],
      "original_question_hash": "245347fd"
    },
    {
      "question": "How do adverse selection and moral hazard differ as sources of agency cost, and why does monitoring the agent's actions after a contract is signed not fully remove those costs?",
      "options": {
        "A": "Adverse selection refers to hidden actions taken after contracting, while moral hazard refers to hidden information about the agent before contracting.",
        "B": "Adverse selection comes from hidden information about the agent's type before the contract is written, whereas moral hazard comes from hidden actions by the agent after contracting; therefore monitoring actions after the contract cannot undo inefficiencies created by pre-contract information gaps.",
        "C": "Both adverse selection and moral hazard arise from the same hidden state, and simply increasing wages will eliminate the agency costs associated with both.",
        "D": "They are the same phenomenon; once the agent's actions become observable, there are no remaining agency costs."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was made more concise and definitions of adverse selection (hidden information before contract) and moral hazard (hidden actions after contract) were clarified; distractors were kept plausible but incorrect. No core concepts were changed.",
      "content_preserved": true,
      "source_article": "Principal–agent problem",
      "x": 1.3196593523025513,
      "y": 0.9723803400993347,
      "level": 2,
      "concepts_tested": [
        "Information asymmetry and incentive misalignment as the root of agency costs",
        "Moral hazard and adverse selection as distinct but related mechanisms driving the problem",
        "Incentive alignment mechanisms and governance (e.g., performance measures, bonding, termination) to mitigate the agency problem"
      ],
      "original_question_hash": "a1f3b401"
    },
    {
      "question": "Why can small perturbations in external forcing cause abrupt, system‑wide reorganization when an Earth system is near a tipping point?",
      "options": {
        "A": "Because nonlinear positive feedbacks among interconnected subsystems can amplify small perturbations, pushing the system past a threshold and producing rapid reorganization (often with hysteresis).",
        "B": "Because Earth's subsystems respond linearly, so small perturbations always produce only proportional, gradual changes.",
        "C": "Because abrupt changes require very large external forcing and internal feedbacks are negligible.",
        "D": "Because strong negative feedbacks always stabilize the system, preventing sudden shifts regardless of the magnitude of forcing."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording was tightened and clarified (e.g., 'external forcing' → 'perturbations in external forcing'); key technical phrases kept (\"nonlinear positive feedbacks\", \"threshold\", \"hysteresis\"); distractors made concise but plausible.",
      "content_preserved": true,
      "source_article": "Earth system science",
      "x": 1.4954091310501099,
      "y": 0.966866135597229,
      "level": 2,
      "concepts_tested": [
        "Interconnected subsystems and feedback mechanisms across atmosphere, hydrosphere, biosphere, lithosphere, etc.",
        "Nonlinear dynamics and thresholds that can cause abrupt Earth system changes.",
        "The integral role of life and cross-sphere connectivity in governing Earth system processes."
      ],
      "original_question_hash": "83bda72d"
    },
    {
      "question": "In a closed population with constant per‑capita birth rate $b$ and death rate $d$, the dynamics are $\\frac{dN}{dt}=(b-d)N=rN$. Which explanation best explains why the instantaneous rate of change is proportional to the current population size $N$?",
      "options": {
        "A": "Because each individual produces a constant expected number of births per unit time, so total births scale with $N$ regardless of population size.",
        "B": "Because births occur at rate $bN$ and deaths at rate $dN$ (each proportional to the number of individuals), so net change $bN-dN$ is proportional to $N$.",
        "C": "Because the environment imposes a fixed carrying capacity $K$ that caps growth, forcing the rate of change to be independent of $N$.",
        "D": "Because immigration and emigration are balanced, making net growth determined by how close $N$ is to carrying capacity rather than by $N$ itself."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was tightened and the math expression kept as inline LaTeX. The correct option (B) was clarified to state births and deaths are proportional to N; distractors were rephrased to remain plausible.",
      "content_preserved": true,
      "source_article": "Population dynamics",
      "x": 1.7165284156799316,
      "y": 1.0955963134765625,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Population change is driven by demographic processes (births, deaths; with possible immigration/emigration), and in a closed system the rate of change is proportional to current population (dN/dt = (b − d)N = rN), linking mechanism to outcome.",
        "Concept 2: Different growth models are related as special cases within a general formulation; historical models (Malthusian, Gompertz, Verhulst, Richards, Lotka–Volterra) illustrate how varying assumptions yield different dynamics, showing a unifying conceptual framework.",
        "Concept 3: Population dynamics employs dynamical systems and differential equations to model time evolution and connects to related fields (epidemiology, evolutionary game theory), reflecting a cohesive modeling approach and interdisciplinary relationships."
      ],
      "original_question_hash": "a10130e3"
    },
    {
      "question": "How does using both qualitative and quantitative methods from multiple disciplines strengthen research in human geography, and which principle describes this benefit?",
      "options": {
        "A": "It removes contextual interpretation to produce purely objective measurements, which purportedly increases generalizability.",
        "B": "It broadens the range of data types and theoretical perspectives, enabling triangulation that cross-checks statistical patterns (numbers) with meanings and context, thus better explaining complex human–place relationships.",
        "C": "It forces researchers to convert all evidence into a single numeric metric, simplifying analysis but sacrificing interpretive nuance.",
        "D": "It limits analysis to a single disciplinary vocabulary to avoid methodological conflicts and preserve consistency."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was condensed and clarified for undergraduate readers; historical details removed; emphasized mixed-methods benefit as 'triangulation' and kept all four options plausible.",
      "content_preserved": true,
      "source_article": "Human geography",
      "x": 1.3062779903411865,
      "y": 0.9414852261543274,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Human-environment spatial relationships — how human activity shapes spaces and how spaces influence human communities.",
        "Concept 2: Environmental determinism and its critiques — a theory linking environment to human behavior and the methodological and ethical criticisms it faced.",
        "Concept 3: Interdisciplinary and mixed-methods approach — the use of qualitative and quantitative methods drawn from multiple disciplines to study human geography."
      ],
      "original_question_hash": "a69e28c7"
    },
    {
      "question": "Why do emergent properties appear in systems as behaviors or patterns that cannot be predicted by examining individual components in isolation?",
      "options": {
        "A": "Because nonlinear interactions and feedback loops among components produce new macro-level patterns that are not reducible to single components.",
        "B": "Because the emergent property is merely the arithmetic average of all component properties as the system scales up.",
        "C": "Because components develop new intrinsic properties only when they are physically isolated from the rest of the system.",
        "D": "Because emergent properties are determined solely by external constraints imposed by a single central agent controlling the system."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened and made more concise; technical terms (e.g., nonlinear interactions, feedback loops, macro-level patterns, reducible) retained. Distractors rewritten to remain plausible and distinct.",
      "content_preserved": true,
      "source_article": "Systems theory",
      "x": 1.481433629989624,
      "y": 1.1052566766738892,
      "level": 2,
      "concepts_tested": [
        "Emergence and synergy: systems are “more than the sum of their parts” due to interactions leading to emergent behavior; tests could ask how and why emergent properties arise.",
        "Interdependence, boundaries, and context: changing one component affects others; causal boundaries and environmental context shape system dynamics; tests could ask how perturbations propagate and how context modulates outcomes.",
        "General principles across domains and dynamic modeling (equifinality, active vs passive systems): aims to develop broadly applicable concepts and model a system’s dynamics, constraints, and relations; tests could ask why general principles are applicable across fields or what equifinality implies for system outcomes."
      ],
      "original_question_hash": "52bc345b"
    },
    {
      "question": "In regional planning, how does arranging transport corridors as a hub-and-spoke network affect the roles of settlements and the timing of infrastructure investment?",
      "options": {
        "A": "It forces all growth into the central hub and prevents development in peripheral towns, which simplifies and centralizes infrastructure planning.",
        "B": "Hubs become higher-accessibility nodes that attract denser, multimodal development, while spokes link outlying towns and allow incremental growth so expensive infrastructure can be staged as demand rises.",
        "C": "It prohibits new development along the corridors to protect green belts, causing development to leapfrog to distant areas outside the corridor system.",
        "D": "It guarantees equal growth and funding across every settlement regardless of network position, producing uniform infrastructure requirements throughout the region."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified the hub-and-spoke mechanism and its effects on accessibility, density, and staged infrastructure investment; removed extraneous phrasing while preserving linkage to green belts and settlement roles.",
      "content_preserved": true,
      "source_article": "Regional planning",
      "x": 1.3393000364303589,
      "y": 0.8226916193962097,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Regional planning uses macro-scale zoning and policy to integrate land use, housing, infrastructure, and environmental protection for sustainable regional growth.",
        "Concept 2: Planning transportation corridors with hubs and spokes shapes accessibility, settlement roles, and future infrastructure needs, guiding regional development.",
        "Concept 3: Designating protected or nuisance land uses (green belts, flood plains, waste disposal sites) influences growth patterns, risk management, and environmental resilience at the regional level."
      ],
      "original_question_hash": "f857640a"
    },
    {
      "question": "Why is balancing competing demands on land among the state, market, and local community considered a core principle of spatial planning?",
      "options": {
        "A": "Because land is a limitless resource and market forces alone can allocate it efficiently without mediation.",
        "B": "Because land is scarce and its use affects economic, social, and ecological objectives, so mediation among state regulation, market signals, and local community needs is required to produce workable and legitimate plans.",
        "C": "Because the state should unilaterally designate land use to ensure consistency across regions, minimizing the role of markets and communities.",
        "D": "Because local communities can decide land use independently of policy, market dynamics, or state planning."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; actors (state, market, local community) made explicit; rationale emphasised (land scarcity and multiple objectives). All four options kept plausible; correct answer unchanged.",
      "content_preserved": true,
      "source_article": "Spatial planning",
      "x": 1.3493595123291016,
      "y": 0.8210391402244568,
      "level": 2,
      "concepts_tested": [
        "Balancing competing demands on land (state, market, local community) as a core principle guiding spatial planning.",
        "The three mechanisms that mark the schools of transformative strategy formulation: stakeholder involvement, integration of sectoral policies, and promotion of development projects.",
        "The evolution and contextual nature of planning systems (how governance structures, actors, and institutions shape planning forms and impacts across time and places)."
      ],
      "original_question_hash": "b26d62f2"
    },
    {
      "question": "Which description best explains how the interplay of social structure and individual agency produces both social stability and social change?",
      "options": {
        "A": "Social stability is fully determined by fixed social structures; individual choices have no lasting effect.",
        "B": "Individual choices alone produce macro social outcomes, so large-scale patterns are simply the sum of micro behaviours independent of structural constraints.",
        "C": "Social order mainly emerges from the routine enactment of existing structures in everyday practices (structure shaping agency); however, when many people adopt new practices or contest norms, those micro-level shifts can accumulate and modify macro structures (agency driving change within constraints).",
        "D": "Macro structures automatically reconfigure to reflect each new individual preference, causing continuous rapid change without historical path dependence."
      },
      "correct_answer": "C",
      "simplification_notes": "Rewrote the stem in concise academic language, highlighted micro/macro interaction and mutual influence, preserved the two-way dynamic (structures shape routines; aggregated agency can modify structures). Distractors were kept plausible by representing common but incorrect positions (structural determinism, methodological individualism, and instant structural responsiveness).",
      "content_preserved": true,
      "source_article": "Sociology",
      "x": 1.2218246459960938,
      "y": 0.9872860312461853,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Structure–agency interplay — how social structures influence individual behavior and, in aggregate, shape social order and social change.",
        "Concept 2: Levels of analysis — the relationship and interaction between micro-level (individual interaction/agency) and macro-level (social systems/structure) explanations.",
        "Concept 3: Digital sociology and technology’s impact — how digital technologies alter social networks, power relations, institutions, and introduce concepts like the digital divide."
      ],
      "original_question_hash": "08e63278"
    },
    {
      "question": "According to the structure–action relationship, how do everyday actions both create social structures and are subsequently shaped by them in a self-reinforcing cycle?",
      "options": {
        "A": "Social structure is fixed by nature; human actions only discover pre-existing patterns without changing them.",
        "B": "Repeated individual actions produce stable patterns that become norms, institutions, and rules; those structures then shape expectations and constrain options, guiding future actions in a self-reinforcing cycle.",
        "C": "Individual actions are entirely independent of any structure; structure appears only later as an external overlay unrelated to earlier actions.",
        "D": "Social structures arise from random processes and do not reflect patterned regularities in individual actions."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was made more direct and concise for undergraduate readers; removed extraneous examples and focused on the mutual emergence and constraint (feedback loop) between action and structure. Option texts were kept plausible and aligned with the original alternatives; the correct answer was preserved.",
      "content_preserved": true,
      "source_article": "Social structure",
      "x": 1.2512801885604858,
      "y": 0.9768516421318054,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The structure–action relationship (structure emergent from and determinant of individual actions; feedback loop between structure and behavior)",
        "Concept 2: Multilevel analysis of social structure (macro, meso, micro scales and how institutions, networks, and norms operate across these levels)",
        "Concept 3: Norms and institutions as mechanisms linking structure to behavior and systems (internalization of norms, durability of patterns, influence on and from social systems)"
      ],
      "original_question_hash": "a34c98e3"
    },
    {
      "question": "Why do firms and industries tend to cluster into dense production and urban areas (agglomeration), and under what conditions do these clusters disperse (deglomeration)?",
      "options": {
        "A": "Because proximity always raises input costs, so firms centralize mainly to capture shared infrastructure benefits despite higher costs.",
        "B": "Because clustering removes the need for skilled labor, so firms aggregate where wage costs are lowest regardless of productivity.",
        "C": "Because proximity lets firms share specialized labor pools, local suppliers, and knowledge spillovers, which raise productivity and market access; clusters disperse when congestion, higher land rents, rising input or commuting costs, or other diseconomies outweigh those benefits.",
        "D": "Because zoning and planning force firms into dense cores, and dispersion occurs only when policies or incentives change to allow relocation."
      },
      "correct_answer": "C",
      "simplification_notes": "Shortened and clarified the question for undergraduates; kept technical mechanisms (shared labor, suppliers, knowledge spillovers) and countervailing forces (congestion, land rents, input costs) while removing historical/model citations and extraneous detail.",
      "content_preserved": true,
      "source_article": "Location theory",
      "x": 1.2991973161697388,
      "y": 0.9420964121818542,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Transport costs shape land use and economic rents, creating distance-based patterns in what goods are produced where (Thünen’s framework).",
        "Concept 2: Agglomeration and deglomeration forces cause clustering or dispersion of production, influencing market areas and urban form (including labor distortions and production scale effects).",
        "Concept 3: Location decisions can be formulated as optimization problems using costs (freight), production functions, and rents to identify optimal plant locations (Weber’s model and related theories)."
      ],
      "original_question_hash": "311a3727"
    },
    {
      "question": "How do faster transportation and improved communications weaken the monocentric city model and promote the formation of multiple urban centers?",
      "options": {
        "A": "They increase the CBD's relative advantage by making travel to the center faster, causing firms and households to concentrate more in the CBD.",
        "B": "They reduce the effective cost of locating away from the CBD—lower commuting costs and better communications let firms and workers operate farther from the center while retaining access to central markets and information, making peripheral centers viable.",
        "C": "They raise land rents uniformly across the metropolitan area, which strengthens CBD dominance and deters the growth of peripheral centers.",
        "D": "They eliminate economies of agglomeration so firms no longer gain from clustering, which makes a single center more efficient than multiple centers."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording made more concise and focused on mechanisms (commuting costs, communications, access to markets). Technical terms like CBD, monocentric, and agglomeration retained; distractors rephrased to remain plausible.",
      "content_preserved": true,
      "source_article": "Urban economics",
      "x": 1.2697348594665527,
      "y": 0.9366488456726074,
      "level": 2,
      "concepts_tested": [
        "Monocentric city model and its drivers: central business district dominance and how faster transportation and improved communications weaken monocentricity by altering commuting and location decisions.",
        "Polycentric urban form and agglomeration economies: explanations for multiple centers, including utility gains from lower land rents and increasing returns from clustering.",
        "Spatial decision-making and allocation across space: how the location choices of households and firms—driven by costs, rents, and accessibility—shape urban structure."
      ],
      "original_question_hash": "f651df93"
    },
    {
      "question": "Why does replacing the objective function f by -f turn the maximization problem \\(\\max_{x\\in A} f(x)\\) into the equivalent minimization problem \\(\\min_{x\\in A} -f(x)\\)? (Recall that \\(f(x_{0})\\ge f(x)\\;\\forall x\\) iff \\(-f(x_{0})\\le -f(x)\\;\\forall x\\).)",
      "options": {
        "A": "Because negation changes the feasible domain so the best point appears unchanged.",
        "B": "Because negating the objective reverses all pairwise comparisons of objective values, so the maximizer of \\(f\\) becomes the minimizer of \\(-f\\) on the same feasible set.",
        "C": "Because the maximum value of \\(f\\) equals the maximum value of \\(-f\\), so minimizing the negative does not affect which point is optimal.",
        "D": "Because the negated objective has the same gradient as the original, allowing the same solver to find the optimum without modification."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded to a concise undergraduate-level question, included the key equivalence in inline LaTeX, and turned distractors into plausible but incorrect statements. Removed wider article background.",
      "content_preserved": true,
      "source_article": "Mathematical optimization",
      "x": 1.6297818422317505,
      "y": 1.1731244325637817,
      "level": 2,
      "concepts_tested": [
        "The optimization problem framework: objective function f, feasible domain A, and the goal of finding x0 that minimizes or maximizes f.",
        "Duality between minimization and maximization: f(x0) ≥ f(x) for all x is equivalent to -f(x0) ≤ -f(x) for all x, enabling conversion between max and min problems.",
        "Discrete vs. continuous optimization and the role of constraints/multimodality: discrete optimization involves countable/structured choices (integers, permutations, graphs), while continuous optimization involves continuous domains; constraints and multimodality affect solution methods."
      ],
      "original_question_hash": "88fa6695"
    },
    {
      "question": "In a multi-tier supply chain, raising a supplier's safety stock can lower stockouts at downstream nodes but increases the supplier's holding costs. Why does this example show that firms should optimize total cost across the whole supply chain rather than just minimize cost at one node?",
      "options": {
        "A": "Because higher upstream inventory always reduces downstream costs in direct proportion, so minimizing a single node's cost will yield aggregate savings.",
        "B": "Because costs and service impacts propagate through interdependent nodes; a local inventory change affects other firms' costs and service levels, so only coordinated, system-level optimization minimizes total cost.",
        "C": "Because holding costs are independent at each node, so upstream inventory decisions do not affect downstream costs or service.",
        "D": "Because demand variability at one node is irrelevant to inventory decisions at other nodes, so local minimization is sufficient."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem to be concise and explicit about the upstream/downstream trade-off; focused on the system-wide optimization principle and kept four plausible answer choices. Removed extended background material but preserved the core trade-off and interdependency concept.",
      "content_preserved": true,
      "source_article": "Supply chain management",
      "x": 1.3967868089675903,
      "y": 0.9920533895492554,
      "level": 2,
      "concepts_tested": [
        "Concept 1: End-to-end integration and synchronization of supply chain activities (design, planning, execution, control, monitoring) to create net value and align demand with supply.",
        "Concept 2: Trade-offs and interdependencies within the chain (e.g., inventory levels vs holding costs) and the goal of minimizing total costs across partners.",
        "Concept 3: Multidisciplinary and broader factors (resilience, sustainability, risk management, transparency, human capital) and how these influence SCM performance."
      ],
      "original_question_hash": "33ee5be6"
    },
    {
      "question": "In acute inflammation, vasodilation and increased capillary permeability cause redness and swelling. Which sequence best explains how these vascular changes both promote clearance of debris and pathogens and can produce a temporary loss of tissue function?",
      "options": {
        "A": "Inflammatory mediators (e.g., histamine, bradykinin) cause endothelial cells to contract, opening gaps in post‑capillary venules so plasma proteins and leukocytes extravasate; plasma mediators (complement, antibodies) and phagocytes clear microbes and debris, while the resulting edema and raised interstitial pressure can impair tissue function.",
        "B": "Arteriolar constriction reduces blood flow and local temperature, which limits leukocyte delivery and thus prevents collateral tissue damage but still allows debris clearance by resident cells.",
        "C": "Lymphatic vessels immediately absorb the exuded fluid, diluting toxins and preventing edema, which rapidly restores tissue function while immune cells act slowly.",
        "D": "Platelets rapidly form a hemostatic plug that seals leaky capillaries, stopping plasma loss and preserving function, but this barrier delays leukocyte entry and slows clearance."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording was condensed and organized into a clear cause→effect sequence; technical terms (extravasation, complement, edema) retained for precision; extraneous background material removed.",
      "content_preserved": true,
      "source_article": "Inflammation",
      "x": 2.0672624111175537,
      "y": 1.1589000225067139,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Inflammation as a coordinated innate immune mechanism to eliminate injury, clear debris, and initiate tissue repair, involving immune cells, blood vessels, and molecular mediators.",
        "Concept 2: Distinction between acute and chronic inflammation, including how acute inflammation progresses and, when prolonged, leads to shifts in cellular composition and simultaneous tissue destruction and healing.",
        "Concept 3: Mechanistic basis for the cardinal signs (heat, pain, redness, swelling, loss of function) through vascular changes and mediators (e.g., increased blood flow, permeability, and chemical mediators like bradykinin and histamine)."
      ],
      "original_question_hash": "5e48bb5c"
    },
    {
      "question": "Why is antigen presentation by innate immune cells required to initiate adaptive T cell responses, and how do innate sensing signals (e.g., PRR activation) influence the quality of that T cell response?",
      "options": {
        "A": "Antigen presentation provides peptide–$MHC$ complexes to naive T cells, while innate-derived co‑stimulatory molecules and PRR‑induced cytokine signals together determine whether T cells activate, become tolerant, and which effector lineage (e.g., Th1, Th2, Th17, Tfh) they differentiate into.",
        "B": "Antigen presentation by innate cells directly triggers B cells to make antibodies without requiring any T cell activation or help.",
        "C": "Antigen presentation occurs only after the adaptive immune system is already active, so innate sensing signals do not affect T cell fate.",
        "D": "Strong peptide–$MHC$–$TCR$ binding alone is always sufficient to activate naive T cells; co‑stimulation and cytokine signals are unnecessary."
      },
      "correct_answer": "A",
      "simplification_notes": "Rewrote the question in concise academic language, made the role of antigen presentation explicit (peptide–MHC and naive T cells), added PRR/cytokine/co‑stimulation details to explain how innate sensing shapes activation and differentiation. Distractors were shortened to remain plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Innate immune system",
      "x": 2.0898425579071045,
      "y": 1.1689776182174683,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Innate immunity mechanisms and their role in activating the adaptive immune system via antigen presentation.",
        "Concept 2: The physical and chemical barriers (skin, mucosa, mucus, peristalsis, cilia, tears, gut flora) as the first line of defense and the mechanisms by which they prevent infection.",
        "Concept 3: Epithelial barrier theory and the idea that barrier dysfunction, environmental exposures, and microbial dysbiosis can drive inflammation and contribute to chronic diseases."
      ],
      "original_question_hash": "ec41c4ef"
    },
    {
      "question": "How does opsonization increase phagocytosis and thereby link innate and adaptive immunity?",
      "options": {
        "A": "Opsonins (antibodies or complement fragments) coat the pathogen surface, improving recognition by phagocyte Fc and complement receptors and accelerating engulfment; the ingested microbe is degraded and peptide fragments are loaded onto MHC class II to activate CD4+ T helper cells, connecting innate clearance to adaptive activation.",
        "B": "Opsonization coats phagocytes with antibodies, causing them to attack host tissues and paradoxically reducing phagocytic efficiency.",
        "C": "Opsonization masks pathogen-associated molecular patterns (PAMPs), preventing detection by pattern-recognition receptors and therefore inhibiting phagocytosis.",
        "D": "Opsonization only neutralizes soluble toxins and has no effect on phagocyte uptake or antigen presentation."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened for clarity; retained technical terms (opsonins, Fc/complement receptors, MHC class II, CD4+ T cells); distractors made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Immunology",
      "x": 2.0838913917541504,
      "y": 1.1545780897140503,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Phagocytosis as a cellular mechanism of immune defense and its role in recognizing and responding to foreign bodies.",
        "Concept 2: The immune system comprises both organ-based (thymus, bone marrow, spleen, lymph nodes, etc.) and cellular components that interact to produce an integrated immune response.",
        "Concept 3: Immunology studies the relationships between host, pathogens, and immunity across health and disease, including how immune components function in different contexts (in vitro, in situ, in vivo)."
      ],
      "original_question_hash": "2f9d57b1"
    },
    {
      "question": "In cybernetic terms, how does negative feedback stabilise a system when it is disturbed?",
      "options": {
        "A": "It detects the deviation (error) from a desired state and adjusts the system's input or control actions to reduce that error, thereby damping the disturbance.",
        "B": "It ignores the measured error and continually reinforces the current output, which tends to amplify deviations and destabilise the system.",
        "C": "It depends on random fluctuations in inputs to slowly nudge the output toward the desired state, a process that is unreliable.",
        "D": "It isolates the system by blocking external inputs entirely so disturbances cannot reach the system."
      },
      "correct_answer": "A",
      "simplification_notes": "Question shortened and clarified; introduced terms 'deviation (error)' and 'control actions'; removed concrete examples (thermostat) while keeping the core mechanism of negative feedback.",
      "content_preserved": true,
      "source_article": "Cybernetics",
      "x": 1.257379412651062,
      "y": 1.0664006471633911,
      "level": 2,
      "concepts_tested": [
        "Circular causality and feedback loops as a mechanism for system stability and adaptation",
        "Information processing, control, and communication as functions linking inputs and outputs",
        "Recursion and self-organization across diverse domains (engineering, biology, society, etc.)"
      ],
      "original_question_hash": "51ba77d3"
    },
    {
      "question": "In an ambiguous situation, a person may be motivated by the need to be accurate (to be right) or by the need to be accepted (to be liked). How do informational and normative social influence differ in their underlying motives and typical outcomes for private belief change versus public conformity?",
      "options": {
        "A": "Informational influence is driven by the need to be liked and leads to private belief change; normative influence is driven by the need to be right and leads to public conformity.",
        "B": "Informational influence is driven by the need to be right and leads to private belief change (internalization); normative influence is driven by the need to be liked and leads to public conformity (compliance/identification).",
        "C": "Informational influence always produces public conformity, while normative influence always produces private belief change.",
        "D": "Both informational and normative influences always produce private belief change, regardless of the person's motivation."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the original into a concise undergraduate-level question. Clarified the motivational terms ('need to be right' and 'need to be liked') and explicitly linked informational→private/internalization and normative→public/compliance to preserve the tested mappings.",
      "content_preserved": true,
      "source_article": "Social influence",
      "x": 1.250878095626831,
      "y": 1.0107418298721313,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Kelman’s three processes of attitude change—compliance, identification, and internalization—and how each leads to different patterns of public behavior versus private belief.",
        "Concept 2: Informational vs normative social influence—how uncertainty and the need to be right or to be liked drive these processes and their typical outcomes (private acceptance vs public conformity).",
        "Concept 3: The relationships among the frameworks—how normative influence tends to produce public compliance and identification, while informational influence leads to private acceptance and internalization, connected to the motivational needs described by Deutsch and Gerard."
      ],
      "original_question_hash": "e3f578f7"
    },
    {
      "question": "How does diffuse power exercised through institutions and discourse shape people's behavior even when no direct threat or force is used?",
      "options": {
        "A": "By defining what actions are legitimate and which choices are feasible—using norms, rules, and language to shape expectations and constrain behavior.",
        "B": "By relying solely on explicit threats of punishment or physical coercion to compel compliance.",
        "C": "By guaranteeing equal power to all actors so that structural constraints are removed.",
        "D": "By requiring explicit, unanimous agreement from every participant before any institutional practice can proceed."
      },
      "correct_answer": "A",
      "simplification_notes": "Clarified 'diffuse power' as power via institutions and discourse; condensed wording; preserved the concept that norms, rules, and language guide choices without coercion. Distractors kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Power (social and political)",
      "x": 1.2504010200500488,
      "y": 0.9713435173034668,
      "level": 2,
      "concepts_tested": [
        "Power as relational and diffuse: power can be exercised through institutions, structures, and language, not only coercion.",
        "Multidimensional power tactics: soft vs hard, rational vs nonrational, unilateral vs bilateral, and how context affects tactic choice.",
        "Balance of power as a relational constraint: power is relative and distributed; analysis focuses on relative strengths and constraints rather than absolute power."
      ],
      "original_question_hash": "b431dc5a"
    },
    {
      "question": "Why is the hierarchical flow of strategy and policy—from senior leaders through middle managers to front-line managers—essential for shaping organizational outcomes?",
      "options": {
        "A": "Because senior leaders set strategy and policy, middle managers translate these into actionable directives for front-line managers, aligning day-to-day operations with strategic goals and creating feedback loops to revise plans as conditions change.",
        "B": "Because governance micromanages front-line tasks by giving direct instructions that bypass middle management, ensuring consistency in daily work.",
        "C": "Because front-line managers independently create policy and senior leaders only review outcomes after the fact, so top-level direction is unnecessary.",
        "D": "Because governance has no effect on day-to-day actions; organizational outcomes are determined solely by external market conditions."
      },
      "correct_answer": "A",
      "simplification_notes": "Rewrote the question in clearer academic language, shortened phrasing, kept technical terms (strategy, policy, feedback loops), and made each option plausible while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Management",
      "x": 1.3694852590560913,
      "y": 0.9897602796554565,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The three-tier management structure (senior, middle, line) and how strategic goals are created at the top and translated into day-to-day actions at lower levels.",
        "Concept 2: Governance versus management and the flow of direction/policy from senior leaders to front-line managers, shaping organizational outcomes.",
        "Concept 3: Evidence-based management as a mechanism for improving decision-making and practices in management across disciplines."
      ],
      "original_question_hash": "d5d80b53"
    },
    {
      "question": "Why do people's motivation and actions typically differ when they act in an organizational role compared with when they act outside the organization?",
      "options": {
        "A": "Because organizational roles carry formal and informal expectations, norms, and accountabilities that shape what counts as acceptable effort and align individual motivation with organizational goals.",
        "B": "Because occupying an organizational role entirely rewrites an individual's core personality traits, so their motivations become fundamentally different.",
        "C": "Because outside organizations there are no norms or accountability mechanisms at all to influence behavior.",
        "D": "Because organizations legally mandate a single, fixed set of behaviors that all members must follow, forcing identical motivation and action."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording was tightened and focused on the central idea that roles embed expectations, norms, and accountability. Distractors were made plausible by overstating alternative causes (personality rewrite, absence of outside norms, legal uniformity).",
      "content_preserved": true,
      "source_article": "Organizational behavior",
      "x": 1.2928954362869263,
      "y": 1.0000430345535278,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Multi-level analysis in organizational behavior (micro, meso, macro) and how interactions across these levels shape outcomes.",
        "Concept 2: Role-based behavior in organizations: individuals behave differently when in organizational roles versus outside the organization, and organizational context shapes motivation and behavior.",
        "Concept 3: Organizational design and its impact on behavior/efficiency: structures like bureaucracy and rational-legal principles influence worker experiences and organizational performance."
      ],
      "original_question_hash": "03fbb39e"
    },
    {
      "question": "When the environment changes, how can that change which heritable traits increase in frequency over generations, and why is that an example of microevolution?",
      "options": {
        "A": "The environment directly rewrites individuals' DNA to produce advantageous traits, so offspring inherit the newly written sequences.",
        "B": "The changed environment alters the relative fitness of different heritable phenotypes; individuals with better-fit traits leave more offspring, so allele/trait frequencies shift across generations (this change in frequencies is microevolution).",
        "C": "The environment immediately forces development into a single phenotype, removing genetic variation in one generation so only that trait remains.",
        "D": "The environment always favours the same traits regardless of context, so trait frequencies remain constant even after environmental change."
      },
      "correct_answer": "B",
      "simplification_notes": "Made language concise and explicit: linked environmental change → altered relative fitness → differential reproduction → change in allele/trait frequencies; added brief statement that changing frequencies is microevolution. Removed historical detail and extraneous terms.",
      "content_preserved": true,
      "source_article": "Natural selection",
      "x": 1.8304734230041504,
      "y": 1.1133588552474976,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Natural selection as a mechanism requiring heritable variation and differential survival/reproduction",
        "Concept 2: Role of environment in shaping which traits are favored, leading to changes in trait frequencies across generations (microevolution vs macroevolution)",
        "Concept 3: Different modes of selection (natural selection, sexual selection, fecundity selection) and how they influence which traits are favored"
      ],
      "original_question_hash": "3f58f392"
    },
    {
      "question": "If a text's meaning is not fixed, why do different literary theories produce different but internally coherent interpretations of the same text?",
      "options": {
        "A": "Because the author's intended meaning changes over time, so readers must continually rediscover the text's purpose under each theory.",
        "B": "Because each theory supplies a distinct set of interpretive assumptions and tools (about language, culture, ideology, power), so readings highlight different aspects of the text.",
        "C": "Because individual readers' personal emotions and reactions determine which parts of the text they accept as meaningful, independent of theoretical frameworks.",
        "D": "Because texts encode a universal, fixed code of meaning and theories simply reorder signs into different sequences without changing the underlying message."
      },
      "correct_answer": "B",
      "simplification_notes": "Question reworded for clarity and concision; retained core idea that nonfixed meaning allows multiple coherent readings. Options were shortened and phrased as plausible alternatives; theoretical language (assumptions, tools, ideology) preserved.",
      "content_preserved": true,
      "source_article": "Literary theory",
      "x": 0.8624365329742432,
      "y": 1.0624544620513916,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Meaning in texts is not fixed; different theories explain how meaning is created and why readings can vary.",
        "Concept 2: Reading is shaped by theoretical frameworks and cultural/ideological contexts (e.g., semiotics, philosophy of language, cultural studies), influencing interpretation.",
        "Concept 3: The aims of theory are varied and contested (close reading vs. culture/ideology critique), reflecting the plural and contextual nature of literary theory."
      ],
      "original_question_hash": "d98dedf0"
    },
    {
      "question": "If culture is acquired and passed on by enculturation and socialization, why do different societies still develop distinct cultures?",
      "options": {
        "A": "Because genetic differences between human populations produce different innate learning tendencies that create distinct cultural norms.",
        "B": "Because norms and practices are learned continuously within each social group, and different groups face different environments, histories of interaction, and social choices, which produce divergent cultural repertoires.",
        "C": "Because cultures are fixed entities that do not change or adapt to new conditions, so differences persist unchanged.",
        "D": "Because global mass media impose a single, uniform global culture that eliminates local variation."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified (asked directly why learned culture varies). Technical terms 'enculturation' and 'socialization' retained. Distractors made plausible (genetics, cultural fixity, globalization) while preserving the original correct option.",
      "content_preserved": true,
      "source_article": "Culture",
      "x": 1.2429252862930298,
      "y": 0.9817147254943848,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Culture is learned and transmitted through social learning (enculturation and socialization), which explains why cultures differ across societies.",
        "Concept 2: Cultural norms function as guidelines that codify acceptable conduct and shape behavior and social expectations, with implications for group cohesion and the risks of monoculture.",
        "Concept 3: Cultural change is a dynamic process driven by internal pressures (factors encouraging/resisting change) and external contact between societies, with heritage preservation (e.g., UNESCO) illustrating responses to cultural change."
      ],
      "original_question_hash": "a87eab5a"
    },
    {
      "question": "Why does granting intellectual property rights for a limited period align a creator's incentive to invest with the goal of wider diffusion of knowledge?",
      "options": {
        "A": "It grants the creator a temporary monopoly to recover investment and earn returns; after the limited term the invention or work enters the public domain, enabling broader diffusion and follow‑on innovation.",
        "B": "It guarantees the inventor permanent exclusive control over the invention or work, preventing others from using it indefinitely.",
        "C": "It keeps the invention completely private forever so the knowledge never diffuses to the public.",
        "D": "It removes the need for any enforcement or policing of rights during the protection term, so creators do not need to protect their work."
      },
      "correct_answer": "A",
      "simplification_notes": "Rephrased the question to be concise and undergraduate accessible; made the mechanism explicit (temporary monopoly → recover costs → public domain → diffusion); kept distractors plausible and retained original correct answer.",
      "content_preserved": true,
      "source_article": "Intellectual property",
      "x": 1.3105686902999878,
      "y": 0.8535875678062439,
      "level": 2,
      "concepts_tested": [
        "The incentive principle: IP law aims to encourage invention and creativity by granting creators exclusive rights for a limited period.",
        "Mechanism of exclusive, time-limited rights: Providing protection and enforcement for a set duration to motivate investment in intellectual assets.",
        "Relationship/contrast with traditional property: Intellectual property is intangible and non-depleting, requiring balancing incentives for creators with broad access and diffusion of information."
      ],
      "original_question_hash": "1feeb3df"
    },
    {
      "question": "Why do large information imperfections and high transaction costs make a firm more likely to vertically integrate certain activities instead of buying them from the market?",
      "options": {
        "A": "Because integration removes the need for price discovery and contracting, eliminating negotiation costs and making transactions effectively instantaneous.",
        "B": "Because internalizing the activity reduces both ex ante and ex post transaction costs by aligning incentives, improving control over information flows, and mitigating hold-up and coordination problems.",
        "C": "Because external suppliers inevitably extract monopoly rents when information is asymmetric, so integration is the only way to avoid supplier exploitation and secure competitive prices.",
        "D": "Because vertical integration always lowers production costs, making it the optimal choice irrespective of market conditions."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened to a clear undergraduate-level formulation; technical terms (ex ante/ex post transaction costs, hold-up, information flows) were retained and clarified. Distractor options were made concise but still plausible.",
      "content_preserved": true,
      "source_article": "Industrial organization",
      "x": 1.2830513715744019,
      "y": 0.9481342434883118,
      "level": 2,
      "concepts_tested": [
        "Firm boundaries and market structure are shaped by transaction costs and information imperfections (why firms exist and when they internalize activities vs. transact).",
        "Strategic behavior in imperfect competition is analyzed through models like game theory, explaining pricing, oligopoly dynamics, vertical integration, and contract design (how these mechanisms produce observed outcomes).",
        "Policy, regulation, and institutional design influence competition outcomes and the governance of markets (why antitrust and regulation affect firm behavior and market performance)."
      ],
      "original_question_hash": "dfc1528f"
    },
    {
      "question": "In an oligopoly, firms are mutually interdependent. Why might a firm refrain from unilaterally changing its price?",
      "options": {
        "A": "Because rivals will immediately match any price change, so the initiating firm gains no competitive advantage from altering price.",
        "B": "Because a unilateral price change changes rivals' best-response strategies, often provoking reactions that reduce the initiating firm’s profits and make price changes risky.",
        "C": "Because government regulators universally ban any price changes by firms in oligopolies.",
        "D": "Because when a few firms dominate, aggregate market demand becomes perfectly inelastic, so changing price has no effect on quantity sold."
      },
      "correct_answer": "B",
      "simplification_notes": "Question rephrased into concise undergraduate-level language emphasising strategic interdependence; distractor options kept plausible but incorrect; core concept and correct answer retained.",
      "content_preserved": true,
      "source_article": "Oligopoly",
      "x": 1.3128647804260254,
      "y": 0.9232719540596008,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Mutual interdependence and strategic behavior — how actions by one firm affect others and lead to reactions, influencing pricing and output decisions.",
        "Concept 2: Collusion as a mechanism for profit maximization — differences between explicit and tacit collusion and how regulation affects their occurrence.",
        "Concept 3: Product homogeneity and substitutes shaping oligopoly types — how identical vs differentiated goods and substitute elasticity determine whether an oligopoly is considered perfect/pure or imperfect."
      ],
      "original_question_hash": "7b5aaf04"
    },
    {
      "question": "Porter says sustainable competitive advantage arises from (1) trade-offs—choosing some activities and rejecting others—and (2) “fit”—aligning activities so they reinforce one another. How do these two elements together explain why a rival cannot simply copy a firm’s configuration of activities and get the same advantage?",
      "options": {
        "A": "Trade-offs force selection and exclusion of activities, and fit makes the chosen activities mutually reinforcing; together they create an interdependent system that is costly and difficult for rivals to replicate exactly.",
        "B": "Trade-offs encourage rivals to copy a firm’s entire set of activities, and fit makes activity choices interchangeable, so rivals can easily reproduce the same advantage.",
        "C": "Trade-offs are negligible and fit simply standardizes activities to industry norms, making copying straightforward.",
        "D": "Trade-offs only shape product features while fit is merely about organizational culture, so neither creates meaningful barriers to replication."
      },
      "correct_answer": "A",
      "simplification_notes": "Removed lengthy background, tightened language to clearly state Porter’s two elements (trade-offs and fit), and posed a single succinct question. All options kept plausible but only A preserves Porter's logic.",
      "content_preserved": true,
      "source_article": "Strategic management",
      "x": 1.3528376817703247,
      "y": 0.9915142059326172,
      "level": 2,
      "concepts_tested": [
        "Porter’s principle of strategy: creating a unique and valuable market position through trade-offs and organizational “fit” of activities.",
        "Mechanism of strategic management: feedback loops that monitor execution and inform the next cycle of planning.",
        "Relationships and levels of strategy: the distinction and linkage between corporate strategy, business strategy, and operational management (and how each answers different questions and governs resource allocation)."
      ],
      "original_question_hash": "3b608a82"
    },
    {
      "question": "In the social determinants of health framework, chronic stress is the pathway linking adverse social and economic conditions to poorer health. Mechanistically, how does chronic stress produce long-term increases in disease risk?",
      "options": {
        "A": "Chronic stress mainly reduces access to medical care directly by causing financial constraints and missed appointments.",
        "B": "Chronic stress activates neuroendocrine and inflammatory pathways, producing cumulative \"wear-and-tear\" on multiple physiological systems (allostatic load) that raises disease risk over time.",
        "C": "Chronic stress changes an individual’s DNA sequence (causes mutations) that permanently increase susceptibility to disease.",
        "D": "Chronic stress only causes short-term or acute illnesses in childhood and has no measurable long-term health effects."
      },
      "correct_answer": "B",
      "simplification_notes": "Phrased the stem more directly for undergraduates, used precise terms (neuroendocrine, inflammatory, allostatic load), kept all four options plausible and aligned with the original concepts.",
      "content_preserved": true,
      "source_article": "Social determinants of health",
      "x": 1.2138139009475708,
      "y": 0.8968653082847595,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Structural determinants (policy and economic arrangements) shape health outcomes and health equity, influencing access to care and disease vulnerability.",
        "Concept 2: Life-course perspective as a framework showing how conditions across life stages accumulate to affect health and functioning.",
        "Concept 3: Chronic stress as a mediating mechanism by which adverse social/economic conditions lead to poorer health outcomes."
      ],
      "original_question_hash": "3611dff8"
    },
    {
      "question": "How do mechanical forces interact with genetic programs to shape tissues during morphogenesis?",
      "options": {
        "A": "Mechanical forces act entirely separately from gene regulatory networks; genes only respond passively to physical deformation without influencing the mechanics.",
        "B": "Mechanical forces modulate cell adhesion, cytoskeletal tension, and migration, which change signaling and gene expression; gene regulatory networks then interpret these mechanical cues and adjust cell behavior, forming a feedback loop that coordinates tissue morphogenesis.",
        "C": "Mechanical forces appear only after morphogenesis is finished and serve solely to remodel or stabilize tissues, not to drive the primary shaping process.",
        "D": "Tissue form is set exclusively by morphogen concentration gradients and gene programs; mechanical factors have no role in influencing cell fate, movement, or tissue patterning."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was shortened and clarified for an undergraduate audience; technical terms (cell adhesion, cytoskeletal tension, gene regulatory networks, morphogen gradients) were retained. The answer options were made concise and plausible while preserving the original logical alternatives.",
      "content_preserved": true,
      "source_article": "Morphogenesis",
      "x": 2.020975351333618,
      "y": 1.1506491899490356,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Morphogenesis is driven by spatial patterning (morphogen gradients and gene regulatory networks) that dictate cell fate and behavior during development.",
        "Concept 2: Mechanical forces and cell movements are integral to morphogenesis and operate in concert with genetic programs to shape tissues.",
        "Concept 3: Morphogenesis occurs across contexts (embryonic development, tissue maintenance/regeneration, and cancer) and dysregulation can lead to abnormal forms, illustrating causal links between signaling, mechanics, and morphology."
      ],
      "original_question_hash": "aa08bf23"
    },
    {
      "question": "How do intragenomic interactions—epistasis, pleiotropy, and heterosis—explain why a gene's observable effect on a trait can change depending on the rest of the genome?",
      "options": {
        "A": "They indicate a gene's effect is constant across genetic backgrounds and independent of other loci.",
        "B": "They show a gene's effect can be modified by the allelic state at other loci (epistasis); a single gene can influence multiple traits (pleiotropy); and heterosis reflects non-additive interactions between alleles—together making phenotypes genome-context dependent.",
        "C": "They imply genome-wide analyses are unnecessary because individual genes alone determine traits in a simple additive way.",
        "D": "They demonstrate that only environmental factors, not genetic interactions, shape complex traits."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording made more concise and direct; each term (epistasis, pleiotropy, heterosis) was explicitly linked to how it changes a gene's phenotypic effect. Distractors kept plausible.",
      "content_preserved": true,
      "source_article": "Genomics",
      "x": 2.0663821697235107,
      "y": 1.1399824619293213,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Genomics as a genome-wide, systems-level study focusing on collective gene interrelations rather than isolated gene effects.",
        "Concept 2: Intragenomic interactions (epistasis, pleiotropy, heterosis) as mechanisms showing that gene effects are context-dependent and interdependent.",
        "Concept 3: The role of technologies (high-throughput sequencing, bioinformatics) in enabling genome-scale analysis and integration into systems biology."
      ],
      "original_question_hash": "ce8af631"
    },
    {
      "question": "How does the principle of \"equality before the law\" operate to constrain political power and prevent arbitrary government action?",
      "options": {
        "A": "By binding rulers and public institutions to the same general rules and due-process procedures, so officials must justify actions under objective legal standards rather than personal whim.",
        "B": "By permitting different enforcement or application of laws for different political groups, letting political considerations determine outcomes.",
        "C": "By replacing due process with discretionary secrecy so government can act more quickly without legal constraints.",
        "D": "By placing international legal norms above domestic law in routine governmental decisions, thereby limiting national discretion."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened and focused on the mechanism; historical and theoretical detail was removed; distractors were made plausible yet incorrect; phrasing made concise and academically clear.",
      "content_preserved": true,
      "source_article": "Rule of law",
      "x": 1.2233461141586304,
      "y": 0.8367442488670349,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Equality before the law / no one above the law as the defining mechanism that constrains power and prevents arbitrariness.",
        "Concept 2: Formalist vs. substantivist interpretations (stability, accessibility, and clarity of laws vs. inclusion of rights and international law).",
        "Concept 3: Relationships and distinctions between the rule of law and related concepts (constitutionalism, Rechtsstaat, rule of man) and the historical development underpinning these ideas."
      ],
      "original_question_hash": "a9a7da77"
    },
    {
      "question": "In the \"waves of democratization\" framework, why do major surges of democratic reform often get followed by waves of de-democratization?",
      "options": {
        "A": "Because economic gains from democratization automatically and permanently consolidate democratic rule, preventing any reversal.",
        "B": "Because newly empowered actors—especially elites who fear losing control—mobilize to roll back reforms, and polarized civil-society dynamics and weak institutions create openings for a backlash.",
        "C": "Because international institutions routinely force countries to reverse democratic reforms after a period of democratization.",
        "D": "Because the electorate immediately endorses further liberalization, eliminating any political pressure or incentive for backsliding."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; core idea preserved emphasizing that empowered actors (elites, civil society) and institutional dynamics drive the backlash. Distractors made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Democratization",
      "x": 1.0294269323349,
      "y": 0.7772293090820312,
      "level": 2,
      "concepts_tested": [
        "Waves of democratization as a macro-level pattern with surges and backlashes (Huntington’s three waves and subsequent de-democratization)",
        "Diverse causal pathways to democratization (elite-driven versus bottom-up civil society and international processes) and how actors influence transitions",
        "Measurement and conceptual debates around democracy (democracy indices, what counts as democracy, and methodological disagreements)"
      ],
      "original_question_hash": "3bee31d9"
    },
    {
      "question": "Why does governance commonly shift from informal leadership in small groups to formal governing bodies as groups grow, and how does this shift help meet governance goals?",
      "options": {
        "A": "Formal rules are unnecessary in small groups because members rely on shared norms; when groups become larger, keeping governance informal preserves flexibility but typically weakens accountability and consistent resource mobilization.",
        "B": "In small groups, direct communication and mutual trust enable coordination without centralized authority; as groups grow, coordination becomes more complex, so delegated authority, standardized rules, and formal oversight are needed to ensure consistency, mobilize resources, and maintain accountability.",
        "C": "Small groups are often influenced by external actors, so when groups grow governance should abandon rules and promote spontaneous, decentralized leadership to reflect diverse interests.",
        "D": "Governance form is independent of group size; any shift from informal to formal governing arrangements is arbitrary and unrelated to coordination, resource allocation, or accountability."
      },
      "correct_answer": "B",
      "simplification_notes": "Phrased the question more directly for undergraduates, clarified the causal link between group size and need for delegation, rules, and oversight; preserved original concepts and kept answer B unchanged.",
      "content_preserved": true,
      "source_article": "Governance",
      "x": 1.2236636877059937,
      "y": 0.9034140706062317,
      "level": 2,
      "concepts_tested": [
        "Governance as a system of rules, structures, and norms that shape decision-making and mobilize resources to address collective needs",
        "The relationship between group size/formality and governance mechanisms (informal leadership in small groups vs. formal governing bodies in larger groups)",
        "The role of power dynamics, relationships, and communication in governance, including external influences and governance without state activity"
      ],
      "original_question_hash": "544290a8"
    },
    {
      "question": "How does global governance secure compliance among interdependent actors when no single global sovereign exists?",
      "options": {
        "A": "By using a layered mix of formal agreements and informal (\"soft\") norms plus monitoring, reporting, and dispute-resolution mechanisms that generate reciprocity and reputational incentives to comply.",
        "B": "By creating a single universal authority with coercive powers that can enforce rules across all actors.",
        "C": "By relying only on market mechanisms and price signals to punish non-compliance in transactions.",
        "D": "By assuming all actors have identical preferences so they comply voluntarily without any enforcement or monitoring."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the prompt for clarity at undergraduate level, clarified 'absence of a single sovereign' as 'no single global sovereign', retained technical terms like 'soft norms' and 'reputational incentives', kept original correct answer and made distractors plausible.",
      "content_preserved": true,
      "source_article": "Global governance",
      "x": 1.2141395807266235,
      "y": 0.8962934017181396,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Regulation and enforcement of rules in the absence of a global sovereign (how governance achieves cooperation among interdependent actors)",
        "Concept 2: Multiactor governance (the shift from state-only to involving NGOs, firms, epistemic communities, etc., and its impact on power dynamics)",
        "Concept 3: Institutional mechanisms for coordination and addressing collective-action problems (how institutions, processes, and relationships enable monitoring, rulemaking, and dispute resolution)"
      ],
      "original_question_hash": "610dcafb"
    },
    {
      "question": "How do immediate, direct degradation pathways (e.g., habitat destruction) and slow, cumulative indirect pathways (e.g., pollutant accumulation or greenhouse‑gas buildup) interact to produce non‑linear, tipping‑point style degradation in ecosystems?",
      "options": {
        "A": "Direct degradation reduces ecosystem resilience, so slowly accumulating indirect stressors push feedbacks past a threshold and trigger abrupt, amplified degradation.",
        "B": "Indirect degradation acts independently and cannot change the trajectory already set by direct degradation.",
        "C": "Tipping points require a single, sudden external shock and cannot result from gradual indirect effects accumulating over time.",
        "D": "The effects of direct and indirect degradation simply add together linearly, with no feedbacks or amplification."
      },
      "correct_answer": "A",
      "simplification_notes": "Question phrased more concisely for undergraduates; 'direct' and 'indirect' pathways given examples; retained emphasis on resilience, thresholds, feedbacks, and tipping points. Options rewritten to be plausible alternatives.",
      "content_preserved": true,
      "source_article": "Environmental degradation",
      "x": 1.5606666803359985,
      "y": 0.9240452647209167,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Human activity (population growth, overconsumption, intensive farming) as a driver of biodiversity loss and broader environmental degradation.",
        "Concept 2: Direct vs. indirect degradation pathways and the role of cumulative processes (e.g., deforestation directly degrades habitat; plastic pollution and greenhouse gas buildup act indirectly and can trigger tipping points in climate systems).",
        "Concept 3: Governance/mismanagement as a driver of degradation and related conflict, and the idea that the environment’s capacity to meet social/ecological needs shapes outcomes."
      ],
      "original_question_hash": "8a92f944"
    },
    {
      "question": "An architectural layout designed so occupants are always visible to authorities (for example, Bentham's Panopticon) is used in Foucauldian analyses as an instance of architecture shaping culture. How would such constant observability most likely affect social norms and everyday behavior?",
      "options": {
        "A": "It forces compliance mainly by physically restricting or suppressing people's actions through direct controls.",
        "B": "It produces a sense of continual surveillance that leads people to self-monitor and internalize norms, altering everyday practices and culture.",
        "C": "It only signals authority and power without changing behavior; people accommodate but are not shaped by the architecture itself.",
        "D": "It chiefly improves operational efficiency, and any cultural change follows from pragmatic shifts toward utilitarian values."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified wording, explicitly referenced the Panopticon and Foucault, removed extended historical detail while preserving the surveillance–culture mechanism; kept all four options plausible.",
      "content_preserved": true,
      "source_article": "Philosophy of architecture",
      "x": 0.7393621802330017,
      "y": 1.0069591999053955,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Architecture as a driver of social values and culture (e.g., Panopticon and Foucauldian analysis of surveillance shaping social practices)",
        "Concept 2: The shift from architecture being seen as pragmatic/engineering to an aesthetic paradigm shaped by Avant-garde movements and mechanization (Constructivism, Functionalism, Cubism, Futurism)",
        "Concept 3: Early distinction between architecture as a function of mental traits (technion) versus its association with the divine or natural (demiorgos), and how this frames the philosophy of architecture within the broader philosophy of art"
      ],
      "original_question_hash": "589a7594"
    },
    {
      "question": "Which mechanism best explains why the adoption of steel or reinforced‑concrete frame construction enabled much taller buildings with large glass façades?",
      "options": {
        "A": "Frame construction channels structural loads through vertical columns and horizontal beams, so exterior walls become non‑load‑bearing and can be largely glazed.",
        "B": "The high tensile strength of steel and reinforced concrete removes the need for any structural framework, allowing completely free‑form shapes without structural constraints.",
        "C": "These materials require much heavier foundations, which favor low, squat building forms to reduce ground pressure and thus limit height.",
        "D": "Because steel and concrete frames are typically prefabricated, architects adopted standardized rectangular box forms to simplify assembly and glazing."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording tightened to focus on structural mechanism; retained four plausible distractors; removed historical narrative and secondary concepts while preserving the tested engineering principle.",
      "content_preserved": true,
      "source_article": "History of architecture",
      "x": 0.1729762703180313,
      "y": 0.2184496521949768,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Technology-driven change in architecture. How material innovations (steel, reinforced concrete, glass, etc.) enable new architectural forms and trends (e.g., Art Nouveau, Beaux-Arts).",
        "Concept 2: Architecture as an interdisciplinary field. Why architecture encompasses urbanism, civil engineering, naval, military, and landscape practice, not just buildings, and how these domains interact.",
        "Concept 3: Evolutionary perspective on shelter/nest-building influencing architectural capability. How early nest-building and shelter practices may have shaped human creativity, construction skill, signaling, and the emergence of architectural behavior (e.g., the concept of a \"home base\")."
      ],
      "original_question_hash": "325a5d73"
    },
    {
      "question": "According to the bundle-of-rights concept, why can a resource be used productively even when no single actor holds every traditional ownership right?",
      "options": {
        "A": "Because ownership is an all-or-nothing legal status—either one party holds the full set of rights or nobody does—so partial rights are irrelevant to productive use.",
        "B": "Because the legal system vests all property rights in the State, which then reallocates or assigns those rights centrally to guarantee productive use.",
        "C": "Because ownership is a modular bundle of separable rights (e.g., use, exclude, transfer, lease, destroy) that can be divided, licensed, or combined among different actors; coordinated allocations and contracts among rights-holders permit productive use.",
        "D": "Because property rights automatically reorganize or contract themselves to produce arrangements that maximize social welfare regardless of who legally holds them."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording shortened and focused on the core bundle-of-rights idea; options rewritten to be concise and plausible for undergraduate readers while preserving legal/institutional terminology (e.g., use, exclude, transfer, lease, license).",
      "content_preserved": true,
      "source_article": "Property",
      "x": 1.2618036270141602,
      "y": 0.8640698790550232,
      "level": 2,
      "concepts_tested": [
        "The bundle-of-rights concept: ownership is a collection of possible rights (use, exclude, transfer, destroy, etc.), and the exact mix defines how a person can control a property.",
        "Forms of property and their social/economic implications: private, public, and collective property shape who has control and how resources are governed.",
        "Property regimes as mediators between individuals, property, and the state: the legal and institutional framework that enforces rights and resolves disputes, defining the relationship among actors."
      ],
      "original_question_hash": "88a25bcf"
    },
    {
      "question": "In law and economics, why is economic efficiency commonly used as the main criterion for evaluating legal rules?",
      "options": {
        "A": "Because it guarantees that every individual is made better off by the legal rule.",
        "B": "Because it aims to maximize total net social benefits by aligning incentives and reducing distortions and transaction costs, thereby increasing overall social welfare.",
        "C": "Because it emphasizes procedural simplicity and faster enforcement of laws rather than the size of social benefits.",
        "D": "Because it requires equalizing outcomes across individuals to achieve fairness."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording shortened and focused on the core rationale; retained key technical terms (incentives, transaction costs, social welfare); removed historical background and branch distinctions to emphasize the conceptual reason efficiency is used.",
      "content_preserved": true,
      "source_article": "Law and economics",
      "x": 0.9283822774887085,
      "y": 0.8967922925949097,
      "level": 2,
      "concepts_tested": [
        "Economic efficiency as a criterion for evaluating legal rules",
        "The two major analytical branches: neoclassical analysis of law and institutional analysis of legal institutions",
        "Application of microeconomic concepts (incentives, costs/benefits, effects) to analyze and predict legal outcomes"
      ],
      "original_question_hash": "6ff3a42b"
    },
    {
      "question": "How does a superior value proposition generate competitive advantage, and how does it influence customer behavior so the advantage is sustained?",
      "options": {
        "A": "By allowing the firm to reduce its production costs more than competitors, so it can offer lower prices to all customers.",
        "B": "By delivering benefits customers value more than available alternatives, which raises loyalty and willingness to pay and creates switching barriers or preferences that rivals find hard to copy.",
        "C": "By securing regulatory protections or legal barriers that prevent competitors from entering the market.",
        "D": "By guaranteeing the firm can always undercut rivals on price without reducing quality, forcing customers to choose it for cost alone."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the original question into clearer academic language for undergraduates; kept core idea that value propositions influence loyalty, willingness to pay, and switching barriers. Options were tightened to remain plausible while preserving the original distractors.",
      "content_preserved": true,
      "source_article": "Competitive advantage",
      "x": 1.345170497894287,
      "y": 0.9698946475982666,
      "level": 2,
      "concepts_tested": [
        "Value proposition as the core mechanism: competitive advantage arises when a firm offers better and greater value to customers, influencing loyalty and choices.",
        "Cost leadership vs differentiation: two primary routes (lower cost for the same products/services vs. offering distinct products/services aligned to customer needs) to achieve competitive advantage.",
        "Sustainability and defense of advantage: competitive advantage is maintained through long-term strategy, defensive positions, ROI, and barriers or unique resources that are hard to imitate."
      ],
      "original_question_hash": "e8802a8a"
    },
    {
      "question": "How does lubrication change the friction mechanism at an interface, and why does it typically reduce the retarding frictional force compared with dry solid–solid contact? (Recall that frictional force equals dissipated energy per distance moved, $F=E/d$.)",
      "options": {
        "A": "It increases the real contact area so more asperities engage, producing higher energy dissipation and thus greater friction.",
        "B": "It creates a thin fluid film that carries much of the load and prevents direct solid–solid asperity contact; energy is then dissipated mainly by viscous shear in the lubricant, reducing the net frictional force.",
        "C": "It causes chemical bonding across the interface that strengthens interfacial adhesion, increasing frictional resistance.",
        "D": "It removes all sources of energy dissipation so motion becomes perfectly frictionless and the retarding force is zero."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording made more concise and academic for undergraduates; retained core physics and added the relation $F=E/d$ to connect friction to energy dissipation; all four options kept plausible but only B remains correct.",
      "content_preserved": true,
      "source_article": "Friction",
      "x": 1.7615982294082642,
      "y": 1.0447992086410522,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Friction arises from multiple microscopic mechanisms (asperity contact/deformation, charges, local structural changes) that dissipate mechanical energy as heat and other forms.",
        "Concept 2: The retarding frictional force is tied to energy dissipation per distance (F = E/d) and is connected to wear and overall energy losses in systems.",
        "Concept 3: Different friction types (dry, fluid, lubricated, skin, internal) embody distinct mechanisms at interfaces, with lubrication altering contact conditions and reducing direct solid–solid friction."
      ],
      "original_question_hash": "9e3ec3e1"
    },
    {
      "question": "According to Robert K. Merton, why do social mechanisms serve as a 'middle ground' between social law and mere description?",
      "options": {
        "A": "Because they function mainly as normative interpretive guides for understanding social data, rather than as empirically testable causal claims.",
        "B": "Because they are social processes that produce designated consequences for designated parts of the social structure, thereby linking macro regularities to micro-level outcomes and supporting causal inference.",
        "C": "Because they are universal, exceptionless laws that predict every social event across contexts.",
        "D": "Because they are purely descriptive labels of social phenomena that do not imply causal relationships."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the question to be concise and clearer for undergraduates; preserved Merton's definition by restating 'social processes having designated consequences' and framed distractors as plausible alternatives (normative guide, universal law, purely descriptive).",
      "content_preserved": true,
      "source_article": "Sociological theory",
      "x": 1.238868236541748,
      "y": 0.9876071810722351,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Social mechanisms as designated consequences within the social structure (Merton's idea of mechanisms) and their role as the middle ground between social law and description.",
        "Concept 2: The distinction between sociological theory and social theory (objectivity, testability, reliance on scientific method vs normative critique).",
        "Concept 3: Dynamic social theory as the idea that institutions and patterns of behavior function as social models that can be replicated or adapted to yield predictable outcomes, akin to theories in the natural sciences."
      ],
      "original_question_hash": "dcde5887"
    },
    {
      "question": "Why is purchasing power parity (PPP) generally considered a more meaningful way to aggregate national GDPs into a single world GDP figure than using official foreign exchange rates?",
      "options": {
        "A": "PPP depends on current foreign exchange market prices to convert values, so it tracks investor sentiment and short-term currency movements.",
        "B": "PPP uses a common basket of goods and services to adjust for national price level differences, so GDP expressed in PPP reflects real volumes of output rather than exchange-rate fluctuations.",
        "C": "PPP treats all currencies as having the same nominal value and therefore ignores differences in domestic prices and purchasing power.",
        "D": "PPP standardizes national GDPs by adjusting for differences in taxes and subsidies, converting output to a uniform fiscal basis across countries."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened and made more direct for undergraduate readers; retained the focus on why PPP is preferred over official exchange rates. Distractors were kept plausible and concise.",
      "content_preserved": true,
      "source_article": "World economy",
      "x": 1.2634422779083252,
      "y": 0.9053763747215271,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Purchasing power parity (PPP) as a method for aggregating global economic activity and why it is used over official exchange rates.",
        "Concept 2: The inclusion of non-market and regulated activities (e.g., black markets) in defining the world economy and why monetary valuation may be incomplete or misleading.",
        "Concept 3: The world economy as an aggregate of national economies and its inseparability from geography and ecology, including how location and environmental factors shape economic activity."
      ],
      "original_question_hash": "efcdb9c1"
    },
    {
      "question": "Everyday neighbor interactions (greetings, favors, informal judgments) occur inside larger structural contexts (housing markets, public policy, demographics). How do these micro-level interactions influence neighborhood change and the subjective meanings residents attach to place?",
      "options": {
        "A": "They are entirely determined by macro-level policies and market forces; micro-level interactions do not shape neighborhood dynamics.",
        "B": "They create shared meaning through reciprocal exchanges and everyday routines, but are constrained and shaped by structural conditions, producing emergent neighborhood norms, identities, and patterns of social ties.",
        "C": "They merely reflect fixed social roles imposed uniformly by urban institutions, so neighborhood dynamics remain essentially unchanged.",
        "D": "They are random and unpredictable events that cannot systematically influence neighborhood meanings or long-term dynamics."
      },
      "correct_answer": "B",
      "simplification_notes": "Removed specialized jargon, shortened phrasing, clarified examples of micro and macro conditions, and emphasized the emergent-norms answer while keeping theoretical core about interaction constrained by structure.",
      "content_preserved": true,
      "source_article": "Urban sociology",
      "x": 1.244774580001831,
      "y": 0.9825426936149597,
      "level": 2,
      "concepts_tested": [
        "Micro-level social interactions in urban settings under broader structural conditions shape neighborhood dynamics and subjective meaning.",
        "Urbanization processes influence class formation, social alienation, and identity in city life, affecting outcomes like segregation, poverty, and neighborhood change.",
        "The Chicago School’s mix of ethnographic fieldwork and quantitative methods represents a shift in studying urban life, highlighting the value of integrating multiple methodologies to understand urban phenomena."
      ],
      "original_question_hash": "5501d3e0"
    },
    {
      "question": "In a market economy, how do price signals—which reflect scarcity—affect a firm's choice between a labor-intensive and a capital-intensive production method?",
      "options": {
        "A": "Prices restrict the set of methods a firm can use, forcing a single production method regardless of relative costs.",
        "B": "Prices reveal the relative costs of inputs and the value of outputs, guiding the firm to the least-cost combination of labor and capital that maximizes profit given its technology.",
        "C": "Prices are arbitrary and do not reflect scarcity, so firms cannot rely on them to choose between methods.",
        "D": "Prices lock firms into fixed technologies that do not adjust when input costs or relative scarcities change."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording condensed and clarified the link between price signals, scarcity, and input choice; emphasized 'least-cost combination' and 'profit given technology'. Options were shortened but kept plausible distractors.",
      "content_preserved": true,
      "source_article": "Economic system",
      "x": 1.246480107307434,
      "y": 0.9384512305259705,
      "level": 2,
      "concepts_tested": [
        "The four fundamental economic problems (what to produce, how to produce, for whom/with what distribution, and when to produce) and how these problems shape economic choices and system design.",
        "Mechanisms of economic decision-making (pricing theory, least-cost production methods, and how seasonal/demand-supply dynamics influence production and distribution).",
        "The network of institutions, information flows, and property rights that coordinate an economic system and its relationship to other social systems (law, politics, culture)."
      ],
      "original_question_hash": "77943e28"
    },
    {
      "question": "How do microbial metabolisms drive nutrient cycling in ecosystems, and why are microbes essential for transforming elements such as carbon and nitrogen?",
      "options": {
        "A": "Microorganisms simply accumulate and store nutrients in their biomass and release them only when they die, so nutrient cycling is dictated mainly by cell turnover timing.",
        "B": "Microbes perform enzyme-catalyzed redox transformations that couple energy harvesting to chemical changes—e.g., carbon fixation, nitrogen fixation, decomposition, denitrification—thereby enabling and accelerating element transformations and fluxes beyond abiotic rates.",
        "C": "Microbial metabolism creates new chemical elements from metabolic energy, increasing the sizes of geochemical reservoirs.",
        "D": "Microbial activity is irrelevant to biogeochemical cycles; abiotic chemical equilibria and physical processes alone drive nutrient transformations."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was condensed and clarified for undergraduate readers; retained technical term 'enzyme-catalyzed redox transformations' and concrete examples (carbon fixation, nitrogen fixation, decomposition, denitrification). Distractors were kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Biogeochemical cycle",
      "x": 1.771303415298462,
      "y": 0.9548467397689819,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Biogeochemical cycles are networks of fluxes that transfer and transform elements between living (biotic) organisms and nonliving (abiotic) reservoirs (air, water, soil, rocks).",
        "Concept 2: Microorganisms and their metabolisms are central drivers of nutrient cycling, enabling key transformations such as carbon fixation, nitrogen fixation, decomposition, and denitrification.",
        "Concept 3: There are multiple element-specific cycles (e.g., carbon, nitrogen, water) with distinct processes and reservoirs, including long-term geological sequestration and human-induced alterations (e.g., fossil fuel combustion, synthetic compounds)."
      ],
      "original_question_hash": "ce448f1c"
    },
    {
      "question": "How does political economy analysis explain why development reforms succeed in some places and fail in others?",
      "options": {
        "A": "By concentrating only on the technical design of policies and assuming political factors do not change outcomes.",
        "B": "By emphasising how power relations, vested interests and misaligned incentives among actors shape policy adoption, resource allocation and implementation, producing path-dependent outcomes.",
        "C": "By treating outcomes as driven solely by external economic shocks, independently of domestic political dynamics.",
        "D": "By claiming that reform success is determined mainly by the volume of international aid received, irrespective of local governance and politics."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording tightened for clarity; retained academic terms (political economy analysis, path-dependent). Distractors made concise and plausible, preserving the original concept tested.",
      "content_preserved": true,
      "source_article": "Development studies",
      "x": 1.1761560440063477,
      "y": 0.8745971918106079,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Interdisciplinary integration of economics and politics to address development issues (the field aims to combine ideas from multiple social sciences rather than relying on economics alone).",
        "Concept 2: Political economy analysis as a methodological approach to explain how political and social factors influence development outcomes and reform success or failure.",
        "Concept 3: The evolution of development studies into a multi-disciplinary field (expanding from economics to include history, political science, education, and other social sciences) to address broader development challenges."
      ],
      "original_question_hash": "a30a3428"
    },
    {
      "question": "How do shifts in media technology (e.g., printing, recording, digital media) change the boundaries of what counts as \"literature\" — for example allowing oral texts and non-fiction to be treated as literature?",
      "options": {
        "A": "They rigidly preserve the traditional canon, excluding new formats regardless of how they are recorded or shared.",
        "B": "They enable new forms to be recorded, preserved, and widely distributed, and — through scholarly and cultural recognition — expand what counts as literature beyond print-only fiction.",
        "C": "They make literature lose artistic value, reducing it primarily to functional or utilitarian texts.",
        "D": "They guarantee that only original authorial invention qualifies as literature, irrespective of medium or mode of transmission."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording made more concise and focused on media technologies (printing, recording, digital); preserved core ideas about recording/preservation, distribution, and scholarly/cultural validation that expand literature to oral and non-fiction forms.",
      "content_preserved": true,
      "source_article": "Literature",
      "x": 0.947955846786499,
      "y": 1.0862157344818115,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Literature as a mechanism for recording, preserving, and transmitting knowledge and entertainment (functional role and social significance).",
        "Concept 2: Evolving boundaries of literature, including inclusion of oral literature and non-fiction, and how media/technology (print, digital) reshape what counts as literature.",
        "Concept 3: Relationships between literature and critical/academic study (literary criticism, textual criticism) and how these approaches evaluate texts and their artifacts."
      ],
      "original_question_hash": "f92d7f54"
    },
    {
      "question": "Why do time-limited exclusive rights (e.g., patents or copyrights) encourage inventors and creators to produce new works, even though information can be copied at near-zero marginal cost?",
      "options": {
        "A": "They grant a temporary monopoly that creates potential profits to recoup large upfront costs and risks of developing and disclosing the work; once the term ends, the work becomes broadly accessible.",
        "B": "They permanently prevent others from using the idea, guaranteeing perpetual profits for the originator.",
        "C": "They make copying illegal forever, so nobody can legally reproduce the work at any time.",
        "D": "They remove the need for market signals by substituting mandatory licensing quotas instead of allowing normal market exchange."
      },
      "correct_answer": "A",
      "simplification_notes": "Rephrased question to undergraduate level, emphasized time-limited exclusive rights and near-zero copying cost; kept core rationale (recouping upfront costs via temporary monopoly) and made distractors plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Intellectual property",
      "x": 1.3096563816070557,
      "y": 0.8623532056808472,
      "level": 2,
      "concepts_tested": [
        "The incentive-based rationale for intellectual property: IP rights are intended to encourage creation and investment by granting temporary exclusive rights, which is argued to stimulate innovation and technological progress.",
        "The indivisibility of IP and the appropriation problem leading to a balancing act: Unlike land, IP can be consumed by many without depletion, yet copying can erode value, so laws must balance strong protection with broad access.",
        "Mechanisms and categories as policy tools: Patents, copyrights, trademarks, and trade secrets are legal mechanisms designed to assign and protect information rights for a limited period to achieve the incentivization goal."
      ],
      "original_question_hash": "3c1c2da3"
    },
    {
      "question": "According to the harm principle described in the article, when is it justified to limit freedom of speech and how should such limits be determined?",
      "options": {
        "A": "Because speech can threaten social order, restrictions are justified whenever it offends or unsettles the majority, with scope set by prevailing public opinion.",
        "B": "Restrictions are justified only to prevent foreseeable, direct harm to others; their scope should be set by balancing the probability and seriousness of harm against the value of the speech and by using the least restrictive means.",
        "C": "The harm principle prohibits any restrictions: freedom of speech should never be limited because harms cannot be reliably anticipated.",
        "D": "Limits should focus primarily on preventing harm to a speaker’s reputation; any speech likely to damage reputations must be banned."
      },
      "correct_answer": "B",
      "simplification_notes": "Question rephrased to a concise, direct prompt; historical and legal background removed; all options rewritten to be short, plausible, and reflect distinctions between harm, offence, absolutism, and reputation-based restrictions. Core concept of the harm principle and balancing test preserved.",
      "content_preserved": true,
      "source_article": "Freedom of speech",
      "x": 1.2006243467330933,
      "y": 0.8227961659431458,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Harm principle and the justification for limiting freedom of speech (why restrictions are allowed to prevent harm to others, and how such limits are determined).",
        "Concept 2: The relationship between freedom of expression and other rights/duties (how rights are balanced with duties, responsibilities, and competing interests like national security, privacy, or reputation).",
        "Concept 3: Mechanisms and governance of speech in practice (how censorship, content moderation, and legal frameworks operate in both traditional and digital contexts, and how they interact with the underlying principle of freedom)."
      ],
      "original_question_hash": "d32d7da7"
    },
    {
      "question": "Why does randomizing participants to exposure versus control groups strengthen causal inference about an exposure's effect on a health outcome?",
      "options": {
        "A": "It mainly increases statistical power by reducing random sampling error.",
        "B": "It ensures that exposure and outcome are measured identically in both groups.",
        "C": "It guarantees temporality by making the exposure occur before the outcome in every participant.",
        "D": "It tends to balance both measured and unmeasured confounders between groups, reducing confounding bias and allowing a closer estimate of the true causal effect."
      },
      "correct_answer": "D",
      "simplification_notes": "Reworded the stem for clarity and brevity; kept technical terms (confounders, causality) appropriate for undergraduates; made all options plausible but preserved the original correct answer.",
      "content_preserved": true,
      "source_article": "Epidemiology",
      "x": 1.0702627897262573,
      "y": 0.908204972743988,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Determinants and risk factors shape the distribution and patterns of disease in populations, providing targets for prevention.",
        "Concept 2: Study design and statistics are used to infer relationships between exposures and health outcomes, moving from observed patterns toward causal understanding.",
        "Concept 3: Syndemics and interdisciplinary influences illustrate how multiple factors and interactions across biology, social science, and other fields shape disease processes and health outcomes."
      ],
      "original_question_hash": "648282bf"
    },
    {
      "question": "Why is the proportionality principle central when assessing the risk–benefit ratio of research involving human participants?",
      "options": {
        "A": "Because the ethical acceptability of a given risk depends on how important the research objective is; more important objectives can ethically justify greater risk, while less important goals justify smaller risk.",
        "B": "Because risk must be reduced to zero in human research; any nonzero risk is ethically unacceptable regardless of the objective’s importance.",
        "C": "Because the risk–benefit ratio is determined solely by statistical measures (e.g., statistical significance and sample size) and therefore does not depend on the importance of the research objective.",
        "D": "Because proportionality requires that risks always equal benefits, so the risk–benefit ratio must always be exactly one."
      },
      "correct_answer": "A",
      "simplification_notes": "Rewrote the question clearly for undergraduates, kept the link to ethical justification (Declaration of Helsinki) and proportionality. Shortened wording and removed extraneous material about risk perception and evaluation types while keeping the tested principle intact. Options made plausible alternatives.",
      "content_preserved": true,
      "source_article": "Risk–benefit ratio",
      "x": 1.4561681747436523,
      "y": 1.0490713119506836,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Proportionality principle in risk-benefit analysis (the importance of the objective relative to the risk; ethical justification for risk in research per the Helsinki Declaration and CONSORT).",
        "Concept 2: Human factors and risk perception mechanisms (voluntary vs involuntary risk, risk aversion, and biases in perceived risk such as misjudging flying vs driving).",
        "Concept 3: Different risk evaluation categories and their relations to evidence (real, statistical, projected, and perceived risk) and how these influence decision-making."
      ],
      "original_question_hash": "23a78f58"
    },
    {
      "question": "In a human-in-the-loop clinical decision support system (CDSS), why is clinician review of the system's suggestions necessary to preserve decision quality and patient safety?",
      "options": {
        "A": "Because the CDSS suggestions are always correct, so clinician review exists only to speed up the workflow.",
        "B": "Because clinicians add domain expertise and knowledge of patient context to interpret, validate, modify, or reject the CDSS suggestions, mitigating errors and aligning decisions with the individual patient.",
        "C": "Because automated CDSS outputs are fully sufficient for diagnosis and treatment, and human review provides no additional value while slowing care.",
        "D": "Because human review forces every decision to follow the CDSS recommendations exactly, making clinical outcomes more deterministic."
      },
      "correct_answer": "B",
      "simplification_notes": "Question was reworded for clarity and concision for undergraduates; technical phrase 'human-in-the-loop' retained; distractors made plausible but incorrect; core rationale preserved.",
      "content_preserved": true,
      "source_article": "Clinical decision support system",
      "x": 1.4090263843536377,
      "y": 1.009924054145813,
      "level": 2,
      "concepts_tested": [
        "Human-in-the-loop decision making: CDSSs provide suggestions that clinicians review and may accept or discard, highlighting the interactive collaboration between clinician and system.",
        "Classification and workflow timing: CDSSs are categorized as knowledge-based vs non-knowledge-based and used at different points in the care process (pre-diagnosis, during diagnosis, post-diagnosis), illustrating how methodology and timing shape use.",
        "Data-driven reasoning mechanisms: CDSSs transform patient data into context-specific advice using approaches like rule-based guidance (DDSS) and case-based reasoning (CBR), illustrating the link between data input, reasoning method, and output."
      ],
      "original_question_hash": "f4bbaad8"
    },
    {
      "question": "When a manager must make an ethical decision, how does normative business ethics differ from descriptive business ethics?",
      "options": {
        "A": "Normative ethics describes what people actually do in practice; descriptive ethics provides standards about how people ought to behave.",
        "B": "Descriptive ethics prescribes what people should do; normative ethics describes how people actually behave in the market.",
        "C": "Normative ethics sets standards for how actors ought to behave and guides decision-making even when actual practice differs; descriptive ethics reports how actors actually behaved without prescribing obligations.",
        "D": "Descriptive ethics enforces laws and penalties; normative ethics measures profits and managerial efficiency."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording shortened and clarified; options rewritten to give concise, plausible contrasts between 'sets standards' (normative) and 'reports behavior' (descriptive); removed broader historical and institutional context while preserving the tested distinction.",
      "content_preserved": true,
      "source_article": "Business ethics",
      "x": 1.301160216331482,
      "y": 0.9560069441795349,
      "level": 2,
      "concepts_tested": [
        "Normative vs. descriptive business ethics: how normative standards guide conduct versus how behavior is described/measured.",
        "Interaction of economics and ethics: how profit-maximizing behavior intersects with non-economic concerns to shape ethical issues.",
        "Institutional mechanisms for ethics: how codes, CSR charters, and regulatory frameworks transmit and enforce ethical norms within organizations."
      ],
      "original_question_hash": "50d1b351"
    },
    {
      "question": "In stimulus diffusion the core idea or solution spreads to another culture even though the original form is modified or rejected. Why can the underlying concept diffuse despite the recipient rejecting the original form, and what does this reveal about how diffusion operates?",
      "options": {
        "A": "Because societies often confront similar problems or needs, so an abstract solution is transferable; recipient cultures reinterpret and implement the concept with their own materials and practices, showing that diffusion travels by functional compatibility rather than exact copying.",
        "B": "Because diffusion requires precise copying of the original artifact or practice; only when the form is preserved does the idea successfully spread.",
        "C": "Because diffusion is driven mainly by coercive processes like conquest, which compel populations to adopt the original cultural package intact.",
        "D": "Because diffusion only occurs when a culture has no previous exposure to related ideas, forcing a full replacement of existing systems rather than adaptation."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened and technical terms clarified. Removed historical examples and streamlined options to focus on why stimulus diffusion works and what it reveals about diffusion mechanisms. Kept the original correct answer and core concept intact.",
      "content_preserved": true,
      "source_article": "Cultural diffusion",
      "x": 1.2327064275741577,
      "y": 1.0082175731658936,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The five diffusion types (Expansion, Relocation, Hierarchical, Contagious, Stimulus) and how each pattern governs the spread of cultural items (e.g., role of distance, social structure, and elites).",
        "Concept 2: The mechanisms/pathways of diffusion (direct diffusion, migration, trade/commerce, media/communication, trans-cultural marriages, skilled labor movement) and how these channels affect rate, reach, and direction of diffusion.",
        "Concept 3: Stimulus diffusion as a diffusion mechanism in which the underlying concept is adopted while the original form is modified or rejected (illustrating how ideas can transfer without copying the exact cultural package)."
      ],
      "original_question_hash": "ce07c7c2"
    },
    {
      "question": "How does a matched-guise experiment demonstrate that language variation encodes social hierarchies?",
      "options": {
        "A": "By showing that listeners judge speakers primarily by the speakers' actual education or language proficiency, regardless of which speech variety is presented.",
        "B": "By having the same speaker present identical content in different language varieties (guises) and asking listeners to rate them; systematic differences in ratings indicate the social meanings and prestige attached to each variety rather than the speaker's intrinsic identity.",
        "C": "By measuring how quickly speakers shift between styles during real conversations to maintain or gain social dominance.",
        "D": "By analyzing long-term demographic and dialect changes across populations to predict shifts in social status."
      },
      "correct_answer": "B",
      "simplification_notes": "Question and options were reworded for clarity and concision; the matched-guise method was explicitly described; distractors were kept plausible but refocused to contrast with the correct methodological mechanism.",
      "content_preserved": true,
      "source_article": "Sociolinguistics",
      "x": 1.2326340675354004,
      "y": 1.070081114768982,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Language variation is socially conditioned, varying across groups (ethnicity, gender, class, age, education) and contexts, signaling social identities.",
        "Concept 2: Variation influences and reflects language change over time, linking social norms and linguistic evolution.",
        "Concept 3: Distinct language varieties (dialects, sociolects, ethnolects) encode social categories and can reinforce or illuminate social hierarchies; interdisciplinary methods (ethnography, matched-guise, corpora analysis) are used to study these relationships."
      ],
      "original_question_hash": "1e74ecf2"
    },
    {
      "question": "According to the uniformitarian principle in linguistics, why can we use patterns of present-day language change to infer how languages changed in the past?",
      "options": {
        "A": "Because past language change was governed by completely different mechanisms, so contemporary patterns are irrelevant.",
        "B": "Because the same cognitive and social pressures that drive language change today (e.g., economy/least effort, analogy, contact) have operated in earlier periods, so regularities seen now can plausibly be extrapolated to reconstruct past changes.",
        "C": "Because only borrowings from other languages drive long-term change, and internal processes like sound change are negligible for historical reconstruction.",
        "D": "Because language change happens instantaneously, so current patterns map directly onto past changes without intermediate variation."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the question to be direct and concise; preserved the uniformitarian principle and examples of mechanisms (economy, analogy, contact). Distractors kept plausible misconceptions.",
      "content_preserved": true,
      "source_article": "Language change",
      "x": 1.2297486066818237,
      "y": 1.0973483324050903,
      "level": 2,
      "concepts_tested": [
        "Uniformitarian principle: Past language change follows the same general principles as present change, guiding historical reconstruction and interpretation.",
        "Economy/least effort as a mechanism: Speech communities simplify utterances to reduce effort, driving phonetic reduction and shaping regular sound changes.",
        "Gradualism and accumulation: Change occurs through extended variation with old and new forms coexisting, eventually accumulating to produce divergent languages and language families."
      ],
      "original_question_hash": "a7a608c3"
    },
    {
      "question": "Why must a strategy align its ends (goals) with its means (resources), and how does inadequate mobilization of resources affect the sequencing and coherence of strategic actions?",
      "options": {
        "A": "Limited resources force prioritization and sequencing so actions are feasible and coherent; if ends exceed available means the plan must be reprioritized or scaled back, otherwise execution becomes fragmented and unsustainable.",
        "B": "Careful planning alone can always achieve end goals regardless of resource levels, so alignment between ends and means is not essential.",
        "C": "Resource mobilization does not influence the order of actions; strategy is primarily about forecasting and declaring intent.",
        "D": "When resources are scarce, emergent improvisation will always produce faster strategic gains than any deliberately aligned plan."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened and made more concise for undergraduate readers; retained the core focus on ends-means alignment, resource mobilization, and effects on action sequencing and coherence. Technical framing (planned vs emergent strategy) was preserved implicitly.",
      "content_preserved": true,
      "source_article": "Strategy",
      "x": 1.348038673400879,
      "y": 0.9842358827590942,
      "level": 2,
      "concepts_tested": [
        "Ends-means alignment and resource mobilization",
        "Planned versus emergent strategy",
        "Strategy as the political-to-military interface"
      ],
      "original_question_hash": "c03afc8f"
    },
    {
      "question": "Why does inter-modal transport — using more than one mode (e.g., ship or train plus truck or car) in a single journey — often move people or freight more efficiently than using just one mode for the entire trip?",
      "options": {
        "A": "Because it lets long-distance segments use high-capacity, lower-cost modes (e.g., ships or trains) while the first/last-mile uses flexible modes (e.g., trucks or cars), reducing overall cost and expanding feasible routes.",
        "B": "Because it reduces the need to coordinate timetables and operations between the different modes.",
        "C": "Because it eliminates transfers and handling, so cargo and passengers never change vehicles.",
        "D": "Because it always minimizes total travel time, regardless of transfer or waiting times."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was shortened and clarified for undergraduate readers; 'long-haul leg' changed to 'long-distance segments' and 'first/last-mile' used for final/initial legs. Distractor options were kept plausible but incorrect. Core concept about cost, capacity, and mode suitability preserved.",
      "content_preserved": true,
      "source_article": "Transport",
      "x": 1.6325688362121582,
      "y": 0.824397623538971,
      "level": 2,
      "concepts_tested": [
        "Inter-modal transport and mode choice: how combining modes based on cost, capability, and route creates efficient movement of people/cargo.",
        "Regulation and governance of transport systems: how financing, policies, and public/private ownership influence operations and infrastructure development.",
        "Transport, trade, and urban-environmental relationships: transport enabling economic growth while impacting pollution, land use, and city planning, with planning needed to manage flow and sprawl."
      ],
      "original_question_hash": "0f6ded2e"
    },
    {
      "question": "How do optimization models and simulation reduce costs in logistics networks?",
      "options": {
        "A": "By automatically accelerating shipments to cut lead times regardless of the cost consequences.",
        "B": "By explicitly modeling trade-offs among inventory holding, transportation, and facility costs, and by testing policies under uncertainty to find cost-minimizing configurations and robust service levels.",
        "C": "By eliminating the need for local operational knowledge and expert judgment in decision making.",
        "D": "By guaranteeing a correct, optimal solution even when input data are inaccurate or incomplete."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the original question to a clear undergraduate-level form, kept mention of optimization and simulation, and preserved the core idea that models represent cost trade-offs and test policies under uncertainty. Distractors were reformulated to remain plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Logistics",
      "x": 1.4155994653701782,
      "y": 0.9667268395423889,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Logistics manages the efficient forward and reverse flow of goods, services, and information to meet customer needs (principle of flow and customer alignment).",
        "Concept 2: Logistics has a defined scope within the supply chain—focused on movement between facilities and not on internal production flow—highlighting its boundary and relationship to production planning.",
        "Concept 3: Logistics significantly affects organizational costs, and optimization through modeling/simulation is used to minimize resource use and improve efficiency."
      ],
      "original_question_hash": "99f9fee6"
    },
    {
      "question": "In a general optimization problem, why does replacing a minimization $\\min_{x\\in A} f(x)$ by the maximization $\\max_{x\\in A} -f(x)$ preserve the set of optimal solutions?",
      "options": {
        "A": "Because negation reverses the ordering of objective values: if $f(x^*)\\le f(x)$ for all $x\\in A$, then $-f(x^*)\\ge -f(x)$ for all $x\\in A$, so $x^*$ maximizes $-f$ and therefore is also a minimizer of $f$.",
        "B": "Because multiplying the objective by $-1$ leaves the numerical values of $f$ unchanged, so the same points remain optimal.",
        "C": "Because the equivalence only holds when $f$ is convex, since convexity guarantees that any argmin maps to an argmax under negation.",
        "D": "Because negating the objective relaxes the constraints and effectively enlarges the feasible set $A$, allowing the original optimum to be found more easily."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording was made more concise and focused on the key algebraic reason; math notation converted to inline LaTeX. Distractors were made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Mathematical optimization",
      "x": 1.6265658140182495,
      "y": 1.170825719833374,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The general optimization formulation as selecting x0 in a set A to minimize or maximize a real-valued function f, highlighting the role of the feasible domain and objective.",
        "Concept 2: The equivalence between minimization and maximization through negating the objective function, showing a fundamental relationship that allows reformulation of problems.",
        "Concept 3: The subdivision of optimization into discrete vs continuous, and the inclusion of constrained and multimodal problems within the continuous case, illustrating how problem structure influences approach and complexity."
      ],
      "original_question_hash": "be4ebea4"
    },
    {
      "question": "How does the division of labour generate economic interdependence and motivate trade among producers?",
      "options": {
        "A": "Because it reduces the range of goods each producer makes, shrinking local markets and forcing them to exchange with others.",
        "B": "Because specialization raises productivity on narrow tasks, so producers depend on others to obtain goods and services they no longer produce efficiently.",
        "C": "Because it enables every producer to produce all goods more cheaply than anyone else, eliminating the need for exchange.",
        "D": "Because it removes the need for coordination among producers, making each independent."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording tightened to explicitly link 'division of labour' with 'producers' and 'trade'. Distractors were made concise and plausible while preserving the original intended options. Technical idea of specialization causing dependence was retained.",
      "content_preserved": true,
      "source_article": "Division of labour",
      "x": 1.2507442235946655,
      "y": 0.9218617081642151,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Specialization increases productivity by aligning limited training and capabilities with specific tasks and assets (tools, skills, equipment).",
        "Concept 2: Division of labour is the motive for trade and the source of economic interdependence, linking producers through exchange.",
        "Concept 3: The division of labour evolves with technology and institutions, contributing to economic growth, capitalism, and more complex production systems (historical progression from ancient to industrial contexts)."
      ],
      "original_question_hash": "8968e7fe"
    },
    {
      "question": "Which scenario best illustrates technology as the systemic application of knowledge to practical, reproducible ends (not just a single tool)?",
      "options": {
        "A": "A device that can perform a task but has no standardized methods for use, replication, or integration.",
        "B": "A production line where knowledge is encoded into repeatable procedures, sensors monitor quality, and process improvements are propagated across all units.",
        "C": "A stand-alone gadget that performs a function but cannot be scaled, reproduced reliably, or connected to other systems.",
        "D": "An experimental material engineered for a purpose but never implemented in an actual production process."
      },
      "correct_answer": "B",
      "simplification_notes": "Question language was tightened and focused on 'systemic application of knowledge'; options were made concise and clarified to emphasize reproducibility, standardization, scalability, and implementation.",
      "content_preserved": true,
      "source_article": "Technology",
      "x": 0.8639495968818665,
      "y": 0.18079327046871185,
      "level": 2,
      "concepts_tested": [
        "Technology as the systemic application of knowledge to practical ends (not just tools)",
        "The social, economic, and ethical relationships and trade-offs arising from technology",
        "Historical progression enabling broader communication and societal change (technology’s role in shaping the knowledge economy and connectivity)"
      ],
      "original_question_hash": "38f1c4a2"
    },
    {
      "question": "How did the printing press's mass production of sheet music change access to and distribution of musical works, and the relationship among composer, performer, and audience?",
      "options": {
        "A": "It raised the cost and exclusivity of music distribution, limiting access mainly to wealthy patrons and institutions.",
        "B": "It decoupled composition from any single performance, allowing many performers in different places to access and perform the same works and thus reach wider audiences.",
        "C": "It fixed musical texts into standardized editions that constrained performers' interpretive freedom and creativity.",
        "D": "It reduced the need to attend live performances by widely circulating printed editions, thereby diminishing demand for communal concerts."
      },
      "correct_answer": "B",
      "simplification_notes": "Question shortened and clarified to focus on how mass-produced sheet music affected access and the composer–performer–audience relationship; options reworded for concision while keeping original meanings and plausibility.",
      "content_preserved": true,
      "source_article": "Music technology",
      "x": 0.08997741341590881,
      "y": 1.4661002159118652,
      "level": 2,
      "concepts_tested": [
        "Technology as a driver of music distribution and access (e.g., printing press for sheet music; gramophone records; radio for broader listening).",
        "Technological capabilities enabling new practices and genres (e.g., overdubbing with multitrack recording; Musique concrète as electronic composition).",
        "Interaction between instrument/notation innovations and musical evolution (e.g., keyboard instrument development shaping orchestration; sheet music dissemination influencing performance and style)."
      ],
      "original_question_hash": "d57e7bb5"
    },
    {
      "question": "In ethnomusicology, long-term fieldwork and participant observation immerse the researcher in a music-making community. Why does this kind of immersion improve understanding of music as a cultural practice?",
      "options": {
        "A": "It ensures the researcher remains neutral and detached from local meanings and practices.",
        "B": "It grants access to tacit knowledge, everyday social routines, and locally held meanings that cannot be obtained from recordings or scores alone.",
        "C": "It guarantees findings can be generalized and applied without modification across different cultures.",
        "D": "It primarily speeds up collection of quantitative performance metrics (e.g., tempo, note durations) for statistical analysis."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; jargon reduced (e.g., \"embedded\" → \"immersed\"); preserved reference to fieldwork and participant observation; answer choices rewritten to be concise and plausible.",
      "content_preserved": true,
      "source_article": "Ethnomusicology",
      "x": 1.2234143018722534,
      "y": 1.024134635925293,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Music is studied as a cultural phenomenon embedded in social context, using a holistic, interdisciplinary framework.",
        "Concept 2: Fieldwork and participant observation are central mechanisms for knowledge production, involving researchers as engaged participants within the community.",
        "Concept 3: The field’s evolution and its relationships to other disciplines, including ethical/power dynamics (e.g., informant vs. consultant, politics shaping ethnomusicology)."
      ],
      "original_question_hash": "c73ccbc1"
    },
    {
      "question": "Why is analyzing only the physical acoustical properties of a musical sound (e.g., frequencies, amplitudes, timbre) insufficient to predict what a listener will experience?",
      "options": {
        "A": "Because perception and memory actively organize and interpret the acoustic signal; identical acoustic input can produce different experiences depending on context, expectations, and prior learning.",
        "B": "Because the physical acoustical properties fully determine the auditory experience, making perception and memory unnecessary.",
        "C": "Because memory only stores melodies and therefore does not influence real-time listening or perceptual organization.",
        "D": "Because perception ignores temporal structure (rhythm and meter) and attends only to static pitch-related features."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified examples of physical properties; emphasized role of perception, memory, context, and prior experience. Distractors rewritten as plausible but incorrect misconceptions.",
      "content_preserved": true,
      "source_article": "Psychology of music",
      "x": 1.119644284248352,
      "y": 1.0694855451583862,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Perception as the primary means to understand music (music can be understood through human perception and memory, not solely through physical properties).",
        "Concept 2: Empirical methods combined with theoretical modeling (advancement through data from systematic observations and modeling of musical structures like melody, harmony, rhythm).",
        "Concept 3: Multidisciplinary relevance and practical connections (applications to performance, education, therapy, attitude/skill, health, and social aspects of music)."
      ],
      "original_question_hash": "9457da33"
    },
    {
      "question": "Cultural psychologists (e.g., Baumann, Shweder) describe culture as an abstract analytic concept that helps explain how people make sense of the world and act, rather than as a concrete force that directly produces specific behaviors. What does this imply about explaining behavioral differences across cultures?",
      "options": {
        "A": "Culture is a universal blueprint that deterministically patterns every individual's actions.",
        "B": "Culture is a shared interpretive framework that shapes meanings and thus influences cognition and action probabilistically, without directly causing specific behaviors in a one-to-one way.",
        "C": "Culture has no influence on mind or behavior, so cross-cultural differences are simply coincidences.",
        "D": "Culture directly prescribes exact behaviors for all contexts, functioning like a set of strict rules everyone follows."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording made more concise and academic; clarified 'culture as an abstract analytic concept' and emphasized mutual-constitution and non-deterministic influence; retained original concepts and correct option.",
      "content_preserved": true,
      "source_article": "Cultural psychology",
      "x": 1.2314918041229248,
      "y": 1.0051013231277466,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Mind and culture are inseparable and mutually constitutive; people are shaped by culture, and culture is shaped by its people.",
        "Concept 2: Culture is an abstract analytic notion (not a real, normative, or predictive thing) that helps explain how people understand and act, rather than directly causing behavior.",
        "Concept 3: Cultural traditions and social practices regulate, express, and transform the human psyche, leading to diversity in mind, self, and emotion across cultures."
      ],
      "original_question_hash": "52b3f2f9"
    },
    {
      "question": "How does the legal requirement that forensic methods meet standards for admissible evidence shape how forensic scientists validate and report their methods and results?",
      "options": {
        "A": "It pressures scientists to prioritize speed over methodological rigor so results reach court more quickly.",
        "B": "It requires validation to include quantified error rates, documented traceability (chain of custody), and reproducibility so judges and jurors can assess the reliability of the evidence.",
        "C": "It permits presenting findings as a single definitive conclusion while minimizing or hiding uncertainties to avoid confusing the court.",
        "D": "It allows forensic laboratories to keep analytical methods proprietary and unpeer‑reviewed because courtroom admissibility outweighs scientific scrutiny."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording tightened to focus on the legal–science link; extraneous historical detail removed. Options rewritten to remain plausible and concise while preserving the original distractors and the correct choice.",
      "content_preserved": true,
      "source_article": "Forensic science",
      "x": 1.32110595703125,
      "y": 0.925808846950531,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Forensic science operates under the principle that scientific methods used must meet legal standards of admissible evidence, linking scientific validity to judicial decision-making.",
        "Concept 2: The forensic process creates a relationship between field collection, laboratory analysis, and expert testimony, illustrating how evidence moves from crime scenes to court.",
        "Concept 3: Forensic science comprises multiple sub-disciplines that develop in response to investigative and legal needs, reflecting how evolving needs shape methodological frameworks."
      ],
      "original_question_hash": "1db1f635"
    },
    {
      "question": "Why is technology transfer better characterized as a collaborative, dynamic (non-linear) process rather than a fixed, linear sequence of steps?",
      "options": {
        "A": "Because organizations uniformly align incentives and follow identical procedures, so a single linear path is efficient.",
        "B": "Because stakeholders have diverse motives, resources, and constraints that create feedback loops and changing opportunities, so transfer paths adapt over time.",
        "C": "Because once a technology is transferred it remains unchanged and therefore fits a fixed order of activities.",
        "D": "Because robust IP rights eliminate the need for collaboration, reducing transfer to a straightforward contract negotiation."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording tightened for undergraduate readers; preserved core idea that diverse stakeholder motives/capabilities and changing conditions make TT non-linear and collaborative. Options rewritten to be concise and plausible.",
      "content_preserved": true,
      "source_article": "Technology transfer",
      "x": 1.2949386835098267,
      "y": 0.9750421643257141,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Technology transfer is a collaborative, dynamic (non-linear) process influenced by organizational context and motives, not a strict step-by-step sequence.",
        "Concept 2: Intellectual property rights and licensing are mechanisms that enable sharing, commercialization, and dissemination of technologies, shaping transfer pathways.",
        "Concept 3: The core relationship and aim of technology transfer is to connect invention creators with broad user bases (public and private) across sectors and borders to develop real-world products and societal impact."
      ],
      "original_question_hash": "bda26ce5"
    },
    {
      "question": "Why does statistical inference start by choosing a statistical model of how the data were generated, and how does that model choice affect the meaning and construction of interval estimates (e.g. confidence or credible intervals)?",
      "options": {
        "A": "The model is chosen mainly to simplify calculations; the interpretation and validity of interval estimates do not depend on which model is used.",
        "B": "The model encodes assumptions about the population and the data-generating process; those assumptions determine how sampling variability is translated into uncertainty, so different models can yield different interval estimates from the same observed data, reflecting different plausible explanations.",
        "C": "Choosing a model fixes the true population parameter, so once a model is adopted interval estimates become exact rather than uncertain.",
        "D": "Selecting a model forces the observations to be identically distributed, which makes interval estimates identical regardless of which modeling choices are made."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was shortened and clarified to focus on why a model is chosen and how it affects interval estimates; technical phrasing like \"sampling variability is mapped into uncertainty\" was kept but made more explicit. Distractors were reworded to be plausible yet incorrect.",
      "content_preserved": true,
      "source_article": "Statistical inference",
      "x": 1.605008840560913,
      "y": 1.1357169151306152,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Inference relies on selecting a statistical model of data generation and using it to deduce propositions about a population.",
        "Concept 2: Inference yields different forms of propositions (point estimates, interval estimates like confidence/credible intervals, hypothesis tests, clustering/classification) derived from the model.",
        "Concept 3: The distinction between inferential statistics and descriptive statistics, anchored in the assumption that observed data come from a larger population and are obtained via sampling."
      ],
      "original_question_hash": "a31dab17"
    },
    {
      "question": "In international politics, how does \"soft power\" allow an actor to shape other actors' preferences and set the agenda without using coercion?",
      "options": {
        "A": "By projecting attractive culture, political values, and legitimate foreign policies that make others want the actor's preferred outcomes and voluntarily emulate its example.",
        "B": "By coercing change through threats, sanctions, and overt pressure that force compliance.",
        "C": "By exploiting information gaps and manipulating audiences into compliant behaviour through deceptive messaging.",
        "D": "By relying exclusively on military force and the threat of armed intervention to compel other actors to change."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording made more concise and academic; emphasized core soft-power resources (culture, values, legitimate policies) and voluntary emulation; distractor options kept plausible (hard power, manipulation, military coercion).",
      "content_preserved": true,
      "source_article": "Soft power",
      "x": 1.2176573276519775,
      "y": 0.9600984454154968,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Soft power operates through attraction and co-option rather than coercion, shaping others' preferences.",
        "Concept 2: The sources/resources of soft power are a country's culture, political values, and foreign policies, especially when perceived as legitimate and morally authoritative; credibility is a key asset.",
        "Concept 3: Soft power leads to desirable outcomes by making others want what you want, enabling agenda-setting and influence, and it can be exercised by non-state actors as part of broader power dynamics (the “second face of power”)."
      ],
      "original_question_hash": "557442c2"
    },
    {
      "question": "How have globalization and information and communication technologies (ICT) changed the way states try to influence foreign publics?",
      "options": {
        "A": "They strengthen centralized, one-way government messaging directed at international audiences.",
        "B": "They require multi-actor engagement: state messages are co-constructed and amplified by non-state actors (media, NGOs, corporations) across global networks.",
        "C": "They reduce the importance of direct state messaging, making non-state actors exclusively responsible for shaping foreign public opinion.",
        "D": "They shift emphasis entirely to coercive hard-power tools, rendering public diplomacy irrelevant."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; 'information and communication technologies' abbreviated to ICT; options kept plausible and aligned with the article's emphasis on multi-actor public diplomacy.",
      "content_preserved": true,
      "source_article": "Public diplomacy",
      "x": 1.108792781829834,
      "y": 0.841228187084198,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Public diplomacy as a mechanism to inform and influence foreign publics (and other actors) in order to advance a state's strategic objectives, including the role of propaganda.",
        "Concept 2: The instruments and channels of public diplomacy (personal contact, media, internet, educational exchanges) as the practical mechanisms through which influence is attempted.",
        "Concept 3: The evolving relational landscape of public diplomacy (expansion beyond governments to include media, NGOs, corporations, and other non-state actors) driven by globalization, ICT, and changing international order."
      ],
      "original_question_hash": "6aad2b65"
    },
    {
      "question": "How does the public sphere convert private concerns into political action that can influence policy while remaining distinct from the economic market?",
      "options": {
        "A": "By aggregating private preferences through market price signals and using those prices to legitimize policy decisions.",
        "B": "By functioning as a discursive space where individuals articulate concerns, deliberate, and form public opinion—through media, meetings, and publications—that policymakers can respond to, independently of market exchange.",
        "C": "By relying on government-issued edicts to shape public discourse and impose policy outcomes.",
        "D": "By converting popularity on social media into immediate, non-deliberative policy changes."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened and jargon reduced; explicit mention of channels (media, meetings, publications) was added to option B to reflect mechanisms from the article. All distractors were kept plausible and aligned with article critiques; the correct answer remains unchanged.",
      "content_preserved": true,
      "source_article": "Public sphere",
      "x": 1.2335221767425537,
      "y": 0.9708138108253479,
      "level": 2,
      "concepts_tested": [
        "The public sphere as a discursive space mediating between private individuals and the state, enabling critique and influence on policy.",
        "Distinctions and interactions among the public sphere, the state, the private realm, and the economy (the public sphere is not the market; it functions as a site for discourse and democratic deliberation shaping governance).",
        "Mechanisms of public debate and the formation of public opinion into political action (channels such as mass media, meetings, social media, publications) and how these channels affect legitimacy and policy outcomes."
      ],
      "original_question_hash": "e0db5509"
    },
    {
      "question": "In a liberal democracy, civil society is an autonomous \"third sector\" separate from the state and the market. Which mechanism best explains how civil society can check or constrain government power while remaining independent and not being co-opted?",
      "options": {
        "A": "By formally incorporating political leaders or elites into civil-society institutions so elite preferences become aligned with citizens' interests.",
        "B": "By aggregating diverse citizen interests into independent advocacy and association, monitoring government actions, supplying critical information, and imposing reputational or political costs for abuses or failures.",
        "C": "By substituting formal constitutional checks with informal social norms and market sanctions that are enforced through peer pressure.",
        "D": "By monopolizing access to political resources and communication channels so a single, unified public voice controls policy outcomes."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; historical background and examples removed. Options rephrased to be concise and academically clear while preserving original meanings; correct answer retained.",
      "content_preserved": true,
      "source_article": "Civil society",
      "x": 1.2166736125946045,
      "y": 0.9427086114883423,
      "level": 2,
      "concepts_tested": [
        "Civil society as an autonomous third sector that can advance citizens' interests and potentially constrain or check government power.",
        "Normative civic values (e.g., freedom of speech, independent judiciary) that underpin democratic society and civil society.",
        "Historical and political role of civil society as opposition or counterbalance to state power, particularly in Eastern Europe and during democratic transitions."
      ],
      "original_question_hash": "a51dc042"
    },
    {
      "question": "According to the public forum doctrine, why can government regulate expressive activity differently in a publicly owned park versus a privately owned shopping mall?",
      "options": {
        "A": "Public parks force the government to permit all expression without restriction, while privately owned malls may lawfully ban any speech at the owner's discretion.",
        "B": "Publicly owned parks are treated as public forums where government restrictions on speech must be content‑neutral and narrowly tailored to serve important interests; privately owned malls are private property and not subject to the same constitutional limits, so owners may more freely restrict expressive activity.",
        "C": "Both parks and privately owned malls are treated the same by law, so the government may regulate speech equally in both settings.",
        "D": "In publicly owned parks speech is entirely unrestricted, whereas in privately owned malls the government has broader authority to limit expression."
      },
      "correct_answer": "B",
      "simplification_notes": "Question condensed to a single sentence; legal language clarified (\"public forum doctrine\", \"content‑neutral\", \"narrowly tailored\"); extraneous examples and jurisdictional details removed. Options made concise but plausible.",
      "content_preserved": true,
      "source_article": "Public space",
      "x": 1.2961819171905518,
      "y": 0.9092790484428406,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Public space as a democratic/participatory arena where speech and assembly are exercised, including the public forum concept and the idea that government action can restrict or permit expression differently in public vs. private spaces.",
        "Concept 2: The ownership/access relationship shaping public space, including how privately owned spaces can function as public spaces (or be restricted), and how features like shared space and visual landscape affect use.",
        "Concept 3: Cross-cultural and legal frameworks defining public space (e.g., allemansrätten in Nordic countries, UK public place definition, US public forum principles) and how these definitions influence rights and restrictions."
      ],
      "original_question_hash": "8d6c83c5"
    },
    {
      "question": "How do well-defined, credibly enforced property rights—within an institution—affect transaction and production costs, and why does that matter for economic performance?",
      "options": {
        "A": "They increase transaction costs by adding legal formalities and compliance burdens, which discourages investment.",
        "B": "They reduce opportunistic behavior and information asymmetries, lowering transaction and production costs and thereby improving incentives for long-term investment and economic growth.",
        "C": "They merely shift costs from producers to consumers so there is no net effect on overall efficiency or investment incentives.",
        "D": "They can protect incumbents, creating higher barriers to entry, raising production costs, and reducing innovation."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was tightened to a single clear question; extraneous background removed. Key terms (property rights, enforcement, transaction/production costs, opportunism, information asymmetry, investment incentives) were retained and made explicit. All four options remain plausible alternatives.",
      "content_preserved": true,
      "source_article": "Institution",
      "x": 1.2465996742248535,
      "y": 0.9813414812088013,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Institutions as socially sanctioned constraints that shape behavior through rights, obligations, and enforcement, enabling predictable social interactions.",
        "Concept 2: The formal–informal spectrum of institutions (formal laws/rules vs. informal norms/conventions) and how this affects enforceability and coordination.",
        "Concept 3: Institutions influence economic performance by altering transaction and production costs through their rules and incentive structures."
      ],
      "original_question_hash": "4ee5fa41"
    },
    {
      "question": "According to Talcott Parsons, how does the \"sick role\" function as a social mechanism to regulate how illness is experienced and managed within society?",
      "options": {
        "A": "It frames illness primarily as a biological condition with no social expectations, emphasizing medical treatment alone.",
        "B": "It defines a temporary social status that grants rights (e.g., exemption from normal duties) and imposes obligations (e.g., desire to recover and to seek and follow medical advice), coordinating expectations among patients, physicians, and society.",
        "C": "It advises keeping illness hidden to avoid disturbing social order and to prevent stigma.",
        "D": "It requires that an ill person permanently withdraw from social roles, making illness a lasting social identity."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduate audience; preserved Parsons' core idea of a temporary social status with specific rights and obligations and its coordinating role among patient, doctor, and society; removed broader historical and methodological details.",
      "content_preserved": true,
      "source_article": "Sociology of health and illness",
      "x": 1.2376519441604614,
      "y": 0.9483994841575623,
      "level": 2,
      "concepts_tested": [
        "The social construction of health and illness and critique of the purely biomedical model, including the biopsychosocial model.",
        "Parsons' sick role as a social mechanism with rights and obligations guiding how illness is experienced and managed within society.",
        "Health in relation to social context across the life-course, emphasizing social determinants and the interaction of cultural, economic, political, and environmental factors with wellbeing."
      ],
      "original_question_hash": "f7251be8"
    },
    {
      "question": "Why might a clinically effective intervention take a long time to become accepted as standard care, and through what social mechanism do social structures delay or speed its adoption?",
      "options": {
        "A": "Because empirical effects can be ambiguous or context-dependent, so evidence alone does not immediately resolve questions of validity.",
        "B": "Because professional interests, power relations, and funding and publishing incentives shape which findings are validated, disseminated, taught, and funded; this gatekeeping by institutions and professions can accelerate or slow adoption.",
        "C": "Because patient demand and popular preferences determine what becomes standard care regardless of the scientific evidence.",
        "D": "Because randomized controlled trials (RCTs) are treated as the definitive evidence and the time required to run and publish RCTs delays adoption into standard practice."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem for clarity and concision; introduced the term 'gatekeeping' to label the mechanism; shortened options while keeping them plausible and tied to concepts from the article (professional power, evidence ambiguity, patient demand, RCT timing).",
      "content_preserved": true,
      "source_article": "Medical sociology",
      "x": 1.2273181676864624,
      "y": 0.9715525507926941,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Social determinants of health and access to care — how class, race/ethnicity, immigration status, gender/sex, sexuality, and age shape disparities in health and medical resources.",
        "Concept 2: Social production of medical knowledge — how knowledge in medicine is shaped by social structures, professional interests, and theoretical frameworks (e.g., conflict theory, sociology of knowledge) and how this can influence what is accepted as valid.",
        "Concept 3: Social organization and interactions in medical practice — how doctor–patient interactions, professional roles, and the broader social context (including public health, ethics, and policy) influence medical practice and its effects."
      ],
      "original_question_hash": "0c19754f"
    },
    {
      "question": "Why do goal-setting, staged progress (incremental steps), and feedback together support sustained personal development across the lifespan?",
      "options": {
        "A": "They force behavior into an inflexible plan that cannot be adapted.",
        "B": "They create a feedback-driven loop: goals give direction, incremental steps make change manageable, and feedback guides adjustments, enabling ongoing adaptive change.",
        "C": "They mainly boost short-term motivation but do not alter long-term habits or trajectories.",
        "D": "They eliminate the need for self-reflection and personal accountability in development."
      },
      "correct_answer": "B",
      "simplification_notes": "Rephrased the original question into clear academic language for undergraduates; retained emphasis on goal-setting, staged progress, and feedback as mechanisms for sustained change; shortened wording and made options concise and plausible.",
      "content_preserved": true,
      "source_article": "Personal development",
      "x": 1.248103380203247,
      "y": 0.9974703788757324,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Personal development as a lifelong process that relies on mechanisms like goal setting, staged progress, and feedback to support sustained change.",
        "Concept 2: The distinction between personal development and personal growth (different scope: \"what\" evolves vs. broader morals/values).",
        "Concept 3: The role of organizational contexts (tools, programs, assessment) and professional roles (teacher/mentor) in facilitating personal development, illustrating relationships between individuals and institutions."
      ],
      "original_question_hash": "de00a810"
    },
    {
      "question": "Why does Maslow describe self-actualization as intrinsic growth rather than the pursuit of a fixed, externally specified goal?",
      "options": {
        "A": "Because it is driven by externally defined benchmarks of success rather than by internal development.",
        "B": "Because it reflects the organism's internal tendency to actualize latent capacities, so growth unfolds according to the individual's potential rather than toward a pre-set endpoint.",
        "C": "Because it occurs only after all lower-order deficiency needs are satisfied, making it a later stage in need fulfillment.",
        "D": "Because it is primarily a social construct shaped by cultural norms and expectations."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem to undergraduate academic language, clarified that the contrast is between intrinsic development and fixed external goals, and made all distractors plausible while keeping the original correct choice.",
      "content_preserved": true,
      "source_article": "Self-actualization",
      "x": 1.2383066415786743,
      "y": 1.0113894939422607,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Growth-motivated actualization and prerequisite needs — self-actualization is described as growth-motivated rather than deficiency-motivated, and typically requires lower-order needs to be satisfied beforehand.",
        "Concept 2: Realizing potential as intrinsic growth — self-actualization involves becoming more of what one is capable of (intrinsic growth of the organism) rather than pursuing a fixed content or goal.",
        "Concept 3: Conceptual relationships and historical interpretations — contrasts between Maslow and Goldstein, the extension to self-transcendence, and the idea of “psychopathology of normality” explaining why self-actualization is rare."
      ],
      "original_question_hash": "814cea60"
    },
    {
      "question": "How do data governance, formal standards (e.g., ISO 8000), and data cleansing jointly define and improve data quality, particularly through standardization?",
      "options": {
        "A": "By adding more data sources and increasing data volume; broader coverage is assumed to make the data higher quality through sheer breadth.",
        "B": "By creating and enforcing agreed definitions and standards across data assets, and applying cleansing to standardize formats and correct inconsistencies so data aligns with a defined target state and is fit for use.",
        "C": "By locking all data into a single fixed schema so no variation is permitted, thereby eliminating the need for ongoing cleansing or governance.",
        "D": "By concentrating solely on measuring data quality metrics and assuming that producing metrics alone will automatically drive improvements without changing data integration or cleansing processes."
      },
      "correct_answer": "B",
      "simplification_notes": "Question shortened and clarified; referenced standards (ISO 8000) as an example; preserved core idea that governance, standards and cleansing create agreed definitions and standardize/correct data to improve fitness for use. Distractors kept plausible.",
      "content_preserved": true,
      "source_article": "Data quality",
      "x": 1.4349770545959473,
      "y": 1.0483304262161255,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Fitness for use as the defining criterion for data quality, varying by consumer, business, and standards perspectives.",
        "Concept 2: Data governance, standards, and data cleansing as mechanisms to establish definitions and improve data quality (including standardization).",
        "Concept 3: The relationship between the number of data sources and internal data consistency, and the idea of quality dimensions as a framework for evaluating quality."
      ],
      "original_question_hash": "6b4efac2"
    },
    {
      "question": "In survey research, a representative sample has a composition that matches the target population. Why does poor representativeness weaken inference about that population, and how is this problem normally addressed in survey design?",
      "options": {
        "A": "It mainly increases random sampling variance, so the appropriate remedy is simply to increase the sample size.",
        "B": "It introduces selection bias because some subgroups are over- or under-represented, producing biased population estimates; typical mitigation is probability-based sampling and using survey weights or post-stratification to align the sample with known population margins.",
        "C": "It only reduces precision for the mis-sampled subgroups and does not bias overall population estimates; therefore mitigation is unnecessary if weights are applied.",
        "D": "It compromises the questionnaire's validity (what the questions measure); the usual mitigation is to redesign the questionnaire to improve measurement."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the original multi-sentence prompt into one clear question; clarified 'representativeness' and focused on the two-part test (why inference is harmed and how designers mitigate it). Kept technical terms (selection bias, probability sampling, weights, post-stratification) and preserved the original correct answer.",
      "content_preserved": true,
      "source_article": "Survey methodology",
      "x": 1.3196672201156616,
      "y": 1.0105262994766235,
      "level": 2,
      "concepts_tested": [
        "Representativeness of the sample and its impact on inference about the target population",
        "How survey design choices (sampling, instruments, data collection) relate to types of survey errors (systematic vs. random)",
        "The trade-off between cost constraints and achieving data quality in survey research"
      ],
      "original_question_hash": "8f622024"
    },
    {
      "question": "When planning information‑gathering, why should you distinguish market research from marketing research, and how does that distinction change the kinds of data you collect and the decisions the research supports?",
      "options": {
        "A": "Market research focuses on the market—size, segments, needs and distribution—to inform strategic decisions (e.g., market entry, product–market fit); marketing research focuses on marketing processes (advertising, pricing, salesforce effectiveness) to optimize campaigns and messaging, so each drives different data collection and decision use.",
        "B": "They are the same activity, so the distinction does not affect what data you collect or which decisions the research supports.",
        "C": "Market research collects only quantitative data while marketing research collects only qualitative data, and this strict split explains why they are used for different purposes.",
        "D": "Marketing research is mainly about supply‑chain and operational problems, whereas market research is only about consumer psychology, so they address entirely separate concerns with no overlap."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the original question into a concise undergraduate level prompt; clarified the core contrast as strategic (market) versus tactical/process (marketing) and summarized how that affects data types and decision targets. Options were made plausible but only A preserves the article's distinction.",
      "content_preserved": true,
      "source_article": "Market research",
      "x": 1.3398877382278442,
      "y": 0.9843630790710449,
      "level": 2,
      "concepts_tested": [
        "Market research as a systematic tool for informing business strategy and decision making by identifying market needs and opportunities",
        "The relationship and distinction between market research and marketing research (scope, purpose, and overlap)",
        "Methodological duality: qualitative vs. quantitative techniques and primary vs. secondary data as mechanisms for gathering and interpreting information to gain insights"
      ],
      "original_question_hash": "dc96ca83"
    },
    {
      "question": "In a media planning process, what role does the evaluation and follow-up phase play in keeping the plan aligned with the target market?",
      "options": {
        "A": "It confirms the original target market is unchanged and that no new audiences exist, so the initial plan remains untouched.",
        "B": "It gathers performance data and audience feedback, then uses that evidence to revise objectives, media choices, scheduling or budget so the campaign stays aligned with an evolving target market and goals.",
        "C": "It concentrates only on reducing media costs, ignoring shifts in audience behavior to maintain budget discipline.",
        "D": "It locks the media plan after launch, preventing any post-launch changes to the media mix or strategy."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording shortened and clarified to focus on the evaluation phase’s corrective role; removed lengthy background examples while keeping the idea that evaluation informs adjustments to objectives, media selection, scheduling and budget.",
      "content_preserved": true,
      "source_article": "Media planning",
      "x": 1.3380120992660522,
      "y": 0.9848076105117798,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The media planning process is a structured, stage-based framework (market analysis -> objectives -> strategy/implementation -> evaluation) that aims to synchronize contributing elements to convey a message.",
        "Concept 2: Budget setting and allocation (including zero-based budgeting or historical budgeting) shape which media can be used and how resources are distributed to achieve objectives.",
        "Concept 3: Alignment between target market analysis and media objectives/strategy, with an evaluation/follow-up loop that informs adjustments to the plan."
      ],
      "original_question_hash": "9600ffe3"
    },
    {
      "question": "How does the mutual interaction between natural environments and human activities produce a cultural landscape?",
      "options": {
        "A": "The landscape is merely a backdrop for visible cultural symbols; the physical environment has little influence on people's practices.",
        "B": "The physical environment fully determines cultural practices, leaving no space for human alteration of landscapes.",
        "C": "Human activities (for example, agriculture, settlement, industry) transform the environment, and those transformed landscapes in turn reshape later cultural practices and social organization, creating a continuous feedback loop.",
        "D": "Cultural practices alter landscapes in arbitrary ways that have no influence on subsequent practices or the organization of space."
      },
      "correct_answer": "C",
      "simplification_notes": "Rephrased the stem to undergraduate level, added concrete examples (agriculture, settlement, industry), tightened option wording for clarity while preserving plausibility and the original correct choice.",
      "content_preserved": true,
      "source_article": "Cultural geography",
      "x": 1.2519344091415405,
      "y": 0.9695373177528381,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The cultural landscape as the product of reciprocal influence between natural environments and human activity (humans shape landscapes and landscapes shape cultures).",
        "Concept 2: The methodological shift from environmental determinism to culturally informed explanations, including interdisciplinarity and concepts like sense of place.",
        "Concept 3: The non-monolithic nature of culture (no single definition) and the rejection of reductionist views, reflecting varied interpretations and practices across regions."
      ],
      "original_question_hash": "0643ffc5"
    },
    {
      "question": "When an imperial power does not settle large numbers of its own people in a territory, how can it still maintain a centre–periphery hierarchy and keep distant regions under control?",
      "options": {
        "A": "By using direct military conquest and mass settlement of the periphery by settlers from the centre to establish permanent governance.",
        "B": "By embedding the periphery in unequal economic and political networks — for example trade dependence, debt and investment control, plus diplomatic or military alignment — letting the metropolitan centre dominate without large-scale settlement.",
        "C": "By granting formal independence but preserving influence through ceremonial ties, symbolic agreements, and non‑binding arrangements that limit real autonomy.",
        "D": "By relying solely on cultural and diplomatic soft power to manufacture consent, removing the need for economic or military levers."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified the question for undergraduates, replaced long passages with a concise prompt about centre–periphery control without settlers, and tightened each option while keeping the original meanings; correct answer letter preserved.",
      "content_preserved": true,
      "source_article": "Imperialism",
      "x": 0.6716118454933167,
      "y": 0.5606006979942322,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Imperialism as a power-projection system using hard power (military/economic) and soft power (diplomatic/cultural) to establish or maintain hegemony.",
        "Concept 2: Center/periphery dynamic and the distinction from colonialism, focusing on domination from a metropolitan center over distant territories (potentially without settlement).",
        "Concept 3: The breadth and debates around imperialism (neocolonialism, expansion for expansion, applicability to non-territorial domains) illustrating how definitions and frameworks shape understanding."
      ],
      "original_question_hash": "7fa22602"
    },
    {
      "question": "In an experiment researchers randomly assign units to a treatment or control group and then compare outcomes. Why does random assignment support stronger causal inference than an observational study that compares pre-existing groups?",
      "options": {
        "A": "It ensures the sample is representative of the target population, so observed differences reflect population effects.",
        "B": "Random assignment, on average, balances both observed and unobserved confounding factors between groups, so differences in outcomes can be attributed to the treatment.",
        "C": "It guarantees that the results will generalize to every other setting and population.",
        "D": "It eliminates all sources of random variation in outcome measurements, leaving only the treatment effect."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was shortened and clarified for undergraduate readers; technical idea of randomization balancing confounders was preserved; distractors made concise and plausible.",
      "content_preserved": true,
      "source_article": "Statistics",
      "x": 1.5612690448760986,
      "y": 1.1366366147994995,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The rationale for descriptive versus inferential statistics and why representative sampling enables generalization from a sample to a population.",
        "Concept 2: The difference between experimental and observational studies and how experimental manipulation (and design) affects causal inference.",
        "Concept 3: The hypothesis testing framework (null vs alternative) and the meaning/significance of Type I and Type II errors in drawing conclusions from data."
      ],
      "original_question_hash": "2eaea9f6"
    },
    {
      "question": "Predictive models estimate likely outcomes. Why does prescriptive analytics—which pairs prediction with optimization—typically provide more actionable guidance for decisions than using predictive models alone?",
      "options": {
        "A": "Because predictive models forecast outcomes for alternative actions, and prescriptive analytics combines those forecasts with constraints and an objective to compute the action or policy that maximizes expected value.",
        "B": "Because prescriptive analytics removes the need for predictive models by hard-coding a single set of optimal decisions into the system.",
        "C": "Because optimization alone can produce reliable decisions without predictive input, by simply using historical averages instead of forecasts.",
        "D": "Because prescriptive analytics guarantees perfect prediction of future outcomes, eliminating uncertainty and making decisions trivial."
      },
      "correct_answer": "A",
      "simplification_notes": "Rephrased the stem to state the predictive role clearly and emphasized that prescriptive analytics adds optimization under constraints to produce actions; wrong options were kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Business analytics",
      "x": 1.3681505918502808,
      "y": 0.9983800053596497,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Distinction between business analytics and business intelligence (description vs. prediction/prescription) and the implications for decision making.",
        "Concept 2: The role of analytical modeling (explanatory/predictive/prescriptive) in BA and its connection to decision making and optimization.",
        "Concept 3: The dual role of BA in decision processes (as input to human decisions vs. driving automated decisions) and the idea of decision support versus automated action."
      ],
      "original_question_hash": "afa25c1a"
    },
    {
      "question": "Why, in stakeholder theory, does salience come from the interaction of power, legitimacy, and urgency rather than from any single attribute, and what follows for managers?",
      "options": {
        "A": "Because managers face bounded rationality and must allocate limited attention across many claimants; a stakeholder is most salient when power, legitimacy, and urgency combine, which helps managers decide which concerns to address first.",
        "B": "Because any one of the attributes alone guarantees salience; if a stakeholder has power, legitimacy, or urgency, they will always be prioritized equally.",
        "C": "Because only legitimacy determines salience; power and urgency only affect long-term strategy and do not draw immediate managerial attention.",
        "D": "Because salience is purely normative and independent of empirical attributes; managers should decide whom to treat as stakeholders solely based on values, not on power, legitimacy, or urgency."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified; technical terms (power, legitimacy, urgency, bounded rationality, salience) retained; removed extended historical context and references; kept the core idea that salience arises from attribute interaction and implies prioritized managerial attention.",
      "content_preserved": true,
      "source_article": "Stakeholder theory",
      "x": 1.339457631111145,
      "y": 0.9714767336845398,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Integration of stakeholder theory with resource-based and market-based views plus a socio-political layer to inform strategy.",
        "Concept 2: Normative vs. descriptive dimensions — identifying who counts as stakeholders and understanding how/when managers treat them as stakeholders (salience).",
        "Concept 3: The principle that stakeholders’ needs should be prioritized in organizational action, challenging traditional, shareholder-centric analysis frameworks and aligning with CSR and social contract ideas."
      ],
      "original_question_hash": "5cef9d10"
    },
    {
      "question": "According to Porter's competitive strategy framework, why are trade-offs (deciding what not to do) necessary, and how does \"fit\" among a firm's activities help sustain its competitive position?",
      "options": {
        "A": "Trade-offs force a firm to focus on a single best practice, and \"fit\" means activities are standardized and tightly controlled to minimize deviation.",
        "B": "Trade-offs prevent the firm from trying to be all things to all customers by limiting which activities it pursues, and \"fit\" links activities so they mutually reinforce the chosen position, making the strategy harder for rivals to imitate.",
        "C": "Trade-offs reduce the need for consistent resource commitments by permitting ad-hoc choices, and \"fit\" means activities can be performed independently without coordination.",
        "D": "Trade-offs are primarily about lowering costs across the board, and \"fit\" ensures each activity simply reaches industry-average performance."
      },
      "correct_answer": "B",
      "simplification_notes": "Question shortened and rephrased for clarity; Porter's terms (trade-offs, fit) were defined briefly; options rewritten to be concise and plausible while preserving the tested distinctions.",
      "content_preserved": true,
      "source_article": "Strategic management",
      "x": 1.3449594974517822,
      "y": 0.9839823842048645,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Strategy as a dynamic, integrated process (formulation, implementation, resource allocation) with feedback loops that adapt to internal and external environments.",
        "Concept 2: Porter's core mechanisms for competitive strategy—creating a unique/valuable position, making trade-offs (what not to do), and achieving fit among activities to reinforce the chosen strategy.",
        "Concept 3: The relationship between corporate strategy and business strategy, and how strategic management relates to operational management (scope and focus distinctions)."
      ],
      "original_question_hash": "2c62737b"
    },
    {
      "question": "Under bounded rationality, decision makers have limited cognitive capacity and incomplete information. How do these constraints typically affect how choices are reached?",
      "options": {
        "A": "They trigger an exhaustive search of all possible options until the true optimum is found.",
        "B": "They encourage simple heuristics and satisficing; people take shortcuts and settle for “good enough” choices rather than the global optimum.",
        "C": "They cause individuals to ignore probabilities and risks, leading to essentially random selection among options.",
        "D": "They guarantee that agents will behave as if perfectly rational and always choose the utility-maximizing option."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was shortened and focused on the causal link between cognitive limits/incomplete information and decision processes. Technical terms (bounded rationality, heuristics, satisficing) were retained and clarified; distractors were made plausibly similar.",
      "content_preserved": true,
      "source_article": "Behavioral economics",
      "x": 1.274804711341858,
      "y": 0.9976612329483032,
      "level": 2,
      "concepts_tested": [
        "Bounded rationality and deviation from homo economicus: why real decision-making diverges from purely rational models and how cognitive/ informational limits shape choices.",
        "Integration of psychology/neuroscience into economic modeling: how psychological insights alter predictions and provide mechanisms for decision-making processes within economic theory.",
        "Loss aversion and behavioral biases under risk: how losses loom larger than gains and how such biases drive deviations from traditional expected utility theory."
      ],
      "original_question_hash": "075866c3"
    },
    {
      "question": "In cognitive models of attention and memory, why is the assumption of a finite processing bottleneck necessary to explain observed performance limits when people try to multitask?",
      "options": {
        "A": "Because it permits unlimited parallel processing across tasks, so the models predict no interference or performance trade-offs.",
        "B": "Because it assumes a shared, limited processing resource must be allocated across tasks, producing interference and predictable trade-offs in speed and accuracy.",
        "C": "Because it asserts memory retrieval is perfect and instantaneous, removing any need for processing constraints to explain limits.",
        "D": "Because it claims cognitive operations are fully modular and occur independently, so multiple tasks should proceed without interference."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was shortened and clarified for undergraduates; historical background and peripheral details were removed. Technical idea preserved: models require a limited shared processing resource (bottleneck) to explain multitasking interference.",
      "content_preserved": true,
      "source_article": "Cognitive psychology",
      "x": 1.2960718870162964,
      "y": 1.0278383493423462,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The cognitive revolution—the move from behaviorist focus on observable behavior to studying unobservable mental processes using models to explain and predict behavior.",
        "Concept 2: Cognitive modeling of mental processes—the idea that attention, memory, language, and problem solving can be explained and understood through theoretical models of mental processing.",
        "Concept 3: Localization of language in the brain—Broca’s and Wernicke’s areas linking neural substrates to language production and comprehension, with aphasia illustrating the cognitive-behavioral-neural relationship."
      ],
      "original_question_hash": "e6bbec45"
    },
    {
      "question": "In the logistic population-growth model, the per-capita growth rate declines as population size $N$ approaches the carrying capacity $K$. Why does the per-capita growth slow, and what does this tell us about the role of $K$ in limiting population dynamics?",
      "options": {
        "A": "Because birth rates increase with density through social effects, causing overshooting and fluctuations around $K$.",
        "B": "Because immigration grows with $N$, driving the population toward $K$ regardless of resource availability.",
        "C": "Because higher density increases competition for limited resources, lowering fecundity and/or raising mortality so per-capita growth falls as $N\\to K$; $K$ functions as the environmental ceiling on population size.",
        "D": "Because the carrying capacity $K$ is a fixed constant that ensures continued, unlimited growth once the population reaches it."
      },
      "correct_answer": "C",
      "simplification_notes": "Condensed the original stem to a direct question about the logistic model; kept mathematical symbols $N$ and $K$ in inline LaTeX; shortened distractors and preserved biological mechanisms (density dependence, immigration, social effects, misinterpretations of K).",
      "content_preserved": true,
      "source_article": "Population ecology",
      "x": 1.7009392976760864,
      "y": 1.0657122135162354,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Population growth is driven by demographic rates (births, deaths, immigration, emigration) and the intrinsic rate of increase, shaping changes in population size.",
        "Concept 2: Carrying capacity and density dependence constrain populations; classic models relate intrinsic growth rate (r), carrying capacity (K), and initial size (N0) to describe how populations approach limits.",
        "Concept 3: The practical application of population ecology to conservation through population viability analysis, linking theoretical dynamics to predictions of long-term persistence in a habitat."
      ],
      "original_question_hash": "da92a7dd"
    },
    {
      "question": "Why do the strength and direction of the relationship between landscape pattern (e.g., patch size, edge density) and an ecological process (e.g., species dispersal) typically change when examined at different spatial scales?",
      "options": {
        "A": "Larger patches always increase dispersal for all species, so the pattern–process relationship simply strengthens as spatial scale increases.",
        "B": "Because ecological processes integrate effects of multiple patches differently at different scales; a pattern observed at one scale can obscure or reverse causal links since scale determines which patches contribute via connectivity, edge effects, or source–sink dynamics.",
        "C": "Landscape pattern is independent of ecological processes and only abiotic factors matter, so changing scale does not alter their relationship.",
        "D": "Processes operate identically across scales due to universal ecological laws, so scale has negligible influence on pattern–process coupling."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording condensed and clarified for undergraduate readers; retained examples (patch size, edge density, species dispersal); removed extraneous background, tightened options to be concise but still plausible.",
      "content_preserved": true,
      "source_article": "Landscape ecology",
      "x": 1.5444141626358032,
      "y": 0.9709839224815369,
      "level": 2,
      "concepts_tested": [
        "Pattern–process–scale coupling: how the spatial arrangement of landscape elements (pattern) affects ecological processes and how these relationships change across different spatial/temporal scales.",
        "Landscape heterogeneity and ecological outcomes: how spatial diversity and patch structure influence organism abundance, behavior, and overall landscape functioning.",
        "Human–environment coupling in landscape ecology: how land use/land cover changes and socioeconomic factors interact with biophysical processes to shape landscape diversity, sustainability, and potential disease dynamics."
      ],
      "original_question_hash": "fbee71ce"
    },
    {
      "question": "When the same underlying hazard and risk are described in different formats — a qualitative label (low/medium/high), an absolute probability (e.g., $2\\%$ per year), or a relative risk (e.g., $2\\times$ higher) — why can these different presentations lead to different decisions about acceptability and mitigation?",
      "options": {
        "A": "Because qualitative labels provide no consistent basis for comparing risks across contexts, so they are unreliable for guiding decisions.",
        "B": "Because the formats highlight different information (absolute magnitude, baseline risk, time horizon, or change relative to a comparator), which shifts perceived urgency, acceptability, and thus which mitigation actions are chosen.",
        "C": "Because absolute probabilities always cause more conservative (risk-averse) decisions than relative-risk statements.",
        "D": "Because expressing risk numerically removes all uncertainty, so the presentation format should no longer influence decisions."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened for clarity; examples converted to inline LaTeX ($2\\%$, $2\\times$); options made concise and retained plausibility. Core concept that format affects perception and choices was preserved.",
      "content_preserved": true,
      "source_article": "Risk assessment",
      "x": 1.5853919982910156,
      "y": 1.1081820726394653,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The risk assessment framework and its sequence (hazard identification → likelihood/consequences estimation → risk evaluation → mitigation; hazard analysis as the first stage; risk management as the broader framework).",
        "Concept 2: Outputs and decision-making (qualitative vs. quantitative risk outputs and how the way statistics are expressed affects interpretation and decisions).",
        "Concept 3: System-level risk and complexity (distinction between individual risk assessment and systems risk assessment, including linear vs. nonlinear systems and how complexity affects predictability)."
      ],
      "original_question_hash": "c26e06af"
    },
    {
      "question": "Why can a pollution-pricing policy (e.g., a carbon tax) produce broader environmental benefits than a policy that directly regulates emissions from a single source?",
      "options": {
        "A": "Because a tax sets a fixed emissions cap across all sectors and thus guarantees identical emission outcomes everywhere.",
        "B": "Because price signals motivate many actors to find the most cost‑effective emissions reductions across multiple sources, yielding spillover environmental gains beyond the one regulated source.",
        "C": "Because taxes remove the need for monitoring and enforcement by relying on voluntary compliance from firms and consumers.",
        "D": "Because regulating a single source automatically forces all other sources to adopt the same technology, spreading benefits uniformly."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified (\"price-based\" → \"pollution-pricing\"), retained the contrast between price instruments and single-source regulation, and made all four options plausible while preserving the original correct option (B).",
      "content_preserved": true,
      "source_article": "Environmental policy",
      "x": 1.373854160308838,
      "y": 0.8689056038856506,
      "level": 2,
      "concepts_tested": [
        "Interconnectedness of environmental problems and the need for holistic policy approaches (spillover effects across issues)",
        "Mechanisms by which policy influences human activity to prevent environmental harm (cause-effect relationship between policy instruments and environmental outcomes)",
        "The environment–policy nexus as an integrated framework (environmental, social, and economic dimensions informing policy design)"
      ],
      "original_question_hash": "0541b723"
    },
    {
      "question": "Which statement best summarizes how stage-based psychosocial theories (e.g., Erikson) differ from dynamic-systems approaches in explaining a major identity-related transition in adolescence, and what this implies for predicting individual developmental paths?",
      "options": {
        "A": "Stage-based theories posit a sequence of relatively fixed, age-linked crises that produce discrete shifts in identity and therefore relatively predictable trajectories; dynamic-systems approaches treat identity as emerging from continuous, context-sensitive interactions among biology, cognition, and environment, yielding nonlinear and highly variable individual paths.",
        "B": "Stage-based theories claim identity change is continuous and uniform across individuals, while dynamic-systems approaches claim identity change occurs abruptly at fixed ages.",
        "C": "Stage-based theories argue the environment alone determines identity development, whereas dynamic-systems approaches argue genes alone determine identity development.",
        "D": "Stage-based theories assume no identity transition occurs during adolescence, and dynamic-systems approaches assume identity is permanently fixed and never changes."
      },
      "correct_answer": "A",
      "simplification_notes": "Condensed the original prompt, clarified terms (stage-based psychosocial vs dynamic systems), removed extended examples, and stated implications for predictability more directly while keeping academic terminology.",
      "content_preserved": true,
      "source_article": "Developmental psychology",
      "x": 1.2794612646102905,
      "y": 1.0128989219665527,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Nature vs. nurture and biopsychosocial interactions as the engine of development (how genes, environment, and personal characteristics interact to shape changes over time)",
        "Concept 2: Interdependence of the three developmental dimensions (physical, cognitive, social-emotional) and how changes in one dimension influence others across the lifespan",
        "Concept 3: Theoretical frameworks for development (stage-based psychosocial theories like Erikson vs. dynamic systems/alternative models) and how they account for change and transitions"
      ],
      "original_question_hash": "bdd32eb5"
    },
    {
      "question": "How do a parent's own history and psychopathology primarily influence a child's development via parental sensitivity?",
      "options": {
        "A": "They alter the child's genetic predispositions so that parental responsiveness no longer affects developmental outcomes.",
        "B": "They reduce the parent's emotional availability and timely responsiveness to the child's cues, lowering parental sensitivity; this undermines attachment quality and thereby shapes the child's subsequent development.",
        "C": "They increase parental control and strictness to such a degree that parental sensitivity becomes irrelevant to the child's developmental trajectory.",
        "D": "They determine child outcomes mainly through broader environmental factors outside the parent–child relationship, making parental sensitivity a negligible mediator."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the stem to focus on the mediation pathway, clarified language about emotional availability, responsiveness, attachment, and developmental outcomes; removed unrelated background details.",
      "content_preserved": true,
      "source_article": "Parenting",
      "x": 1.2529520988464355,
      "y": 0.9927597641944885,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The four parenting styles (authoritative, authoritarian, permissive, uninvolved) are defined by combinations of acceptance/responsiveness and demand/control, and these styles relate to different child mental health and well-being outcomes.",
        "Concept 2: Parental history and psychopathology influence parental sensitivity, which mediates child outcomes (attachment quality and subsequent development).",
        "Concept 3: Warm adoptive parenting is associated with reductions in internalizing and externalizing problems over time for adopted children, illustrating how parenting climate affects long-term child outcomes."
      ],
      "original_question_hash": "f727cfd2"
    },
    {
      "question": "How does adding systematic assessment of economic and social conditions to land-use planning change how planners select and compare options, compared with using only physical land potential?",
      "options": {
        "A": "It causes decisions to focus only on maximizing physical land use, while ignoring market forces and community needs.",
        "B": "It makes trade-offs between efficiency, equity, and feasibility explicit, so options are screened to align with economic realities and social goals before informing the comprehensive plan.",
        "C": "It mainly slows down decision-making but does not change which option is ultimately chosen.",
        "D": "It guarantees the cheapest option will always be selected because economic criteria dominate the choice."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the contrast between assessing only physical potential versus including economic/social conditions; retained the idea of trade-offs and alignment with comprehensive planning.",
      "content_preserved": true,
      "source_article": "Land-use planning",
      "x": 1.3543610572814941,
      "y": 0.8264645338058472,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Regulation and zoning as mechanisms to influence land-use behavior to achieve environmental, social, and efficiency goals.",
        "Concept 2: Systematic planning process (assessing land/water potential, alternatives, economic/social conditions) to select the best land-use options and inform a comprehensive plan.",
        "Concept 3: Context-dependency of planning outcomes—the idea that the benefits of land-use regulation depend on location and the specific regulations in place."
      ],
      "original_question_hash": "648626b1"
    },
    {
      "question": "How does an urban growth boundary (UGB) implement smart growth by changing development incentives and land-use outcomes?",
      "options": {
        "A": "It limits where new development can occur by setting a boundary, making infill and higher-density projects inside the boundary relatively more attractive and better aligned with transit and existing infrastructure.",
        "B": "It automatically raises allowable zoning densities everywhere inside the boundary, regardless of local zoning rules or market demand.",
        "C": "It redirects growth to greenfield sites just outside the boundary, encouraging outward expansion to avoid restrictions.",
        "D": "It removes public infrastructure planning inside the boundary, leaving infrastructure provision entirely to private market decisions."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording tightened for clarity; removed extended background and focused on how a UGB changes incentives and land-use patterns. Distractors made plausible by reflecting common misunderstandings about UGB effects.",
      "content_preserved": true,
      "source_article": "Smart growth",
      "x": 1.3756908178329468,
      "y": 0.8625004291534424,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Compact, walkable, transit-oriented, mixed-use development as a core principle to reduce sprawl and expand transportation/housing options.",
        "Concept 2: Infill and redevelopment preferred over greenfield development to protect natural resources and leverage existing infrastructure.",
        "Concept 3: Regulatory tools (e.g., urban growth boundaries) as mechanisms to implement smart growth principles and shape development outcomes."
      ],
      "original_question_hash": "d9404d75"
    },
    {
      "question": "Why does using semantic structure and explicit labels (e.g., meaningful headings, labeled form fields, accessible controls) matter both for direct (unassisted) access and for indirect access via assistive technology?",
      "options": {
        "A": "It only makes the visual layout more consistent across devices and has no effect on assistive technologies.",
        "B": "It lets assistive technologies reinterpret content but adds complexity that can hinder unassisted users' ability to use the interface.",
        "C": "It creates a shared, well-structured content model that helps unassisted users navigate the interface and allows assistive technologies to expose equivalent information and controls to users who rely on them.",
        "D": "It only benefits people with disabilities and is unnecessary for most other users."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording was shortened and clarified while retaining technical terms (semantic structure, explicit labels, assistive technology, direct/indirect access). Examples (headings, labeled fields, controls) were kept. Distractor options were rewritten to be plausible but preserve the original correct choice C.",
      "content_preserved": true,
      "source_article": "Accessibility",
      "x": 1.3775540590286255,
      "y": 1.0428146123886108,
      "level": 2,
      "concepts_tested": [
        "Direct vs. indirect access and the role of assistive technology in accessibility (how design enables unassisted use and compatibility with assistive tech)",
        "The relationship and distinction between accessibility and usability (why they are not the same and how they complement each other)",
        "Universal design vs. accessible design (how these approaches differ philosophically and in practice, and how they aim to maximize usable access for broad user groups)"
      ],
      "original_question_hash": "e0ee7c47"
    },
    {
      "question": "How does the choice of resource-allocation mechanism affect whether a Pareto-efficient outcome can be achieved?",
      "options": {
        "A": "The mechanism shapes which final allocations are feasible and what information and incentives participants face; some Pareto-efficient allocations may be unreachable or unstable under certain rules, while other mechanisms can steer behavior toward them.",
        "B": "Pareto efficiency depends only on initial resource endowments; the allocation mechanism does not matter because any efficient outcome can always be reached.",
        "C": "Allocation mechanisms only determine who receives which resources, not whether an allocation is Pareto-efficient; they cannot change whether someone can be made better off without harming another.",
        "D": "An allocation is Pareto-efficient only if it maximizes the total sum of individual welfare, so efficiency is independent of the mechanism and mechanisms cannot influence efficiency."
      },
      "correct_answer": "A",
      "simplification_notes": "Rephrased the original question into a concise undergraduate-level sentence, simplified wording and sentence structure, removed extended examples while keeping the core idea that mechanisms affect feasibility, information, and incentives.",
      "content_preserved": true,
      "source_article": "Resource allocation",
      "x": 1.4265565872192383,
      "y": 1.0322095155715942,
      "level": 2,
      "concepts_tested": [
        "Pareto efficiency as a guiding principle for resource allocation and how it depends on the chosen allocation mechanism.",
        "The contrast and interplay between allocation mechanisms (markets, planning, algorithmic approaches) and their impact on resource distribution and efficiency.",
        "Auction-based allocation and proportional share scheduling as concrete mechanisms, including how bidding or shares influence resource distribution."
      ],
      "original_question_hash": "82fa01b7"
    },
    {
      "question": "Why does earned media generally have higher perceived credibility than paid media, and how does this affect control over where and when the message appears?",
      "options": {
        "A": "Earned media is created internally and placed by the organization with guaranteed distribution, so controlled messaging increases its perceived credibility.",
        "B": "Earned media comes from independent third-party sources and is not paid for, so it acts as a third-party endorsement that boosts credibility; however, the organization has less control over timing and placement, making exposure less predictable. Paid media gives guaranteed placement and timing but is seen as promotional and less credible.",
        "C": "Earned media is more credible solely because it always reaches a larger audience than paid media.",
        "D": "Earned and paid media are equally credible; the only difference is in budgeting and resource allocation."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording made more concise and academic; preserved the causal link that earned media's independence increases credibility while reducing control over placement/exposure; kept all four plausible options and the original correct answer.",
      "content_preserved": true,
      "source_article": "Public relations",
      "x": 1.3140901327133179,
      "y": 0.9809756875038147,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Earned media vs. paid media as PR mechanisms and their impact on credibility and exposure",
        "Concept 2: The relationship between PR, publicity, and journalism, including differing objectives (promotional vs. objective reporting) and how that shapes information flow",
        "Concept 3: Media relations and stakeholder targeting as mechanisms to inform and persuade audiences to maintain a positive organizational perception"
      ],
      "original_question_hash": "8d707f31"
    },
    {
      "question": "According to hermeneutics, how does the hermeneutic circle explain how we come to understand a text, and why does that view make preconceptions both necessary and open to revision?",
      "options": {
        "A": "Understanding is produced by applying explicit textual clues and fixed rules; interpretation advances linearly via universal decoding methods, so prior assumptions are irrelevant.",
        "B": "Understanding develops circularly: we use interpretations of parts to inform our sense of the whole and use the whole to reinterpret the parts; this iterative process requires initial preconceptions but continuously refines and can revise them.",
        "C": "Understanding consists in reconstructing the author's single intended meaning; the interpreter's preconceptions must be suspended and remain unchanged until that intent is recovered.",
        "D": "Interpretation is entirely subjective and arbitrary, so there is no stable meaning to discover; preconceptions neither help guide nor get revised during understanding."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened and made more concise for undergraduate readers; retained the hermeneutic-circle idea that parts and whole inform one another and that preconceptions are necessary starting points but revisable. Distractors were kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Hermeneutics",
      "x": 0.20822978019714355,
      "y": 0.5782473087310791,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3"
      ],
      "original_question_hash": "8c4b9762"
    },
    {
      "question": "Why do environmental humanities reject a strict nature/culture split, and how does that stance shape their approach to knowledge and solving environmental problems?",
      "options": {
        "A": "Because it reveals that environmental problems are entangled with justice, labor, and politics across Western, Eastern, and Indigenous worldviews, prompting methodological synthesis that integrates diverse perspectives.",
        "B": "Because it preserves a clear separation between human culture and natural processes, protecting ecological data from cultural interpretation.",
        "C": "Because it allows ethical questions to be treated separately from empirical conditions, so humanities scholars can address values without examining material realities.",
        "D": "Because it treats Indigenous knowledge as the sole legitimate epistemology, excluding other scientific and cultural sources."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was shortened and clarified for undergraduate readers while retaining technical terms (e.g., entangled, methodological synthesis, Indigenous). Distractors were kept plausible and aligned with original misconceptions. Correct answer letter unchanged.",
      "content_preserved": true,
      "source_article": "Environmental humanities",
      "x": 1.236242651939392,
      "y": 0.9394885897636414,
      "level": 2,
      "concepts_tested": [
        "Interdisciplinary integration: combining environmental sub-disciplines in the humanities to address environmental problems.",
        "Critical relationships to core binaries and worldviews: resisting the nature/culture divide and incorporating justice, labor, politics, and Western/Eastern/Indigenous perspectives.",
        "Methodological synthesis: blending methods from multiple fields to create new ways of thinking about environmental problems."
      ],
      "original_question_hash": "ebac58e0"
    },
    {
      "question": "How does strict adherence to formal rules affect an administrator's capacity to handle cases not anticipated by those rules, and why is this trade-off important for governance?",
      "options": {
        "A": "It increases adaptability because administrators must continually reinterpret rules to fit each new situation.",
        "B": "It promotes consistency and accountability but reduces flexibility, so genuinely novel cases may remain inadequately resolved.",
        "C": "It removes bias by producing identical outcomes for all cases, regardless of differing contexts.",
        "D": "It obviates human discretion altogether, enabling automated procedures to resolve unforeseen problems."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the original complex prompt into a concise undergraduate-level question, kept key terms (formal rules, administrators, governance, trade-off), and made each option a plausible interpretation of the rule-versus-discretion trade-off.",
      "content_preserved": true,
      "source_article": "Bureaucracy",
      "x": 1.2416404485702515,
      "y": 0.9393437504768372,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Autonomy vs accountability in bureaucratic control – why should civil servants be autonomous or directly accountable to political leaders, and how does this trade-off affect governance.",
        "Concept 2: Rule-based procedures vs discretionary judgment – how strict adherence to rules limits or enables responses to cases not foreseen in advance.",
        "Concept 3: Rationalization and efficiency vs individual freedom (Weber’s iron cage) – how a highly organized, rule-governed system can improve efficiency but potentially constrain personal liberty."
      ],
      "original_question_hash": "11a2e97d"
    },
    {
      "question": "Why is nucleation described as a stochastic barrier-crossing process that requires a critical nucleus before a new phase can grow?",
      "options": {
        "A": "Because microscopic fluctuations continually form and dissolve small clusters; most collapse because creating an interface costs free energy, but occasionally a fluctuation produces a cluster larger than the critical size where bulk free-energy gain exceeds the surface cost (the barrier $\\Delta G^{*}$), so growth becomes favorable. The nucleation rate is set by the probability of these rare barrier-crossing events (≈ $e^{-\\Delta G^{*}/k_{B}T}$).",
        "B": "Because nucleation is determined solely by the magnitude of the thermodynamic driving force (e.g., supersaturation or supercooling) and proceeds deterministically, independent of fluctuations.",
        "C": "Because every cluster either dissolves or grows deterministically as soon as it appears, so there is no stochastic barrier-crossing involved in nucleation.",
        "D": "Because nucleation only happens at surfaces or impurities, and those fixed sites make the outcome deterministic rather than stochastic."
      },
      "correct_answer": "A",
      "simplification_notes": "Reduced wording to clear undergraduate language; explicitly named the free-energy barrier $\\Delta G^{*}$ and the exponential dependence of the rate; kept contrasts with heterogeneous nucleation and the role of the driving force.",
      "content_preserved": true,
      "source_article": "Nucleation",
      "x": 1.8238525390625,
      "y": 1.0328969955444336,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Nucleation as a stochastic, barrier-crossing process that requires a critical nucleus to trigger a phase transition.",
        "Concept 2: The distinction between heterogeneous and homogeneous nucleation and how surfaces/impurities affect nucleation rates.",
        "Concept 3: The relationship between driving forces (temperature/supercooling or supersaturation) and nucleation rates, and the role/limitations of classical nucleation theory in predicting these rates."
      ],
      "original_question_hash": "bb49a0ee"
    },
    {
      "question": "Why, at a phase transition point, do the two competing phases have equal Gibbs free energies $G$ (i.e. $G_{\\text{phase 1}} = G_{\\text{phase 2}}$), and what does that equality imply about the system's equilibrium and its sensitivity to small changes in temperature or pressure?",
      "options": {
        "A": "Because the transition point is defined by $G_{\\text{phase 1}}=G_{\\text{phase 2}}$, so there is no net thermodynamic driving force to convert one phase into the other; both phases can coexist in equilibrium, and arbitrarily small changes in temperature or pressure will favor the phase whose $G$ becomes slightly lower.",
        "B": "Because entropy must be maximal at the transition, which forces identical microscopic configurations and equal probabilities of finding the system in either phase.",
        "C": "Because at the transition all kinetic barriers disappear, so the system rapidly and randomly switches between phases with equal likelihood regardless of small changes in external conditions.",
        "D": "Because the transition is a purely geometric or structural rearrangement independent of thermodynamics; the equality of free energies is coincidental and does not imply coexistence."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording tightened for undergraduate level, explicitly used Gibbs free energy notation $G$, clarified that equality means no net thermodynamic driving force and implies coexistence and sensitivity to small T/P changes. Distractors kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Phase transition",
      "x": 1.7990329265594482,
      "y": 1.06143319606781,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Phase stability and the role of free energy at the transition point (two phases have identical free energies and are equally likely to exist).",
        "Concept 2: Phase diagrams and metastability (how changing temperature/pressure moves systems across regions, causing transitions; the possibility of diabatic vs. adiabatic paths leading to superheating/supercooling).",
        "Concept 3: Structural/solid-solid phase transitions (allotropy/polymorphism, and displacive transformations such as martensitic transitions as mechanisms for changing crystal structure without changing composition)."
      ],
      "original_question_hash": "e3c27880"
    },
    {
      "question": "Why does grounding scientific inquiry in testable hypotheses and predictions allow science to accumulate reliable knowledge?",
      "options": {
        "A": "Because it ensures every hypothesis will eventually be proven true if tested enough times.",
        "B": "Because it provides criteria to falsify competing explanations: results that contradict predictions force revision or rejection of hypotheses, producing more robust theories.",
        "C": "Because it guarantees that repeated experiments will always produce data confirming the initial hypothesis.",
        "D": "Because it eliminates all ambiguity in interpreting observations, forcing a single correct reading of the data."
      },
      "correct_answer": "B",
      "simplification_notes": "Question and options were made more concise and targeted to undergraduates; retained technical terms like 'falsify', 'hypothesis', and 'theory'; distractors were phrased as common misconceptions about testing and reproducibility.",
      "content_preserved": true,
      "source_article": "Science",
      "x": 1.2820433378219604,
      "y": 1.06809401512146,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The scientific method as a framework that uses testable hypotheses and predictions to build knowledge.",
        "Concept 2: The relationships and methodological differences among branches of science (natural, social, formal, applied) and how these distinctions reflect underlying approaches (empirical vs. deductive reasoning).",
        "Concept 3: The historical progression of science (from natural philosophy to modern science, through the Scientific Revolution) illustrating how ideas, practices, and institutions co-evolve to shape scientific knowledge."
      ],
      "original_question_hash": "7172da19"
    },
    {
      "question": "How does the arm's‑length principle operate to protect institutional autonomy between religious bodies and the state, and why does that arrangement support a pluralist public sphere?",
      "options": {
        "A": "It requires a formal merger of religious and governmental leadership so the same authorities determine public policy and religious doctrine.",
        "B": "It creates overlapping, interchangeable functions that allow religious organizations to veto or directly control state policy decisions.",
        "C": "It treats the state and religious bodies as two largely independent organizations that interact through limited, clearly defined channels, preventing coercion by either side and allowing diverse religious and secular voices to coexist.",
        "D": "It obliges the state to fund and administer all religious activities equally, achieving neutrality by universal subsidization."
      },
      "correct_answer": "C",
      "simplification_notes": "Condensed and clarified the original question; defined 'arm's‑length' as protecting institutional autonomy; kept technical terms and the core idea that limited, bounded interaction preserves pluralism; all answer choices made concise and plausible. Correct answer unchanged.",
      "content_preserved": true,
      "source_article": "Separation of church and state",
      "x": 0.43993571400642395,
      "y": 0.6674001216888428,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Separation of church and state as a principle defining political distance and the idea of a secular state (including disestablishment).",
        "Concept 2: Arm's length principle as a mechanism by which church and state operate as independent entities with limited interference.",
        "Concept 3: Different models of church-state relationships (laïcité, official state church, accommodationism) and how they alter public interaction between religion and government."
      ],
      "original_question_hash": "b26240f8"
    },
    {
      "question": "Under suggestion theory, how do emotionally charged propaganda messages shape people's judgments, and why do they often evade deliberate, rational thinking?",
      "options": {
        "A": "They evoke rapid, automatic affective responses that bias judgments before deliberate analysis occurs, because affective processing is faster and requires fewer cognitive resources than systematic evaluation.",
        "B": "They supply extensive factual evidence and logical arguments that audiences reconstruct into reasoned conclusions, thereby promoting deliberate, rational processing.",
        "C": "They work mainly through repeated exposure that strengthens memory traces, but do not directly produce affective responses or immediate judgmental bias.",
        "D": "They present contradictory information to induce cognitive dissonance, causing audiences to vacillate rather than form stable judgments."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the question for clarity, replaced informal phrasing with concise academic terms (e.g., 'affective processing' clarified as 'rapid, automatic affective responses'), and kept the focus on suggestion theory and the bypassing of deliberate reasoning. Options made equally plausible.",
      "content_preserved": true,
      "source_article": "Propaganda",
      "x": 1.1983801126480103,
      "y": 0.956258237361908,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Propaganda as an influence mechanism that uses selective information and loaded language to shape perception and advance an agenda.",
        "Concept 2: Media and technology as enablers of propaganda, including the shift from traditional media to digital/computational methods (bots, fake news, chatbots) and their impact on dissemination.",
        "Concept 3: Theoretical explanations of propaganda effects (e.g., suggestion theory) focusing on how emotionally resonant messages can influence judgments versus rational processing."
      ],
      "original_question_hash": "4faa93c7"
    },
    {
      "question": "At subsonic speeds, how does increasing the angle of attack (AoA) primarily increase the lift produced by a wing, and why does a stall occur at high AoA?",
      "options": {
        "A": "Increasing AoA deflects more air downward, increasing the downward momentum change of the airflow and therefore lift (Newton's third law); at high AoA the boundary layer separates from the upper surface, reducing the suction/pressure differential and causing stall.",
        "B": "Lift increases solely because the flow over the wing’s upper surface speeds up (a Bernoulli effect); stall happens when the airflow accelerates so much that the pressure drop can no longer increase.",
        "C": "Lift is determined only by the wing’s curvature (camber); AoA does not change lift significantly, and stall occurs when the wing’s camber limit is reached.",
        "D": "Lift increases mainly because steeper AoA raises viscous frictional forces and form drag that push the wing upward; stall occurs when drag becomes dominant and the airflow can no longer stay attached."
      },
      "correct_answer": "A",
      "simplification_notes": "Focused the question on subsonic AoA effects; used ‘AoA’ abbreviation; emphasized momentum change and boundary-layer separation as the correct mechanism; rewrote distractors as common misconceptions (Bernoulli-only, camber-only, drag-driven lift).",
      "content_preserved": true,
      "source_article": "Aeronautics",
      "x": 1.71843683719635,
      "y": 0.6987758874893188,
      "level": 2,
      "concepts_tested": [
        "Aerodynamics: the interaction of air flow with moving objects (aircraft) and how this governs lift, drag, and stability.",
        "Energy/Propulsion for sustained flight: why muscular effort is insufficient and how propulsion energy enables continued flight.",
        "Theory-to-practice progression in aeronautics: how rational study of flight (from birds to early engineers like da Vinci and Cayley) shaped the design and understanding of aircraft."
      ],
      "original_question_hash": "c2f8c722"
    },
    {
      "question": "Many economic activities occur outside recorded monetary transactions. How do non-monetary exchanges and social norms affect economic outcomes beyond price signals?",
      "options": {
        "A": "They operate independently of markets, so production and allocation are determined solely by market prices.",
        "B": "They create social constraints and expectations that shape who participates, how work is organized, and how risk is shared, thereby influencing production, allocation, and innovation beyond what prices alone dictate.",
        "C": "They are fully replaced by formal contracts and institutions, so everyday economic activity is unaffected by non-monetary factors.",
        "D": "They only encourage generosity or reciprocity and do not alter resource allocation or economic efficiency."
      },
      "correct_answer": "B",
      "simplification_notes": "Question and options were shortened and clarified for undergraduate readers; technical phrasing (e.g., 'price signals') used; core idea that social norms and non-monetary exchanges shape production, allocation, and innovation was preserved.",
      "content_preserved": true,
      "source_article": "Economy",
      "x": 1.2559552192687988,
      "y": 0.9473756551742554,
      "level": 2,
      "concepts_tested": [
        "Economy as an embedded social domain: interconnected with culture, values, education, political/legal institutions, and natural resources; these factors condition how an economy functions.",
        "Drivers and mechanisms of economic activity: production uses natural resources, labor, and capital; technology, innovation, and changes in industrial relations shape economic evolution.",
        "Scope of economic activity beyond money: monetary transactions are only a small part of the domain; non-monetary exchanges and other forms of value contribute to the economy."
      ],
      "original_question_hash": "cf32337d"
    },
    {
      "question": "How does differential stress in the crust produce faults, and how does this process drive the progressive evolution of crustal structure over time? (Use principal stresses $\\sigma_{1}$ and $\\sigma_{3}$ where appropriate.)",
      "options": {
        "A": "When the difference between principal stresses ($\\sigma_{1}-\\sigma_{3}$) exceeds the rock's brittle strength, the rock fails along discrete planes to form faults; these abrupt failures redistribute and reorient the local stress field and create new structural boundaries that channel subsequent deformation, so repeated faulting progressively reshapes crustal architecture.",
        "B": "Rocks always accommodate differential stress by slow, ductile flow regardless of pressure and temperature, so discrete fault planes do not form from differential stress.",
        "C": "Faults are formed mainly by chemical dissolution or weakening of minerals that creates random mechanical planes, so fault orientations are largely independent of the ambient stress field.",
        "D": "Faults are produced only by surface weathering and erosion and therefore do not record deeper tectonic stresses or contribute to long‑term crustal evolution."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording tightened to undergraduate level, introduced principal stress notation ($\\sigma_{1},\\sigma_{3}$), and made each option concise and plausible while preserving the original correct choice and core concepts.",
      "content_preserved": true,
      "source_article": "Geology",
      "x": 1.7009109258651733,
      "y": 0.9515277147293091,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Geological processes shape and alter the Earth's structure over time, linking structure to ongoing dynamics.",
        "Concept 2: Reconstructing geological history requires integrating multiple methods (petrology, crystallography, paleontology, geochemistry, geophysics) to establish relative and absolute ages.",
        "Concept 3: Geological evidence underpins large-scale theories (plate tectonics, evolutionary history of life, past climates) and relies on cross-disciplinary data."
      ],
      "original_question_hash": "2c5ec3e7"
    },
    {
      "question": "How does the distinctive blue-and-white shield emblem (Article 16 of the 1954 Hague Convention) function to protect cultural property in armed conflict, and what principle explains its main strength and its main vulnerability?",
      "options": {
        "A": "The emblem creates a legally binding immunity that automatically prevents any attack on marked property under all circumstances.",
        "B": "The emblem is an internationally recognized visual signal that indicates a site or object is protected, guiding combatants to spare it; its effectiveness relies on widespread compliance with humanitarian norms and on the emblem being correctly displayed and not misused.",
        "C": "The emblem denotes state ownership, converting the property into state-controlled assets whose protection is guaranteed by domestic law rather than by international humanitarian rules.",
        "D": "The emblem is a decorative or symbolic mark without practical effect on battlefield decisions; protection of cultural property therefore depends on post-conflict restitution and legal remedies."
      },
      "correct_answer": "B",
      "simplification_notes": "Removed historical detail and legal text; clarified that the emblem is a visual signal under the 1954 Hague Convention and emphasized dependence on compliance and correct use. Wording simplified and made more direct for undergraduates.",
      "content_preserved": true,
      "source_article": "Cultural property",
      "x": 0.9643685817718506,
      "y": 0.3382428288459778,
      "level": 2,
      "concepts_tested": [
        "The relationship between cultural property and broader cultural heritage, and how legal mechanisms (international agreements, national laws) enable its protection.",
        "The inclusion of both tangible and intangible elements in cultural heritage and the implications this has for identification and protection.",
        "The emblem-based protection mechanism (Article 16) as a practical tool for marking and safeguarding cultural property during armed conflict."
      ],
      "original_question_hash": "2f63697c"
    },
    {
      "question": "According to cultural studies, why is culture regarded as dynamic and processual rather than fixed and bounded?",
      "options": {
        "A": "Because culture is a stable collection of artifacts and practices that never changes over time.",
        "B": "Because culture is determined entirely by a single dominant tradition that imposes uniform meanings on everyone.",
        "C": "Because culture emerges from ongoing interactions among practices, meanings, institutions, and power relations, so it is continually reconstituted and changes.",
        "D": "Because culture is unrelated to social structures and merely reflects individual preferences."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; retained technical terms (practices, institutions, power relations); distractors made plausible and concise; correct option preserved verbatim in essence.",
      "content_preserved": true,
      "source_article": "Cultural studies",
      "x": 1.1922972202301025,
      "y": 0.9765444993972778,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Culture is dynamic and processual, not fixed; cultures are constituted by interacting practices and changing processes.",
        "Concept 2: Culture, power, and meaning: meaning is generated, disseminated, contested, and bound up with social, political, and economic power, involving ideology and hegemony.",
        "Concept 3: Interdisciplinary, theory-rich approach: cultural studies employs multiple frameworks (e.g., Marxism, semiotics, postcolonialism, feminist theory) to analyze cultural phenomena and their relation to broader structures."
      ],
      "original_question_hash": "ea630cea"
    },
    {
      "question": "Why do the scope and mechanisms of regional development change when a region is defined or perceived differently by policymakers and stakeholders?",
      "options": {
        "A": "Because changing the region's boundaries changes the distribution of natural resources, and those resource maps alone automatically determine policy priorities.",
        "B": "Because different definitions change which actors, institutions, and partner jurisdictions are included, which reshapes governance arrangements, coordination challenges, and the set of policy instruments that are feasible.",
        "C": "Because perceptions of a region are purely subjective and therefore have no practical effect on how policies are designed or implemented.",
        "D": "Because the underlying national economy stays the same regardless of regional boundaries, so regional policy choices do not need to change."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the question for clarity and concision for undergraduates; kept technical terms (actors, institutions, governance) and preserved the original concept that definitions affect which stakeholders and tools are relevant. Options made more plausible and concise.",
      "content_preserved": true,
      "source_article": "Regional development",
      "x": 1.3109123706817627,
      "y": 0.8401470184326172,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Regional development aims to reduce disparities through policy interventions and resource allocation targeted at underdeveloped areas.",
        "Concept 2: The scope and mechanisms of regional development depend on how a region is defined and perceived, affecting boundaries and policy design.",
        "Concept 3: There has been a shift from domestically focused inward investment policies to international assistance and cooperation (involving organizations like OECD, UN, IMF) as a core mechanism in regional development."
      ],
      "original_question_hash": "b6d75624"
    },
    {
      "question": "How do product design choices and material specifications fundamentally determine the sequence of steps in a manufacturing process?",
      "options": {
        "A": "Because, given enough labor and time, manufacturers can arbitrarily rearrange steps to produce any design, so the design offers little guidance on process sequencing.",
        "B": "Because design details and material properties constrain which processing methods, tools, energy inputs, tolerances, and costs are feasible; those constraints together define a viable process chain and its sequence of steps.",
        "C": "Because market timing or delivery deadlines alone decide which steps are performed, independent of design or materials.",
        "D": "Because superficial attributes such as the final product’s color alone determine the order of processing operations."
      },
      "correct_answer": "B",
      "simplification_notes": "Question and options were rewritten in clearer, concise academic language for undergraduates; technical constraints from the article (methods, tools, energy, tolerances, costs) were retained in option B. Distractors were made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Manufacturing",
      "x": 1.4729691743850708,
      "y": 0.950214684009552,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Manufacturing begins with product design and materials specification, and these inputs determine the steps of the manufacturing process to produce a finished product.",
        "Concept 2: Manufacturing serves as the bridge between the primary sector (raw materials) and distribution/consumption, spanning a spectrum from handicraft to high-tech and influencing whether goods are sold to other manufacturers or end users.",
        "Concept 3: The manufacturing field is closely connected with engineering and industrial design, with manufacturing engineering focusing on designing and optimizing the steps that transform materials into products."
      ],
      "original_question_hash": "1f585129"
    },
    {
      "question": "How does applying the Six Sigma DMAIC (Define, Measure, Analyze, Improve, Control) improvement cycle conceptually reduce waste in a production system?",
      "options": {
        "A": "By increasing automation of all tasks to eliminate human error.",
        "B": "By enforcing disciplined measurement and data-driven analysis to identify and remove root causes of process variation, thereby reducing defects and rework (forms of waste).",
        "C": "By maximizing throughput by pushing production capacity beyond current customer demand.",
        "D": "By guaranteeing immediate, universal standardization of every process without pilot testing or controls."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the question; explicitly named DMAIC steps; made each option concise and plausible while keeping the original correct choice (B) unchanged.",
      "content_preserved": true,
      "source_article": "Industrial engineering",
      "x": 1.4236921072006226,
      "y": 1.0117719173431396,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Industrial engineering as the optimization of integrated systems (people, materials, information, equipment, energy) to improve efficiency, productivity, and quality.",
        "Concept 2: Methodologies (lean manufacturing, Six Sigma, process capability) as mechanisms to reduce waste and enhance performance.",
        "Concept 3: Interdisciplinary foundations and relationships among engineering, mathematics, social sciences, and business that enable design, analysis, and management of complex processes, including overlaps with related fields."
      ],
      "original_question_hash": "42592cc6"
    },
    {
      "question": "Why does organising co-creation as two distinct steps—(1) open contribution of ideas by many participants and (2) a separate selection process that chooses which ideas to implement—tend to create more value than a single-step approach where the same actor both proposes and selects ideas? How do the contribution and selection steps interact to improve outcomes?",
      "options": {
        "A": "Because it widens the pool of ideas by attracting diverse contributions, and then applies an independent or goal-aligned evaluation to filter for feasibility, desirability and strategic fit, yielding higher-value choices.",
        "B": "Because concentrating both proposal and decision power in one actor speeds up development and ensures the best idea is chosen by an expert central authority.",
        "C": "Because separating stages reduces transparency about how ideas are judged, which discourages participation and therefore increases the quality of remaining submissions.",
        "D": "Because splitting steps locks ideas in early and prevents iterative improvement or further feedback, forcing firms to choose sooner rather than refine proposals."
      },
      "correct_answer": "A",
      "simplification_notes": "Rewrote the question in clear academic language for undergraduates, emphasising the two-step sequence (contribution then selection) and how they interact. Removed historical and typology detail while keeping the core mechanism and outcome. Options made plausible alternatives.",
      "content_preserved": true,
      "source_article": "Co-creation",
      "x": 1.3186079263687134,
      "y": 1.0002882480621338,
      "level": 2,
      "concepts_tested": [
        "The two-step mechanism of co-creation: contribution (content ideas) and selection (which ideas are used), and how this sequence drives value creation.",
        "The typology of co-creation types based on openness and leadership (Collaborating, Tinkering, Co-designing, Submitting) and how different configurations influence participation and outcomes.",
        "The relational/organizational principle of participatory, bottom-up processes extending to urban planning to achieve inclusive, democratic, and sustainable transformations."
      ],
      "original_question_hash": "40a73a26"
    },
    {
      "question": "Using the two-dimensional framework of innovation — (1) degree of novelty (e.g., new to the firm, market, industry, or world) and (2) kind of innovation (process vs product‑service) — why might a very high‑novelty process improvement fail to generate value while a moderately novel product‑service redesign succeeds?",
      "options": {
        "A": "Because the degree of novelty alone determines value creation; the kind of innovation (process vs product‑service) does not affect implementation or value.",
        "B": "Because value realization requires alignment between novelty degree and the firm’s capabilities and the market’s readiness; very novel process changes often demand new skills, routines, or ecosystem shifts the firm lacks, whereas a moderately novel product‑service redesign can use existing channels, competencies, and customer acceptance.",
        "C": "Because process innovations inherently produce more value than product‑service innovations, so a highly novel process should always succeed regardless of required capabilities or market conditions.",
        "D": "Because only innovations that are new to the world can create value; innovations that are merely new to the firm or market never produce real value."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the question; explicitly named the two dimensions (degree of novelty and kind) and emphasized alignment with organizational capabilities and market readiness as the explanatory mechanism. Options made concise and plausible.",
      "content_preserved": true,
      "source_article": "Innovation",
      "x": 1.3131284713745117,
      "y": 0.9911307692527771,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Invention vs. innovation — innovation involves practical implementation and value realization, and not all innovations require a new invention.",
        "Concept 2: Two dimensions of innovation — degree of novelty (new to firm, market, industry, or world) and kind of innovation (process vs. product-service system), shaping classification and impact.",
        "Concept 3: Innovation as a process and outcome — production/adoption, assimilation, and exploitation of value-added novelty, leading to changes in products, services, processes, and markets."
      ],
      "original_question_hash": "3ac896a2"
    },
    {
      "question": "Functional fixedness reduces problem-solving effectiveness by making a person treat an object only in terms of its conventional function, thereby narrowing the set of candidate strategies. Which intervention most directly counteracts functional fixedness?",
      "options": {
        "A": "Reframe the problem to prompt thinking about unconventional or multiple uses for the object, thereby expanding the solution search space.",
        "B": "Increase emphasis on the object's most familiar use so solvers commit to the standard, well-practiced procedure.",
        "C": "Make the task more constrained and specify a single acceptable solution to reduce ambiguity.",
        "D": "Assign the task to a domain expert who will apply established procedures and standard uses of the object."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the stem for clarity and concision while preserving technical terms (functional fixedness, search space); kept the core concept and correct answer. Options made plausible but only A directly addresses overcoming fixation.",
      "content_preserved": true,
      "source_article": "Problem solving",
      "x": 1.3237224817276,
      "y": 1.0522172451019287,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Problem structure dictates approach and planning (well-defined vs ill-defined problems influence the strategies and extent of initial planning).",
        "Concept 2: Cognitive processes and impediments (mental techniques to identify/analyze/solve; obstacles like confirmation bias and functional fixedness that hinder solution).",
        "Concept 3: Domain and resource context (problem solving across domains such as mathematical vs. personal problems; impact of resources and knowledge on solvability)."
      ],
      "original_question_hash": "0ad723da"
    },
    {
      "question": "Why is transfer of learning better understood as an ongoing part of learning rather than a separate outcome, and which instructional design most effectively promotes generalized transfer?",
      "options": {
        "A": "Treat transfer as a separate event that happens only after a task is fully mastered, and promote it by practicing exclusively in contexts identical to the original learning situation.",
        "B": "Treat transfer as intrinsic to learning: it arises when learners practice in varied contexts and are guided to abstract and apply underlying principles across different tasks.",
        "C": "Assume transfer requires identical surface features between training and transfer tasks, and promote it by restricting practice to the exact conditions of the test.",
        "D": "Consider transfer a byproduct of extensive repetition in a single context, and promote it by maximizing repetitive practice of the same task without variation."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was condensed and clarified for undergraduate readers; preserved core concepts (transfer as a continuum, identical-elements vs. abstraction, and instructional implications). Emphasized varied contexts and abstraction as the effective design principle.",
      "content_preserved": true,
      "source_article": "Transfer of learning",
      "x": 1.278234601020813,
      "y": 1.0206695795059204,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Identical elements theory — transfer likelihood depends on the overlap/similarity of elements between the original learning task and the transfer task; how and why similarity facilitates transfer.",
        "Concept 2: Transfer as a relationship/continuum with learning — transfer is not a separate event but interconnected with learning; why/how transfer may occur as part of learning and under what conditions it generalizes.",
        "Concept 3: Instructional design implications from transfer theories — how curricula and practice conditions (e.g., training in similar contexts vs. exact conditions) influence the ease and extent of transfer; why certain designs promote or hinder transfer."
      ],
      "original_question_hash": "4f7746d9"
    },
    {
      "question": "How do career tenure and political neutrality of civil servants promote continuity of public policy when governments change?",
      "options": {
        "A": "Because civil servants are elected officials who change only when electoral mandates change, so they always reflect the current political leadership.",
        "B": "Because civil servants are temporary employees whose contracts expire at each election, requiring new staff and new policies.",
        "C": "Because civil servants are career professionals appointed on merit with tenure; they implement government policy regardless of party, provide institutional memory, and maintain nonpartisan administration across political transitions.",
        "D": "Because civil servants are formally required to switch their party allegiance to align with the new ruling party after elections, ensuring policy alignment."
      },
      "correct_answer": "C",
      "simplification_notes": "Question rewritten to be concise and academically clear; options kept plausible misconceptions (A, B, D) while preserving the correct concept that merit-based tenure and neutrality provide institutional memory and nonpartisan implementation (C).",
      "content_preserved": true,
      "source_article": "Civil service",
      "x": 0.6100896596908569,
      "y": 0.49519750475883484,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Continuity and neutrality of administration — civil service tenure tends to survive political transitions, and civil servants answer to the government rather than to a political party.",
        "Concept 2: Structural classification and governance mechanisms — distinctions between central vs. local government employees, the role of non-departmental public bodies, and how terms/conditions are regulated and measured.",
        "Concept 3: International civil service governance — international staff are governed by internal regulations, with immunity and access to dedicated tribunals (e.g., ICSC, Administrative Tribunal of the ILO) and not directly bound by national legislation."
      ],
      "original_question_hash": "89de996c"
    },
    {
      "question": "Why does the indirectness of neuroscientific measurements (e.g., fMRI BOLD signals, lesion studies, single-unit recordings, computational models) constrain how strongly we can explain cognitive processes, and which evaluation criterion best captures a robust explanation under that constraint?",
      "options": {
        "A": "Because indirect measures provide direct one-to-one mappings without background assumptions, so any model that fits the observed data consistently is sufficient.",
        "B": "Because these measurements are proxies for complex neural activity and must be interpreted via explicit assumptions about how neural states generate mental states; a robust explanation should show convergence across different methods and be transparent about those assumptions.",
        "C": "Because indirect measurements automatically establish causation among neural events and cognition, so a single convincing dataset is enough for a robust explanation.",
        "D": "Because indirectness makes multiple models equally compatible with the data, so robust explanations cannot legitimately prefer one mechanism over another."
      },
      "correct_answer": "B",
      "simplification_notes": "Reduced and clarified the original multipart question, gave concrete examples of indirect methods, and made the evaluation criterion explicit (cross-method convergence + transparency). Distractors were rephrased to remain plausible.",
      "content_preserved": true,
      "source_article": "Neurophilosophy",
      "x": 1.6747517585754395,
      "y": 1.1235193014144897,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Mind-brain relationship and neural underpinnings — whether the physical brain is necessary (and/or sufficient) to explain mental phenomena, and what this implies for the mind–body problem.",
        "Concept 2: Indirectness of methods and epistemological constraints — how neuroscientific methods (e.g., fMRI, single-unit recording, computational modeling) shape the interpretation of data and the limits on claiming explanations about cognition.",
        "Concept 3: Binding problem and localization of cognitive function — how the brain integrates information to produce unified experiences and whether cognitive functions are localized or distributed across regions."
      ],
      "original_question_hash": "992053d6"
    },
    {
      "question": "Cosmopolitanism claims there are universal moral duties that apply to all people, but different cultures have different norms. Which mechanism best reconciles these universal duties with cultural diversity to advance global solidarity?",
      "options": {
        "A": "Imposing identical legal and moral standards worldwide so every society must follow the same rules regardless of local customs.",
        "B": "Interpreting universal duties through context-sensitive moral reasoning that respects local norms while upholding core ethical commitments, enabling cross-cultural cooperation without cultural imperialism.",
        "C": "Relying on global economic integration (market forces) to align behaviors, assuming markets will automatically make societies conform to universal duties.",
        "D": "Abolishing national sovereignty and creating a single global authority that enforces universal duties on all cultures."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the prompt, removed detailed historical/philosophical references, clarified the tension (universal duties vs cultural norms), and kept four plausible mechanisms (legal uniformity, contextual interpretation, market integration, global authority).",
      "content_preserved": true,
      "source_article": "Cosmopolitanism",
      "x": 1.1919498443603516,
      "y": 0.9720008969306946,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Cosmopolitanism as a universal human community with mutual respect across diverse beliefs and locations.",
        "Concept 2: The multiplicity of forms (moral, political, cultural, economic) and how they interact or conflict in realizing cosmopolitan aims.",
        "Concept 3: The role of global political structures and universal standards as mechanisms to privilege global sociality and coordinated engagement among all humans."
      ],
      "original_question_hash": "736357ce"
    },
    {
      "question": "When an education policy is 'borrowed' from one country and implemented in another, outcomes often differ. Which explanation best accounts for these mixed results?",
      "options": {
        "A": "Because a policy's success depends only on its technical design, and those technical elements cannot be exactly reproduced in a different setting.",
        "B": "Because effectiveness depends on contextual fit — differences in governance, institutional structures, incentives, administrative capacity, and local conditions change implementation and results.",
        "C": "Because policies require identical cultural values to succeed, so transfers will fail unless cultures are perfectly aligned.",
        "D": "Because cross-context transfers are inherently invalid and any observed outcomes are artifacts of measurement error or biased evaluation."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem for clarity and concision; emphasized 'policy borrowing' and replaced long explanation with the concise concept of 'contextual fit'. Options kept plausible distractors but were shortened; key mechanisms (governance, institutions, incentives, capacity) retained in option B.",
      "content_preserved": true,
      "source_article": "Comparative education",
      "x": 1.230762004852295,
      "y": 0.9598444104194641,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The education–society relationship (how educational systems both reflect and shape social structures, processes, and outcomes)",
        "Concept 2: Policy borrowing and cross-context transfer (how ideas and policies move between contexts, benefits and criticisms, and mechanisms like international comparisons and centralized vs. decentralized systems)",
        "Concept 3: Generalization across contexts and macro-analysis (the aim to establish statements valid beyond a single country, and the role of large-scale, cross-national data in forming such generalizations)"
      ],
      "original_question_hash": "450e27f3"
    },
    {
      "question": "Why do multiple stressors (overexploitation, pollution, flow modification, habitat loss, invasive species) often produce impacts on freshwater biodiversity that are greater than the sum of their individual effects, particularly when climate change (e.g., ~1 °C warming and reduced ice cover) is involved?",
      "options": {
        "A": "Climate warming uniformly raises species' physiological tolerances, so organisms become less sensitive to other stressors and combined impacts are reduced.",
        "B": "Each stressor acts independently on separate components of the system, so their impacts simply add together without interacting.",
        "C": "Stressors interact across ecological processes and life stages; climate change shifts temperatures and habitat conditions and alters species' tolerances, producing synergistic effects that can push communities past ecological thresholds.",
        "D": "Freshwater systems are buffered by large volumes of water that dilute pollutants and stressors, preventing interactions and limiting combined effects."
      },
      "correct_answer": "C",
      "simplification_notes": "Condensed the original question, emphasized the five stressors and climate-change example, clarified 'synergistic' as interactions across processes and thresholds; answer choices reworded to be concise and plausible.",
      "content_preserved": true,
      "source_article": "Freshwater ecosystem",
      "x": 1.6458758115768433,
      "y": 0.943327784538269,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The relationship between habitat type and ecosystem structure/function (how lentic vs. lotic vs. wetlands, through factors like water movement, depth, and saturation, shape community composition and ecological processes).",
        "Concept 2: The drivers of freshwater biodiversity decline (how overexploitation, pollution, flow modification, habitat loss, and invasive species interact with climate change to reduce biodiversity and ecosystem health).",
        "Concept 3: The rationale and mechanics of ecosystem monitoring (why biological indicators such as macroinvertebrates, macrophytes, and fish provide integrated assessments of health, and how monitoring has evolved from chemical indicators to multi-taxa bioassessment)."
      ],
      "original_question_hash": "272534d3"
    },
    {
      "question": "How can internal ecosystem processes produce feedbacks that change how external climate-driven inputs (e.g., water, nutrients, light) influence resource availability and thus ecosystem structure and function?",
      "options": {
        "A": "Internal processes are passive and do not alter resource availability; external climate inputs alone set resources and determine ecosystem structure.",
        "B": "Internal processes (e.g., plant uptake, decomposition, root competition, shading) actively modify the availability, distribution, and retention of water, nutrients and light, creating feedbacks that can buffer, concentrate, or redirect external inputs and thereby change ecosystem structure and function.",
        "C": "External inputs determine all ecosystem dynamics; internal processes only react after major disturbances and do not affect ongoing resource flows.",
        "D": "Internal processes always amplify external fluctuations, increasing variability and causing greater instability in resource availability and ecosystem structure."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording shortened and clarified for undergraduates; explicit examples (plant uptake, decomposition, root competition, shading) were added; distractors made plausible while keeping the original correct choice.",
      "content_preserved": true,
      "source_article": "Ecosystem",
      "x": 1.653399109840393,
      "y": 0.960132360458374,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Energy flow and nutrient cycling as core mechanisms linking biotic and abiotic components (e.g., energy enters via photosynthesis; decomposition recycles nutrients and releases carbon).",
        "Concept 2: Internal vs. external factors and their feedbacks shaping ecosystem structure and processes (external climate inputs influence resources; internal processes regulate and respond to those inputs).",
        "Concept 3: Disturbance, succession, resistance, and ecological resilience as dynamic properties governing how ecosystems change and recover over time."
      ],
      "original_question_hash": "78cb069a"
    },
    {
      "question": "How do societal norms regulate acceptable behavior to enable cooperation within a group when formal enforcement (laws, police) is weak or absent?",
      "options": {
        "A": "They convert most behaviors into formal, codified laws that are uniformly enforced, removing individual discretion.",
        "B": "They create shared expectations about acceptable conduct, which reduces uncertainty about others' actions and lowers the costs of coordinating behavior.",
        "C": "They function mainly through genetic predispositions that compel individuals to comply with group rules.",
        "D": "They replace material incentives entirely by depending solely on moral approval or social praise, rendering economic rewards irrelevant."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened and clarified (explicitly mentioning weak or absent formal enforcement). Options were rephrased to be concise and plausible alternatives; technical terms like 'coordination costs' retained. Core concept and correct answer preserved.",
      "content_preserved": true,
      "source_article": "Society",
      "x": 1.265217661857605,
      "y": 0.9733117818832397,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Societal norms as mechanisms that regulate acceptable behavior and enable cooperation within a group.",
        "Concept 2: Labor specialization and patterns of social relations as foundations of social structure and stratification in larger, more complex societies.",
        "Concept 3: Bidirectional shaping between society and individuals, with technology/economic activity influencing social organization and, in turn, individuals influencing societal change."
      ],
      "original_question_hash": "f1b6ec1f"
    },
    {
      "question": "Why does interdependence between two people make their relationship quality change together over time, rather than be determined by isolated events?",
      "options": {
        "A": "Because each partner's actions, emotions, and reactions influence the other's across repeated interactions, producing a feedback loop anchored in their shared history and context.",
        "B": "Because once social rules or norms are established, those external regulations fully determine partners' behavior and they no longer influence each other.",
        "C": "Because relationship quality is fixed by initial attraction (or first impressions) and stays constant regardless of later interactions.",
        "D": "Because mutual influence necessarily increases conflict, so interdependence always leads to a steady decline in relationship quality."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the stem to say \"change together over time\" instead of \"co-vary\" and tightened option wording. Removed explicit citations of Sternberg, Fisher, and other theories while preserving the core idea of mutual influence and feedback over time. Made distractors plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Interpersonal relationship",
      "x": 1.2612677812576294,
      "y": 0.9988553524017334,
      "level": 2,
      "concepts_tested": [
        "Interdependence and mutual influence: Relationships are interdependent and mutual influence shapes relationship quality, driven by interactions and shared history/context.",
        "Theoretical models of love and relationship structure: Sternberg’s Triangular Theory of Love (intimacy, passion, commitment) and Fisher’s stages of love (attraction, romantic love, attachment) as frameworks to explain how and why relationships develop and vary.",
        "Dimensional variation and social regulation: Relationships differ along dimensions like intimacy, self-disclosure, duration, reciprocity, and power, and are regulated by law, custom, or mutual agreement, influencing the formation and maintenance of different relationship types."
      ],
      "original_question_hash": "53d2370f"
    },
    {
      "question": "If the same nominal dose of a chemical is given by different routes (oral, inhalation, dermal), how can the route of exposure change the observed dose–response relationship?",
      "options": {
        "A": "It alters the fraction that reaches systemic circulation and the site of action: different routes have different absorption barriers and may involve first‑pass metabolism, so the effective internal dose and observed toxicity change.",
        "B": "It does not affect toxicity; only the nominal (administered) dose matters because the chemical is the same.",
        "C": "It changes the chemical's identity at the site of action, so different routes produce different toxic compounds.",
        "D": "It only affects the time to onset of effect, not the magnitude, because all routes eventually deliver the same total amount."
      },
      "correct_answer": "A",
      "simplification_notes": "Shortened and clarified the question, added route examples (oral, inhalation, dermal), distinguished nominal versus effective (internal) dose, and made each option concise and plausible.",
      "content_preserved": true,
      "source_article": "Toxicology",
      "x": 1.8464996814727783,
      "y": 1.004227876663208,
      "level": 2,
      "concepts_tested": [
        "Dose-response principle: The toxicity of a substance depends on the dose and related exposure characteristics (e.g., duration, route).",
        "Modulating factors in toxicity: Factors such as dosage, duration, route of exposure, species, age, sex, and environment influence the outcome.",
        "The dose makes the poison (threshold/context dependence): Only the appropriate dose yields a toxic effect, highlighting that harm is a function of dose rather than the substance alone."
      ],
      "original_question_hash": "ea3886ca"
    },
    {
      "question": "Why do typical time-series models express the current observation as depending on past observations rather than future observations?",
      "options": {
        "A": "Because time imposes a natural one-way ordering and causal direction: present values are influenced by past (observed) values, not by future (unobserved) values.",
        "B": "Because time-series analysis primarily compares identical time points across different individuals (a cross-sectional perspective), so models align observations by the same time index.",
        "C": "Because future observations are always independent of the past, making it unnecessary to model dependence on the future.",
        "D": "Because data are usually recorded in reverse chronological order, so past values are more accessible for modeling than future ones."
      },
      "correct_answer": "A",
      "simplification_notes": "Rewrote the question in concise academic language, clarified the role of one-way temporal ordering and causality, and made distractors plausible by referencing cross-sectional comparison, (false) independence, and data ordering.",
      "content_preserved": true,
      "source_article": "Time series",
      "x": 1.6289494037628174,
      "y": 1.122033953666687,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Temporal dependence and one-way time direction — observations close in time are more related, and models typically express current values as deriving from past values rather than future values.",
        "Concept 2: Distinction from cross-sectional and spatial data due to natural temporal ordering — time series analysis focuses on within-series relationships over time, not on ordering across individuals or locations.",
        "Concept 3: Forecasting and stochastic-process modeling — time series analysis uses models (often stochastic) to predict future values from past observations, and differentiates this from general regression that links different series or cross-sectional attributes."
      ],
      "original_question_hash": "cb756ca1"
    },
    {
      "question": "Why does reduced rolling resistance at the wheel–rail interface enable railways to operate longer trains while using less energy?",
      "options": {
        "A": "Reduced rolling resistance lowers the energy required per tonne‑kilometre ($\\mathrm{energy}/(\\text{tonne}\\cdot\\text{km})$), so a locomotive with a given power/tractive effort can haul more cars (longer trains) for the same energy input, increasing overall efficiency.",
        "B": "Lower rolling resistance greatly increases wheel and rail wear and damage, forcing operators to shorten trains to limit maintenance, which reduces efficiency.",
        "C": "Lower rolling resistance raises the initial tractive force needed to overcome static resistance, making it harder to start and accelerate longer trains.",
        "D": "Lower rolling resistance increases aerodynamic drag on the moving train at speed, which offsets any energy savings and reduces efficiency."
      },
      "correct_answer": "A",
      "simplification_notes": "Removed historical context and auxiliary details; focused the wording on the core physical relation between rolling resistance, energy per tonne‑km, locomotive power and train length; clarified technical terms.",
      "content_preserved": true,
      "source_article": "Rail transport",
      "x": 1.741544246673584,
      "y": 0.44491592049598694,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Friction and track design enable higher capacity and efficiency (lower rolling resistance allows longer trains and better energy use).",
        "Concept 2: Evolution of propulsion (steam to diesel to electric) and electrification as mechanisms for improved performance, speed, and emissions.",
        "Concept 3: Economic impact and market integration (how rail lowers shipping costs and fosters national/international trade and industrial development)."
      ],
      "original_question_hash": "ac8c0059"
    },
    {
      "question": "In systems thinking within systems engineering, why does improving a single component in isolation often fail to maximize overall system performance?",
      "options": {
        "A": "Because subsystems are linked by interfaces and feedback loops, so a local improvement can change interactions and trade-offs, producing emergent effects that degrade system-level performance.",
        "B": "Because modularity guarantees that optimizing one component in isolation always produces the best overall system performance.",
        "C": "Because overall system performance depends only on external requirements and not on internal interactions among components.",
        "D": "Because global or system-level optimization can be postponed until later design stages without affecting current system behavior or lifecycle outcomes."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened for clarity and concision; jargon retained (e.g., interfaces, feedback, emergent effects) appropriate for undergraduates. Distractors were made plausible but incorrect. Core concept about interactions and holistic optimization was preserved.",
      "content_preserved": true,
      "source_article": "Systems engineering",
      "x": 1.4522749185562134,
      "y": 1.050655722618103,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Systems thinking and holistic integration (viewing a system as an integrated whole; interactions among components and the importance of shared objectives and “ilities”)",
        "Concept 2: Discovery-driven, problem-first approach (starting by identifying real problems and probable failures; contrasts with a purely manufacturing mindset)",
        "Concept 3: Life-cycle perspective and cross-disciplinary coordination (design, integration, management across the system’s life cycle; coordinating multiple disciplines and stakeholders)"
      ],
      "original_question_hash": "6aa0c18e"
    },
    {
      "question": "How does banning predatory pricing by a dominant firm help sustain competitive processes in a market?",
      "options": {
        "A": "It forces all firms to set prices above average total cost ($ATC$) at all times, guaranteeing long-run profits for incumbents.",
        "B": "It stops a dominant firm from temporarily pricing below cost to drive rivals out, thus preserving market entry, contestability, and consumer choice.",
        "C": "It permits the dominant firm to engage in price discrimination without consequence, increasing surplus for its preferred customers.",
        "D": "It eliminates the need for ongoing regulatory oversight once a firm achieves dominance."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened and made more concise; predatory pricing is referenced as below-cost exclusion; technical term 'contestability' retained. Options rewritten to be clear and plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Competition law",
      "x": 1.2911211252212524,
      "y": 0.8757821321487427,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Prohibiting agreements or cartels to prevent restriction of free trading and competition.",
        "Concept 2: Regulating abusive behavior by dominant firms (e.g., predatory pricing, tying, price gouging, refusals to deal) to maintain competitive processes.",
        "Concept 3: Mergers and acquisitions oversight and remedies (e.g., divestitures, licensing) to prevent harm to competition."
      ],
      "original_question_hash": "7fce2d86"
    },
    {
      "question": "How does the option for an author to self-publish when traditional publishers refuse illustrate the central principle of freedom of the press?",
      "options": {
        "A": "It shows that freedom of the press is about access to publication — the right for works to be published (including via self-publishing) — not only the absence of government censorship.",
        "B": "It shows that freedom of the press guarantees equal funding and financial support for every publishing project.",
        "C": "It proves that press freedom is identical to freedom of speech and that self-publishing is not covered by press rights.",
        "D": "It implies private publishers have unchecked control over published content and that self-publishing automatically removes that private control."
      },
      "correct_answer": "A",
      "simplification_notes": "Stem shortened and clarified to focus on self-publishing as an alternative when publishers refuse; language made more concise and academic while preserving the key idea that press freedom concerns access to publication as well as limits on censorship.",
      "content_preserved": true,
      "source_article": "Freedom of the press",
      "x": 1.153627634048462,
      "y": 0.9205676913261414,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Freedom of the press as a fundamental legal right designed to minimize censorship and prior restraint, protected by laws or constitutions.",
        "Concept 2: The relationship between press freedom and freedom of speech, including the idea that they are often governed by the same laws to ensure equal treatment of spoken and published expression.",
        "Concept 3: The relationship between press freedom and publishing practices, including the role of publishers (and the option of self-publishing when publishers refuse), illustrating that press freedom encompasses access to publication, not just absence of interference from outsiders."
      ],
      "original_question_hash": "96a545b8"
    },
    {
      "question": "How do transmission or technological regulations handle conflicts between different national rules, and what is a main trade-off compared with relying only on content-based censorship?",
      "options": {
        "A": "They impose a universal, content‑agnostic technical rule set that applies identically everywhere, which would remove cross‑border legal disputes but undermine local cultural and legal norms.",
        "B": "They force platforms to use a single global moderation policy for all users and content, resolving legal divergence at the cost of reducing platform responsiveness to local markets and laws.",
        "C": "They regulate platform architecture and enforcement tools (e.g., geoblocking, data‑routing controls, differential data flows, and automated moderation) so operators can comply with multiple jurisdictions without adjudicating each item of content; the trade‑off is increased automated removals, algorithmic bias, and greater privacy/data‑flow concerns.",
        "D": "They ban cross‑border data transfers to eliminate jurisdictional conflicts, which perfectly conforms to local law but cripples global information exchange and digital services."
      },
      "correct_answer": "C",
      "simplification_notes": "Rewrote the stem in clearer academic language, gave concrete technical examples (geoblocking, data routing, automated moderation), and condensed legal context. Maintained original trade‑off about automated overreach and privacy.",
      "content_preserved": true,
      "source_article": "Mass media regulation",
      "x": 1.250142216682434,
      "y": 0.9051834344863892,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The objectives of mass media regulation (protecting public interest, promoting competition, establishing technical standards) and how regulators evaluate success.",
        "Concept 2: The mechanisms of regulation (content regulation/censorship vs. transmission/technological regulation) and how they interact, including cross-jurisdictional conflicts.",
        "Concept 3: Trade-offs and rights implications (balancing positive/negative liberties, digital rights, privacy, and the use of AI in moderation, with potential biases)."
      ],
      "original_question_hash": "ba6ace3e"
    },
    {
      "question": "Why is a conflict of interest (COI) commonly defined as a risk that a secondary interest may unduly influence a primary interest, rather than as proof that a decision was actually biased?",
      "options": {
        "A": "Because the objective is to protect the integrity of decisions by addressing situations where a secondary interest could unduly influence judgment; such risks can be identified by objective indicators (e.g., competing roles or financial ties) even when actual bias has not been proved.",
        "B": "Because only proven bias matters for ethics rules; defining COI as a risk is unnecessary and makes enforcement harder.",
        "C": "Because secondary interests are always immoral and therefore must be eliminated regardless of whether there is evidence they affected a decision.",
        "D": "Because COI determinations depend primarily on the individual's intention to influence outcomes, not on objective evidence like relationships or financial interests."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was shortened and clarified for undergraduates; kept the core idea that COI is risk-based and identifiable via objective signs (competing roles, financial ties). Distractors were made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Conflict of interest",
      "x": 1.26700758934021,
      "y": 0.9168710708618164,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Primary vs. secondary interests and their potential to unduly influence professional judgment in decision-making contexts.",
        "Concept 2: The risk-based nature of conflicts of interest (COI) — COI is about the possibility of influence, not necessarily actual misconduct, and can be identified via objective evidence.",
        "Concept 3: Management mechanisms for COI (e.g., identifying contending interests, recusal, or relinquishing conflicting roles) to preserve integrity of decisions."
      ],
      "original_question_hash": "6d527e59"
    },
    {
      "question": "How do attributing information to sources and avoiding overt judgments help journalists pursue objectivity?",
      "options": {
        "A": "They prevent the journalist’s personal views from shaping the story, allowing readers to evaluate the facts and the sources themselves.",
        "B": "They ensure a single source’s account is presented as the definitive truth, eliminating ambiguity.",
        "C": "They require journalists to present facts together with explicit value judgments to guide the audience.",
        "D": "They reduce the need for independent verification by relying on the journalist’s authority and credibility."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was shortened and clarified for an undergraduate audience; technical phraseology simplified while keeping the concepts of attribution, avoidance of judgment, and audience empowerment. All answer options made plausible alternatives and original correct choice preserved.",
      "content_preserved": true,
      "source_article": "Journalistic objectivity",
      "x": 1.215501070022583,
      "y": 1.0040290355682373,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Truthfulness, neutrality, and detachment as interrelated components of objective journalism.",
        "Concept 2: Mechanisms to achieve objectivity (attributing information, avoiding overt partiality/judgment) and their practical role in reporting.",
        "Concept 3: The institutional/relational role of objectivity (as a fourth estate function) and its aim to empower audiences to form their own conclusions."
      ],
      "original_question_hash": "5533a7a3"
    },
    {
      "question": "When solving a novel problem (e.g., in mathematics or design), how do discursive (formal) reasoning and intuitive reasoning typically interact, and why is each necessary for progress?",
      "options": {
        "A": "Intuition supplies fast heuristic answers, while formal logic is too slow and should be postponed until later stages.",
        "B": "Discursive formal logic alone can generate new theorems or designs without any intuitive input; intuition is unnecessary.",
        "C": "Intuition supplies heuristic starting points and pattern recognition to guide exploration, while discursive reasoning subjects those hunches to rigorous testing and validation; both are usually required.",
        "D": "Intuition always conflicts with formal logic, so one must choose either intuitive reasoning or formal reasoning, never both."
      },
      "correct_answer": "C",
      "simplification_notes": "Clarified the original question, shortened phrasing, made example domains explicit (mathematics, design), retained technical contrast between heuristic/intuition and formal/discursive reasoning and their complementary roles.",
      "content_preserved": true,
      "source_article": "Reason",
      "x": 1.1961004734039307,
      "y": 1.0697746276855469,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Forms of reasoning (deductive, inductive, abductive) and the goal of extrapolating from knowledge to generate new conclusions, including the role of logic in producing valid arguments.",
        "Concept 2: Relationship between discursive (formal) reasoning and intuitive reasoning, including how intuition can be complementary or clash with formal logic and its role in creativity (e.g., mathematical proofs).",
        "Concept 3: Reasoning as part of decision making and agency, including its connection to self-determination, goals, beliefs, and the study of reasoning across psychology, cognitive science, and automated/artificial reasoning."
      ],
      "original_question_hash": "6ec35575"
    },
    {
      "question": "Why does the truth of the formula $(P \\land Q)\\to P$ depend only on its logical form and not on the particular meanings of $P$ and $Q$?",
      "options": {
        "A": "Because truth depends on the actual meanings of $P$ and $Q$, not on the form of the formula.",
        "B": "Because a logically valid form preserves truth under any substitution of $P$ and $Q$ with arbitrary propositions, so the structure alone fixes its truth conditions.",
        "C": "Because $P$ and $Q$ must both be true for $(P \\land Q)\\to P$ to be true.",
        "D": "Because the antecedent $P\\land Q$ can be false depending on what $P$ and $Q$ assert, so the formula's truth would vary with their meanings."
      },
      "correct_answer": "B",
      "simplification_notes": "Rephrased the question in concise undergraduate-level language, converted symbolic expressions to inline LaTeX, and tightened option wording while keeping all four choices plausible and preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Logic",
      "x": 1.3673951625823975,
      "y": 1.118519902229309,
      "level": 2,
      "concepts_tested": [
        "Logical form and content-independence: how the truth of some propositions depends on their logical structure rather than the meanings of their parts.",
        "Premises–conclusion relationship and argument validity: how premises support a conclusion, with deductive arguments delivering necessary truth and others offering weaker, ampliative support.",
        "Types of reasoning (deductive, inductive, abductive) and their justificatory strength: how each form provides different kinds of evidence (necessary vs. probabilistic vs. explanatory)."
      ],
      "original_question_hash": "2faad961"
    },
    {
      "question": "Why does a narrator's decision to present events out of simple chronological order (rather than as a straightforward timeline) change how an audience understands the causal links between those events?",
      "options": {
        "A": "Because changing the order alters which events are foregrounded as causes, guiding the audience's inferences about cause–effect and shaping narrative tension.",
        "B": "Because any ordering is equally informative; audiences infer causality solely from the final outcome of the story.",
        "C": "Because the narrator's voice or claimed reliability alone determines the credibility of events, so reordering does not affect perceived causal relations.",
        "D": "Because changing the sequence actually changes the events themselves and therefore changes the historical facts reported."
      },
      "correct_answer": "A",
      "simplification_notes": "Clarified prompt to focus on non-chronological presentation and causal inference; shortened wording; removed broader media/fiction distinctions while preserving the core idea that order shapes emphasis and suspense.",
      "content_preserved": true,
      "source_article": "Narrative",
      "x": 0.9473984837532043,
      "y": 1.1538634300231934,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Narrative as the sequence of connected events presented to an audience by a narrator (the relationship between events, narration, and audience).",
        "Concept 2: The fiction vs. nonfiction distinction and how this affects narrative purpose and reliance on facts/history.",
        "Concept 3: The relationship between medium/form and narrative (narratives can be expressed across speech, text, images, film, etc., with the sequence of events as the throughline)."
      ],
      "original_question_hash": "1ab045f1"
    },
    {
      "question": "In interactive media (for example, video games), world-building, causal event design, and character development let players influence what happens. Why does that lead to multiple, individualized narrative experiences instead of one fixed story?",
      "options": {
        "A": "Designers predefine and encode every possible outcome in advance so all players follow the same narrative trajectory.",
        "B": "Players' actions and choices dynamically change the sequence of events and fill interpretive gaps, producing different story paths and experiences for different players.",
        "C": "Causality is replaced by randomness so outcomes are entirely unpredictable and unique on each playthrough.",
        "D": "Narrativity in games is only a surface effect; player involvement does not alter the underlying story."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened and clarified for undergraduates; retained mention of world-building, causal design, and character development. Distractors were rephrased to remain plausible while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Narratology",
      "x": 0.8582143187522888,
      "y": 1.1083292961120605,
      "level": 2,
      "concepts_tested": [
        "How structuralist analyses of narrative (focusing on the narration of events) differ from cognitive narratology (focusing on how humans experience and make sense of stories).",
        "The distinction between a narrative object and narrativity—as a quality that can be evoked by media (e.g., video games) even if they aren’t conventional narratives.",
        "The mechanisms by which interactive media create narrativity (world-building, causal event design, character development) and how these drive multiple, individualized narrative experiences."
      ],
      "original_question_hash": "d0790be9"
    },
    {
      "question": "How does the reciprocal relationship between telling myths and performing rituals help maintain social cohesion in a society that preserves traditions?",
      "options": {
        "A": "Myths provide narrative templates that rituals enact; by performing rituals repeatedly these narratives become embedded in collective memory and shared meaning, creating a feedback loop that reinforces both social norms and group identity.",
        "B": "Myths are merely stories and rituals are independent practices; there is no mutual influence between them.",
        "C": "Each ritual performance generates a new mythic account, which gradually erodes the authority of established traditional narratives.",
        "D": "The relationship is one-way: myths prescribe social norms, while rituals only reflect those norms without influencing the myths themselves."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the original question into clearer academic language for undergraduates, removed complex phrasing, preserved the idea of a bidirectional (reciprocal) myth–ritual feedback loop and its role in sustaining social norms and identity; distractors kept plausible alternatives.",
      "content_preserved": true,
      "source_article": "Myth",
      "x": 1.1223212480545044,
      "y": 1.0535306930541992,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Myths have a social function and are meaningful beyond verifiable truth (they play a fundamental societal role).",
        "Concept 2: There is a close, bidirectional relationship between myth recital and ritual enactment (myths and rituals reinforce each other).",
        "Concept 3: Origin/etiological and national myths explain the origins of customs, institutions, taboos, and a society’s values or identity."
      ],
      "original_question_hash": "109aaf84"
    },
    {
      "question": "Why does informal, community-based transmission both preserve folklore and produce multiple locally distinct variants across different social groups?",
      "options": {
        "A": "Because informal transmission depends on a centralized authority that enforces uniform versions of the tradition.",
        "B": "Because informal transmission lacks centralized standardization, so individuals reinterpret and adapt artifacts to local contexts, and social networks spread these diverse variants to other groups.",
        "C": "Because only formal institutions generate multiple variants; informal, community-based transmission tends to reduce variation and enforce conformity.",
        "D": "Because the absence of written records ensures that no version survives beyond a single telling, preventing stable transmission across groups."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded for clarity and concision at an undergraduate level; emphasized mechanisms (lack of central standardization, reinterpretation/adaptation, and social networks) while keeping original answer choices and core concept.",
      "content_preserved": true,
      "source_article": "Folklore",
      "x": 0.6788979172706604,
      "y": 1.0756088495254517,
      "level": 2,
      "concepts_tested": [
        "Informal, community-based transmission as the mechanism by which folklore persists and evolves, including the role of multiple variants.",
        "Folklore artifacts span verbal, material, and customary lore, forming an integrated expressive culture.",
        "Folklore as a function of shared community identity that evolves with new groups, contrasting with formal high culture and institutional transmission."
      ],
      "original_question_hash": "8850d94e"
    },
    {
      "question": "How does a cross‑disciplinary information governance framework enable effective governance across the information lifecycle when different domains (e.g., legal, IT, security, compliance) have different goals and metrics?",
      "options": {
        "A": "By forcing all domains to adopt a single uniform metric so every decision uses the same measure.",
        "B": "By aligning domains to expose interdependencies and trade‑offs, enabling policies that balance risk, value, compliance, and usability across creation, usage, storage, and disposition.",
        "C": "By making IT the sole decision‑maker and disregarding security, privacy, and legal concerns.",
        "D": "By outsourcing governance entirely to external standards bodies and removing internal coordination and control."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the question, added stakeholder examples, used consistent lifecycle terms (creation, usage, storage, disposition), and kept all four plausible options while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Information governance",
      "x": 1.3467609882354736,
      "y": 0.9699174761772156,
      "level": 2,
      "concepts_tested": [
        "Concept 1: IG as a principle of balancing risk with the value of information, implemented through policies and procedures to support compliance, transparency, and cost management.",
        "Concept 2: IG as a holistic, cross-disciplinary framework that integrates multiple domains (security, privacy, data quality, analytics, IT, etc.) across the information lifecycle.",
        "Concept 3: The existence and role of codified principles/standards (e.g., ARMA’s Generally Accepted Recordkeeping Principles and maturity models) as mechanisms to guide consistent IG practices across organizations."
      ],
      "original_question_hash": "15123bd8"
    },
    {
      "question": "Let \\{X_i\\} be a family of random variables. If the index i runs over a one-dimensional set (e.g. i \\in T \\subset \\mathbb{R}) we usually call the object a stochastic process with sample paths t \\mapsto X_t. If i runs over a two-dimensional set (e.g. (s,t) \\in \\mathbb{R}^2) we call it a random field with realizations (s,t) \\mapsto X_{s,t}. Which statement best explains why changing the dimension of the index set alters the terminology and the interpretation of realizations?",
      "options": {
        "A": "The dimension of the index set fixes which measurable function spaces one uses on it, so realizations on a 1D index are called paths while on higher dimensions they are fields.",
        "B": "Index dimension determines the expected dependence structure: a 1D (time) index suggests sequential evolution with possible Markov or martingale structure, while a multi-dimensional (spatial) index suggests spatial correlations that depend on distance, hence the 'random field' viewpoint.",
        "C": "The index-set dimension has no conceptual effect; the choice of the terms 'stochastic process' or 'random field' is only conventional naming.",
        "D": "A 1D index forces the random variables to be independent, whereas a 2D index forces them to be dependent, so the terminology changes accordingly."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and focused the stem; converted set notation to inline LaTeX (e.g. \\{X_i\\}, i \\in T \\subset \\mathbb{R}, (s,t) \\in \\mathbb{R}^2); clarified that the key distinction is the nature of dependence (time vs. spatial) while keeping all four options plausible and preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Stochastic process",
      "x": 1.6446605920791626,
      "y": 1.1524431705474854,
      "level": 2,
      "concepts_tested": [
        "Concept 1: A stochastic process is a family of random variables indexed by time (or another index set), representing systems that evolve randomly.",
        "Concept 2: The indexing set determines terminology and relationships (time-indexed processes vs. plane-indexed random fields) and links to the idea of random functions.",
        "Concept 3: Central models (Brownian/Wiener motion and Poisson process) serve as fundamental building blocks that inform theory and applications in stochastic processes."
      ],
      "original_question_hash": "ce7c2821"
    },
    {
      "question": "If a person explicitly rejects a stereotype, how can implicit stereotypes still alter their judgments?",
      "options": {
        "A": "They work via controlled, effortful processing in which the person consciously substitutes the stereotype with a deliberate evaluation.",
        "B": "They exist as associative networks in semantic (knowledge) memory that automatically activate links between a social group and certain attributes, biasing judgments before conscious consideration.",
        "C": "They only affect decisions when the person is consciously aware of them; without awareness they have no impact.",
        "D": "They are identical to a person's explicit beliefs and therefore always match what the person says they believe."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem for undergraduate clarity, shortened sentences, retained key technical term 'associative networks in semantic memory', and made all options plausible while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Stereotype",
      "x": 1.2352972030639648,
      "y": 0.9964442253112793,
      "level": 2,
      "concepts_tested": [
        "Explicit vs implicit stereotypes: differences in conscious awareness, control, and how bias might be mitigated or persist.",
        "Stereotypes as cognitive shortcuts: how they simplify information processing and their potential for inaccuracy and resistance to updating.",
        "Implicit stereotypes and associative networks: how automatic activation links groups to domains/attributes and can influence judgments despite explicit beliefs."
      ],
      "original_question_hash": "488e4be8"
    },
    {
      "question": "According to labeling theory, how does labeling a person as 'deviant' cause them to adopt and continue a deviant role?",
      "options": {
        "A": "The label itself alters the objective or legal nature of the act, making the behavior deviant by definition.",
        "B": "The label changes how others treat the person and how the person sees themself, reduces legitimate opportunities, and thus reinforces deviant behavior (a self‑fulfilling prophecy).",
        "C": "The label is a neutral descriptor with no causal effect on the individual's behavior or identity.",
        "D": "The label immediately produces rehabilitation or support services that prevent further deviant behavior."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; emphasized the mechanism as changing social reactions and self‑concept (internalization and reduced opportunities). Distractors kept plausible and aligned with common misconceptions.",
      "content_preserved": true,
      "source_article": "Labeling theory",
      "x": 1.2505429983139038,
      "y": 1.0021092891693115,
      "level": 2,
      "concepts_tested": [
        "The labeling mechanism: how attaching a label to an individual alters self-concept and can lead to deviant behavior (self-fulfilling prophecy).",
        "Deviance as a product of social labeling: deviance arises from societal reactions and labeling of acts, not from the acts themselves.",
        "The role of stigma and labeling processes (e.g., tagging, dramatization of evil) within interactionist and constructionist frameworks and their impact on identity and social status."
      ],
      "original_question_hash": "1467dcbb"
    },
    {
      "question": "Role theory holds that people act according to socially defined roles (sets of rights, duties, expectations, norms). If one person occupies multiple roles whose expectations conflict, how does this explain why their behavior changes across social contexts?",
      "options": {
        "A": "They combine the conflicting expectations into one single norm that they apply in all situations.",
        "B": "They choose which role’s expectations to follow in each context, producing selective behavior and often tension between roles.",
        "C": "They reject role expectations and act only on personal preferences, ignoring social norms.",
        "D": "Their behavior becomes unpredictable or random because the competing expectations cancel each other out."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the premise and question, retained definition of roles and role conflict; clarified that the question asks how this leads to context-specific behavior. Made all options plausible alternatives.",
      "content_preserved": true,
      "source_article": "Role theory",
      "x": 1.2679829597473145,
      "y": 0.9859323501586914,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Roles as socially defined categories with rights, duties, expectations, norms, and behaviors that guide action.",
        "Concept 2: Context-specific behavior: how behavior is shaped by social position and the surrounding social structure.",
        "Concept 3: Role conflict: the tension that arises when multiple roles carry contradictory expectations, affecting how one acts."
      ],
      "original_question_hash": "9ae54b68"
    },
    {
      "question": "Why do shared assumptions—often summarized as \"the way things get done around here\"—shape both how individuals interpret situations and how the organization performs?",
      "options": {
        "A": "They remove uncertainty by providing explicit rules for every possible situation, guaranteeing identical responses from all members.",
        "B": "They function as implicit cognitive lenses that filter how events are interpreted, guide decision making and habitual behaviours, and thereby influence coordination and organizational performance.",
        "C": "They are merely descriptive labels and have no causal effect on people's behaviour or organizational outcomes.",
        "D": "They impose a single corporate identity that eliminates variation in values and norms across all teams and subgroups."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem for clarity, kept technical idea of implicit assumptions guiding cognition and performance; options were made concise and plausible while preserving original meanings.",
      "content_preserved": true,
      "source_article": "Organizational culture",
      "x": 1.2994452714920044,
      "y": 0.9875512719154358,
      "level": 2,
      "concepts_tested": [
        "Shared assumptions and patterns: culture is a \"pattern of basic assumptions\" or \"the way things get done around here\" that are learned and guide thinking and action.",
        "Culture's influence on behavior and outcomes: culture affects how people interact, how decisions are made, and contributes to organizational performance (e.g., competitive advantage, internal alignment).",
        "Subcultures and organizational identity: cultures can be diverse with subcultures, and identity/values help differentiate and shape stakeholder perceptions and behavior within the organization."
      ],
      "original_question_hash": "1443597b"
    },
    {
      "question": "Why do emergent properties appear in complex systems, and why can't you predict them by examining only individual components?",
      "options": {
        "A": "They come from linear, additive effects of each part, so the system is merely the sum of its components' behaviors.",
        "B": "They arise from nonlinear interactions, feedback, and the network of relationships among components, producing system-level properties that cannot be deduced from any single part.",
        "C": "They appear only when a central controller or external rule overrides local component behavior and creates new system-level properties.",
        "D": "They appear because each component operates independently and identically, so system behavior is simply predictable from single-component behavior."
      },
      "correct_answer": "B",
      "simplification_notes": "Question language was tightened and technical terms (nonlinear interactions, feedback, network relationships) preserved; extraneous examples and historical context removed. Options were kept plausible but clarified.",
      "content_preserved": true,
      "source_article": "Complex system",
      "x": 1.5554652214050293,
      "y": 1.109225869178772,
      "level": 2,
      "concepts_tested": [
        "Emergence: system-level properties arise from interactions among parts and nonlinear relationships.",
        "Feedback loops and adaptation: mechanisms like information feedback and adaptive change drive dynamics and self-organization.",
        "Relational/network perspective: understanding via interactions and networks (and environment) rather than strict reductionism."
      ],
      "original_question_hash": "f830de0e"
    },
    {
      "question": "In the attractor–noise view of self-organization, how do random fluctuations (\"noise\") affect a system's movement toward organized states?",
      "options": {
        "A": "Noise suppresses the system's dynamics and keeps it trapped in its initial local attractor.",
        "B": "Noise causes the system to wander aimlessly, destroying attractors and preventing any organized states from forming.",
        "C": "Noise enables stochastic exploration of different basins of attraction, helping the system escape shallow attractors and, via nonlinear feedback, settle into deeper, more stable attractors.",
        "D": "Noise deterministically selects a single global attractor independent of the system's initial conditions."
      },
      "correct_answer": "C",
      "simplification_notes": "Wording shortened and clarified: 'random fluctuations' labeled as 'noise'; phrasing made explicit that noise enables stochastic exploration and escape from shallow attractors; removed extraneous background while keeping attractor/noise mechanism intact.",
      "content_preserved": true,
      "source_article": "Self-organization",
      "x": 1.6449075937271118,
      "y": 1.1208395957946777,
      "level": 2,
      "concepts_tested": [
        "Emergence: global order arising from decentralized, local interactions governed by nonlinear dynamics and feedback.",
        "Four ingredients: strong dynamical non-linearity (with positive/negative feedback), balance of exploitation and exploration, multiple interactions among components, and energy availability.",
        "Attractor/Noise framework: systems move toward attractors in state space, with noise facilitating exploration to reach deep attractors (order from noise)."
      ],
      "original_question_hash": "79379b43"
    },
    {
      "question": "Why does higher-quality logistics tend to improve combat performance but still not guarantee victory, and what does this imply about the role of logistics in overall operational success?",
      "options": {
        "A": "Because logistics increases tempo, sustainment, and unit readiness, enabling more effective and flexible operations; however, final success also depends on strategy, enemy actions, terrain, and unpredictable battlefield contingencies that logistics alone cannot control.",
        "B": "Because logistics only affects the speed of delivering supplies, not the condition, morale, or effectiveness of forces, so it cannot meaningfully change combat outcomes beyond timing of shipments.",
        "C": "Because superior logistics can always compensate for tactical or strategic errors by supplying unlimited materiel and thus guarantees victory in every engagement.",
        "D": "Because logistics is separate from operational design, so improvements in logistics do not influence strategic or tactical decisions and therefore do not affect combat results."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; preserved technical idea that logistics enables tempo, sustainment, and readiness while other factors govern victory; options rephrased to be concise and plausible.",
      "content_preserved": true,
      "source_article": "Military logistics",
      "x": 1.3778064250946045,
      "y": 0.9470551609992981,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Logistics as an enabler of military operations and the cause–effect relationship between logistics quality and combat outcomes (poor logistics can lead to defeat; even excellent logistics cannot guarantee victory).",
        "Concept 2: The integrated scope of logistics (movement, supply, maintenance, transport, facilities, services, medical support) and the interdependence of these domains in supporting operations.",
        "Concept 3: The fitness-for-purpose principle in logistics (resources should be adequate to the mission; being perfectly optimized is not always necessary)."
      ],
      "original_question_hash": "1ea6f75d"
    },
    {
      "question": "How does framing disaster management as a life cycle with interphase, event, response, and recovery phases improve planning and operational execution?",
      "options": {
        "A": "It causes planners to allocate resources only during the disaster event, conserving resources before and after the event.",
        "B": "It mandates a strict linear progression so each phase must end before the next begins, reducing overlap and flexibility.",
        "C": "It highlights activities that cross phases and creates continuous feedback loops, enabling proactive planning across phases and rapid adaptation during the event.",
        "D": "It shifts emphasis away from policy development and coordination toward immediate clinical care only."
      },
      "correct_answer": "C",
      "simplification_notes": "Question and options were rewritten to be more concise and academically clear; phase names retained; distractors rephrased to be plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Disaster medicine",
      "x": 1.4234139919281006,
      "y": 0.8542295694351196,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Disaster life cycle as a framework guiding planning, response, and recovery, with distinct phases and cross-phase activities.",
        "Concept 2: The liaison/coordination role of disaster medicine, illustrating the mechanism by which clinical care, emergency management, incident command, and policy interact.",
        "Concept 3: The unique practice scope of disaster medicine (emergency-based practice) and its implications for policy development, planning, and training."
      ],
      "original_question_hash": "85109343"
    },
    {
      "question": "In continuum mechanics, constitutive laws relate stress and strain using tensor fields. Why does formulating these laws with tensors ensure that the predicted material response is independent of an observer's choice of coordinate system, and what fundamental property of tensors provides this invariance?",
      "options": {
        "A": "Because tensor components have the same numerical values in every coordinate frame.",
        "B": "Because tensor components transform according to tensor transformation rules so that a change of coordinates (e.g. rotation) produces new components that represent the same physical quantity and same physical response.",
        "C": "Because tensor algebra commutes with all spatial differential operators, guaranteeing the constitutive relations are unchanged under coordinate changes.",
        "D": "Because tensors are always rank-0 (scalars) and so are inherently independent of coordinates."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording shortened and clarified (e.g., 'observer's coordinate system' → 'choice of coordinate system'); preserved technical terms 'constitutive laws', 'tensor', and 'transformation rules'; removed extraneous details while keeping the core test of why tensor formulation gives frame-independence.",
      "content_preserved": true,
      "source_article": "Continuum mechanics",
      "x": 1.7525031566619873,
      "y": 1.0843026638031006,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Continuum hypothesis and infinitesimal elements; bulk behavior described by differential equations because matter is treated as continuously distributed at scales larger than atomic spacing.",
        "Concept 2: Constitutive relationships and tensorial material properties; material behavior is encoded via state-dependent stresses, strains, and other properties using tensors, with independence from the observer’s coordinate system.",
        "Concept 3: Governing conservation principles and coordinate-free formulation; mass, momentum, and energy conservation underlie elasticity, plasticity, and fluid mechanics within a framework that is invariant to coordinate choices."
      ],
      "original_question_hash": "d414b16d"
    },
    {
      "question": "In a feedback-configured amplifier, why can the same feedback path both provide regenerative gain (boost the desired output) and also cause unwanted hum or noise?",
      "options": {
        "A": "Because feedback reduces the net loop gain in a way that selectively cancels noise while increasing the desired signal amplitude.",
        "B": "Because the feedback loop multiplies whatever is fed back — both desired signal and any hum/noise. If the loop phase and gain reinforce the signal (regeneration), they will also reinforce noise present in the loop, which can degrade the signal-to-noise ratio.",
        "C": "Because feedback only changes the system's response speed or bandwidth, not amplitudes; any hum must come from external interference rather than the feedback path.",
        "D": "Because using feedback necessarily makes the amplifier nonlinear, and that nonlinearity always worsens signal quality by producing distortion and noise."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was tightened to an undergraduate level: focus shifted to loop gain and phase as the mechanism that can amplify both signal and noise. Extraneous historical and application details were removed; technical terms (loop, phase, SNR) retained.",
      "content_preserved": true,
      "source_article": "Feedback",
      "x": 1.580588459968567,
      "y": 1.0928349494934082,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Feedback creates a closed-loop, where outputs influence future inputs, making simple linear cause-and-effect reasoning insufficient.",
        "Concept 2: Feedback mechanisms act as regulators in physical systems (e.g., float valve, centrifugal governors) to maintain or control a variable (water level, speed).",
        "Concept 3: Feedback can both enhance performance (regenerative amplification) and introduce unwanted effects (hum), illustrating the dual, trade-off nature of feedback in systems."
      ],
      "original_question_hash": "c963248c"
    },
    {
      "question": "In navigation, angular bearings to known landmarks are used to locate yourself on a chart. Why does a single bearing constrain your position to a line of possible locations, while two bearings to different known landmarks can give a unique position?",
      "options": {
        "A": "Each bearing defines a locus of possible positions (a line of position); two independent bearings intersect at a single point, yielding your position.",
        "B": "Because taking two bearings removes measurement error completely, so the exact position is recovered; a single bearing cannot eliminate that error.",
        "C": "A single bearing is always insufficient unless you also know your speed or recent motion; taking two bearings supplies that motion information.",
        "D": "The second bearing tells you how far you moved relative to the first bearing, converting the line into a point."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified the terms 'bearing' and 'line of position'; removed historical and background text; options rewritten to be concise and plausible while preserving the tested concept.",
      "content_preserved": true,
      "source_article": "Navigation",
      "x": 1.7032620906829834,
      "y": 1.0428972244262695,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Position estimation relative to known references or patterns (e.g., charts, stars, landmarks, GNSS) as the core principle of navigation.",
        "Concept 2: Use of measurements and instruments (compass, astrolabe, charts, GNSS) to convert observations into position and direction for guiding movement.",
        "Concept 3: Domain-specific reference frames and tools (marine charts, land maps/marks, flight progress in aeronautics) and how technological evolution enabled navigation principles."
      ],
      "original_question_hash": "b06303c9"
    },
    {
      "question": "Why is iteration with feedback essential in the engineering design process?",
      "options": {
        "A": "To fix requirements early and prevent any later changes to the design.",
        "B": "Because it lets designers refine the problem understanding, objectives, and feasible solutions as new data or test results appear, reducing the risk of a poor fit before committing to a final design.",
        "C": "To ensure the final design is produced in a single phase without intermediate testing or revisions.",
        "D": "To reduce stakeholder involvement so the project can proceed faster without interruptions."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the question for clarity at undergraduate level; kept technical focus on iteration and feedback; options rewritten to remain plausible while preserving the original correct choice (B).",
      "content_preserved": true,
      "source_article": "Engineering design process",
      "x": 1.4525220394134521,
      "y": 1.0594204664230347,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The design process is iterative and feedback-driven; cycles repeat as needed before progressing.",
        "Concept 2: Establishing objectives and criteria drives design decisions, synthesis, analysis, testing, and evaluation.",
        "Concept 3: There are multiple, non-linear framings/models of the design process, indicating variability in steps and their overlap."
      ],
      "original_question_hash": "e1a0131e"
    },
    {
      "question": "How does knowledge management act as a strategic asset that speeds organizational learning across organizational boundaries (e.g., in supply chains)?",
      "options": {
        "A": "By centralizing all knowledge into a single repository and assuming knowledge will be automatically disseminated to every unit.",
        "B": "By enabling structured information flows, codification, and socialization mechanisms that convert tacit knowledge into shareable forms, thereby increasing absorptive capacity and creating feedback learning loops.",
        "C": "By prioritizing archival retention and passive storage over interactive sharing, based on the assumption that tacit knowledge cannot be transferred.",
        "D": "By restricting knowledge sharing to protect competitive advantage, which preserves secrets but inhibits cross‑organizational learning."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; retained domain terms (codification, tacit knowledge, absorptive capacity) and example (supply chains); distractors kept plausible and aligned with original incorrect options.",
      "content_preserved": true,
      "source_article": "Knowledge management",
      "x": 1.3469089269638062,
      "y": 1.0148155689239502,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Knowledge management as a set of processes to create, share, and apply knowledge to support organizational goals, and its role in enabling organizational learning.",
        "Concept 2: The relationship between knowledge management and organizational learning, including how KM serves as a strategic asset and facilitates information sharing that drives OL.",
        "Concept 3: Transorganizational knowledge in supply chains and the challenges it presents, including how Industry 4.0 and digital transformation affect knowledge flows across organizational boundaries."
      ],
      "original_question_hash": "8a524478"
    },
    {
      "question": "When a public sector unit is corporatized into a state-owned corporation, why does this change normally increase incentives for efficiency even though the government remains the owner?",
      "options": {
        "A": "It transfers ownership to private investors so profits are aligned with public objectives.",
        "B": "It creates a separate legal entity with its own board, commercial accounting and performance reporting, enabling performance-based management and clearer separation from direct political control while the government retains ownership.",
        "C": "It eliminates government budgeting and oversight entirely, allowing the entity to operate as a fully private firm in the market.",
        "D": "It requires the enterprise to become a nonprofit foundation funded by government grants, removing profit motives."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem for clarity and concise academic language; preserved technical terms (corporatization, state-owned corporation, board, performance reporting). Options were made concise and all kept plausible. Correct answer unchanged.",
      "content_preserved": true,
      "source_article": "State ownership",
      "x": 1.2911571264266968,
      "y": 0.9013239741325378,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Why governments own enterprises (to address natural monopolies, promote development/industrialization, and influence public budgets) and how this affects the intended objectives and performance of state-owned entities.",
        "Concept 2: The mechanisms and transformations of state ownership (state-owned enterprises, corporatization, nationalization, municipalization) and how these change ownership structure, incentives, and potential transitions toward privatization.",
        "Concept 3: The relationship between public ownership and broader economic systems (social ownership and socialism; state capitalism; planned vs. market economies) and what public ownership signifies within those frameworks."
      ],
      "original_question_hash": "a9fc80ea"
    },
    {
      "question": "In cells, many anabolic (biosynthetic) reactions are thermodynamically unfavorable and require input of energy. Which mechanism best explains how energy released by catabolic reactions is used to drive these unfavorable anabolic steps?",
      "options": {
        "A": "By increasing the concentrations of reactants so the equilibrium is driven toward product formation regardless of the reaction's free-energy change",
        "B": "By coupling the unfavorable reaction to exergonic processes (for example ATP hydrolysis or NADH oxidation), so the combined reaction has a negative free energy ($\\Delta G<0$) and proceeds spontaneously",
        "C": "By raising the temperature of the cell to speed up reactions and overcome energetic/activation barriers",
        "D": "By spatially separating catabolic and anabolic pathways to prevent direct energy exchange between them, forcing indirect control of fluxes"
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was tightened for clarity, thermodynamic language added (\\Delta G) in the correct option, and distractors were rephrased to remain plausible; the original core concept and correct answer letter were preserved.",
      "content_preserved": true,
      "source_article": "Metabolism",
      "x": 1.997929573059082,
      "y": 1.0789332389831543,
      "level": 2,
      "concepts_tested": [
        "Enzyme-catalyzed regulation of metabolic reactions: how enzymes speed reactions and regulate their rate in response to cellular conditions and signals.",
        "Catabolic vs. anabolic metabolism and energy coupling: how energy-releasing (catabolic) processes power energy-consuming (anabolic) biosynthesis.",
        "Evolutionary conservation and organization of metabolic pathways: why core pathways and intermediates are shared across diverse organisms due to early appearance and sustained efficacy."
      ],
      "original_question_hash": "8f84d20c"
    },
    {
      "question": "At the molecular level, how does coupling an endergonic reaction to ATP hydrolysis enable the endergonic process to proceed?",
      "options": {
        "A": "Energy released by ATP hydrolysis is dissipated as heat, raising local temperature and accelerating the endergonic step.",
        "B": "A phosphate from ATP is transferred to a substrate or enzyme, forming a high-energy phosphorylated intermediate that lowers the activation barrier and makes the overall coupled sequence thermodynamically favorable.",
        "C": "ATP hydrolysis donates new carbon skeletons to substrates, increasing their chemical energy and driving the endergonic reaction.",
        "D": "The energy released by ATP hydrolysis increases the concentrations of reactants, shifting the equilibrium of the endergonic step toward product formation."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was shortened and clarified to focus on the molecular mechanism of coupling; technical phrasing retained (e.g., 'phosphorylated intermediate', 'activation barrier'); irrelevant broader context removed.",
      "content_preserved": true,
      "source_article": "Bioenergetics",
      "x": 1.8283681869506836,
      "y": 1.0609499216079712,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Energy conservation and transformation in metabolism (first law of thermodynamics; breaking/forming bonds to release usable energy; ATP as energy currency)",
        "Concept 2: Coupling of catabolic and anabolic processes via metabolic pathways and energy transduction (exergonic reactions driving endergonic ones; role of energy flow in growth and work)",
        "Concept 3: Sources and flow of bioenergetic energy across organisms (autotrophs vs. heterotrophs; photosynthesis as energy input; respiration and metabolic networks that connect energy from sunlight to ATP)"
      ],
      "original_question_hash": "3817664b"
    },
    {
      "question": "Why does a modest change in cellular pH often produce a large change in an enzyme's activity when substrate concentration remains constant?",
      "options": {
        "A": "Because small pH shifts change the protonation (ionization) states of active-site amino acids and of the substrate, altering electrostatic interactions and the active-site geometry that stabilizes the transition state, thereby changing catalytic efficiency.",
        "B": "Because pH shifts induce mutations in the protein sequence, altering its primary structure and so its activity.",
        "C": "Because enzymes are rigid and insensitive to pH, so any observed activity change is due to experimental error.",
        "D": "Because pH changes mainly alter bulk solvent properties (e.g., water structure or ionic strength) rather than active-site chemistry and enzyme–substrate interactions."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording shortened and clarified for undergraduates; retained technical terms (protonation, electrostatic interactions, transition-state stabilization); distractors made plausible but incorrect; correct answer preserved.",
      "content_preserved": true,
      "source_article": "Biochemistry",
      "x": 1.9766260385513306,
      "y": 1.0713821649551392,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Structure–function relationships of biological macromolecules (proteins, nucleic acids, carbohydrates, lipids) underpin cellular processes.",
        "Concept 2: Metabolism as the network of chemical reactions that enables cells to harvest and use energy.",
        "Concept 3: Biochemistry as an interdisciplinary field connecting chemistry, biology, and applied areas (medicine, nutrition, agriculture, biotechnology)."
      ],
      "original_question_hash": "185679df"
    },
    {
      "question": "In a circular economy, why does the \"product-as-a-service\" business model tend to keep materials and products in use longer than a conventional ownership model?",
      "options": {
        "A": "The producer retains ownership and ongoing responsibility for maintenance, upgrades, and end-of-life recovery, creating incentives to design for durability, refurbishment, and take-back loops.",
        "B": "Customers pay more upfront for the service, which discourages frequent replacements and therefore reduces product turnover.",
        "C": "Service contracts typically prevent refurbishment or recycling by third parties, which limits reuse and forces products to be discarded sooner.",
        "D": "Switching to a service model removes the need to change product design for longevity, so companies keep existing short-life designs."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the original scenario into a concise question focusing on how product-as-a-service affects material use; shortened and clarified each option while preserving plausibility and the original correct answer.",
      "content_preserved": true,
      "source_article": "Circular economy",
      "x": 1.4031109809875488,
      "y": 0.8895013332366943,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3"
      ],
      "original_question_hash": "b4d56411"
    },
    {
      "question": "Why do ISO 14040/14044 LCA guidelines not produce identical results across different studies, and what is the practical implication for interpreting and comparing LCAs?",
      "options": {
        "A": "The standards set a consistent framework but allow practitioner choices (e.g., system boundaries, allocation methods, data sources/quality, selected impact categories); consequently, studies that follow ISO can still produce different results, so valid comparison requires transparent documentation of those methodological choices.",
        "B": "The standards prescribe exact data values and fixed system boundaries, so if researchers follow them strictly identical results should be produced and any difference indicates calculation error.",
        "C": "ISO mandates cradle-to-grave analysis and forbids including the use phase, so differences between studies arise only from varying data availability for stages not covered by the standard.",
        "D": "The standards eliminate uncertainty by providing standardized datasets, therefore any remaining differences between studies signal non-compliance rather than legitimate methodological choices."
      },
      "correct_answer": "A",
      "simplification_notes": "Reduced wording to a concise undergraduate-level question, emphasized that ISO gives a framework but allows methodological choices (boundaries, allocation, data, impact categories), and clarified the practical implication: need for transparent documentation when comparing LCAs. Kept original correct answer.",
      "content_preserved": true,
      "source_article": "Life-cycle assessment",
      "x": 1.4450273513793945,
      "y": 0.958297610282898,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Cradle-to-grave (life-cycle) framing as the holistic basis for assessing environmental impacts across all stages of a product’s life.",
        "Concept 2: Inventory and emissions linkage to cumulative environmental impact and its use in comparing carbon footprints.",
        "Concept 3: Standardization through ISO 14040/14044 and the associated boundaries/criticisms, highlighting how guidelines shape but do not perfectly constrain outcomes."
      ],
      "original_question_hash": "2a1cccb4"
    },
    {
      "question": "How does the choice of methodology constrain or shape the conclusions you can legitimately draw from the same dataset?",
      "options": {
        "A": "Because some methods prioritize speed or procedural simplicity over rigor, producing weaker or less reliable conclusions.",
        "B": "Because different methodologies carry distinct assumptions about what counts as evidence, how data link to claims, and which generalizations are legitimate, so the same data can support different defensible conclusions under different inferential frameworks.",
        "C": "Because data are objective and unambiguous, so applying standard formulas produces the same interpretation regardless of the method used.",
        "D": "Because the dataset inherently admits multiple, equally valid interpretations, and different methods simply expose these different truths."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was tightened and made more direct for undergraduates; technical idea preserved that methodology encodes assumptions about evidence, inference, and generalization. Distractors made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Methodology",
      "x": 1.342572569847107,
      "y": 1.0472161769866943,
      "level": 2,
      "concepts_tested": [
        "The relationship between method choice and the conclusions/claims that can be drawn from data (how different methods can lead to different interpretations).",
        "The distinction and integration of quantitative and qualitative methodologies, including the value and rationale for mixed-methods.",
        "The role of philosophical background assumptions in determining what counts as evidence and how methods are evaluated."
      ],
      "original_question_hash": "20a42321"
    },
    {
      "question": "Why do welfare states commonly finance social programs with public money but deliver them through private providers instead of operating all services solely via government agencies?",
      "options": {
        "A": "It allows combining public financing with private delivery to leverage administrative efficiency and specialized expertise while preserving universal access through public regulation and funding.",
        "B": "It guarantees uniform service quality across regions because private competition produces identical standards everywhere.",
        "C": "It removes political oversight, since private providers alone determine who is eligible for benefits.",
        "D": "It makes welfare services fully self-sustaining so they no longer require public subsidies."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was shortened and clarified for undergraduate readers; retained policy vocabulary (public financing, private providers, universal access). Distractor options were kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Welfare state",
      "x": 1.2235982418060303,
      "y": 0.9065580368041992,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Welfare state as a mixed economy with state intervention and private delivery (public programs implemented with private entities; varying levels of government involvement).",
        "Concept 2: Core normative objectives guiding welfare states (equal opportunity, equitable wealth distribution, public responsibility for citizens lacking provisions) and how these principles shape policy design.",
        "Concept 3: Historical drivers and trajectory of expansion (influence of World War I, the Great Depression, World War II; emergence of public pensions/social insurance in the 1880s; expansion after WWII) and the mechanisms by which these events spurred reform."
      ],
      "original_question_hash": "85cfacae"
    },
    {
      "question": "How does regulatory capture permit rent-seeking by creating a coercive monopoly, and why does that outcome typically reduce market efficiency?",
      "options": {
        "A": "By increasing competition among firms and breaking up monopolistic power, which raises productivity and overall welfare.",
        "B": "By allowing a regulatory agency to be dominated by a few incumbent firms so that rules (licenses, quotas, tariffs, etc.) create legal barriers or exclusive privileges that yield above-normal profits without increasing output, thereby transferring wealth from consumers and competitors and lowering social welfare.",
        "C": "By ensuring regulations are applied neutrally and uniformly to all firms, which minimizes rents and increases public trust in markets.",
        "D": "By improving transparency and equal access to rules so all firms can compete fairly, which eliminates opportunities for rent extraction."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the original multi-sentence question into a single clearer sentence; preserved technical terms (regulatory capture, coercive monopoly, above-normal profits) and kept the causal chain: capture → legal barriers/exclusive privileges → rents without added output → reduced welfare. Options were rephrased for clarity but remain plausible distractors.",
      "content_preserved": true,
      "source_article": "Rent-seeking",
      "x": 1.2854315042495728,
      "y": 0.9364534616470337,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Rent-seeking as extraction of economic rent through policy/institutional manipulation, i.e., wealth growth without productivity gains, and its typical effects on efficiency and resource allocation.",
        "Concept 2: The relationship between rent-seeking and profit-seeking, highlighting how rent-seeking redistributes existing wealth rather than creating new wealth.",
        "Concept 3: Regulatory capture as a specific mechanism that enables rent-seeking (leading to coercive monopolies and competitive disadvantages), and its broader implications for markets and public trust."
      ],
      "original_question_hash": "5b884073"
    },
    {
      "question": "Digital distribution reduces the marginal cost of delivering an additional music unit to (almost) zero. How does this change the economics and strategic choices of the music industry, and why does it create both new opportunities for creators and new competitive pressures?",
      "options": {
        "A": "It reduces marginal delivery cost, shifting revenue models toward per‑stream monetization and data‑driven audience engagement. This lowers barriers for independent artists to reach listeners but intensifies competition and increases dependence on large digital platforms.",
        "B": "It increases the marginal cost of distributing music, which discourages broad distribution and makes piracy a smaller concern.",
        "C": "It makes direct‑to‑consumer sales the only viable channel, eliminating the need for record labels and intermediaries.",
        "D": "It guarantees that piracy will always outperform legitimate channels, preventing sustainable monetization for creators."
      },
      "correct_answer": "A",
      "simplification_notes": "Question shortened and language clarified; kept key mechanisms (near‑zero marginal cost, shift to per‑stream/data models, opportunities for independents, greater competition and platform reliance). Options made plausible distractors but preserved the original correct choice.",
      "content_preserved": true,
      "source_article": "Music industry",
      "x": 0.47213947772979736,
      "y": 1.7297617197036743,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The shift from sheet music to recorded music as the primary product transformed the structure and business model of the modern music industry.",
        "Concept 2: Market concentration and power dynamics (a few major labels and a dominant live/promotions ecosystem) shape who controls distribution, revenue, and access to markets.",
        "Concept 3: Digital distribution (including legal stores and piracy) disrupted traditional monetization and distribution channels, creating new opportunities and challenges for revenue and competition."
      ],
      "original_question_hash": "bc462f4c"
    },
    {
      "question": "Why is the deliberate exploration of multiple plausible futures central to the methodology of futures studies?",
      "options": {
        "A": "Because it increases forecasting accuracy by isolating a single most likely future outcome.",
        "B": "Because the field emphasizes extrapolating historical trends to determine the only possible path forward.",
        "C": "Because it broadens decision-makers' mental models to include diverse trajectories, allowing strategies that are robust under different possible conditions.",
        "D": "Because it avoids making normative judgments so that forecasts remain purely descriptive and value-free."
      },
      "correct_answer": "C",
      "simplification_notes": "Question rephrased for clarity and concision; retained core idea that futures studies uses plural, plausible scenarios to improve foresight and strategy. Distractor options were made plausible misconceptions but do not change the tested concept. No factual content altered; answer letter preserved.",
      "content_preserved": true,
      "source_article": "Futures studies",
      "x": 1.144371509552002,
      "y": 1.041351318359375,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Emphasis on exploring multiple plausible futures (plural futures) rather than sole prediction, as a core methodological principle.",
        "Concept 2: Pattern-based analysis of sources, patterns, and causes of change and stability to develop foresight.",
        "Concept 3: The methodological status and legitimacy debate of futures studies (art vs. science, interdisciplinarity, and its relation to social sciences and history)."
      ],
      "original_question_hash": "829ed6d3"
    },
    {
      "question": "Why do genres proliferate and change as audiences and creators change, and what does that reveal about genre's role in interpreting and organizing artistic works?",
      "options": {
        "A": "Genres are fixed templates enforced by cultural institutions; changes happen only when those institutions formally revise rules, so proliferation reflects top-down control.",
        "B": "Genres are dynamic, socially negotiated frameworks that adapt as audience expectations and creators' practices shift; this evolution shows genre is a flexible tool for interpreting and organizing new artistic output.",
        "C": "Genres are largely arbitrary marketing labels with limited interpretive value; their growth mainly reflects commercial trends rather than substantive shifts in audiences or creators.",
        "D": "Genres are stable, formal classifications analogous to mathematical categories; any proliferation therefore signals a failure of proper categorization, not adaptive interpretation."
      },
      "correct_answer": "B",
      "simplification_notes": "Question was condensed and clarified for undergraduate readers. Historical and institutional examples were removed for brevity. Each option was rewritten to be concise and plausible while preserving the original argument alternatives; the correct answer remains B.",
      "content_preserved": true,
      "source_article": "Genre",
      "x": 0.9168002009391785,
      "y": 1.1538052558898926,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Genre conventions are socially agreed-upon frameworks that guide what a genre is and how information is communicated; these conventions can evolve over time.",
        "Concept 2: Genres are dynamic and proliferate in response to changes in audiences and creators, using genre as a tool to interpret and organize artistic output.",
        "Concept 3: The historical/theoretical framework of genres (e.g., hierarchy of genres and its institutional backing) shapes how genres are evaluated, organized, and taught within art history and criticism."
      ],
      "original_question_hash": "b028d502"
    },
    {
      "question": "How do the middle ear ossicles enable efficient transmission of sound from airborne waves into the fluid-filled cochlea, and which mechanical features implement this function?",
      "options": {
        "A": "They reduce energy loss by damping vibrations across the frequency spectrum, thereby preventing reflection at the air–fluid interface.",
        "B": "They convert airborne vibrations into neural signals within the middle ear by resonance of the tympanic membrane.",
        "C": "They overcome the air–fluid impedance mismatch by increasing pressure at the oval window, using the tympanic membrane–stapes area ratio $A_{TM}/A_{stapes}$ together with the lever action of the malleus–incus.",
        "D": "They selectively amplify high-frequency sounds through resonant tuning of the ossicular chain, improving sensitivity only at those frequencies."
      },
      "correct_answer": "C",
      "simplification_notes": "Reworded the original question for clarity and concision while retaining technical terms (impedance mismatch, area ratio, lever action). Converted area ratio to inline LaTeX $A_{TM}/A_{stapes}$. Kept all four options plausible and preserved the original correct answer.",
      "content_preserved": true,
      "source_article": "Hearing",
      "x": 1.6154621839523315,
      "y": 1.0913314819335938,
      "level": 2,
      "concepts_tested": [
        "Outer ear function and localization: the pinna and ear canal focus/filter sound and provide vertical localization cues due to asymmetry.",
        "Middle ear impedance matching: the ossicles (malleus, incus, stapes) overcome impedance mismatch between air and cochlear fluids to efficiently transmit vibrations.",
        "Inner ear transduction: the organ of Corti in the cochlea converts mechanical vibrations into neural impulses via hair cells, enabling auditory perception."
      ],
      "original_question_hash": "c90001c0"
    },
    {
      "question": "Why might an intended (planned) strategy be realized as emergent patterns of action rather than by strict, literal execution of a formal plan?",
      "options": {
        "A": "Because the operating environment is fully predictable, so managers can follow the written plan exactly without deviation.",
        "B": "Because individuals and organizational units interpret the plan using local information and constraints, learn from feedback, and adapt their behavior, producing recurring patterns that achieve the plan's goals.",
        "C": "Because formal plans are fundamentally defective and therefore must be abandoned in favor of ad-hoc decisions that disregard the original objectives.",
        "D": "Because emergent strategy requires stopping all planning and responding only with reactive moves to events in the market."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the stem to be concise for undergraduates, emphasized local interpretation, learning and adaptation as mechanisms for emergence; removed historical and tool-specific details while keeping the core contrast between intended and emergent strategy.",
      "content_preserved": true,
      "source_article": "Strategic planning",
      "x": 1.3688288927078247,
      "y": 0.9796237349510193,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The relationship and distinction between strategic planning (analytical, coordinating formulation and implementation) and strategy formation (synthesis, connecting the dots), and how they work together.",
        "Concept 2: Intended vs emergent strategy, including how a plan can be realized as patterns of activity in response to environment and competition.",
        "Concept 3: Planning maturity progression (financial planning → forecast-based planning → externally oriented planning → strategic management) and how each stage relates to strategic thinking and resource allocation."
      ],
      "original_question_hash": "a2709282"
    },
    {
      "question": "In the resource-based view, isolating mechanisms prevent valuable resources from spreading to rivals and so help sustain competitive advantage. Which statement best explains why these mechanisms are essential and how they operate?",
      "options": {
        "A": "They accelerate diffusion of resources among competitors, making those resources common and reducing differences between firms.",
        "B": "They restrict how easily competitors can copy, acquire, or substitute valuable resources, thereby preserving the firm’s distinctive resource bundle and its sustained advantage.",
        "C": "They convert tacit, firm-specific knowledge into easily codified assets so that imitation by rivals becomes trivial.",
        "D": "They rely primarily on formal legal and contractual protections (e.g., patents, exclusive contracts) to block competitors from accessing key resources."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened and jargon reduced while preserving technical terms. Options were rephrased to be concise and plausible distractors; the correct answer retained its original meaning.",
      "content_preserved": true,
      "source_article": "Resource-based view",
      "x": 1.3414074182510376,
      "y": 0.9724573493003845,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Resource heterogeneity across firms leads to different strategies and competitive positions.",
        "Concept 2: Internal resources (assets, capabilities, and competencies) are the source of competitive advantage; firms should identify and leverage them.",
        "Concept 3: Isolating mechanisms protect valuable resources from diffusion, helping sustain competitive advantage."
      ],
      "original_question_hash": "0d0713c6"
    },
    {
      "question": "Why does legitimacy matter for a political authority's capacity to act and implement its decisions?",
      "options": {
        "A": "Legitimacy is irrelevant to enforcement; an authority's capacity to act derives solely from its ability to coerce.",
        "B": "Legitimacy supplies a normative justification for obedience, so compliance is more voluntary and the authority can achieve goals beyond what coercion alone would allow.",
        "C": "Legitimacy confines authority to ceremonial or symbolic duties, preventing it from taking substantive action.",
        "D": "Legitimacy guarantees that all citizens will consent to every policy the authority adopts."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened and clarified (\"capacity to act\" → \"ability to implement decisions\"); language made more concise and academic; distractors rephrased to remain plausible while preserving the core idea that legitimacy increases voluntary compliance versus coercion.",
      "content_preserved": true,
      "source_article": "Authority",
      "x": 1.2396384477615356,
      "y": 0.842140793800354,
      "level": 2,
      "concepts_tested": [
        "Legitimacy as the basis for authority and its relation to the power to act",
        "Institutional frameworks and jurisdiction (branches of government, sovereignty) as mechanisms that structure and limit authority",
        "The relationship between authority, freedom, and political obligations (e.g., the idea of limited government)"
      ],
      "original_question_hash": "0e6ae8dd"
    },
    {
      "question": "How does the interdependence of people, organizations, and information systems influence how we design and use information systems?",
      "options": {
        "A": "By assuming technology alone determines outcomes, so design should optimize technical performance independently of users and organizational context.",
        "B": "By treating users as adapters who must fit the system after deployment, leaving organizational workflows largely unchanged.",
        "C": "By recognizing that user needs, organizational routines, and information flows form feedback loops, so design must jointly address tasks, policies, and the movement of data.",
        "D": "By prioritizing security and technical controls above all else, treating people and organizations mainly as constraints on system operation."
      },
      "correct_answer": "C",
      "simplification_notes": "Reduced length and jargon, clarified the core question, preserved the systems-thinking perspective; options reworded to be concise and plausible distractors.",
      "content_preserved": true,
      "source_article": "Information science",
      "x": 1.2587233781814575,
      "y": 0.9981171488761902,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Systems-thinking and stakeholder-centered problem solving (tackling systemic problems rather than isolated technologies)",
        "Concept 2: Transdisciplinary integration across technical, organizational, and human dimensions (bridging informatics, library science, HCI, ethics, etc.)",
        "Concept 3: Interdependence of people, organizations, and information systems (how user needs, organizational contexts, and information flow shape design and use)"
      ],
      "original_question_hash": "3940a6c4"
    },
    {
      "question": "According to the \"front doors\" principle in information architecture, a website should offer multiple entry points to its content. Why does providing multiple \"front doors\" improve findability for users who have different goals?",
      "options": {
        "A": "It funnels users into a single top-level navigation path, reducing apparent complexity by limiting the routes they can take.",
        "B": "It creates varied, goal-aligned entry points that match different users' mental models and let them access relevant content directly.",
        "C": "It forces all users to rely on a single global search box so every item is discoverable through one interface.",
        "D": "It removes the need for clear labeling because users can randomly browse until they eventually find what they want."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the stem; preserved technical terms like \"findability\" and \"mental models\"; kept all four options plausible while retaining the original correct answer (B).",
      "content_preserved": true,
      "source_article": "Information architecture",
      "x": 1.3886654376983643,
      "y": 1.050351858139038,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Information architecture as the structural design that uses organization, labeling, search, and navigation to improve usability and findability in information environments.",
        "Concept 2: The Principles of information architecture (objects, choices, disclosure, exemplars, front doors, multiple classification, focused navigation, growth) as a design framework guiding how information should be organized and accessed.",
        "Concept 3: IA as an interdisciplinary, cross-domain framework linking design, architecture, and information science, and its role across web, information systems, data architecture, and enterprise contexts."
      ],
      "original_question_hash": "ed8f9514"
    },
    {
      "question": "How does creating a broad master plan at the start of a landscape architecture project help ensure that built (hard) and planted (soft) systems work together later in the process?",
      "options": {
        "A": "It defines spatial relationships and performance criteria early, which guide later technical decisions so infrastructure, drainage, vegetation and public spaces are coordinated into a coherent system.",
        "B": "It removes all uncertainties so no further design or technical coordination is needed later in the project.",
        "C": "It focuses primarily on visual composition at the outset, leaving structural, engineering, and ecological coordination to later stages.",
        "D": "It concentrates on reducing short-term construction costs, while ignoring long-term ecological and functional constraints."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified; ‘built’ and ‘planted’ explained as ‘hard’ and ‘soft’; options rewritten to be concise and plausible without changing the tested concept.",
      "content_preserved": true,
      "source_article": "Landscape architecture",
      "x": 0.7025012373924255,
      "y": 0.9748724699020386,
      "level": 2,
      "concepts_tested": [
        "Interdisciplinary integration: Landscape architecture combines urban design, ecology, engineering, horticulture, psychology, and more to achieve multiple outcomes.",
        "Early concept/master planning as pivotal: The initial concept and master plan drive subsequent design, technical work, and implementation, highlighting a cause–effect relationship between early ideas and project outcomes.",
        "Built/green system integration and sustainability: The discipline balances hard (built) and soft (planted) elements and emphasizes ecological sustainability in design decisions."
      ],
      "original_question_hash": "663592b0"
    },
    {
      "question": "How does embedding the network of relationships on a site plan—property boundaries, setbacks, rights-of-way, utilities, and the placement of buildings, roads, and landscape features—affect the design process?",
      "options": {
        "A": "It lets designers explore aesthetic options unconstrained, postponing regulatory and infrastructure checks to later stages.",
        "B": "It enables early multidisciplinary coordination so siting, circulation, and landscape decisions align with infrastructure and regulations, reducing costly changes later.",
        "C": "It replaces the need for separate technical studies, meaning no further analysis is required once the plan is drawn.",
        "D": "It locks in a single arrangement that cannot be adapted if regulations or site conditions change."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording condensed for clarity while retaining technical terms (boundaries, setbacks, rights-of-way, utilities). Question reframed to ask about the effect on the design process; distractors kept plausible and aligned with common misconceptions.",
      "content_preserved": true,
      "source_article": "Site plan",
      "x": 1.3776278495788574,
      "y": 0.8506210446357727,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Site analysis as a data-driven precursor that determines where and how development should occur (e.g., avoiding floodplains or steep slopes, leveraging favorable conditions).",
        "Concept 2: The site plan as a map of relationships among built form, infrastructure, landscape elements, and regulatory constraints (boundaries, setbacks, rights of way) that shapes design decisions.",
        "Concept 3: Multidisciplinary integration and representation (architectural, landscape, engineering, urban planning inputs) expressed through a scaled top-down drawing to coordinate improvements."
      ],
      "original_question_hash": "c2a5f47f"
    },
    {
      "question": "According to Mackinder's Heartland Theory, why would control of the Eurasian interior (the \"Heartland\") confer global dominance, and how does that mechanism contrast with the logic of sea power?",
      "options": {
        "A": "Because control of the Heartland secures contiguous overland mobility and access to vast hinterland resources that are less vulnerable to naval blockade, enabling projection of power across continents; this contrasts with sea-power logic, which emphasizes control of sea lanes and naval reach to project power globally.",
        "B": "Because the interior is sparsely populated and therefore easier to govern from afar, a dynamic that parallels sea power’s reliance on distant coaling and supply stations.",
        "C": "Because land power is inherently superior in all terrains, making maritime routes and naval supremacy strategically irrelevant.",
        "D": "Because modern technology has largely nullified geographic constraints, so control of the Heartland does not provide a distinctive strategic advantage."
      },
      "correct_answer": "A",
      "simplification_notes": "Shortened and clarified the prompt, defined 'Heartland', and emphasized the contrast between overland/resource-based power and sea-lane/naval-based power; removed extended historical context while keeping the causal mechanism.",
      "content_preserved": true,
      "source_article": "Political geography",
      "x": 1.2513872385025024,
      "y": 0.9449913501739502,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Spatial structures and political processes are interrelated; geography shapes political outcomes and politics shapes spatial organization.",
        "Concept 2: Heartland Theory as a geopolitical framework: control of the Eurasian heartland yields global power, contrasting with sea-power paradigms.",
        "Concept 3: Environmental determinism and Lebensraum as mechanisms linking the physical environment to state expansion and imperialist policy."
      ],
      "original_question_hash": "305cf0d8"
    },
    {
      "question": "According to Marx's historical materialism, how does the economic \"base\" bring about changes in politics, law, and culture (the \"superstructure\"), and what mechanism links them?",
      "options": {
        "A": "The superstructure (ideas, culture, institutions) drives economic change: cultural norms and political programs reshape production methods and thus reconfigure the base.",
        "B": "The economic base—especially the relations of production and property—produces class interests and power blocs that dominate political and legal institutions and create dominant ideologies, thereby shaping the superstructure.",
        "C": "The base and the superstructure influence each other equally and independently, with no single dominant causal direction.",
        "D": "The base affects only technical tools and productive technology; politics and culture develop autonomously from ideas rather than being determined by economic relations."
      },
      "correct_answer": "B",
      "simplification_notes": "Made wording more concise and academic; clarified 'relations of production' as the causal mechanism (class interests and power blocs); removed historical examples and extra exposition while keeping the core causal claim intact.",
      "content_preserved": true,
      "source_article": "Marxism",
      "x": 0.9139242768287659,
      "y": 0.9605815410614014,
      "level": 2,
      "concepts_tested": [
        "Historical materialism and the base–superstructure model as mechanisms linking economy to all other aspects of society",
        "Class struggle and exploitation as engines of social conflict and change",
        "Internal contradictions of capitalism leading to crises and a potential proletarian revolution toward socialism/communism"
      ],
      "original_question_hash": "cf56ef5f"
    },
    {
      "question": "How do remedial measures such as quotas address structural discrimination, and what principal controversy do they typically provoke?",
      "options": {
        "A": "They create targeted opportunities (for jobs, education, or representation) to counteract entrenched disparities and rebalance access to resources; the main controversy is that they are perceived as unfair to non-targeted groups and provoke debates about merit and reverse discrimination.",
        "B": "They replace merit-based selection with automatic advancement for members of the targeted group, thereby eliminating all inequities but generating controversy about competence and lowered standards.",
        "C": "They concentrate on changing individual attitudes rather than altering institutional opportunity structures, so the main dispute is about the effectiveness of persuasion rather than questions of fairness.",
        "D": "They instantly eliminate all legal and institutional barriers to equality, thereby solving structural discrimination without producing any public opposition or disagreement."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened and academic jargon reduced; kept focus on quotas as structural remedies and their controversy over fairness/merit. Distractor options were kept plausible but clearly incorrect or extreme.",
      "content_preserved": true,
      "source_article": "Discrimination",
      "x": 1.185056447982788,
      "y": 0.9044979810714722,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Discrimination as a causal process that leads to deprivation or unfair treatment of groups based on perceived characteristics.",
        "Concept 2: Structural and policy-level nature of discrimination, including how laws, traditions, and policies encode discriminatory practices and how remedial measures (e.g., quotas) aim to address them—and the controversies they provoke.",
        "Concept 3: The normative frameworks for evaluating discrimination (moralized vs non-moralized definitions) and how those frameworks shape our understanding of whether discrimination is inherently wrong."
      ],
      "original_question_hash": "21686d62"
    },
    {
      "question": "How do iconography and formal analysis work together to produce meaning in an artwork, and why can the same formal arrangement (composition, line, color, etc.) convey different meanings in different historical contexts?",
      "options": {
        "A": "The formal arrangement encodes universal psychological truths, so historical or cultural context does not affect meaning.",
        "B": "Iconography supplies culturally specific symbols and conventions that viewers bring to an artwork, while formal elements (line, color, composition) shape how those symbols are perceived; together they allow the same formal arrangement to signal different ideas in different contexts.",
        "C": "Formal analysis alone determines meaning because form is self-contained and independent of symbolism, history, or audience.",
        "D": "Iconography and formal analysis are entirely separate, so meaning comes only from the literal subject or narrative depicted, not from form or symbolic conventions."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the wording, clarified examples of formal elements (composition, line, color), and made the roles of iconography and formal analysis explicit while preserving the original distinctions and correct answer.",
      "content_preserved": true,
      "source_article": "Art history",
      "x": 1.0869985818862915,
      "y": 1.027099847793579,
      "level": 2,
      "concepts_tested": [
        "Contextual analysis and relationships: how artworks relate to their historical, social, political, and economic contexts, including creators and patrons influencing meaning.",
        "The role of theoretical frameworks: how approaches like historical materialism or feminist/critical theory shape interpretation and reveal power, ideology, and cultural dynamics.",
        "Iconography and formal analysis as meaning-making: how symbols, motifs, line, color, and composition contribute to meanings within specific historical contexts."
      ],
      "original_question_hash": "af44940c"
    },
    {
      "question": "According to the \"Pictorial Turn,\" why do images—rather than verbal language—become central for understanding culture, and how does technological mediation contribute to that shift?",
      "options": {
        "A": "It claims images are inherently more truthful than texts, so cultural interpretation should prioritize visual material.",
        "B": "It argues images act as active mediators: they shape perception, produce meaning, and circulate through visual technologies, thereby reconfiguring how knowledge is produced and shared.",
        "C": "It maintains that language remains the primary framework for meaning, with images merely reflecting pre-existing textual ideas.",
        "D": "It contends that visual culture ignores social and political context and concerns itself only with aesthetic form."
      },
      "correct_answer": "B",
      "simplification_notes": "Removed historical background and lengthy quotes; tightened wording to focus on the two core points: images as primary vectors (Pictorial Turn) and the role of visual technologies in mediating perception and knowledge. Language made concise and academic for undergraduates.",
      "content_preserved": true,
      "source_article": "Visual culture",
      "x": 1.1677789688110352,
      "y": 1.010263442993164,
      "level": 2,
      "concepts_tested": [
        "The mediation of visual experience by technology: how visual technologies alter perception, meaning, and the production/dissemination of images.",
        "The Pictorial Turn / iconic turn as paradigm shifts: why images, rather than language, become central to understanding culture and knowledge.",
        "Interdisciplinary scope and conceptual framework of visual culture: how visual culture connects with multiple fields and treats “visual events” as interfaces where information, meaning, or pleasure are produced."
      ],
      "original_question_hash": "f6083db6"
    },
    {
      "question": "Why is the differential $df_{x_0}$ regarded as the best linear approximation to a differentiable function $f$ near the point $x_0$?",
      "options": {
        "A": "Because for small increments $h$, $f(x_0+h)=f(x_0)+df_{x_0}(h)+o(\\|h\\|)$, so $df_{x_0}$ captures the first-order change while higher-order terms are negligible compared with $\\|h\\|$.",
        "B": "Because it minimizes the maximum error of any linear approximation over a fixed small neighborhood around $x_0$.",
        "C": "Because it is determined by the second derivatives of $f$ and therefore encodes the curvature of $f$ near $x_0$.",
        "D": "Because it is the exact inverse of the Jacobian matrix at $x_0$, providing a perfect linear correction."
      },
      "correct_answer": "A",
      "simplification_notes": "Rephrased the question for clarity at undergraduate level, converted math to inline LaTeX, kept original correct option (A). Distractors were kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Linear algebra",
      "x": 1.6785765886306763,
      "y": 1.1969066858291626,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Linear equations, linear maps, and their representations in vector spaces and matrices are interconnected aspects of modeling linear relationships.",
        "Concept 2: Linear algebra functions as a unifying framework linking algebra, geometry, and analysis (including its role in geometry and as a basis for function spaces in functional analysis).",
        "Concept 3: Local linear approximation of nonlinear systems through the differential, i.e., the linear map that best approximates a function near a point."
      ],
      "original_question_hash": "40076b99"
    },
    {
      "question": "How do checks and balances prevent the concentration of state power, and what is the primary mechanism by which they work?",
      "options": {
        "A": "By assigning each branch wholly exclusive powers and preventing any institutional influence between them, thus guaranteeing total independence.",
        "B": "By requiring inter-branch consent, vetoes, or oversight so that any attempt to expand power must overcome resistance from other branches; this creates bargaining costs that deter unilateral power grabs.",
        "C": "By placing final constitutional authority in a single supreme court that unilaterally decides the legality of actions by other branches.",
        "D": "By eliminating separate deliberation and merging functions into a single, faster fused process that avoids inter-branch conflict."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the stem to ask how checks and balances prevent power concentration; specified the mechanism as inter-branch consent/veto/oversight and 'bargaining costs'. Distractors were reworded to remain plausible while preserving the original concepts.",
      "content_preserved": true,
      "source_article": "Separation of powers",
      "x": 1.180764079093933,
      "y": 0.7746780514717102,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Distinction of state functions as a constitutional principle (legislative, executive, judicial) to preserve the integrity of each function and prevent conflation of powers.",
        "Concept 2: Mechanisms of checks and balances (inter-branch independence, reciprocal constraints, and the idea of fusion vs. separation) to prevent the concentration or abuse of power.",
        "Concept 3: Historical and theoretical foundations (Polybius, Locke, and the evolution toward a tripartite system) that explain why the model emerged and how it is applied in practice (e.g., early English instruments, development of judicial separation)."
      ],
      "original_question_hash": "56245d1a"
    },
    {
      "question": "In a homeostatic control system, why does negative feedback provide stability, and how does increasing the system's $gain$ change its response to a larger disturbance?",
      "options": {
        "A": "Because sensors amplify deviations and effectors further drive the change, making the system more sensitive to perturbations.",
        "B": "Because sensors detect the deviation and effectors act to oppose it, returning the variable toward its set point; increasing the system's $gain$ reduces residual error and speeds recovery after larger disturbances.",
        "C": "Because the set point simply shifts toward the perturbation, so the system no longer needs to correct the change.",
        "D": "Because external disturbances always push the system away and the feedback loop cannot oppose them, so negative feedback does not stabilize the variable."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording shortened and clarified: kept focus on negative feedback, sensors/effectors roles, and the effect of increasing system $gain$ on residual error and recovery speed. Technical terms (homeostasis, gain, residual error) retained for precision.",
      "content_preserved": true,
      "source_article": "Physiology",
      "x": 1.9079105854034424,
      "y": 1.1079388856887817,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Structure–function interdependence in physiology (anatomy dictates function and vice versa)",
        "Concept 2: Homeostasis and control mechanisms as central to physiological stability",
        "Concept 3: Multilevel organization and integration (from biomolecules to populations) and intercellular communication"
      ],
      "original_question_hash": "297b55e0"
    },
    {
      "question": "In an experiment testing whether fertilizer concentration (the independent variable) affects plant growth, researchers keep watering, light, soil type, and plant age identical across treatment groups. Why is holding these factors constant essential for inferring that fertilizer caused any observed growth differences, and what risk arises if one of those other factors differs between groups?",
      "options": {
        "A": "Holding those factors constant isolates the effect of fertilizer so observed growth differences can be attributed to fertilizer concentration; if another factor varied, that factor could be responsible for the differences, producing a confounded result.",
        "B": "Keeping conditions identical improves the experiment's presentation and reduces reviewer skepticism, increasing the likelihood of publishing positive results.",
        "C": "It guarantees no growth differences will be observed, because with all variables the same across groups only fertilizer can vary and must produce identical outcomes.",
        "D": "It eliminates the need for replication or further statistical controls, since constant conditions provide a definitive single result."
      },
      "correct_answer": "A",
      "simplification_notes": "Clarified that fertilizer is the independent variable; condensed wording; made the question explicitly ask both why controls are needed and what risk arises if they vary; rewrote all options into concise, plausible statements while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Experiment",
      "x": 1.3560352325439453,
      "y": 1.0534862279891968,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Cause-and-effect through variable manipulation and controls (why/how manipulating an independent variable with controls isolates its effect).",
        "Concept 2: The role of experiments in testing hypotheses within the scientific method (why/how experiments can support or disprove hypotheses and why they cannot prove them).",
        "Concept 3: Variability in experimental design (natural vs. highly controlled experiments) and the importance of repeatability and logical analysis for reliability."
      ],
      "original_question_hash": "639a2313"
    },
    {
      "question": "In inductive generalization, why does biased sampling weaken the strength of a conclusion about a population?",
      "options": {
        "A": "Because biased sampling increases sampling variability and random error, reducing the precision of the estimate.",
        "B": "Because biased sampling introduces systematic differences between the sample and the population, so the sample statistic is not a reliable estimate of the population parameter, weakening the generalization.",
        "C": "Because biased sampling prevents the use of standard inferential tools (e.g., valid margins of error and confidence intervals), making the numerical estimate unreliable.",
        "D": "Because biased sampling mainly distorts causal inferences and therefore does not significantly affect simple descriptive inductive generalizations."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording made more concise and direct for undergraduates; removed extended examples and historical context; retained core idea that representativeness determines inductive strength. Options rewritten to be plausible distractors while preserving the correct choice.",
      "content_preserved": true,
      "source_article": "Inductive reasoning",
      "x": 1.435678243637085,
      "y": 1.106186032295227,
      "level": 2,
      "concepts_tested": [
        "Inductive generalization: inferring population attributes from sample data; strength depends on sample size and representativeness.",
        "Probabilistic inference mechanisms: Bayesian updating and maximum likelihood estimation as methods to quantify uncertainty and identify the most likely population distribution given observed data.",
        "Fallacies and limitations: hasty generalization and biased sampling as factors that weaken or invalidate inductive conclusions."
      ],
      "original_question_hash": "30b64cf1"
    },
    {
      "question": "Determinants of health such as poverty, migration, trade, and environmental change cross national borders. Why does addressing these transnational determinants require coordinated global action to improve health equity?",
      "options": {
        "A": "Because these cross-border determinants create interdependencies: policies or conditions in one country can affect health outcomes in others, so aligned international action and policy are needed to reduce inequities.",
        "B": "Because if every country simply adopted the same medical technologies, health equity would automatically be achieved without any cross-border coordination.",
        "C": "Because global action is primarily about providing financial aid, which has little ability to change the social and structural determinants inside countries.",
        "D": "Because health equity is determined entirely by a nation's income level, so international coordination cannot change health outcomes."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording was shortened and clarified for undergraduate readers; examples (poverty, migration, trade, environmental change) were kept; distractors were rewritten to remain plausible but incorrect; core concept of cross-border interdependence requiring coordinated action was preserved.",
      "content_preserved": true,
      "source_article": "Global health",
      "x": 1.208701252937317,
      "y": 0.888480544090271,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Determinants and distribution of health across borders influence health equity and require global, coordinated responses (why/how do socioeconomic and cross-border factors shape global health outcomes?).",
        "Concept 2: Interdisciplinary governance and collaboration among medicine, public health, economics, social sciences, and international organizations shape global health interventions (how do different disciplines and organizations interact to address health challenges?).",
        "Concept 3: One Health and global health security as integrative frameworks that connect human, animal, and environmental health to prevent and respond to global threats (how does integrating multiple domains improve global health security?)."
      ],
      "original_question_hash": "de2a9e41"
    },
    {
      "question": "Open data is often said to improve reproducibility. How does making data open interact with reproducibility, and what additional condition must accompany open data to enable others to reproduce the work?",
      "options": {
        "A": "Open data by itself guarantees reproducibility simply by being available to anyone.",
        "B": "Open data improves reproducibility only when it is accompanied by thorough metadata, method documentation, and clear data provenance so others can understand and reuse it correctly.",
        "C": "Open data reduces reproducibility because public access increases the risk of misinterpretation and errors by nonexperts.",
        "D": "Open data improves reproducibility only if access is restricted to peer-reviewed projects to prevent misuse."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem to be concise and direct for undergraduates; kept technical accuracy. Options were shortened and made equally plausible while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Open science",
      "x": 1.291959524154663,
      "y": 1.0173134803771973,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3"
      ],
      "original_question_hash": "57857a08"
    },
    {
      "question": "According to the social-constructionist view of identity, why do people often behave in line with group norms even when their private preferences differ?",
      "options": {
        "A": "Personal autonomy predominates; group norms have little causal influence on behaviour.",
        "B": "Because group membership activates normative expectations and potential social sanctions, and people regulate themselves to preserve a coherent self-concept, leading them to conform.",
        "C": "Social constructions are merely descriptive labels and therefore do not affect how people behave.",
        "D": "Identity is fixed and unchanging, so behaviour remains the same regardless of group norms."
      },
      "correct_answer": "B",
      "simplification_notes": "Made the question more concise and direct for undergraduates; clarified mechanisms (norms, sanctions, self-concept, self-regulation) while keeping original meaning and correct answer. Distractors were kept plausible but shortened.",
      "content_preserved": true,
      "source_article": "Identity (social science)",
      "x": 1.240106463432312,
      "y": 0.9962090849876404,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Identity as a self-regulatory structure that provides meaning, direction, and behavioral guidance, influencing adaptation and long-term goals.",
        "Concept 2: The social construction of identity, including the interplay between personal identity and collective/group identities shaped by social/cultural factors and how these identities guide behavior.",
        "Concept 3: The dynamic, context-dependent nature of identity—its stability and continuity alongside situational change across life stages, as illustrated by psychosocial development theories (e.g., Erikson)."
      ],
      "original_question_hash": "b6cc4ef9"
    },
    {
      "question": "In prophase I of meiosis homologous chromosomes exchange segments (crossing over). How does crossing over increase genetic variation in gametes beyond the variation produced by independent assortment of whole chromosomes?",
      "options": {
        "A": "It produces recombinant chromatids that carry new combinations of alleles from both homologues on the same chromosome, increasing the variety of genotypes passed to offspring.",
        "B": "It changes the total number of chromosomes that end up in each gamete, altering ploidy.",
        "C": "It generates novel mutant alleles that were not present in either parent.",
        "D": "It causes maternal and paternal whole chromosomes to align and segregate more randomly during meiosis I."
      },
      "correct_answer": "A",
      "simplification_notes": "Language simplified and sentence structure shortened; 'crossing over' clarified as segment exchange; technical terms (recombinant chromatids, alleles) retained; distractors rephrased to be concise and plausible.",
      "content_preserved": true,
      "source_article": "Reproduction",
      "x": 1.9905306100845337,
      "y": 1.1460233926773071,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Mechanisms of sexual reproduction—how meiosis produces haploid gametes, how fertilization combines parental genomes, and how these processes generate genetic variation in offspring.",
        "Concept 2: Mechanisms of asexual reproduction—how mitosis-based processes (binary fission, budding, fragmentation, parthenogenesis, vegetative reproduction) produce genetically identical or nearly identical offspring without genetic input from another parent.",
        "Concept 3: Evolutionary trade-offs of reproduction modes—why sexual reproduction persists despite costs (e.g., two-fold cost) and how genetic variation from sex can confer adaptive advantages."
      ],
      "original_question_hash": "d735d21d"
    },
    {
      "question": "For a mathematical model formulated with differential equations, why do the initial and boundary conditions matter for its predictive power?",
      "options": {
        "A": "They fix the system's starting state and external constraints, which determine a unique, physically meaningful solution that can be used for prediction.",
        "B": "They only influence numerical stability of solvers and do not change the model's predicted behavior.",
        "C": "They are arbitrary choices that do not alter the differential-equation solution.",
        "D": "They transform a nonlinear differential system into a linear one, making prediction simpler."
      },
      "correct_answer": "A",
      "simplification_notes": "Rephrased the question in concise undergraduate-level language; made each option plausible but distinct; retained emphasis that initial/boundary conditions provide uniqueness and physical relevance of solutions.",
      "content_preserved": true,
      "source_article": "Mathematical model",
      "x": 1.5765589475631714,
      "y": 1.1384687423706055,
      "level": 2,
      "concepts_tested": [
        "A mathematical model is an abstract, math-based representation of a system used to characterize, predict, and solve problems, with quality linked to agreement with experimental results.",
        "Model structure comprises governing/defining equations, constitutive/sub-models, assumptions/constraints, and initial/boundary conditions, illustrating how a model encodes a system’s rules and limits.",
        "Linearity vs. nonlinearity is context-dependent and has implications for decomposability and for phenomena like chaos and irreversibility."
      ],
      "original_question_hash": "159901f0"
    },
    {
      "question": "Why does calibrating radiocarbon dates with dendrochronology data produce more accurate calendar ages?",
      "options": {
        "A": "Dendrochronology gives exact calendar-year ages for individual tree rings, so radiocarbon ages can be read directly as calendar years.",
        "B": "Atmospheric $^{14}\\mathrm{C}$ concentrations have varied through time; dendrochronology provides a year-by-year calendar sequence of tree rings whose measured $^{14}\\mathrm{C}$ values produce a calibration curve that converts radiocarbon years into true calendar years.",
        "C": "Dendrochronology changes the decay rate of $^{14}\\mathrm{C}$ to reflect climate conditions, thereby correcting radiocarbon ages.",
        "D": "Combining the two methods simply averages their independent results, reducing random error but not correcting systematic variations in atmospheric $^{14}\\mathrm{C}$."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the question; emphasized that dendrochronology provides an annual tree-ring sequence used to build a calibration curve; added $^{14}\\mathrm{C}$ notation; kept all four options plausible and preserved the original correct answer.",
      "content_preserved": true,
      "source_article": "Chronology",
      "x": 0.04421889781951904,
      "y": 0.5079593062400818,
      "level": 2,
      "concepts_tested": [
        "Calibration and interdependence of dating methods: how dendrochronology provides calibration curves for radiocarbon dating to produce accurate ages.",
        "Relationship between calendar systems and dating of events: how Julian vs. Gregorian calendars and the Anno Domini era affect the dating of historical events.",
        "Chronology as an integration of timekeeping, historiography, and era systems: how time measurement, historical writing, and calendar eras together establish the actual temporal sequence of events."
      ],
      "original_question_hash": "4e9d0034"
    },
    {
      "question": "How do checks and balances keep elected representatives accountable in a representative democracy, and under what circumstances can these constraints fail?",
      "options": {
        "A": "They depend on the executive having exclusive lawmaking control so it can enforce compliance quickly.",
        "B": "Authority is distributed across separate institutions (executive, legislature, judiciary) that can check one another; constitutional rules, judicial review, and where available direct-democratic tools (initiative, referendum, recall) add restraints. These mechanisms work best when institutions are formally independent, the rule of law is enforced, rules are clear, and political norms encourage restraint; they fail when institutions are weak or captured, oversight is absent, or a dominant party undermines independence.",
        "C": "No separate institutions are needed; periodic elections alone provide continuous accountability and therefore fully constrain representatives.",
        "D": "Constraints depend on a single party controlling all branches so it can uniformly implement mandates without internal resistance, ensuring compliance."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was shortened and clarified; technical detail about judicial review, initiatives/referenda, and failure conditions (weak institutions, capture, dominant party) was preserved; historical examples and excess background were removed.",
      "content_preserved": true,
      "source_article": "Representative democracy",
      "x": 1.1478867530822754,
      "y": 0.8541262745857239,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Representation as delegated authority and accountability — elected representatives act on behalf of the people, with accountability and power constrained by a constitution and checks and balances.",
        "Concept 2: Checks and balances as mechanisms to limit power — independent judiciary, constitutional rules, and potential direct democratic instruments that influence how representative power is exercised.",
        "Concept 3: The role of parties and electoral systems in shaping governance — how party-centric elections affect representation, accountability, and the functioning of a multi-party system within a representative framework (and related idea of polyarchy)."
      ],
      "original_question_hash": "c7ffe6df"
    },
    {
      "question": "How do checks and balances operate to limit power and promote accountability among the branches of government?",
      "options": {
        "A": "They require every policy decision to receive simultaneous approval from all branches, producing unanimous consent for action.",
        "B": "They allocate powers so that each branch can review, constrain, or sometimes override the actions of the others, creating interdependence and institutional accountability.",
        "C": "They concentrate all decision-making authority in a single branch, with the other branches only able to provide advice after decisions are made.",
        "D": "They prioritize rapid, largely unchecked action by the executive branch to ensure efficient government operations."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the question to a single clear sentence; removed historical and comparative detail; preserved focus on power distribution and mechanisms (review, constrain, override) that produce accountability; made all options plausible alternatives.",
      "content_preserved": true,
      "source_article": "Separation of powers",
      "x": 1.1859577894210815,
      "y": 0.7746538519859314,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Functional differentiation of state power (legislative, executive, judicial/administrative) and the need for clear allocation to prevent fusion and preserve the integrity of each function.",
        "Concept 2: Mechanisms of checks and balances and structural independence that constrain power and enable accountability among branches.",
        "Concept 3: Variants and evolution of the model (triangular/political system, unity/fusion vs. separation, historical development) showing how the principle is implemented differently across contexts."
      ],
      "original_question_hash": "a19d3325"
    },
    {
      "question": "During an outbreak, how does switching from passive to active public health surveillance change the data collection method, timeliness, completeness, and resource use, and why is active surveillance commonly selected in outbreaks?",
      "options": {
        "A": "Active surveillance reduces operational costs and accelerates reporting but leads to poorer data completeness.",
        "B": "Passive surveillance consists of proactive case-finding and detailed record review, which improves timeliness but is resource-intensive.",
        "C": "Active surveillance uses proactive, systematic data collection (e.g., visiting facilities and reviewing records) that improves timeliness and completeness but requires substantially more resources; this is why it is preferred for epidemics or elimination campaigns.",
        "D": "Passive surveillance automatically captures every case through routine reporting, providing maximal completeness and the fastest possible response."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording was tightened and made more direct; retained focus on surveillance type, effects on timeliness/completeness/resources, and rationale for choosing active surveillance. Technical examples (visiting facilities, record review) preserved.",
      "content_preserved": true,
      "source_article": "Public health surveillance",
      "x": 1.318280816078186,
      "y": 1.0027111768722534,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The surveillance cycle and its role in informing public health action (data collection → analysis → interpretation → planning/implementation/evaluation → feedback).",
        "Concept 2: Passive vs. active surveillance and the trade-offs/appropriate contexts for each (cost, time, completeness, and epidemic response).",
        "Concept 3: The integration of diverse data sources and informatics (cancer/disease registries, syndromic surveillance, automated adverse event reporting) and how these support detection, monitoring, and policy decisions."
      ],
      "original_question_hash": "c0dcbfa0"
    },
    {
      "question": "In a value chain, if inputs and the final product remain the same, how does performing a primary activity more efficiently affect costs and profits?",
      "options": {
        "A": "It reduces the cost per unit of that activity (lowering total costs), which increases profit if the selling price is unchanged.",
        "B": "It raises the product's inherent or perceived value to customers, allowing the firm to charge a higher price.",
        "C": "It changes the legal or contractual sequence of steps so a different party becomes responsible for input costs.",
        "D": "It renders earlier activities irrelevant to the final price because the last activity dominates customer value."
      },
      "correct_answer": "A",
      "simplification_notes": "Question shortened and clarified for undergraduate readers; preserved 'primary activity' and 'value chain' terms; options rephrased to be concise and plausible alternatives.",
      "content_preserved": true,
      "source_article": "Value chain",
      "x": 1.3754098415374756,
      "y": 0.9778385162353516,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Value is created through a chain of activities that add value at each step, with cumulative value arising from the sequence.",
        "Concept 2: The value chain is a system with inputs, transformation processes, and outputs; how activities are performed determines costs and profits.",
        "Concept 3: Competitive advantage can be achieved by optimizing any of the primary activities (inbound logistics, operations, outbound logistics, marketing and sales, service), and the appropriate analytical level is the business unit rather than the entire company."
      ],
      "original_question_hash": "5a4c2096"
    },
    {
      "question": "How do accreditation bodies, professional associations, and codes of conduct enforce standards and conformity within a profession?",
      "options": {
        "A": "They centralize decision-making so uniform, fast judgments can be applied to all cases, regardless of context.",
        "B": "They set common standards, monitor members' compliance, and impose sanctions for violations, which aligns individual practice with collective norms and legitimizes the profession's authority.",
        "C": "They mainly function as marketing or reputation tools that let professionals demand higher fees without substantive changes to practice.",
        "D": "They weaken public oversight by insulating professionals from external scrutiny and accountability."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording shortened and clarified for undergraduates; preserved the role of credentialing, monitoring, sanctions, occupational closure, and professional authority. Distractors made plausible and concise.",
      "content_preserved": true,
      "source_article": "Professionalization",
      "x": 1.2780810594558716,
      "y": 0.9576087594032288,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Occupational closure / credentialism as a mechanism that restricts entry to a profession and defines who is qualified.",
        "Concept 2: Accreditation, professional associations, and codes of conduct as instruments to enforce standards and conformity within a profession.",
        "Concept 3: The social power and status dynamics created by professionalization, including the elite positioning of professionals and their autonomous relationship with society."
      ],
      "original_question_hash": "43f9a3fb"
    },
    {
      "question": "What role does the idea of \"disinterested pleasure\" play in aesthetic judgment, and what problem is it supposed to solve when we move from a subject's personal experience to claims about an artwork's value?",
      "options": {
        "A": "It holds that aesthetic judgments should be based on responses to an artwork's formal and perceptual features that are detached from personal desires or practical interests, thereby providing a basis for comparing and criticizing judgments across different observers.",
        "B": "It requires that aesthetic judgments be certified by credentialed experts or institutions to make them objective and standardized.",
        "C": "It claims that aesthetic value is determined only by the intensity of an observer's emotional reaction, so judgments depend entirely on transient moods.",
        "D": "It asserts that an aesthetic judgment is legitimate only if it aligns with the artist's stated intentions or declared meaning."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording simplified for clarity: defined \"disinterested pleasure\" as detachment from desires/practical concerns and emphasized its role in making subjective responses comparable; distractor options made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Aesthetics",
      "x": 1.1276756525039673,
      "y": 1.0717506408691406,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The objectivity vs subjectivity of aesthetic properties and judgments, including the idea of disinterested pleasure in aesthetic experience.",
        "Concept 2: The relationship between artworks and interpretation/criticism—how meaning, emotion, and authorial intent are identified and analyzed.",
        "Concept 3: The interdisciplinary and cross-cultural relationships of aesthetics—how aesthetics relates to ethics, religion, psychology, and comparative traditions."
      ],
      "original_question_hash": "0fda0267"
    },
    {
      "question": "How does a state's monopoly on the legitimate use of physical force enable a centralized authority to form and make the state distinct from households or kinship groups?",
      "options": {
        "A": "Because it keeps coercive power spread among many groups, preventing a stable central authority and forcing governance to remain local.",
        "B": "Because it creates a credible, centralized enforcement capacity that can back rules across the territory, enabling unified coordination and making the state the sole legitimate wielder of coercion.",
        "C": "Because it lets individuals choose which group enforces rules for them, producing flexible but inconsistent enforcement across communities.",
        "D": "Because it legitimizes violence by permitting local groups to decide what counts as legitimate force, removing a common central standard."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording reduced and focused on Weber/Tilly idea of monopoly of legitimate force; removed broader historical mechanisms and background while keeping the core causal question and plausible distractors.",
      "content_preserved": true,
      "source_article": "State formation",
      "x": 1.2005034685134888,
      "y": 0.9014818072319031,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Monopoly of legitimate use of force as a defining feature of the state (Weber). How does this capacity enable centralized authority and distinguish the state from other organizations?",
        "Concept 2: Multiple mechanisms driving state formation (warfare, commerce, contracts, cultural diffusion). How do these mechanisms interact to produce centralized institutions, and why might their prominence vary across historical periods?",
        "Concept 3: State as a coercive, territorially bounded organization distinct from households/kinship groups (autonomy and differentiation). Why is centralized coercion and territoriality essential for state functioning?"
      ],
      "original_question_hash": "b4e3b916"
    },
    {
      "question": "In a hierarchical type system (as used to avoid Russell's paradox), which structural rule most directly prevents the formation of \"the set of all sets that do not contain themselves\"?",
      "options": {
        "A": "Elements can only belong to collections of a strictly higher type, so no collection can be a member of itself; this blocks the self-reference that produces the paradox.",
        "B": "All collections are required to be finite, so the infinite self-containing universal collection needed for the paradox cannot exist.",
        "C": "The system bans any self-referential definitions by forbidding predicates from referring to their own truth value, preventing such constructions.",
        "D": "The theory postulates a single universal type that contains every object, and this single-type view eliminates the possibility of the paradox."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording was tightened for clarity and concision; the central idea that hierarchical typing prevents self-membership was emphasized. Distractors were kept plausible but incorrect alternatives.",
      "content_preserved": true,
      "source_article": "Type theory",
      "x": 1.5462026596069336,
      "y": 1.1796894073486328,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Hierarchical typing as a mechanism to avoid paradoxes (e.g., Russell's paradox) by restricting how entities of different types are constructed and preventing self-reference.",
        "Concept 2: Type theory as an alternative foundation for mathematics and its relationships to set theory and constructive mathematics (e.g., intuitionistic type theory) as a theoretical framework.",
        "Concept 3: The use of type theory in computational proof systems (e.g., calculus of constructions, Coq, Lean) to encode mathematics and support formal verification."
      ],
      "original_question_hash": "2d4c08fe"
    },
    {
      "question": "How does \"internal fit\"—the alignment of a firm's activities—help make a competitive strategy sustainable, and why does weak or broken fit undermine that sustainability?",
      "options": {
        "A": "By standardizing activities across the firm and eliminating variation so execution is simpler and easier to control.",
        "B": "By creating reinforcing linkages among activities so success in one area strengthens others, making the overall strategy harder to imitate and more durable; if fit is weak, misaligned activities destroy synergies and dilute the firm’s value proposition.",
        "C": "By shifting emphasis from creating customer value to cutting costs; the primary benefit of fit is improved operational efficiency.",
        "D": "By guaranteeing that any new activity will automatically conform to the strategy, removing the need for deliberate coordination and oversight."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the question to define \"internal fit\" as activity alignment and focused on how reinforcing linkages create durability and imitation barriers; distractor options kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Strategic management",
      "x": 1.36787748336792,
      "y": 0.988638162612915,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The strategy process is iterative and adaptive, using feedback loops to monitor execution and inform the next round of planning.",
        "Concept 2: Competitive strategy rests on three related mechanisms: creating a unique market position, making trade-offs (what not to do), and achieving internal “fit” by aligning activities to support the chosen strategy.",
        "Concept 3: There is a relationship and distinction between levels of strategy—corporate strategy (portfolio/choices about what business(es) to be in) and business strategy (how to compete within a chosen business)."
      ],
      "original_question_hash": "4a9ef385"
    },
    {
      "question": "Why is the boundary between military and civilian technology often described as porous, and what are the main ways technologies typically move from the military to civilian sectors?",
      "options": {
        "A": "Because many military inventions are dual-use; they move into civilian markets through licensing, commercial spin-offs, and adaptation to civilian needs.",
        "B": "Because military technologies are usually kept secret and are never transferred to civilian industry.",
        "C": "Because civilian industries fully fund military research and directly dictate technology adoption, so transfers do not require separate mechanisms.",
        "D": "Because military technologies that fail in combat are immediately repurposed for civilian applications."
      },
      "correct_answer": "A",
      "simplification_notes": "Condensed wording; defined 'porous' as overlap between sectors and emphasized primary transfer mechanisms (licensing, spin-offs, adaptation) while keeping distractors plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Military technology",
      "x": 1.8711339235305786,
      "y": 0.787945032119751,
      "level": 2,
      "concepts_tested": [
        "The porous boundary between military and civilian technology and how tech transfers occur between sectors",
        "The role of military funding in driving science and technology development",
        "Armament engineering as an interdisciplinary field coordinating design, testing, and lifecycle management of weapons systems"
      ],
      "original_question_hash": "49be4303"
    },
    {
      "question": "How does combining the physical design of a public space with deliberate management and programming change how diverse users actually use and experience that space?",
      "options": {
        "A": "Because management alone determines usage patterns regardless of the physical layout or design.",
        "B": "Because design creates affordances (what activities are possible) while management and programming signal social norms and rules; together they shape which activities occur, who feels welcome, and how safe the space feels.",
        "C": "Because aesthetic appeal alone attracts all user groups equally, making management or programming unnecessary.",
        "D": "Because only economic incentives and market forces determine usage, while design and management have minimal impact on experience."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduate readers; technical term 'affordances' briefly defined in-place; distractors kept plausible but concise.",
      "content_preserved": true,
      "source_article": "Urban design",
      "x": 1.3343162536621094,
      "y": 0.9152858853340149,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Public space as the central focus of urban design, and the relationship between design/management of spaces and how they are used and experienced.",
        "Concept 2: Interdisciplinary integration across architecture, landscape architecture, urban planning, civil engineering, sociology, law, economics, etc., to produce holistic urban design outcomes.",
        "Concept 3: Design objectives (equitable, beautiful, performative, sustainable) that guide decisions and define success in urban design."
      ],
      "original_question_hash": "e5c6f0b5"
    },
    {
      "question": "Using White's formula $P = E/T$ (where $P$ is a development index, $E$ is energy consumed, and $T$ is the technical efficiency factor), why does a society's $P$ increase when it improves tool efficiency (i.e., lowers $T$) while $E$ remains constant?",
      "options": {
        "A": "Because lowering $T$ causes $E$ to rise.",
        "B": "Because $P$ depends only on $E$ and not on $T$.",
        "C": "Because $P = E/T$ implies that reducing $T$ while holding $E$ fixed increases the ratio $E/T$, so $P$ increases.",
        "D": "Because making tools more efficient reduces energy use and that reduction in energy always lowers $P$."
      },
      "correct_answer": "C",
      "simplification_notes": "Made the question explicit and concise, presented the formula in inline LaTeX, clarified variable meanings, and kept all four options plausible while preserving the original correct choice (C).",
      "content_preserved": true,
      "source_article": "History of technology",
      "x": 0.8375959992408752,
      "y": 0.1705569326877594,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The technology–science nexus — how new knowledge enables tools and, in turn, how tools/instruments enable scientific inquiry.",
        "Concept 2: Energy as driver of cultural and technological evolution — the idea that advancing stages of energy harnessing (muscle, animals, plants, fossil fuels, nuclear) track development, with measures like the P=E/T framework illustrating this relationship.",
        "Concept 3: Information/knowledge as a core driver of development — the notion that the amount and control of information shape how advanced a society becomes."
      ],
      "original_question_hash": "0cc745db"
    },
    {
      "question": "Why does a close, logical fit between a sponsor and the sponsoree make a sponsorship more effective than simple exposure alone?",
      "options": {
        "A": "It strengthens the cognitive link between brand and event, creating coherent event-linked memory associations so that thinking of the event more readily evokes the brand and favorable perceptions.",
        "B": "It guarantees higher sales because the sponsorship communicates specific product attributes directly to consumers.",
        "C": "It removes the need for any other marketing activities, since sponsorship alone will produce all desired outcomes.",
        "D": "It lets the sponsor avoid explaining the reason for the sponsorship to the audience, because the fit makes justification unnecessary."
      },
      "correct_answer": "A",
      "simplification_notes": "Rephrased 'logical match' as 'close, logical fit'; simplified sentence structure; retained technical terms 'cognitive link' and 'event-linked memory associations'; made incorrect options plausible but clearly inconsistent with the article.",
      "content_preserved": true,
      "source_article": "Sponsor (commercial)",
      "x": 1.3373030424118042,
      "y": 0.9998077750205994,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3"
      ],
      "original_question_hash": "9276925b"
    },
    {
      "question": "In computational creativity, how can a fully programmed system still be considered creative when it appears to 'break rules' or depart from convention?",
      "options": {
        "A": "Because any departure from programmed rules just injects randomness into outputs, and randomness is what makes results creative.",
        "B": "Because creativity can emerge when the system explores beyond conventional constraints (e.g., via stochastic search, recombination, remixing, or generative models) to produce novel, potentially valuable artifacts; a separate evaluation phase then selects outputs that are useful.",
        "C": "Because genuine rule-breaking requires conscious intent, which machines lack, so any apparent deviation is not real creativity but pre-programmed behavior.",
        "D": "Because creativity is simply rule-breaking, so any deviation from rules is automatically creative regardless of novelty or usefulness."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the question in clearer undergraduate language; emphasized generation/evaluation stages and mechanisms (stochastic search, recombination, generative models) described in the article; preserved the original correct answer and the core concepts of novelty/usefulness and the debate over machine rule-following.",
      "content_preserved": true,
      "source_article": "Computational creativity",
      "x": 1.4283311367034912,
      "y": 1.1098560094833374,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Criteria for computational creativity (e.g., novelty, usefulness, value) and the existence of multiple frameworks (Newell-Shaw-Simon vs. Boden) that operationalize creativity.",
        "Concept 2: The philosophical/mechanistic question of whether machines can be creative if they are constrained by programming, including the role of rule-breaking or deviation from convention.",
        "Concept 3: The relationship between theoretical inquiry and practical system design in computational creativity—how theory informs implementations and practical work, and vice versa."
      ],
      "original_question_hash": "09f25e1d"
    },
    {
      "question": "Why did the introduction of GPU architectures accelerate progress in deep learning, and which technical principle explains why GPUs are especially effective for training modern neural networks?",
      "options": {
        "A": "They increased single-thread CPU clock speeds, enabling faster serial execution of training algorithms.",
        "B": "They provide massive data-level parallelism (many cores / SIMD) that maps naturally to the dense linear algebra—large matrix multiplications—used in neural network training, making large-scale training on big datasets practical.",
        "C": "They introduced new, GPU-only training algorithms that replaced backpropagation and thereby sped up learning.",
        "D": "They dramatically reduced per-model memory usage so much larger networks could fit on the same hardware without adding more memory."
      },
      "correct_answer": "B",
      "simplification_notes": "Question shortened and made more direct for undergraduates; preserved technical terms (data-level parallelism, matrix multiplications) and reframed options to be plausible alternatives.",
      "content_preserved": true,
      "source_article": "Artificial intelligence",
      "x": 1.3562893867492676,
      "y": 1.0899471044540405,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The relationship between AI goals (learning, reasoning, perception, planning) and the techniques used to achieve them (search, optimization, formal logic, neural networks, statistics). How do specific goals drive the choice of methods, and how do different subfields align with particular tools?",
        "Concept 2: The causal role of hardware and architectural advances in AI progress (GPUs enabling deep learning; transformer architecture enabling generative AI; the AI boom after 2020). Why/how did progress accelerate at these points, and what roles do hardware vs. algorithms play?",
        "Concept 3: The emergence of ethical concerns and regulatory considerations in response to AI capabilities and potential harms (long-term effects, existential risks, safety policies). Why/how do capabilities lead to policy discussions, and how might regulation influence future development?"
      ],
      "original_question_hash": "bf24934d"
    },
    {
      "question": "Why does a short, emotionally framed message repeated across multiple media channels tend to shift public opinion more effectively than a longer, fact-heavy argument?",
      "options": {
        "A": "Because repetition makes a message seem familiar and therefore more novel and credible to audiences than new or different information.",
        "B": "Because repetition increases processing fluency and framing ties the message to identity and emotion, reducing resistance and making the claim easier to accept even when the factual content is unchanged.",
        "C": "Because repeated messages typically provoke audience distrust, prompting people to seek opposing views and thereby amplifying debate rather than changing minds directly.",
        "D": "Because people always ignore factual arguments and base opinions solely on emotional appeals, so only emotion-driven messages can change public opinion."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduate readers; retained technical terms from the article (framing, processing fluency, media effects); distractor options made plausible but distinct; core explanation and correct answer preserved.",
      "content_preserved": true,
      "source_article": "Public opinion",
      "x": 1.2290785312652588,
      "y": 0.9521849155426025,
      "level": 2,
      "concepts_tested": [
        "Concept 1: How public opinion is formed and influenced by media, advertising, and rhetoric (mechanisms of opinion formation).",
        "Concept 2: The relationship between public opinion and governance/legitimacy, including social conformity norms (opinion as a driver or constraint on political power).",
        "Concept 3: Methods for studying public opinion (e.g., sentiment analysis) and the role/challenge of misinformation in measuring or interpreting public opinion."
      ],
      "original_question_hash": "752a65cd"
    },
    {
      "question": "Which statement best explains how using predefined performance indicators allows comparison across organizational units but can limit evaluation tailored to each unit? What is the resulting trade-off?",
      "options": {
        "A": "Predefined indicators guarantee universal accuracy, so measurements are unaffected by local conditions.",
        "B": "Standard indicators set identical targets for all units, increasing comparability but making evaluations insensitive to local constraints and needs.",
        "C": "Using predefined indicators creates a common measurement framework that enables cross-unit comparisons but may overlook unique local constraints and priorities, trading contextual relevance for standardization.",
        "D": "Standard indicators remove subjectivity entirely, producing fully objective assessments that are independent of local context."
      },
      "correct_answer": "C",
      "simplification_notes": "Shortened and clarified the question to focus explicitly on the comparability vs. contextual relevance trade-off; removed references to specific framework names and historical definitions while preserving the core concept; kept all four options plausible and concise.",
      "content_preserved": true,
      "source_article": "Performance measurement",
      "x": 1.3615344762802124,
      "y": 0.9747732877731323,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The purpose-driven nature of performance measurement. Why the objective of measuring (forward-looking, retrospective, evaluative) changes what gets measured and how results are used.",
        "Concept 2: Standards vs. context in performance measurement. How predefined indicators enable comparability but may constrain context-specific assessment, and the trade-offs involved.",
        "Concept 3: Frameworks and alignment of measures with performance. How organizing frameworks integrate indicators across an organization and link measurement to overall performance, including the reality of divergent viewpoints on how to define/use measures."
      ],
      "original_question_hash": "55caed99"
    },
    {
      "question": "How does designing industrial systems as closed loops improve overall resource efficiency?",
      "options": {
        "A": "Because it guarantees zero waste by perfectly recycling every material back into its original form.",
        "B": "Because it shifts optimization from individual processes to the network level: outputs from one process are reused as inputs for others, reducing the need for virgin resources and lowering waste generation.",
        "C": "Because it eliminates external energy inputs by producing all required energy solely from waste streams.",
        "D": "Because closed-loop systems depend exclusively on biological cycles to recycle materials, preventing material degradation over time."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the prompt to ask directly how closed-loop design improves resource efficiency; clarified option B to emphasise network-level optimization and reuse of outputs as inputs; kept all four options plausible but incorrect ones concise.",
      "content_preserved": true,
      "source_article": "Industrial ecology",
      "x": 1.4843944311141968,
      "y": 0.9412277936935425,
      "level": 2,
      "concepts_tested": [
        "Closed-loop systems / industrial metabolism: transforming waste inputs into new resources; moving from open-loop to closed-loop industrial processes.",
        "Systems-based emergent behavior with ecosystem metaphor: viewing industrial and natural systems as integrated, interdependent networks whose collective behavior emerges from component interactions.",
        "Life-cycle thinking and eco-design for sustainability: using life-cycle planning and design choices (dematerialization, decarbonization, eco-design) to reduce resource use and environmental impact."
      ],
      "original_question_hash": "60f4eecc"
    },
    {
      "question": "In a sealed box containing $N$ identical molecules evolving under time‑symmetric Newtonian dynamics, the gas is prepared in a low‑entropy state and its entropy then increases monotonically. Why does macroscopic irreversibility (an observed arrow of time) appear even though the microscopic laws are time‑reversal symmetric?",
      "options": {
        "A": "Because microscopic interactions produce effective dissipative forces that single out a preferred time direction, making the time‑reversed evolution physically impossible.",
        "B": "Because the number of microstates corresponding to high‑entropy macrostates is astronomically larger than that for low‑entropy macrostates, so almost any evolution from a typical low‑entropy microstate moves toward higher entropy; the exact time‑reversed microstate is extraordinarily unlikely.",
        "C": "Because measurement or observation forces the system onto a single history (collapse), creating an apparent one‑way time direction not implied by the underlying dynamics.",
        "D": "Because the gravitational potential (or other external long‑range fields) ensures the system's energy is minimized only in the forward temporal direction, forcing entropy to increase with time."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the scenario in clearer undergraduate language, used inline LaTeX for $N$, kept the focus on time‑symmetric microscopic laws vs macroscopic entropy increase. Options were rephrased to be concise and all remain plausible alternative explanations; the correct statistical/microstate counting answer (B) is preserved.",
      "content_preserved": true,
      "source_article": "Arrow of time",
      "x": 1.7203365564346313,
      "y": 1.1153637170791626,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Time-reversal symmetry vs macroscopic irreversibility; how microscopic laws can be time-symmetric while macroscopic processes are not, due to entropy production.",
        "Concept 2: Entropy and the second law as the statistical mechanism behind the arrow of time; increasing entropy corresponds to decreasing free energy and a preferred temporal direction.",
        "Concept 3: The role of initial/boundary conditions (e.g., low-entropy Big Bang) and cosmological context in establishing or constraining the arrow of time, including how global conditions influence local time asymmetry."
      ],
      "original_question_hash": "3fa6f0cf"
    },
    {
      "question": "Why does copyright protect an author's original expression of an idea rather than the underlying idea itself?",
      "options": {
        "A": "Because protecting ideas would unduly restrict creative reuse—ideas are general concepts others must be free to use.",
        "B": "Because ideas are abstract and not always fixed in a tangible form, whereas an author's expression is fixed and can be identified.",
        "C": "Because copyright grants exclusive rights only to the specific form of an author's expression, while the underlying idea remains available for others to reuse and adapt.",
        "D": "Because once a work is created the underlying idea automatically enters the public domain and therefore cannot be protected."
      },
      "correct_answer": "C",
      "simplification_notes": "Reworded the stem for clarity and concision; options rewritten to be concise and plausible while preserving the tested idea–expression distinction and the exclusivity of rights.",
      "content_preserved": true,
      "source_article": "Copyright",
      "x": 1.2604055404663086,
      "y": 0.9052388072013855,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Copyright protects the original expression of an idea, not the idea itself (principle driving what is protected).",
        "Concept 2: Exclusive rights granted to rights holders (reproduction, distribution, derivative works, public performance, moral rights) and the role of limitations (e.g., fair use/fair dealing) as mechanisms balancing protection with public interest.",
        "Concept 3: Territorial nature and duration of rights (copyright as a territorial right; cross-border implications; eventual entry into the public domain)."
      ],
      "original_question_hash": "2bc6efcb"
    },
    {
      "question": "According to the Matthew effect, why do power gaps between social groups with unequal initial resources tend to persist and grow over time?",
      "options": {
        "A": "Initial resource advantages increase access to influential networks, institutions, and means of production, creating reinforcing feedback loops (cumulative advantage) that further enhance those groups' resources and power.",
        "B": "Social systems have neutral norms and automatic redistributive mechanisms that correct initial imbalances, causing power differences to shrink over time.",
        "C": "Resource-rich groups merely hoard assets without converting them into broader influence or network ties, so power inequalities stay roughly constant rather than widening.",
        "D": "Large external shocks (e.g., wars, economic crises) randomly redistribute resources across groups, making changes in the power gap independent of initial endowments."
      },
      "correct_answer": "A",
      "simplification_notes": "Condensed the original wording to a single clear question, kept the term 'Matthew effect' and core concept of cumulative advantage, and made all four options plausible alternatives while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Social conflict",
      "x": 1.1902081966400146,
      "y": 0.9553793668746948,
      "level": 2,
      "concepts_tested": [
        "Resource competition and power struggles between social groups",
        "Economic structure (mode of production/base-superstructure) shaping social life and consciousness",
        "Cumulative advantage: resources empower some groups to maintain or increase power (Matthew effect)"
      ],
      "original_question_hash": "71b641b8"
    },
    {
      "question": "How does a coherent business-model narrative (a \"recipe\") do more than merely describe a firm? In what way does it shape entrepreneurial action and coordination beyond description?",
      "options": {
        "A": "It provides a concise blueprint that translates the value-creation logic into shared constraints and signals, guiding resource allocation, decisions, and coordination across the firm and its partners.",
        "B": "It is only a storytelling device with no practical effect on resource choices, organizational decisions, or partner alignment.",
        "C": "It guarantees rapid scaling and profitability whenever the narrative is consistent, regardless of market or operational conditions.",
        "D": "It removes the need to test assumptions by substituting narrative for formal contracts, governance, or experimental validation."
      },
      "correct_answer": "A",
      "simplification_notes": "Phrase shortened and clarified; jargon reduced while retaining key terms (\"business-model narrative\", \"recipe\", \"value-creation logic\", \"shared constraints\"); options rewritten to be concise and plausible; core concept (narrative guides action and coordination) preserved.",
      "content_preserved": true,
      "source_article": "Business model",
      "x": 1.3645720481872559,
      "y": 0.98843914270401,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The function of a business model as the design for creating, delivering, and capturing value to generate profit.",
        "Concept 2: Business model innovation as a mechanism for adapting to opportunities and aligning with business strategy.",
        "Concept 3: The role of narrative/coherence (and the idea of model “recipes”) as mechanisms that communicate the business logic and guide entrepreneurial action."
      ],
      "original_question_hash": "9253a61c"
    },
    {
      "question": "Why does a behavior- and experience-centered design approach — one that imagines how interactions could be instead of only describing how they are now — tend to produce more usable and more innovative products?",
      "options": {
        "A": "Because it anchors designs to current user practices, minimizing disruption and making adoption easier.",
        "B": "Because imagining new interactions puts aesthetic novelty above functional concerns, which drives innovation.",
        "C": "Because it lets designers explore a wider space of interaction possibilities, contexts, user goals, and system states, surfacing latent needs and solutions that analysis of current behavior can miss.",
        "D": "Because it guarantees faster development by allowing teams to skip iterative user testing and validation."
      },
      "correct_answer": "C",
      "simplification_notes": "Compressed and clarified the question to focus on imagining interactions vs. analysing current systems; used terms like ‘latent needs’, ‘contexts’, ‘user goals’, and ‘system states’; made all four options plausible while keeping the original correct answer C.",
      "content_preserved": true,
      "source_article": "Interaction design",
      "x": 1.3730930089950562,
      "y": 1.0727747678756714,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Behavior- and experience-centered design; designing by imagining how interactions could be rather than only analyzing how they are.",
        "Concept 2: Managing complexity in interactive systems across multiple users, contexts, and states, including using design tools to handle that complexity.",
        "Concept 3: Interdisciplinary, constraint-aware approach that aligns user needs with technical and business constraints to create usable experiences."
      ],
      "original_question_hash": "52660aea"
    },
    {
      "question": "Two games use the same core mechanics (the same rules), but one gives immediate, obvious feedback for each player action while the other gives delayed, subtle feedback. Which statement best describes what this difference demonstrates about the relationship between mechanics, gameplay, and player experience?",
      "options": {
        "A": "Immediate feedback changes the underlying rules, producing different gameplay behavior and therefore a different player experience.",
        "B": "Delayed feedback exposes hidden mechanics that were not in the rules, effectively adding new gameplay elements.",
        "C": "Feedback timing changes how players interact with the same mechanics (the gameplay) and how they feel during play (the player experience), without altering the core rules.",
        "D": "Feedback timing has no effect on gameplay or player experience; it only affects surface aesthetics."
      },
      "correct_answer": "C",
      "simplification_notes": "Clarified that 'core mechanics' means 'rules' and that 'gameplay' means player interaction; shortened and focused the scenario to isolate feedback timing as the variable.",
      "content_preserved": true,
      "source_article": "Game design",
      "x": 1.391275405883789,
      "y": 1.270129680633545,
      "level": 2,
      "concepts_tested": [
        "The relationship between game mechanics/systems, gameplay, and player experience, and how each influences the others.",
        "The iterative design process, including prototyping and playtesting, as a mechanism for refining rules and mechanics.",
        "The division of design/development roles (designer, developer, artist) and how their collaboration translates a concept into a final game."
      ],
      "original_question_hash": "c67e0fce"
    },
    {
      "question": "Why can homologous anatomical structures be recognized as related across different species even when they perform different functions?",
      "options": {
        "A": "Convergent evolution: similar selective pressures produce superficially similar structures in unrelated lineages.",
        "B": "Shared ancestry: a common structural blueprint is inherited and then modified by divergent evolution so the same organ serves different functions in different species.",
        "C": "Parallel genetic changes: similar mutations in separate lineages produce comparable structures independently.",
        "D": "Random fixation: identical mutations became fixed by chance in both lineages, yielding the same structures."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened and made more concise; kept technical terms (homology, divergent/convergent evolution). Options were rewritten to be clear and plausible while preserving the original distractors and the correct choice.",
      "content_preserved": true,
      "source_article": "Homology (biology)",
      "x": 1.8167668581008911,
      "y": 1.1162359714508057,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Homology reflects shared ancestry and can persist while structures are modified for different functions through divergent evolution.",
        "Concept 2: There are multiple kinds of homology (anatomical/structural, developmental/serial, and sequence/genetic), connected by the idea that common ancestry underlies similarity across levels.",
        "Concept 3: Evidence and methods (e.g., sequence similarity, orthologs vs paralogs, sequence alignments) are used to infer homology and reconstruct evolutionary relationships."
      ],
      "original_question_hash": "06afabed"
    },
    {
      "question": "According to reason‑responsiveness, a belief is rational when an agent updates it in response to relevant reasons. Which scenario best shows why responding to reasons matters for forming rational beliefs?",
      "options": {
        "A": "An agent retains a belief because changing it would disrupt their broader system of commitments, even though strong contrary evidence is available.",
        "B": "An agent revises a belief when presented with strong, relevant new evidence, despite prior commitments or initial intuitions favoring the old belief.",
        "C": "An agent counts a belief as rational only if it matches the majority view, regardless of the evidence.",
        "D": "An agent fixes all beliefs to maximize cognitive efficiency, keeping them even when there are compelling reasons to reconsider."
      },
      "correct_answer": "B",
      "simplification_notes": "Focused the stem on reason‑responsiveness and clarified each option to map to competing motives (coherence/conservatism, evidential revision, conformity, bounded rationality). Wording shortened and made more direct while preserving original choices and the correct answer.",
      "content_preserved": true,
      "source_article": "Rationality",
      "x": 1.27496337890625,
      "y": 1.05368971824646,
      "level": 2,
      "concepts_tested": [
        "Reason-responsiveness as a criterion for rationality: rationality defined by responsiveness to reasons and how reasons justify actions or beliefs.",
        "Coherence-based vs reason-responsive accounts: internal coherence among mental states versus responsiveness to external reasons, including how they handle contradictions and justify beliefs/intentions.",
        "Types and limits of rationality (theoretical vs practical, ideal vs bounded, internalist vs externalist, normative questions): how different domains and constraints shape what counts as rational and when rationality may be limited or context-dependent."
      ],
      "original_question_hash": "374a0975"
    },
    {
      "question": "Why does including Scope 3 emissions give a more complete picture of an organization's carbon footprint, and how does that affect mitigation planning?",
      "options": {
        "A": "Because Scope 3 covers only on-site energy use and facility emissions, so including it narrows mitigation to energy-supply contracts and facility upgrades.",
        "B": "Because Scope 3 covers indirect emissions across the value chain from sources the organization does not own or control; including it reveals supplier, customer, and product-life-cycle hotspots and directs mitigation toward procurement, product design, and collaborative supplier engagement.",
        "C": "Because Scope 3 simply duplicates Scope 2 calculations, so adding it does not alter the footprint or change mitigation priorities.",
        "D": "Because Scope 3 emissions are reported separately from an organisation's main accounts, including them complicates reporting and therefore should be avoided in mitigation planning."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; Scope definitions made explicit; options rephrased to be concise yet plausible; technical terms (value chain, procurement, product life cycle) retained.",
      "content_preserved": true,
      "source_article": "Carbon footprint",
      "x": 1.3535118103027344,
      "y": 0.8510311245918274,
      "level": 2,
      "concepts_tested": [
        "Life cycle perspective (cradle-to-grave): emissions included from production through supply chain, use, and disposal; rationale for including the full life cycle to enable meaningful comparisons and mitigation planning.",
        "Emissions scopes for organizations (Scope 1, 2, and 3): direct vs indirect emissions and the importance of including indirect (especially Scope 3) for a comprehensive footprint and for guiding mitigation strategies.",
        "CO2-equivalent aggregation across greenhouse gases: using CO2e as a common unit to sum diverse greenhouse gases; considerations about which gases are included and how this affects comparability and interpretation of the footprint."
      ],
      "original_question_hash": "0e83fdd4"
    },
    {
      "question": "Why does conducting a collaborative, stakeholder-centered program evaluation make the evaluation findings more useful and more likely to be used to improve the program?",
      "options": {
        "A": "It allows stakeholders to collect data themselves and bypass the evaluator's methods.",
        "B": "It ensures the evaluation questions, measures, and interpretation address decision-relevant concerns, builds legitimacy with users, and increases the likelihood that findings will inform program improvements.",
        "C": "It guarantees the evaluator's preferred methodological approach is selected regardless of stakeholder needs.",
        "D": "It eliminates all potential biases by requiring unanimous stakeholder agreement on the results."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduate readers; preserved core idea that stakeholder collaboration improves relevance, legitimacy, and use of findings; all four options made plausible but only B remains correct.",
      "content_preserved": true,
      "source_article": "Program evaluation",
      "x": 1.302912712097168,
      "y": 0.9668228626251221,
      "level": 2,
      "concepts_tested": [
        "Purpose and value focus of evaluation (assessing effectiveness, efficiency, and value for money to inform decisions and improvements)",
        "Stakeholder-centered, collaborative evaluation practice (best practice of joint projects between evaluators and stakeholders)",
        "Methodological pluralism across the program life-cycle (use of quantitative and qualitative methods with questions that evolve at different stages)"
      ],
      "original_question_hash": "ed6a928d"
    },
    {
      "question": "Why is it inappropriate to treat randomized controlled trials (RCTs) and quantitative data as the single best form of evidence for all policy areas in evidence-based policy?",
      "options": {
        "A": "Because quantitative methods yield perfect, context-independent certainty for policy decisions.",
        "B": "Because different policy domains have different epistemic aims, normative requirements, and practical constraints, so some domains need qualitative, normative, or deliberative evidence in addition to or instead of RCTs.",
        "C": "Because RCTs are inherently unethical for evaluating public policies in every case.",
        "D": "Because the quality of evidence depends solely on sample size, making the study design (like RCT vs qualitative) irrelevant."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the question for clarity and concision; emphasised domain-dependence, normative and practical reasons from the article. Kept original intent and correct answer. Options made plausible distractors reflecting common misconceptions.",
      "content_preserved": true,
      "source_article": "Evidence-based policy",
      "x": 1.3377076387405396,
      "y": 1.018660306930542,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The three conditions for labeling a policy as evidence-based (comparative evidence vs alternatives; alignment with the policy area’s preferences; a sound account linking evidence and preferences)",
        "Concept 2: The domain-dependence of evidence (RCTs and quantitative data are not universally optimal; some areas require different forms of evidence and may involve moral/philosophical reasoning)",
        "Concept 3: The prerequisites and dynamics of effectiveness (quality data, analytical skills, political backing; and how power/politics influence the production and use of evidence)"
      ],
      "original_question_hash": "17afe7d5"
    },
    {
      "question": "Why do regulations that target point sources typically achieve more dependable pollution reductions than regulations aimed at nonpoint sources?",
      "options": {
        "A": "Because point sources concentrate emissions at identifiable locations, so regulators can monitor them, issue permits, and enforce limits, whereas nonpoint sources are spatially and temporally diffuse and harder to measure or regulate.",
        "B": "Because point sources produce far less pollution than nonpoint sources, so regulating them is sufficient to protect public health and the environment.",
        "C": "Because nonpoint-source pollution can be eliminated by a single cleanup or repair at one site, making formal regulation unnecessary.",
        "D": "Because regulatory agencies prefer to focus on sources where penalties and compliance actions are administratively simpler to apply, regardless of the pollution characteristics."
      },
      "correct_answer": "A",
      "simplification_notes": "Condensed the question and clarified the distinction between point and nonpoint sources; emphasized monitoring, permitting, and enforcement as the key reason; distractor options were reworded to be plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Pollution",
      "x": 1.7814215421676636,
      "y": 0.9691889882087708,
      "level": 2,
      "concepts_tested": [
        "Pollution sources and forms with management implications: how human activities create contaminants and how point vs. nonpoint sources affect mitigation strategies.",
        "Persistence and legacy of pollution: how pollutants can remain in environments and continue affecting health and ecosystems after emissions stop.",
        "Health, ecological impacts and policy responses: how pollution leads to health and environmental outcomes and drives regulation, environmental justice considerations, and international cooperation."
      ],
      "original_question_hash": "71ddd1d2"
    },
    {
      "question": "A person solves a spatial puzzle by mentally rotating a visual image while at the same time rehearsing a sequence with auditory imagery. The visual and auditory images can be varied independently, and both influence the final answer. Which view of the architecture of mental representations does this pattern best support?",
      "options": {
        "A": "A single, amodal \"mentalese\" store that is accessed identically by all modalities (one abstract symbolic system for vision, hearing, etc.).",
        "B": "Distinct, modality-specific representations (e.g. visual vs. auditory) that can operate independently but interact during reasoning and problem solving.",
        "C": "No persistent stored representations: mental content is reconstructed anew each time, so apparent independence reflects on-the-fly reconstruction.",
        "D": "Imagery is epiphenomenal and cannot causally guide problem solving; performance must rely on non-imagery processes instead."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the scenario and clarified that visual and auditory imagery are manipulated independently; framed options in clear, graduate-level cognitive terms (amodal mentalese, modality-specific interaction, reconstructive account, epiphenomenal imagery). Kept original core contrast and correct answer.",
      "content_preserved": true,
      "source_article": "Mental representation",
      "x": 1.1980589628219604,
      "y": 1.0765488147735596,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Representations as intermediaries between the observing subject and external reality (representationalism vs. direct realism)",
        "Concept 2: The internal symbolic structure of thought (the idea of mentalese with syntax and semantics, internal language of thought)",
        "Concept 3: Multimodal mental imagery and the cognitive mechanisms for using representations (visual/auditory/etc. imagery for problem solving and the debates about how content is interpreted and stored in the brain)"
      ],
      "original_question_hash": "31cae8d8"
    },
    {
      "question": "According to postmodernism's view that knowledge is socially constructed, why can different cultures and disciplines disagree about the same phenomenon?",
      "options": {
        "A": "There is one objective reality that is simply misinterpreted; disagreements are just errors or mistakes.",
        "B": "Knowledge emerges from discursive practices embedded in power relations, so different contexts can legitimate multiple interpretations.",
        "C": "Scientific progress will eventually unify all interpretations, so current disagreements are temporary.",
        "D": "All interpretations are equally arbitrary and thus there is no basis for evaluating one against another."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened to be direct and concise for undergraduates; retained key terms ('socially constructed', 'discursive practices', 'power relations', 'interpretations'). Distractors were kept plausible and aligned with common alternatives or criticisms.",
      "content_preserved": true,
      "source_article": "Postmodernism",
      "x": 0.5757351517677307,
      "y": 1.125283122062683,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Skepticism toward universal truths and grand narratives; instability of meaning and relativism.",
        "Concept 2: Social construction of knowledge and reality; plurality of perspectives and critique of Enlightenment progress.",
        "Concept 3: Reflexivity, pastiche, and eclecticism as mechanisms of postmodern expression (use of irony and style-blending to challenge norms)."
      ],
      "original_question_hash": "cb18e290"
    },
    {
      "question": "Eclecticism gains complementary insights by choosing useful elements from multiple theories instead of creating a single universal paradigm. Which mechanism best explains how eclecticism preserves flexibility and avoids dogmatism?",
      "options": {
        "A": "It commits to a single overarching theory and requires all other elements to conform to it.",
        "B": "It selects ideas on the basis of their merit and context-specific usefulness without forcing them into a single unified framework.",
        "C": "It rejects theoretical approaches altogether and relies solely on empirical methods.",
        "D": "It merges chosen elements into a new, fully unified system that replaces earlier frameworks."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; retained contrast between selecting elements vs. forming a unified system; options made explicitly plausible and aligned with distinctions between eclecticism and syncretism.",
      "content_preserved": true,
      "source_article": "Eclecticism",
      "x": 1.1207072734832764,
      "y": 1.0717051029205322,
      "level": 2,
      "concepts_tested": [
        "Eclecticism as a principle: adopting multiple theories/ideas rather than a rigid single paradigm to gain complementary insights or apply different theories to specific cases.",
        "Relationship to syncretism: eclecticism selects elements from various systems without necessarily forming a single unified framework, unlike syncretism which merges traditions into one system.",
        "Origin and mechanism: the practice originated with ancient Greek/Roman philosophers who selected doctrines they found reasonable and combined them; the term means \"choosing the best\" and implies merit-based selection rather than creating a single tradition."
      ],
      "original_question_hash": "b47725ce"
    },
    {
      "question": "How does the fact that a time-varying magnetic field induces an electric field (Faraday's law) illustrate Maxwell's unification of electricity and magnetism?",
      "options": {
        "A": "It shows electric and magnetic fields are inseparable components of a single electromagnetic field: a time-varying magnetic field produces an electric field and, conversely, a time-varying electric field produces a magnetic field (via Maxwell's displacement current), allowing energy to propagate as self-sustaining electromagnetic waves.",
        "B": "It proves that only moving charges feel magnetic forces, so a changing magnetic field directly accelerates charges regardless of their state of motion.",
        "C": "It implies magnetic fields can exist without any electric fields and therefore do not influence electric phenomena.",
        "D": "It demonstrates that static electric fields alone are sufficient to explain all electromagnetic phenomena, making changing magnetic fields unnecessary."
      },
      "correct_answer": "A",
      "simplification_notes": "Question was restated to refer explicitly to Faraday's law and Maxwell's framework; wording was tightened and technical terms (displacement current, electromagnetic waves) kept for precision; distractors were rephrased to remain plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Electromagnetism",
      "x": 1.7926831245422363,
      "y": 1.0761419534683228,
      "level": 2,
      "concepts_tested": [
        "The unification of electric and magnetic phenomena into electromagnetic fields described by Maxwell's equations, which predict electromagnetic waves and light.",
        "The Lorentz force as the microscopic mechanism by which Charges experience forces in electric and magnetic fields, linking electrostatics and magnetism at the particle level.",
        "The broad role of electromagnetism as a fundamental force that explains physical and chemical phenomena and enables technologies, through dynamic interactions of changing electric and magnetic fields."
      ],
      "original_question_hash": "0a61afb6"
    },
    {
      "question": "Which statement best describes the co-production (mutual shaping) view that technology and society shape each other?",
      "options": {
        "A": "Technologies arise to meet fixed societal goals, and society passively adapts to whatever technology appears.",
        "B": "Social needs, policies, and institutions influence which technologies are developed and adopted; those technologies then reshape social practices and institutions, creating new needs and governance that guide future technological choices.",
        "C": "Market and economic forces alone determine which technologies exist, with social norms and governance playing little or no role.",
        "D": "Once a technology is created, social norms, institutions, and governance remain essentially unchanged."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified the question term \"co-production\" and shortened option wording for clarity while keeping the original meanings and nuances; maintained technical terms like \"institutions\" and \"governance.\"",
      "content_preserved": true,
      "source_article": "Technology and society",
      "x": 1.2658908367156982,
      "y": 0.9710628390312195,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Mutual shaping of technology and society (co-evolution/co-production) — how societal needs and governance influence technology, and how technology, in turn, reshapes social practices and institutions.",
        "Concept 2: Technology as a driver of social and economic transformation — how innovations (e.g., printing press, Internet) enable information flow, globalization, and changes in productivity and social classes.",
        "Concept 3: Frameworks and debates for evaluating technology’s impact — diverse orthodoxies (transhumanism, techno-progressivism, Neo-Luddism) and interdisciplinary approaches (science and technology studies) that conceptualize ethics, governance, and development."
      ],
      "original_question_hash": "8b196396"
    },
    {
      "question": "Why do larger integrated regional markets typically reduce firms' average production costs, and under what condition can those cost advantages be limited?",
      "options": {
        "A": "Greater competition in a bigger market forces firms to lower prices, which in turn reduces marginal costs uniformly across all output levels.",
        "B": "A larger market enables firms to specialize and scale up production, spreading fixed costs over more units and gaining from learning-by-doing; these economies of scale and specialization gains can be limited if essential inputs (e.g., skilled labor, intermediate goods) are scarce or if transport/logistical and cross-border frictions constrain higher output.",
        "C": "Regional harmonization of standards eliminates product variety, and this uniformity always reduces unit costs regardless of industry technology or demand patterns.",
        "D": "Higher internal tariffs within the region incentivize relocating production to lower-cost countries, which inevitably lowers domestic unit costs through cheaper imported goods."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem for clarity; emphasized mechanisms (spread fixed costs, learning-by-doing, specialization) and realistic limits (input scarcity, logistical/cross-border frictions). Made distractors plausible but technically incorrect.",
      "content_preserved": true,
      "source_article": "Economic integration",
      "x": 1.2741605043411255,
      "y": 0.9162166714668274,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Comparative advantage as the basis for gains from specialization and trade within integrated economies.",
        "Concept 2: Economies of scale as a mechanism by which larger, integrated markets reduce average costs and boost productivity.",
        "Concept 3: The Second Best framework for economic integration (removing barriers as a pragmatic option when full free trade is infeasible) and its implications for welfare outcomes."
      ],
      "original_question_hash": "2a2bd9bf"
    },
    {
      "question": "Why do border delays and non‑tariff barriers (NTBs) matter for international trade beyond the effect of tariffs, and how do they influence a firm’s choice of foreign markets to serve?",
      "options": {
        "A": "They impose extra time, compliance, and uncertainty costs per shipment, raising the delivered price and reducing the set of profitable markets—especially for time‑sensitive or high‑value goods; these cross‑border frictions are much smaller or absent in domestic trade.",
        "B": "They mainly work by changing currency exchange rates, and this exchange‑rate channel is the primary reason firms reduce foreign market participation.",
        "C": "They generally raise product quality and safety standards across borders, which makes foreign markets uniformly more attractive to exporters.",
        "D": "They apply only to services trade and not to goods, so manufacturing exporters’ market choices are unaffected."
      },
      "correct_answer": "A",
      "simplification_notes": "Rewrote the prompt in clearer academic language, focused on how NTBs and delays raise delivered costs (time, compliance, uncertainty) and thereby narrow profitable markets; preserved original alternatives but made them concise and plausible.",
      "content_preserved": true,
      "source_article": "International trade",
      "x": 1.2844713926315308,
      "y": 0.8984416723251343,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Trade costs and barriers (tariffs, border delays, non-tariff barriers, language/cultural differences, legal systems) that make international trade more complex than domestic trade.",
        "Concept 2: Role of institutions and governance (e.g., WTO, statistical agencies) in facilitating, standardizing, and providing data for international trade.",
        "Concept 3: Drivers of the trade system (globalization, advanced technology, outsourcing, multinational corporations) and their impact on the scale and nature of international trade."
      ],
      "original_question_hash": "d9702ec4"
    },
    {
      "question": "Why must freedom of religion explicitly protect non-belief (freedom from religion) in addition to belief, practice, and worship, and how does that protection support the principle of state neutrality?",
      "options": {
        "A": "Because protecting non-belief enables the state to establish and enforce a single official creed by law.",
        "B": "Because without protection for non-belief, government endorsement or favouring of a particular religion or belief can coerce individuals and make religious freedom dependent on assent, thereby undermining state neutrality.",
        "C": "Because protecting non-belief ensures every religious group is equally constrained by secular law, which effectively removes religious liberty rather than protecting it.",
        "D": "Because non-belief is independent of the state's treatment of belief or practice and therefore does not affect whether the state is neutral."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the original multipart question into a concise prompt; clarified key terms (non-belief, state neutrality); removed historical examples and legal case references while preserving the conceptual link between protecting non-belief and preventing state coercion.",
      "content_preserved": true,
      "source_article": "Freedom of religion",
      "x": 0.49871110916137695,
      "y": 0.44860801100730896,
      "level": 2,
      "concepts_tested": [
        "The components and interplay of freedom of religion (belief, practice, worship, and non-belief) and why each is protected.",
        "The relationship between religious liberty and secular law, including how conflicts are resolved (e.g., exemptions, court decisions).",
        "The interaction between state religion and FoRB, and how establishment affects the rights of minority or non-state-religion communities."
      ],
      "original_question_hash": "f054923f"
    },
    {
      "question": "Why is intellectual property (IP) described as the 'irreducible core' that links creativity to economic value in the creative industries?",
      "options": {
        "A": "Because IP guarantees that every creative work will be profitable and removes market risk or the need for licensing.",
        "B": "Because IP ensures equal access for everyone to use a work, eliminating the need for licenses or contracts.",
        "C": "Because IP grants exclusive, time-limited rights to exploit a work, creating a temporary monopoly that allows licensing and sale of rights—thus enabling financiers to invest and creators to monetize their output.",
        "D": "Because IP makes creative activity inherently valuable regardless of market demand or exploitation, so value arises automatically."
      },
      "correct_answer": "C",
      "simplification_notes": "Clarified the original question to ask why IP is called the 'irreducible core'; removed broader article context, kept focus on how IP enables exclusivity, licensing and financing; options rephrased to be concise and plausible misconceptions retained.",
      "content_preserved": true,
      "source_article": "Creative industries",
      "x": 1.2850552797317505,
      "y": 0.9536318182945251,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Intellectual property as the core mechanism linking creativity to economic value in the creative industries (the “irreducible core” of IP exchange for finance/rights).",
        "Concept 2: Competing definitions (creative industries vs. cultural industries vs. creative economy) and their policy implications for what activities are included or excluded.",
        "Concept 3: Creativity as a driver of the knowledge-based economy, and the debates over which activities (e.g., education, gastronomy, engineering-related creative tasks) belong in the domain, affecting how wealth and jobs are created."
      ],
      "original_question_hash": "7c767f10"
    },
    {
      "question": "How does social constructionism account for the existence and stability of social categories (for example, money or gender) when there is no single natural essence behind them?",
      "options": {
        "A": "They are universal natural facts that are discovered by empirical science and exist independently of social interaction.",
        "B": "They arise from ongoing social interactions and negotiated meanings that become stabilized as shared norms and institutions guiding behavior, but they can change when discourse or power relations shift.",
        "C": "They are only linguistic labels with no real effect on how people behave in everyday life.",
        "D": "They are arbitrary labels that vanish as soon as scientific explanations supersede them."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened and domain jargon reduced; the core idea—that social categories are produced and sustained by social interaction and negotiation—was preserved. Options were made concise and equally plausible.",
      "content_preserved": true,
      "source_article": "Social constructionism",
      "x": 1.210485577583313,
      "y": 1.0101699829101562,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Reality is socially constructed through ongoing interactions and negotiations rather than being determined by empirical observation alone.",
        "Concept 2: Meanings arise from mental and linguistic representations; there is no independent external foundation for many phenomena.",
        "Concept 3: Social constructs are maintained and can vary across cultures through collective consensus and cultural practices, with individuals both internalizing and shaping narratives."
      ],
      "original_question_hash": "fb5791cf"
    },
    {
      "question": "Two neighboring communities that once had different languages and rituals intermarry over generations, adopt a common language, blend customs, and begin to identify as a single, new collectivity with its own traditions. Which mechanism best explains the formation of this new ethnicity?",
      "options": {
        "A": "Endogamy-based maintenance of the original groups (continued in‑group marriage that preserves separate identities)",
        "B": "Amalgamation — the fusion of previously distinct groups into a new, shared ethnicity",
        "C": "Language shift leading to assimilation of one group into the existing other group (no new distinct ethnicity)",
        "D": "Ethnogenesis driven by persistence of primordial traits (formation tied to ancient, unchanged characteristics)"
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the scenario to its essential processes (intermarriage, language adoption, cultural mixing) and clarified each option with short definitions; preserved key theoretical terms (amalgamation, endogamy, language shift, ethnogenesis, primordial traits).",
      "content_preserved": true,
      "source_article": "Ethnicity",
      "x": 1.1926079988479614,
      "y": 0.9643174409866333,
      "level": 2,
      "concepts_tested": [
        "Ethnicity maintenance: Ethnicity is sustained through long-term social practices and boundaries such as endogamy, shared language, culture, traditions, religion, and social treatment.",
        "Ethnicity change and formation: Ethnic identities can shift or merge through processes like assimilation, amalgamation, language shift, intermarriage, adoption, and religious conversion, leading to ethnogenesis or panethnicity.",
        "Theoretical framing: Ethnicity is viewed through competing theories—primordialism (enduring, real traits) versus constructivism (socially constructed, situational identity)."
      ],
      "original_question_hash": "83d04407"
    },
    {
      "question": "Compared with passive portfolio capital, how does the FDI requirement for active management participation and transfer of know‑how change the investor's decision problem?",
      "options": {
        "A": "It makes the investor focus mainly on local market size and short‑term returns rather than long‑term value.",
        "B": "It requires the investor to obtain direct control and transfer technology/management so they can coordinate long‑run value creation—something passive capital cannot easily achieve.",
        "C": "It reduces the need for formal governance mechanisms because control is exercised informally and the investor need not oversee daily operations.",
        "D": "It severs the link between the investment and any transfer of technology or managerial practices, making it equivalent to portfolio investment."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; emphasized contrast between FDI (active control, know‑how transfer) and passive portfolio flows; distractors reworded to remain plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Foreign direct investment",
      "x": 1.3057540655136108,
      "y": 0.8984997868537903,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Control/ownership threshold as the defining feature of FDI, enabling lasting managerial influence.",
        "Concept 2: FDI involves active participation (management, joint ventures, technology/know-how transfer) beyond passive capital.",
        "Concept 3: Theoretical rationale for FDI based on production cost differences and factor endowments (labor vs capital) leading to specialization and international investment."
      ],
      "original_question_hash": "d222fb4d"
    },
    {
      "question": "How does an electronic, paperless Single Window change the cost and speed of cross-border trade, and why is it central to trade facilitation?",
      "options": {
        "A": "By centralizing submission of trade data and enabling automated, cross-agency checks and risk-based processing, it removes duplicate data requests and reduces clearance delays, lowering trade transaction costs and speeding up processing.",
        "B": "By abolishing regulatory checks so goods move without verification, guaranteeing immediate clearance and eliminating delays.",
        "C": "By forcing all data through a single agency without coordinated inter-agency processing, which can create a new bottleneck and slow clearance.",
        "D": "By adding more manual forms and steps for traders to comply with, increasing compliance costs and lengthening processing times."
      },
      "correct_answer": "A",
      "simplification_notes": "Condensed the original multipart question into a single clear sentence; retained technical terms (Single Window, paperless trade, cross-agency checks, trade transaction costs); made options concise and plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Trade facilitation",
      "x": 1.2851378917694092,
      "y": 0.8949767351150513,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Simplification, standardization, and harmonization of trade procedures and information flows as the core mechanism to reduce costs and increase efficiency.",
        "Concept 2: The regulatory interface between government bodies and traders (e.g., customs administration) as a focal point for reforms that improve border procedures and reduce delays.",
        "Concept 3: Implementation of electronic, paperless trade systems (e.g., single windows) as concrete mechanisms that translate procedural reforms into lower trade costs and faster processing, influencing competitiveness."
      ],
      "original_question_hash": "fc2dd01f"
    },
    {
      "question": "In marketing, brand consonance means aligning all brand touchpoints (ads, products, stores, employees) so they communicate the same message. Why does this alignment strengthen a corporation's identity?",
      "options": {
        "A": "Because it increases the number of distinct messages across channels, which supposedly improves overall brand recall.",
        "B": "Because it reduces consumers' cognitive load, enabling faster recognition and greater trust since the consistent signal is easier to process.",
        "C": "Because it allows each department to craft separate brand messages, increasing perceived diversity and choice.",
        "D": "Because it relies on frequent logo and identity changes to keep the brand feeling new and relevant."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified 'brand consonance' and 'touchpoints' for undergraduates, used clearer concise wording, preserved the original options' intent and plausibility, and kept the correct answer unchanged.",
      "content_preserved": true,
      "source_article": "Corporate identity",
      "x": 1.3433252573013306,
      "y": 0.9877581000328064,
      "level": 2,
      "concepts_tested": [
        "Brand consonance: alignment of all brand touchpoints to convey a unified message, strengthening corporate identity.",
        "Integrated Marketing Communications (IMC): a process/framework to achieve clarity and consistency in brand positioning across the organization.",
        "Role of guidelines and multi-sensory elements: how logos, colors, typography, décor, and employee presentation contribute to a consistent corporate identity."
      ],
      "original_question_hash": "98f71fd8"
    },
    {
      "question": "If an offender receives cognitive‑behavioral skills training (CBT) together with stable social supports (e.g., steady employment, stable housing, and pro‑social networks), how does this combination affect their likelihood of reoffending, and why does it address both individual decision processes and environmental risk factors?",
      "options": {
        "A": "It implies that harsher punishment alone deters future crime and there is no need to change cognition or social conditions.",
        "B": "CBT improves decision‑making and coping skills while social supports reduce situational stressors, together shifting the offender’s perceived costs and benefits toward lawful choices and promoting lasting desistance.",
        "C": "It only succeeds if the offender formally admits guilt and accepts responsibility; without that admission the combined intervention will not work.",
        "D": "It operates by isolating offenders from society; removing offenders from social environments alone is sufficient to prevent recidivism."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified the intervention components (CBT + examples of social supports), shortened and focused the question on the dual mechanism (individual cognition + environment), and rephrased options to be concise and plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Criminology",
      "x": 1.179487705230713,
      "y": 0.9305343627929688,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Etiology of crime and the personality of criminals (causation of crime) — understanding why crime occurs and what personality factors are involved.",
        "Concept 2: Nature, administration, and development of criminal law — how laws are formed, applied, and evolve within societies.",
        "Concept 3: Mechanisms of crime control and rehabilitation — how the justice system suppresses crime and rehabilitates offenders through institutions and policies."
      ],
      "original_question_hash": "50ed1040"
    },
    {
      "question": "According to nationalism’s idea that a nation should govern itself, how does “nation–state congruence” create political legitimacy?",
      "options": {
        "A": "By prioritizing centralized coercive power over identity, legitimizing whoever controls the military or state apparatus regardless of the nation.",
        "B": "By grounding sovereignty in a self‑defined national community and aligning the state’s borders with that nation, so political authority is legitimate only when it derives from that community, reducing competing internal claims.",
        "C": "By requiring recognition or approval from other states or external powers to make a government legitimate.",
        "D": "By treating legitimacy as solely the economic success of a centralized government rather than its origin in national identity."
      },
      "correct_answer": "B",
      "simplification_notes": "Reduced abstract phrasing, defined 'nation–state congruence' in plain academic terms, removed historical detail, and made each distractor a plausible alternative view while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Nationalism",
      "x": 1.1856051683425903,
      "y": 0.9506867527961731,
      "level": 2,
      "concepts_tested": [
        "Nation-state congruence and sovereignty: nationalism asserts the nation should govern itself and be the sole source of political power, highlighting the relationship between nation, state, self-determination, and sovereignty.",
        "National identity formation and its bases: the aim to build a single national identity from shared characteristics (culture, language, homeland, etc.) and the distinction between ethnic vs. civic nationalism as different foundations for unity and political legitimacy.",
        "Positive vs. negative political implications: nationalism can promote unity, cultural revival, and pride, but can also legitimize divisions, suppress minorities, undermine human rights, and trigger conflict or war, relating to debates about patriotism and cosmopolitanism."
      ],
      "original_question_hash": "8647e517"
    },
    {
      "question": "Why may secular law limit freedom of religion, and what legal standard do courts normally use to decide when a restriction is permissible?",
      "options": {
        "A": "Religious freedom is absolute; courts must invalidate any secular law that limits religious belief or practice.",
        "B": "Only religious beliefs are protected; any secular restriction justified by public safety or public order is illegitimate.",
        "C": "Religious freedom is not absolute. Courts permit restrictions when a law is neutral and generally applicable; if a law targets religion, the state must show a compelling interest and that the restriction is narrowly tailored, with courts balancing competing rights.",
        "D": "The state may regulate religious practice (but not belief); courts always require proof that the religious group violated anti‑discrimination laws before allowing any limitation."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; retained legal terms (neutral and generally applicable, compelling interest, narrowly tailored) while removing historical examples and lengthy background.",
      "content_preserved": true,
      "source_article": "Freedom of religion",
      "x": 0.5016083717346191,
      "y": 0.4472223222255707,
      "level": 2,
      "concepts_tested": [
        "The tripartite/quadruple structure of FoRB: freedom of belief, freedom of practice, freedom of worship, and the right not to profess or practice—how they interrelate and under what circumstances they may diverge or be limited.",
        "The relationship between religious liberty and secular law/government: how practices motivated by religion may interact with or challenge secular legal frameworks, and how courts balance competing rights and interests.",
        "The role of international and national legal frameworks in protecting FoRB: how universal rights are implemented domestically and the implications for policy, governance, and enforcement across jurisdictions."
      ],
      "original_question_hash": "bf9df9bf"
    },
    {
      "question": "In a civil fraud claim, why must the plaintiff prove that they relied on the defendant's misrepresentation, and how does that reliance connect to the harm the plaintiff seeks to recover?",
      "options": {
        "A": "Proving reliance only helps show the defendant's intent; if reliance is absent, the court still assumes damages.",
        "B": "Reliance connects the misrepresentation to the plaintiff's decision, creating the causal chain from deception to loss; that causal link is required to recover damages.",
        "C": "Reliance is unnecessary for contract-related misrepresentations; in those cases harm is presumed without proving reliance.",
        "D": "Reliance is irrelevant in civil fraud—any false statement by the defendant automatically establishes liability and entitles the plaintiff to damages."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the question to emphasize the need to prove reliance and its causal role in establishing loss; options were tightened to present plausible mistaken views about reliance, causation, and contract exceptions while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Fraud",
      "x": 1.295838475227356,
      "y": 0.8476226329803467,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The core element of civil fraud—intentional misrepresentation or concealment relied upon by the victim, leading to detriment or harm.",
        "Concept 2: The civil vs. criminal distinction in fraud, including differences in remedies (rescission, damages, punitive damages) and penalties (fines, imprisonment) and how the same conduct can trigger both pathways.",
        "Concept 3: The evidentiary burden in fraud cases, including the difficulty of proving intent and, in some jurisdictions, the need for clear and convincing evidence."
      ],
      "original_question_hash": "422f1ce2"
    },
    {
      "question": "When risk is defined as the variance of portfolio returns $Var(R_p)$, why does diversification typically reduce portfolio risk?",
      "options": {
        "A": "Because individual asset variances cancel each other when combined, so total portfolio variance necessarily falls regardless of correlations.",
        "B": "Because portfolio variance depends on both individual variances and covariances; adding assets that are not perfectly correlated introduces covariance terms that can offset some volatility, reducing overall variance. Lower correlations give greater potential reduction.",
        "C": "Because diversification entirely removes idiosyncratic (asset-specific) risk, so portfolio variance approaches zero as more assets are added.",
        "D": "Because diversification lowers the portfolio's expected return, and a lower expected return automatically reduces the risk of suffering a loss."
      },
      "correct_answer": "B",
      "simplification_notes": "I shortened and clarified the question, explicitly noted risk as $Var(R_p)$, and made each option a concise, plausible explanation (keeping the original correct answer). Distractors reflect common misconceptions about variance, covariances, and return.",
      "content_preserved": true,
      "source_article": "Financial risk",
      "x": 1.3461557626724243,
      "y": 0.9040278792381287,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Modern portfolio theory defines risk as the portfolio variance and links diversification to risk management.",
        "Concept 2: The risk-return trade-off, including the equity risk premium, explaining why higher risk investments may offer higher expected returns.",
        "Concept 3: Mechanisms and relationships among market risk factors (equity, interest rate, currency, commodity) and their impact on asset values and firm profitability, including cause-effect examples like interest rate changes affecting banks."
      ],
      "original_question_hash": "9700e2bb"
    },
    {
      "question": "A proposed policy produces substantial instrumental benefits (it is useful as a means to other good outcomes) but harms something widely held to have intrinsic value (good in itself). How does the intrinsic/instrumental distinction change the moral assessment of this policy?",
      "options": {
        "A": "The policy is justified whenever the sum of instrumental benefits exceeds the losses, so intrinsic value is overridden by greater net instrumental gain.",
        "B": "Only instrumental value is relevant for moral evaluation, so intrinsic value can be disregarded in judging the policy.",
        "C": "Harms to intrinsically valuable things cannot be justified solely by larger instrumental gains, because intrinsic value places non-negotiable moral limits on what means are permissible.",
        "D": "Intrinsic and instrumental values are treated as interchangeable, so the policy's morality depends only on the overall net outcome, not on the kind of value affected."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording shortened and clarified (policy, instrumental benefits, intrinsic value); options made concise and kept all four plausible; preserved original core distinction and correct answer.",
      "content_preserved": true,
      "source_article": "Value theory",
      "x": 1.2316176891326904,
      "y": 1.0176522731781006,
      "level": 2,
      "concepts_tested": [
        "Intrinsic vs instrumental value: how values can be good in themselves versus valuable as means to other ends.",
        "Realism vs anti-realism about value: whether values exist as objective features of reality or as subjective/constructed phenomena.",
        "Sources of value and value pluralism: different bases for value (hedonism, desire theory, perfectionism) and the idea that multiple intrinsic values across domains may be difficult to compare."
      ],
      "original_question_hash": "9f525917"
    },
    {
      "question": "How do practical universal-design features such as curb cuts demonstrate that accessibility is co-created by physical spaces and diverse users?",
      "options": {
        "A": "They are designed only for people with disabilities, which unintentionally limits other users.",
        "B": "They remove a barrier that many different people face; a feature introduced for one group often benefits others, showing how design and user needs mutually shape accessibility.",
        "C": "They require complex, specialist technologies and knowledge that only accessibility experts can implement.",
        "D": "They make spaces inefficient and costly, reducing everyday usability for the general population."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified while keeping the idea that a design feature introduced for one group can benefit many; distractors were rephrased to remain plausible.",
      "content_preserved": true,
      "source_article": "Universal design",
      "x": 1.339338779449463,
      "y": 0.97359299659729,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Universal design as a rights-based approach focused on inclusion and reducing barriers to participation.",
        "Concept 2: Trade-offs and dilemmas in implementing universal design, including the idea that not every need can be addressed and that different kinds of knowledge are relevant for different purposes.",
        "Concept 3: Practical, cross-user mechanisms (e.g., curb cuts) that illustrate how design intended for one group also benefits others, highlighting the socio-material relationship between spaces and diverse users."
      ],
      "original_question_hash": "3a73d5f6"
    },
    {
      "question": "When buyers lack full information about product quality, why can a well-known brand charge higher prices?",
      "options": {
        "A": "The brand acts as a credible signal of expected quality, reducing information asymmetry so consumers pay a premium for reassurance.",
        "B": "Branding guarantees higher intrinsic product quality through investments, so higher prices simply reflect objectively better products.",
        "C": "The brand mainly shifts preferences via emotional or symbolic appeal, increasing demand regardless of objective product quality.",
        "D": "The brand removes product differentiation, making price the sole determinant of consumer choice."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording was tightened and technical terms preserved (e.g., \"information asymmetry\", \"credible signal\"). The context of imperfect information was kept; distractors were phrased as plausible alternative mechanisms.",
      "content_preserved": true,
      "source_article": "Brand equity",
      "x": 1.3452918529510498,
      "y": 0.9623202681541443,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Brand equity serves as a credible signal of product quality to imperfectly informed buyers, enabling price premiums and influencing price structure (mechanism from information economics).",
        "Concept 2: Cognitive drivers—brand awareness and brand associations—shape how consumers perceive product attributes, thereby linking brand equity to demand and preference (mechanism from cognitive psychology).",
        "Concept 3: Brand equity is driven by factors like brand awareness, brand perspective, and brand attachment, and it is difficult to quantify, with a tension between qualitative judgments and quantitative metrics (relationship between drivers and measurement challenges)."
      ],
      "original_question_hash": "becc5a9d"
    },
    {
      "question": "According to social epistemology, knowledge is often a collective achievement that depends on social structures. Which mechanism best explains how an individual's belief can legitimately count as knowledge because of these social processes, even if the individual lacks full independent justification?",
      "options": {
        "A": "The belief counts as knowledge only if the individual personally verifies every relevant fact by direct observation.",
        "B": "Reliable testimony, trustworthy institutions, and collaborative division of cognitive labor provide justificatory support, allowing the individual’s belief to qualify as knowledge via the social network.",
        "C": "A belief counts as knowledge only when every member of the relevant community unanimously agrees, regardless of any individual’s justification.",
        "D": "Whether a belief is knowledge is determined solely by the original source’s authority, irrespective of the individual’s own engagement or critical assessment."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording made more concise and academic; core idea reframed as a choice between clear mechanisms (personal verification, social justification via testimony/institutions, unanimous agreement, sole source authority). Technical phrase 'collaborative cognitive labor' simplified to 'collaborative division of cognitive labor'.",
      "content_preserved": true,
      "source_article": "Social epistemology",
      "x": 1.2404289245605469,
      "y": 1.0217702388763428,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Knowledge as a collective achievement / the social dimension of knowledge (how social context enables or constrains knowledge and its attribution)",
        "Concept 2: Testimony and epistemic authority (how information from others contributes to knowledge and what criteria assess its reliability)",
        "Concept 3: Group epistemology and social attribution (when and how knowledge can be attributed to groups versus individuals, and the criteria for such attributions)"
      ],
      "original_question_hash": "b618d734"
    },
    {
      "question": "How does mantle convection act as the engine of plate tectonics, and which processes at plate boundaries mainly produce the horizontal motion of lithospheric plates?",
      "options": {
        "A": "Mantle convection uniformly thins the lithosphere everywhere, so plates slide laterally in all directions at the same rate due to reduced thickness.",
        "B": "Mantle convection establishes large-scale mantle flow that couples to the lithosphere; upwelling at mid-ocean ridges creates new crust and produces a ridge-push force, while cooling, sinking slabs at subduction zones exert a slab-pull that drags plates—these boundary-scale forces together drive horizontal plate motion.",
        "C": "Plate motion is driven by wind-like flows in the uppermost atmosphere and ocean surface that push plates laterally, largely independent of mantle dynamics.",
        "D": "The geomagnetic field applies torque to the lithosphere and directly drives plate motion, with mantle convection playing at most a minor role."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified to focus on mantle convection as the driver and the two main boundary forces (ridge push and slab pull). Distractor options rephrased to be scientifically plausible but incorrect. Technical terms retained.",
      "content_preserved": true,
      "source_article": "Earth science",
      "x": 1.6336157321929932,
      "y": 0.9944612383842468,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Mantle convection as the engine of plate tectonics. How heat-driven convection in the mantle causes lithospheric plates to move.",
        "Concept 2: Plate boundary types and their geologic consequences. How divergent boundaries create new crust, convergent boundaries recycle crust via subduction, and transform boundaries involve lateral plate movement, with earthquakes linked to plate motion.",
        "Concept 3: Interdisciplinary nature of Earth science. How geology, geochemistry, geophysics, paleontology, geomorphology, and related fields connect to explain Earth’s materials, structures, and processes."
      ],
      "original_question_hash": "35e006b6"
    },
    {
      "question": "Why does a sign (releasing) stimulus reliably trigger a fixed action pattern (FAP) that typically runs to completion, instead of the behavior being gradually changed by experience or learning?",
      "options": {
        "A": "Because the sign stimulus activates a genetically encoded, hard-wired neural program that generates a stereotyped motor sequence which proceeds almost automatically and is largely independent of learning or current context.",
        "B": "Because the sign stimulus becomes linked to a reward through reinforcement, so the response is gradually shaped by repeated pairing with positive outcomes.",
        "C": "Because the animal habituates to the sign stimulus if it is not followed by feedback, causing the response to diminish rather than run to completion.",
        "D": "Because continuous environmental feedback alters motor commands, producing a flexible, context-dependent sequence instead of a fixed, invariant one."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording tightened and clarified 'sign stimulus' and 'FAP'; retained technical terms; distractors rephrased to reflect associative learning, habituation, and environmental modulation.",
      "content_preserved": true,
      "source_article": "Animal behaviour",
      "x": 1.342658519744873,
      "y": 1.0233547687530518,
      "level": 2,
      "concepts_tested": [
        "Fixed action patterns and sign/releasing stimuli as mechanisms linking specific stimuli to complete, invariant behavioral sequences",
        "Habituation as a simple learning process where responses to irrelevant stimuli diminish",
        "Associative learning (classical conditioning) as a mechanism where a neutral stimulus comes to elicit a response after pairing with a meaningful stimulus"
      ],
      "original_question_hash": "747751a1"
    },
    {
      "question": "Why do information systems use multiple metadata types (descriptive, structural, administrative) to describe a resource rather than a single metadata type?",
      "options": {
        "A": "Because the different metadata types simply present the same attributes in different formats, without adding distinct information.",
        "B": "Because descriptive metadata supports discovery and identification; structural metadata records how a resource’s components are organized; and administrative metadata provides management information (e.g., provenance, rights, and technical details).",
        "C": "Because only descriptive metadata is necessary to find and identify resources; structural and administrative metadata are optional add-ons.",
        "D": "Because structural metadata specifies rights and permissions while descriptive metadata defines the physical or logical ordering of a document’s parts."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the question; stated each metadata role succinctly; made incorrect options plausible misconceptions without changing the core tested concept.",
      "content_preserved": true,
      "source_article": "Metadata",
      "x": 1.3817516565322876,
      "y": 1.0369831323623657,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Metadata is data about data and serves to describe, locate, retrieve, and manage the underlying data, illustrating the core relationship between data and its metadata.",
        "Concept 2: Different metadata types (descriptive, structural, administrative, etc.) encode distinct relationships to a resource (discovery/identification, organization, governance), showing how metadata supports various workflows.",
        "Concept 3: Multiple classification models (e.g., Bretherton & Singley; Kimball; NISO) reflect different perspectives on metadata roles (structure vs. guidance; technical vs. business vs. process; descriptive/structural/administrative), indicating that metadata concepts are contextual and model-dependent."
      ],
      "original_question_hash": "6b2f4e30"
    },
    {
      "question": "Why do empiricists insist that the scientific method test hypotheses by observation and experiment instead of relying only on pure a priori reasoning?",
      "options": {
        "A": "Because observations and experiments give a public, repeatable basis to evaluate predictions, allow falsification and iterative revision, and thus tie claims to how the world actually behaves, whereas pure a priori reasoning works within a closed logical space that may not match empirical reality.",
        "B": "Because a priori reasoning yields universally certain conclusions that empirical observations could never contradict, so testing is merely confirmatory.",
        "C": "Because experiments can be designed to eliminate all cognitive biases, producing infallible results that need no further revision.",
        "D": "Because innate ideas provide secure knowledge, so empirical testing is only needed for weaker or derivative claims."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording tightened; maintained technical terms (a priori, falsification); option A preserved the original justification (public/repeatable evidence, falsification, iterative revision) while other options were made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Empiricism",
      "x": 1.1559025049209595,
      "y": 1.0739017724990845,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Knowledge justification derives from sensory experience and empirical evidence, not innate ideas or traditions.",
        "Concept 2: The scientific method in empiricism relies on testing hypotheses through observation and experiment, prioritizing evidence over a priori reasoning or revelation.",
        "Concept 3: Empiricism views knowledge as tentative and probabilistic, continually revised in light of new empirical data and subject to cognitive biases."
      ],
      "original_question_hash": "d8262b51"
    },
    {
      "question": "Economic sociology holds that economic actions are embedded in social relations and shared meanings. Which mechanism best explains how this embeddedness can change the terms and stability (durability) of a market exchange compared with a purely impersonal, rule‑based transaction?",
      "options": {
        "A": "Trust and shared social norms lower information asymmetries and enforcement costs, making longer‑term relationships possible and yielding more favorable terms.",
        "B": "Embedding economic activity in social ties erodes efficiency incentives (favoring in‑group loyalty), thereby producing generally poorer outcomes.",
        "C": "Social networks introduce extra coordination costs and uncertainty, adding friction that makes exchanges less predictable and more fragile.",
        "D": "Embeddedness converts price‑driven decisions into moral obligations, effectively replacing market mechanisms and eliminating normal market behavior."
      },
      "correct_answer": "A",
      "simplification_notes": "Shortened and clarified the stem (kept 'embeddedness' and comparison to impersonal transactions); made each distractor a plausible theoretical claim; preserved the original correct rationale about trust, norms, information asymmetry and enforcement costs.",
      "content_preserved": true,
      "source_article": "Economic sociology",
      "x": 1.195939302444458,
      "y": 0.932757556438446,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Embeddedness of economic activity in social relations and meanings (how social context shapes economic exchanges and how exchanges influence social relations)",
        "Concept 2: The role of social institutions and culture (capitalism, modernity, religion) in shaping economic structures, and vice versa",
        "Concept 3: Social consequences and meanings of economic exchanges (how economic actions generate social effects, norms, and interactions)"
      ],
      "original_question_hash": "cd48ed2b"
    },
    {
      "question": "How can contextual factors make an argument that has a formally valid deductive structure nonetheless fallacious?",
      "options": {
        "A": "Context is irrelevant; if an argument has a formally valid form, that guarantees it is not fallacious.",
        "B": "Context can show that, despite a valid form, the premises are irrelevant, misleading, false, or rest on hidden assumptions, making the argument fallacious in practice.",
        "C": "Whether an argument is fallacious depends only on the speaker's intent; if the error is unintentional, it is not a fallacy.",
        "D": "The presence of specialized or ambiguous language automatically renders an argument fallacious, independent of context."
      },
      "correct_answer": "B",
      "simplification_notes": "Rephrased the original question into a concise undergraduate-level sentence; removed lengthy examples and background, kept the core idea that formal validity differs from practical soundness and that context can affect premises' relevance or truth. Options were made brief and all remain plausible.",
      "content_preserved": true,
      "source_article": "Fallacy",
      "x": 1.3036478757858276,
      "y": 1.0994993448257446,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Formal vs informal fallacies and why a formally valid argument can still be fallacious due to incorrect reasoning or content.",
        "Concept 2: Mechanisms generating fallacies (intentional manipulation, cognitive biases, language limitations, context ignorance) and the role of context in determining fallacy.",
        "Concept 3: Relationship between fallacies and argument evaluation (soundness continuum; evaluating form vs. content; context-dependent assessment)."
      ],
      "original_question_hash": "160f3085"
    },
    {
      "question": "Why does codifying private law in civil‑law systems tend to produce more uniform outcomes across similar disputes than the precedent‑driven approach used in common‑law systems?",
      "options": {
        "A": "Because codification creates explicit, general statutory rules that constrain judicial discretion and directly guide decisions, producing consistent applications; precedent‑based systems derive outcomes from past judicial interpretations, which can vary with factual differences and doctrinal evolution.",
        "B": "Because codification increases judicial discretion by supplying broad, open‑ended principles for judges to develop case‑by‑case, while precedent forces judges to follow narrow prior decisions, reducing variation.",
        "C": "Because civil‑law codes are inherently adaptive, requiring judges to craft new, bespoke rules for each dispute under the code, whereas common law rigidly applies statutes without room for judge‑made adjustments.",
        "D": "Because civil‑law jurisdictions typically delegate private dispute resolution to administrative or nonjudicial tribunals, removing the variability that arises from independent judicial precedent."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and focused on the contrast between codification and precedent; technical terms (codification, precedent, judicial discretion) were retained and clarified; distractors were reworded to be plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Law",
      "x": 1.261707067489624,
      "y": 0.9221591353416443,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Sources and creation of law: statutes, executive orders/regulations, and judicial precedent, plus how codification vs. case law functions in different legal systems.",
        "Concept 2: Public law vs private law and the governance/alt-dispute framework: how the public-private divide shapes legal authorities and problems, and how civil vs. common law reflect different emphases on codification and precedent.",
        "Concept 3: Constitutions, rights, and the normative aims of law: how constitutional frameworks and rights shape legal authority and the law’s role in politics, economics, history, and society (including justice, equality, and fairness)."
      ],
      "original_question_hash": "096cb165"
    },
    {
      "question": "Which statement best expresses the Gestalt claim that \"the whole is something other than the sum of its parts\" in terms of perceptual organization?",
      "options": {
        "A": "Perception is produced by summing elementary sensory components, so the whole equals the numerical sum of its parts.",
        "B": "Perception is organized so that the entire configuration constrains and defines what counts as the parts; parts derive their identity from the whole.",
        "C": "Perception is driven mainly by learned associations between discrete features and operates independently of any global configuration.",
        "D": "Perception breaks objects into fixed elemental features and later reassembles those elements in memory to form a whole."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the original question to a single clear sentence; retained technical terms (whole, parts, configuration); rewrote distractors to reflect structuralist and associationist alternatives.",
      "content_preserved": true,
      "source_article": "Gestalt psychology",
      "x": 1.1217371225357056,
      "y": 1.014275312423706,
      "level": 2,
      "concepts_tested": [
        "Whole over parts principle: Perception is organized so that the whole has priority; parts are defined by the structure of the whole.",
        "Emergent patterns/configurations: Perceptual experience arises from patterns or configurations rather than merely summing individual components.",
        "Opposition to atomism/structuralism: Psychology should not decompose phenomena into basic elements; understanding comes from holistic organization rather than elemental components."
      ],
      "original_question_hash": "a2f9b5ac"
    },
    {
      "question": "Vision is not a literal, one-to-one copy of the retinal image. Which mechanism best explains how the brain constructs the conscious percept from retinal neural signals?",
      "options": {
        "A": "The retina encodes a complete, unambiguous representation of the scene and the brain only transmits this faithful copy to higher visual areas.",
        "B": "Perception depends exclusively on immediate, low-level retinal features (e.g., luminance or local contrast) with no influence from context, prior knowledge, or expectations.",
        "C": "The brain produces perceptual interpretations randomly, so what we see varies without systematic use of prior information or contextual cues.",
        "D": "The brain integrates retinal input with prior knowledge, expectations, and contextual cues—performing unconscious (e.g., Bayesian-like) inference—to construct the most plausible scene representation."
      },
      "correct_answer": "D",
      "simplification_notes": "Wording was made more concise and direct for undergraduates; retained technical terms (retinal signals, inference, context). Added explicit reference to unconscious/Bayesian inference to match the article. Distractors were kept plausible but clarified.",
      "content_preserved": true,
      "source_article": "Visual perception",
      "x": 1.7079027891159058,
      "y": 1.1019374132156372,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Visual transduction in the retina (rods vs cones) converting light into neural signals.",
        "Concept 2: The hierarchical visual processing pathway and the two-streams hypothesis (ventral “what” vs. dorsal “where/how”).",
        "Concept 3: Perception as a brain-constructed interpretation of retinal input, not a direct one-to-one mapping."
      ],
      "original_question_hash": "7a7f7074"
    },
    {
      "question": "Why do the vector-space axioms require closure under both vector addition and scalar multiplication, and how does that ensure linear combinations $a_{1}v_{1}+\\cdots+a_{k}v_{k}$ are well-defined and remain in the space?",
      "options": {
        "A": "Because closure guarantees that any finite linear combination of vectors (sums of scaled vectors) remains inside the same set, so repeated additions and scalings never leave the space.",
        "B": "Because closure under addition alone ensures that rearranging or grouping sums of vectors cannot change their result, making linear combinations unambiguous.",
        "C": "Because closure under scalar multiplication alone guarantees the existence and uniqueness of the zero vector in the space.",
        "D": "Because closure implies every nonzero scalar has a multiplicative inverse inside the space, allowing one to divide vectors by scalars."
      },
      "correct_answer": "A",
      "simplification_notes": "Question shortened and made explicit: asks directly how closure under addition and scalar multiplication makes linear combinations $a_{1}v_{1}+\\cdots+a_{k}v_{k}$ well-defined and remain in the space. Distractor options were phrased as plausible misconceptions.",
      "content_preserved": true,
      "source_article": "Vector space",
      "x": 1.6648296117782593,
      "y": 1.1971302032470703,
      "level": 2,
      "concepts_tested": [
        "Concept 1: A vector space is defined by closure under addition and scalar multiplication and must satisfy eight axioms, which establish the linear structure and enable predictable behavior of combinations of vectors.",
        "Concept 2: Dimension and isomorphism as classification tools—spaces with the same dimension have the same structural properties up to isomorphism, illustrating how independent directions determine the essence of a vector space.",
        "Concept 3: Generalization and interplay with other structures—vector spaces extend beyond Euclidean vectors to infinite dimensions and can be endowed with additional structures (algebras, inner products, norms), linking linear structure to broader mathematical frameworks and applications."
      ],
      "original_question_hash": "cb98db10"
    },
    {
      "question": "In platform-mediated crowdsourcing, why do features like feedback loops, discussion forums, and presentation formats change the quality of the final solutions?",
      "options": {
        "A": "They mainly increase the number of submissions so that, by sheer volume, the best solution is likely to appear.",
        "B": "They shape contributors’ understanding and support iterative improvement, coordination, and visibility, guiding effort toward problem-relevant aspects and thereby improving solution quality.",
        "C": "They transfer ownership or control to the platform, which demotivates contributors and thus reduces the quality of submissions.",
        "D": "They impose rigid formatting and presentation rules that reduce idea diversity and therefore lower solution quality."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; technical phrase 'platform-mediated' retained; options rephrased to be concise and maintain plausibility; core idea that platform features affect understanding, iteration, coordination, visibility preserved.",
      "content_preserved": true,
      "source_article": "Crowdsourcing",
      "x": 1.3093315362930298,
      "y": 1.020599126815796,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Open-call broadcast mechanism—soliciting contributions from a wide public to solve problems, and how this differs from traditional outsourcing.",
        "Concept 2: Platform-mediated collaboration—the role of online platforms, participant communication, and presentation in shaping idea quality and received solutions.",
        "Concept 3: Incentives and ownership—the use of prizes, payments, or recognition and the transfer (or retention) of ownership of solutions, and how this affects participation and outcomes."
      ],
      "original_question_hash": "b8555e82"
    },
    {
      "question": "In open innovation, why must mechanisms that move knowledge across organizational boundaries (e.g., licensing, joint ventures, spin‑offs) be purposively managed and aligned with the firm's business model?",
      "options": {
        "A": "Because alignment ensures external ideas are integrated with the firm's value proposition and revenue model, enabling deliberate exploitation of external knowledge rather than unsystematic spillovers.",
        "B": "Because alignment guarantees every external idea will be absorbed into the firm's internal R&D, thereby increasing internal R&D spending.",
        "C": "Because alignment removes the need for governance and control over collaborations and external contributors.",
        "D": "Because alignment requires that all external knowledge exchanges occur only through acquisitions, preventing other channels like licensing or spin‑offs."
      },
      "correct_answer": "A",
      "simplification_notes": "Rewrote the stem concisely for undergraduates, kept examples of mechanisms, clarified the purpose of purposive management and alignment; options were made plausible but only A preserves the article's rationale.",
      "content_preserved": true,
      "source_article": "Open innovation",
      "x": 1.3174464702606201,
      "y": 1.0167381763458252,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Inbound and outbound open innovation as complementary paths to leverage external ideas and move inventions to market.",
        "Concept 2: Purposely managed knowledge flows across organizational boundaries, including mechanisms like licensing, joint ventures, and spin-offs, aligned with the firm’s business model.",
        "Concept 3: The boundary-permeable ecosystem of innovation, incorporating customers and user innovators and affecting outcomes at the consumer, firm, industry, and societal levels."
      ],
      "original_question_hash": "49966c52"
    },
    {
      "question": "According to the \"space of flows\" perspective, what explains how global finance and media networks function effectively when participants are spread across different locations?",
      "options": {
        "A": "They continue to depend on centralized physical trading floors and co‑located institutions to coordinate activity.",
        "B": "They operate mainly through strictly synchronous, real‑time communication that requires everyone to act simultaneously.",
        "C": "Information and communication technologies (ICT) enable asynchronous and real‑time flows across distances, decoupling activity from location and supporting distributed networks.",
        "D": "Physical proximity is completely replaced by immersive virtual‑reality environments that simulate co‑presence for all participants."
      },
      "correct_answer": "C",
      "simplification_notes": "Reworded the question for clarity, kept technical terms (\"space of flows\", ICT, asynchronous), shortened options while keeping them plausible and distinct.",
      "content_preserved": true,
      "source_article": "Network society",
      "x": 1.1971827745437622,
      "y": 1.0037291049957275,
      "level": 2,
      "concepts_tested": [
        "Space of Flows: technology enables activities across distances without physical proximity, shaping finance, media, and organizational structures.",
        "Timeless Time: disruption of traditional sequences of social time leading to asynchronous interactions and flexible coordination.",
        "Networked Individualism / three-domain focus: individuals operate within networks rather than bounded groups, affecting community, work, and organizations."
      ],
      "original_question_hash": "9ebed1c9"
    },
    {
      "question": "In a model that treats attention as a limited cognitive resource allocated selectively, why does adding a second demanding task typically reduce performance on the primary task?",
      "options": {
        "A": "Because the brain can rapidly switch between tasks, which eliminates interference between them.",
        "B": "Because attentional resources are finite and must be shared; as demand increases, the resources available for each task shrink, producing slower responses or more errors unless one task becomes automatic.",
        "C": "Because attention can be fully divided so both demanding tasks are processed in parallel with no cost, as long as the tasks are unrelated.",
        "D": "Because adding a second task increases physiological arousal, which always improves performance on the primary task."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the stem in clearer academic language, removed extraneous economic wording while keeping the limited-resource and selective-allocation concepts; kept original correct option wording and made all distractors plausible.",
      "content_preserved": true,
      "source_article": "Attention economy",
      "x": 1.2881730794906616,
      "y": 0.9773461818695068,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Attention as a scarce cognitive resource and the mechanism of selective resource allocation (why attention is limited, how resources are allocated when multitasking).",
        "Concept 2: Economic framing of attention (how attention is valued economically, the role of time/value, and why advertising-driven models monetize attention).",
        "Concept 3: Information overload and design filtering (how systems should filter or prioritize information to manage attention, and why mere information abundance can be detrimental)."
      ],
      "original_question_hash": "6952ada0"
    },
    {
      "question": "Why does publishing government data alone not ensure accountability, and what is required to convert that data into effective public oversight?",
      "options": {
        "A": "Because publishing data immediately exposes wrongdoing and automatically triggers sanctions without further action.",
        "B": "Because data must be embedded in governance mechanisms that assign responsibility, ensure data quality and context, enable interpretation, and provide enforcement; without these institutional structures the data can be underused or misinterpreted.",
        "C": "Because technology (analytics and platforms) will by itself detect problems and enforce compliance without human or institutional frameworks.",
        "D": "Because accountability depends solely on political will or leaders' decisions, independent of public access to data."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording made more concise and academic; removed historical examples and extraneous detail; preserved core point that open data alone is insufficient and that governance/institutional mechanisms (responsibility, quality, context, interpretation, enforcement) are essential.",
      "content_preserved": true,
      "source_article": "Open government",
      "x": 1.2453821897506714,
      "y": 0.9922125935554504,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The triad of transparency, participation, and accountability and how they reinforce each other in open government.",
        "Concept 2: The distinction between open data/open data technologies and open government, including why data availability alone does not guarantee accountability.",
        "Concept 3: The role and limits of technology as a facilitator for disclosure and public oversight, highlighting that technology enables information flow but does not automatically create accountability without governance mechanisms."
      ],
      "original_question_hash": "f02036e7"
    },
    {
      "question": "How do proper accounting practices and good records management enable accountability, and why would accountability be undermined without them?",
      "options": {
        "A": "They ensure leaders will always act ethically, so no further oversight is necessary.",
        "B": "They create an auditable trail of decisions, actions, and evidence that lets actors inform others, justify choices, and face sanctions when appropriate; without such records the account-giving relationship becomes opaque and accountability breaks down.",
        "C": "They mainly function as public-relations tools to improve an organization's image, without materially affecting how responsibility is assigned.",
        "D": "They eliminate the need for other accountability mechanisms (e.g., oversight, elections, sanctions) because records alone are sufficient to enforce responsibility."
      },
      "correct_answer": "B",
      "simplification_notes": "Question and options were shortened and clarified for undergraduate readers. Technical phrase 'account-giving relationship' was preserved and rephrased as 'inform others, justify choices, and face sanctions.' Distractor options were framed as plausible misconceptions.",
      "content_preserved": true,
      "source_article": "Accountability",
      "x": 1.2406333684921265,
      "y": 0.9531809091567993,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Accountability as an account-giving relationship involving informing, justifying actions, and accepting potential punishment.",
        "Concept 2: The necessity of proper accounting practices and good records management as preconditions for accountability.",
        "Concept 3: Accountability modes (administrative, political, market, judicial, professional, social) as mechanisms that implement accountability, including specific processes like hierarchical oversight and electoral accountability."
      ],
      "original_question_hash": "fdc0264a"
    },
    {
      "question": "Why do externalities and non-excludable public goods cause market failure? Explain the mechanism by which private decisions diverge from the social optimum.",
      "options": {
        "A": "Private decisions internalize all social costs and benefits, so private and social optima coincide.",
        "B": "Private decisions ignore external costs or benefits, so quantity is determined by private marginal benefit and private marginal cost ($PMB$ and $PMC$) rather than social marginal benefit and social marginal cost ($SMB$ and $SMC$); for non‑excludable public goods, free‑rider behaviour reduces private provision, causing under‑provision relative to the social optimum.",
        "C": "Markets always achieve Pareto efficiency even when externalities or public goods exist, because competitive pressure forces efficient outcomes.",
        "D": "Externalities and public goods are automatically corrected by market price signals alone, so no policy or collective action is needed."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording condensed and clarified for undergraduates; introduced concise notation ($PMB$, $PMC$, $SMB$, $SMC$) and explicitly stated the free‑rider mechanism; distractor options kept plausible.",
      "content_preserved": true,
      "source_article": "Market failure",
      "x": 1.2884156703948975,
      "y": 0.9277800917625427,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Pareto efficiency as the benchmark and how market outcomes can fail to reach it due to externalities, public goods, information asymmetries, or market power.",
        "Concept 2: Externalities and public goods as primary mechanisms driving market failure (why non-private costs/benefits and non-excludability lead to inefficiency).",
        "Concept 3: Government intervention versus government failure (conditions under which policy can improve outcomes and potential downsides)."
      ],
      "original_question_hash": "e0ba0fad"
    },
    {
      "question": "When local cortical activity increases, how does the neurovascular unit usually produce a matching rise in regional cerebral blood flow?",
      "options": {
        "A": "Action potentials released into the bloodstream directly depolarize vascular smooth muscle, causing arteriolar dilation.",
        "B": "Neurotransmitters diffuse from active neurons to vessel walls and produce a rapid, permanent increase in vessel diameter.",
        "C": "Neurons and astrocytes release vasoactive signals in response to synaptic activity; those signals act on nearby arterioles to transiently dilate them and increase blood flow.",
        "D": "Local blood flow is fixed and only changes with systemic blood pressure, not with local neuronal activity."
      },
      "correct_answer": "C",
      "simplification_notes": "Wording shortened and clarified; preserved technical term 'neurovascular unit' and key cell types (neurons, astrocytes, arterioles). Distractors made plausible but incorrect (A: implausible mechanism; B: wrong permanence; D: contradicts neurovascular coupling).",
      "content_preserved": true,
      "source_article": "Nervous system",
      "x": 1.8603817224502563,
      "y": 1.1532543897628784,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Neurons transmit signals via electrochemical impulses along axons, using electrical synapses or chemical synapses with neurotransmitters to influence other cells.",
        "Concept 2: The nervous system is organized into CNS and PNS, with divisions such as somatic vs autonomic (and autonomic’s sympathetic, parasympathetic, and enteric) that coordinate different states and actions.",
        "Concept 3: Neural circuits, networks, and the neurovascular unit illustrate how structure and signaling dynamics generate perception and behavior while meeting energy demands through regulated cerebral blood flow."
      ],
      "original_question_hash": "d94a66eb"
    },
    {
      "question": "Why does treating periodical payments (such as rent) as accruing day by day allow a fair apportionment when a tenancy ends partway through a payment period?",
      "options": {
        "A": "Because it permits the landlord to vary the daily rate based on occupancy, enabling them to increase revenue for the shortened period.",
        "B": "Because it treats the payment as accruing continuously so the total can be apportioned pro rata to the actual number of days the obligation existed, preventing an unjust gain to either party.",
        "C": "Because it imposes a fixed per-period division that ignores the exact number of days each party held the interest, simplifying calculations.",
        "D": "Because it requires the tenant to renegotiate a new annual (or full-period) agreement whenever occupancy or entitlement changes mid-period."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified phrasing to emphasize day-by-day accrual and pro rata allocation; shortened language and made distractors plausible while keeping the original correct choice.",
      "content_preserved": true,
      "source_article": "Apportionment",
      "x": 1.2776234149932861,
      "y": 0.7819695472717285,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Apportionment by act of the parties vs. apportionment by operation of law (what triggers each and how the division of rights/obligations occurs).",
        "Concept 2: Time-based apportionment of rent and other periodical payments (day-to-day accrual and allocation over time).",
        "Concept 3: The role of specific statutes (e.g., Lands Clauses Consolidation Act, Agricultural Holdings Act, Irish Land Act) in enabling or governing apportionment in property contexts and the typical scenarios they address."
      ],
      "original_question_hash": "8acf14b8"
    },
    {
      "question": "In compellence, how does combining credible threats of force with positive inducements alter an adversary's cost–benefit calculation?",
      "options": {
        "A": "It signals the coercer is prepared to escalate to higher levels of force, inducing the opponent to concede immediately to avoid escalation.",
        "B": "It changes the opponent's payoffs by increasing the cost of continued defiance and raising the benefits of compliance, making concession more attractive.",
        "C": "It depends on ambiguous messaging that keeps the opponent uncertain about when force will be used rather than directly changing payoffs.",
        "D": "It guarantees success by removing all uncertainty about the coercer's intentions and ensuring compliance."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and focused on 'cost–benefit calculation'; retained technical terms ('credible threats', 'positive inducements', 'compellence'). All four options were kept plausible but made more concise. Core concepts of credibility, costs/benefits, and bargaining mechanism preserved.",
      "content_preserved": true,
      "source_article": "Compellence",
      "x": 1.2784713506698608,
      "y": 0.9688782095909119,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Compellence vs. deterrence—changing the status quo versus maintaining it.",
        "Concept 2: Credibility and cost of threats—how legitimacy, reputational risk, and the cost of failed threats influence success.",
        "Concept 3: Mechanism of compellence—using threats (and, if needed, limited force) alongside positive inducements within a bargaining framework to alter an adversary’s decision calculus."
      ],
      "original_question_hash": "adebbb14"
    },
    {
      "question": "How does verisimilitude—the balance between artifice and believability—help an audience remain engaged with a fictional world?",
      "options": {
        "A": "By restricting the story to events that could occur in the real world, with no invented elements, so readers do not notice the fiction.",
        "B": "By making invented elements follow coherent internal rules and surrounding them with believable details, allowing readers to suspend disbelief without being jarred by inconsistency.",
        "C": "By requiring every aspect to match actual history and factual reality exactly, leaving no space for invention or interpretation.",
        "D": "By having the author explicitly explain or justify every fantastical feature to remove any potential doubt in the reader."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording condensed and clarified; retained key terms (verisimilitude, suspension of disbelief, worldbuilding). Options rephrased for clarity while preserving original meanings and plausibility.",
      "content_preserved": true,
      "source_article": "Fiction",
      "x": 0.8762577176094055,
      "y": 1.1575214862823486,
      "level": 2,
      "concepts_tested": [
        "Verisimilitude: the balance between creative invention (artifice) and believable elements (believability) within fiction.",
        "Suspension of disbelief: the audience’s willingness to accept elements beyond reality to engage with a fictional work.",
        "Fiction–reality relationship and worldbuilding: how fiction relates to real-world issues and how independent fictional universes are constructed through worldbuilding."
      ],
      "original_question_hash": "cd8dc85e"
    },
    {
      "question": "How does a technology strategy shape an organization's choices about which technologies to invest in?",
      "options": {
        "A": "It provides a prioritized framework that maps technology opportunities to strategic goals and customer value, so investments are chosen based on strategic fit and expected value.",
        "B": "It requires investing only in the newest or most advanced technologies, regardless of market needs or organizational fit.",
        "C": "It treats technology options as a static catalog with no connection to business strategy or customer value.",
        "D": "It outsources all technology investments to external vendors, removing internal alignment with organizational goals."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording shortened and clarified: question focused on how strategy guides investment choices; answer options kept plausible but more concise. No technical content removed; emphasis kept on mapping technologies to goals, value, and common misconceptions.",
      "content_preserved": true,
      "source_article": "Technology management",
      "x": 1.3918023109436035,
      "y": 1.006951928138733,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The role of technology strategy in aligning technology with organizational goals and customer value, and guiding investment decisions.",
        "Concept 2: Technology forecasting and roadmapping as mechanisms to anticipate relevant technologies and map them to business and market needs.",
        "Concept 3: Technology portfolio/project portfolio management and technology transfer as processes for coordinating the use, development, and dissemination of technology to maximize value."
      ],
      "original_question_hash": "28fffe1e"
    },
    {
      "question": "Why does audience-centered risk communication use value-based framing when addressing specific groups?",
      "options": {
        "A": "Because it reduces cognitive load by presenting information as simpler, decontextualized facts that are easier to process.",
        "B": "Because it aligns messages with the group's values and decision-making priorities, making the risk personally meaningful and more likely to prompt protective action.",
        "C": "Because it relies on fear appeals to shock audiences into immediate behaviour change.",
        "D": "Because it standardizes messages so the same content can be delivered to all audiences for consistency."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduate readers; emphasized 'value-based framing' and its link to motivating action; all options rewritten to be concise and plausible alternatives while retaining original correct answer.",
      "content_preserved": true,
      "source_article": "Risk communication",
      "x": 1.297698736190796,
      "y": 0.9810869097709656,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Audience-centered risk communication uses value-based framing to help targeted audiences understand risks and motivate behavior change.",
        "Concept 2: Distinction and relationship between risk communication and crisis communication (risk communication as ongoing, broad risk awareness vs. crisis communication focusing on a specific threat, magnitude, outcomes, and recommended actions).",
        "Concept 3: Integration of risk communication within the risk analysis/risk management framework (and its obligatory role in areas like food safety, linking risk assessment, risk management, and communication to reduce risk)."
      ],
      "original_question_hash": "b1ac382d"
    },
    {
      "question": "According to natural law theory, why and in what way do universal moral principles discovered by human reason limit or override human-made (positive) laws?",
      "options": {
        "A": "Because the legitimacy of positive law rests solely on the sovereign's command; a law is valid regardless of its moral content.",
        "B": "Because practical reason discloses universal moral norms that function as higher law; positive laws are legitimate only insofar as they conform to these norms, and laws that violate them can be judged unjust and may be resisted or disobeyed.",
        "C": "Because human reason cannot reliably discover moral truths, so natural law provides no real constraint on positive law.",
        "D": "Because natural law and positive law are effectively the same set of rules; any apparent conflict is merely a matter of interpretation."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the original question into clearer undergraduate-level prose, removed historical examples and theological framing, and preserved the core distinction that reason-discerned universal norms function as higher law constraining positive law; distractors were made plausible alternatives.",
      "content_preserved": true,
      "source_article": "Natural law",
      "x": 1.238019347190857,
      "y": 0.9739046096801758,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Natural law posits universal moral principles discoverable by reason that underlie and constrain human-made laws (relationship between natural law and positive law).",
        "Concept 2: Reason as a source connected to the divine, granting intrinsic equality and rights to all human beings (principle tying reason, divinity, and universal rights).",
        "Concept 3: Historical and political influence of natural law on Western thought—challenging divine right, justifying social contract, property rights, and revolution (causal relationship to modern legal/political institutions)."
      ],
      "original_question_hash": "0b9da954"
    },
    {
      "question": "Which explanation best explains how a media work can evoke narrativity (prompt a narrative response) without being a conventional, linear narrative?",
      "options": {
        "A": "The work eliminates ambiguity and forces a single, author-intended interpretation on the audience.",
        "B": "The work relies on active sense-making by users: world-building, causal event design, and character cues provide scaffolds that different players interpret and combine into individualized narrative scripts.",
        "C": "Narrativity depends on a strict plot with a clear protagonist and chronological sequence; without those features there is no narrativity.",
        "D": "Narrativity can only come from explicit, author-driven narration; interactive elements cannot produce narrativity."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was tightened and jargon reduced; the distinction between ‘narrative’ and ‘narrativity’ was clarified. Key mechanisms (world-building, causal event design, character cues, player interpretation) from the article were preserved in option B. All options made plausible alternatives.",
      "content_preserved": true,
      "source_article": "Narratology",
      "x": 0.850951611995697,
      "y": 1.1214760541915894,
      "level": 2,
      "concepts_tested": [
        "Structuralist vs. cognitive narratology: different focus on narrative structure vs. human sense-making and experience.",
        "Narrative vs. narrativity: a media can possess narrativity (evoke narrative response) without being a conventional narrative.",
        "Mechanisms of evoking narrative in games: world-building, causal event design, and character development as tools to elicit multiple individualized narrative scripts."
      ],
      "original_question_hash": "00975878"
    },
    {
      "question": "Why does the extended period of dependence on adults during human development enhance cultural learning?",
      "options": {
        "A": "Because it lets juveniles rely mainly on unguided trial-and-error, which speeds individual discovery of skills without social input.",
        "B": "Because it provides repeated opportunities to observe and imitate adults, receive corrective feedback, and practice socially transmitted skills, enabling reliable transfer and refinement of complex behaviors across generations.",
        "C": "Because a longer dependent period increases the genetic encoding of particular cultural practices, making them easier to inherit biologically.",
        "D": "Because it removes the need for social learning by allowing knowledge to be expressed through innate instincts rather than through social interaction."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording condensed and made more direct; mechanisms emphasized (observation, imitation, feedback, practice); distractor options rephrased to remain plausible but incorrect; retained original core concept and correct answer.",
      "content_preserved": true,
      "source_article": "Cultural learning",
      "x": 1.2766252756118774,
      "y": 1.0204135179519653,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3"
      ],
      "original_question_hash": "2ab7feec"
    },
    {
      "question": "Why does designing a product with mass-production constraints early in the process produce economies of scale and typically affect yield, assembly complexity, and time-to-market?",
      "options": {
        "A": "It lets designers add many bespoke features to increase product differentiation, which reduces the drive to use standardized tooling and processes.",
        "B": "It ensures parts are compatible with standard processes and tolerances, reducing the need for custom tooling, simplifying assembly, increasing manufacturing yield, enabling faster iteration, and lowering unit costs.",
        "C": "It argues for postponing manufacturability decisions until after prototyping so creative ideation remains unconstrained.",
        "D": "It causes manufacturing choices to be dominated by marketing priorities for novelty and perceived differentiation rather than by process capability and efficiency."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; technical terms retained (e.g., tolerances, yield); options rephrased to be plausible distractors while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Industrial design",
      "x": 1.3390122652053833,
      "y": 0.9730010628700256,
      "level": 2,
      "concepts_tested": [
        "Relationship between design decisions and mass production constraints",
        "Integration of aesthetics, usability/ergonomics, and broader considerations (marketing, sustainability)",
        "Multidisciplinary collaboration (designers, engineers, business experts, etc.) and its impact on product outcomes"
      ],
      "original_question_hash": "44d62c87"
    },
    {
      "question": "In simultaneous bilingual development, children exposed from early childhood to two languages commonly show unequal proficiency between them. From a learning-mechanism perspective, which explanation best accounts for this pattern?",
      "options": {
        "A": "The brain commits separate, fixed processing channels to each language during early exposure, preventing later reallocation of capacity and thus producing stable asymmetry.",
        "B": "Proficiency reflects the frequency and richness of meaningful input: the language used more often and in more varied, communicative contexts receives more practice and stronger mental representation, while the less-used language stays weaker.",
        "C": "A language that receives less formal instruction is intrinsically harder to learn, so it lags regardless of how much natural exposure the child gets.",
        "D": "Any early imbalance in ability will automatically even out over time without regard to continuing differences in input or use."
      },
      "correct_answer": "B",
      "simplification_notes": "Question phrased more directly for undergraduates; retained the idea of simultaneous bilinguals and unequal proficiency. Options rewritten to be concise and plausible while preserving original alternatives.",
      "content_preserved": true,
      "source_article": "Multilingualism",
      "x": 1.2388726472854614,
      "y": 1.0807987451553345,
      "level": 2,
      "concepts_tested": [
        "Cognitive/learning advantages of multilingualism: multilinguals may be better at language learning than monolinguals.",
        "Developmental dynamics of simultaneous bilingualism: early exposure to multiple languages often yields uneven proficiency across languages, reflecting underlying mechanisms of language acquisition and use.",
        "Multilingualism in computing: a continuum between internationalization and localization, with English’s dominance shaping software development and multilingual versioning."
      ],
      "original_question_hash": "50b143cc"
    },
    {
      "question": "How can niche construction change the geographic range dynamics (expansion or contraction) of other species?",
      "options": {
        "A": "By actively modifying environmental conditions or resource distributions, a niche-constructing species alters habitat suitability and biotic interactions, creating feedbacks that can expand or contract the ranges of other species.",
        "B": "By passively tolerating existing environmental conditions without altering them, so the habitat suitability for other species — and thus their ranges — remain essentially fixed.",
        "C": "By changing the environment in ways that benefit only the constructing species while never affecting other species' interactions or habitat suitability, leaving their ranges unchanged.",
        "D": "By producing random, non-directional environmental disturbances that uniformly suppress all species regardless of their ecological interactions, causing indiscriminate range contractions."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the question for clarity and concision for undergraduates; kept core idea that active environmental modification (niche construction) changes habitat suitability and interactions, producing range feedbacks. Distractors were made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Ecological niche",
      "x": 1.6912513971328735,
      "y": 1.0331627130508423,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The niche as a dynamic interface between a species and its environment, encompassing both the species’ responses to resources/competition and its influence on those same environmental factors (causal feedbacks).",
        "Concept 2: The three major niche frameworks (Grinnellian, Eltonian, Hutchinsonian) that illustrate different mechanisms and emphases for understanding niches: habitat/behavioral adaptation, environmental modification and behavioral ecology, and mathematical characterization for coexistence.",
        "Concept 3: Niche construction as a process by which organisms actively alter their environment and, in turn, affect the distribution and dynamics of other species (impacting range dynamics and community structure)."
      ],
      "original_question_hash": "d3375e4a"
    },
    {
      "question": "How do measurement and control in the BPM lifecycle help governance and continuous improvement of end-to-end processes, instead of only improving individual tasks?",
      "options": {
        "A": "By concentrating measurement on the single most visible task to make governance simpler.",
        "B": "By delivering continuous, data-driven feedback across the entire end-to-end process, enabling targeted improvements and governance over the whole flow.",
        "C": "By allowing automation to replace all human decision-making so the process becomes perfectly efficient.",
        "D": "By isolating parts of the process so improvements in one area do not affect other areas."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the original question into clearer academic language for undergraduates, kept key terms (measurement, control, BPM lifecycle, end-to-end processes, governance, continuous improvement). Options were tightened to be concise and plausible distractors while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Business process management",
      "x": 1.422210454940796,
      "y": 1.0405232906341553,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Process orientation and end-to-end asset management — BPM treats processes as valuable assets that must be understood, managed, and developed to deliver value, spanning across the enterprise.",
        "Concept 2: BPM lifecycle — modeling, automation, execution, control, measurement, and optimization comprise a cohesive discipline aimed at improving process performance and governing end-to-end flows.",
        "Concept 3: People–technology interplay — BPM involves both human and technological elements, with automation (including RPA) acting as an enabler alongside human roles in designing, executing, and improving processes."
      ],
      "original_question_hash": "ef502b38"
    },
    {
      "question": "Cognates are words in related languages that descend from a common etymon but often have different phonological forms. Which mechanism best explains why their sounds can diverge while still indicating a shared origin, and how does this relate to reflexes?",
      "options": {
        "A": "Regular sound changes operating independently in each daughter language produce different reflexes that nonetheless map back to the same etymon.",
        "B": "The languages independently borrowed the word from different unrelated sources, preserving meaning but not a common origin.",
        "C": "The original etymon shifted in meaning at the same time as different phonological changes occurred, causing apparent divergence of forms.",
        "D": "Reflexes must match the etymon exactly; any phonological difference means the words are not cognates."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording was condensed and clarified: emphasized 'regular sound changes', 'reflexes', and 'common etymon'; removed historical background and extraneous examples while keeping the tested idea intact.",
      "content_preserved": true,
      "source_article": "Etymology",
      "x": 1.2183620929718018,
      "y": 1.0844111442565918,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Comparative method and proto-language reconstruction as a mechanism to infer origins and historical forms of words.",
        "Concept 2: Relationships across languages and time—etymon, reflex, cognates, and doublets—and how these explain inheritance and evolution of words.",
        "Concept 3: Interdisciplinary and evidence-based approach (historical linguistics, philology, semiotics, etc., plus textual data) to construct a chronological catalogue of meanings and changes."
      ],
      "original_question_hash": "c37e3b29"
    },
    {
      "question": "How do place names (toponyms) encode historical social contexts in the landscape, and why does that make place-name data useful for reconstructing past population contacts and movements?",
      "options": {
        "A": "Place names are chosen mainly for administrative convenience and therefore preserve a single, unchanging linguistic layer.",
        "B": "Place names accumulate layered linguistic and political information over time (e.g., languages, rulers, settlers), so their etymology and changes reveal historical contacts and movement patterns.",
        "C": "Place names are arbitrary labels and thus cannot indicate social context, making them unreliable for population history studies.",
        "D": "Toponymy treats place names as decorative labels without relation to language, power, or demographic change."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified to focus on how toponyms record layered linguistic/political evidence; choices rewritten to be concise and academically phrased while keeping the original options' intent and the same correct answer.",
      "content_preserved": true,
      "source_article": "Onomastics",
      "x": 0.016496554017066956,
      "y": 0.5537145733833313,
      "level": 2,
      "concepts_tested": [
        "The field studies the etymology, history, and use of names, linking linguistic origin to social meaning.",
        "Branches of onomastics reflect relationships between naming practices and contexts (place names vs personal names vs literature vs society).",
        "Names serve as sources of information for analysis (e.g., named-entity recognition, ethnic affiliation in populations, prosopography)."
      ],
      "original_question_hash": "01a8327c"
    },
    {
      "question": "In the \"conveyor-belt\" model, new oceanic lithosphere is created at divergent (spreading) ridges and old lithosphere is recycled at subduction zones. How does linking these processes make a self-regulating system, and what is the consequence for long-term plate motion?",
      "options": {
        "A": "Crust creation at ridges is independent of destruction at trenches, so Earth's crustal area increases without limit and plate motions accelerate.",
        "B": "Creation and destruction are driven by random mantle fluctuations, so plate motion frequently stalls for long intervals.",
        "C": "Seafloor spreading and subduction are coupled through mantle convection, so production roughly equals destruction, keeping total crustal area nearly constant and sustaining continuous long-term plate motion.",
        "D": "Crust is simply redistributed between continental and oceanic regions with no net area change, so plate motion is driven only by surface gravity contrasts."
      },
      "correct_answer": "C",
      "simplification_notes": "Shortened and clarified the stem to focus on the conveyor-belt coupling of spreading and subduction; language made more direct for undergraduates while preserving the mechanism and long-term implication. Options kept plausible distractors but kept the original correct choice.",
      "content_preserved": true,
      "source_article": "Plate tectonics",
      "x": 1.0761706829071045,
      "y": 0.1523093730211258,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Plates move on the ductile asthenosphere due to mantle convection, with seafloor spreading at divergent boundaries and subduction at convergent boundaries; this creates a self-regulating system that maintains Earth’s surface area.",
        "Concept 2: The type of plate boundary (divergent, convergent, transform) dictates the motion of plates and the associated geologic phenomena (e.g., earthquakes, volcanism, mountain building, trenches).",
        "Concept 3: The “conveyor belt” model connects crust formation at spreading ridges with crust destruction at subduction zones, balancing total crustal area and driving long-term plate motion."
      ],
      "original_question_hash": "972c2980"
    },
    {
      "question": "Why does allocating wealth, opportunities, and burdens through institutions (e.g., taxation, social insurance, public services) encourage cooperative behavior in a society?",
      "options": {
        "A": "Because it shares the costs and benefits of cooperation across members, aligning incentives and reducing the tendency to free-ride.",
        "B": "Because it guarantees equal outcomes for all individuals regardless of their contributions, so people cooperate because results are identical.",
        "C": "Because it depends entirely on private voluntary charity to provide public goods, so cooperation emerges from altruism alone.",
        "D": "Because it allows individuals to opt out of funding public goods without consequences, preserving total individual freedom."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording condensed and clarified institutional examples; preserved the rights–duties and free-rider concepts while making all options plausible.",
      "content_preserved": true,
      "source_article": "Social justice",
      "x": 1.2112224102020264,
      "y": 0.9398850202560425,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Social justice as the distribution of wealth, opportunities, and burdens through institutional mechanisms (e.g., taxation, public services) to enable cooperation.",
        "Concept 2: The rights–duties framework within institutions, enabling individuals to receive benefits and bear burdens as part of a just order.",
        "Concept 3: Relational/cultural foundations of social justice, including reciprocal interdependence between individuals and society and how different traditions emphasize responsibility vs power dynamics."
      ],
      "original_question_hash": "0d017c3f"
    },
    {
      "question": "How does recorded control (formalization) — the use of written rules, procedures, and records — improve coordination and stability in a social organization?",
      "options": {
        "A": "By expanding individual autonomy through decentralized decision rights, encouraging novel initiatives and personal discretion.",
        "B": "By reducing ambiguity through codified procedures and rules, creating a shared understanding of processes that enables consistent actions and smoother turnover.",
        "C": "By concentrating authority in senior leaders so that only top managers make decisions, thereby removing input from frontline members.",
        "D": "By promoting spontaneous, improvised decisions and rapid ad-hoc responses without formal communication, increasing short-term flexibility."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the question for clarity, defined recorded control as formalization (written rules/procedures), and tightened option wording while keeping the original correct answer (B).",
      "content_preserved": true,
      "source_article": "Social organization",
      "x": 1.2678362131118774,
      "y": 0.9655122756958008,
      "level": 2,
      "concepts_tested": [
        "Affiliation and collective resources as a mechanism for organizational power and motivation",
        "Substitutability of individuals as a mechanism to ensure continuity and resilience",
        "Recorded control (formalization) as a mechanism to improve coordination and stability"
      ],
      "original_question_hash": "0d10bc1f"
    },
    {
      "question": "How do the legislature, regulatory agencies, and courts typically interact to enforce labour law so that workplaces comply and workers' rights are protected?",
      "options": {
        "A": "The legislature sets broad employment standards; regulatory agencies turn those standards into detailed rules, inspect and monitor compliance, and enforce administrative sanctions; courts resolve disputes, interpret statutes and regulations, and provide remedies — together offering democratic legitimacy, technical implementation, and due process.",
        "B": "The legislature micromanages every workplace detail so regulators are unnecessary; courts deal only with criminal matters; enforcement depends solely on statutory provisions.",
        "C": "Regulatory agencies write the general standards and carry out enforcement while the legislature only decides how strictly to apply rules; the judiciary has no meaningful role in enforcement or rights protection.",
        "D": "Courts create and define workplace standards through rulings, regulators impose penalties, and the legislature delegates enforcement mainly to private actors and agreements."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording condensed to an undergraduate level and focused on the three institutional roles (legislature, regulators, courts). Distractors rewritten to be plausible but incorrect; historical and peripheral material removed.",
      "content_preserved": true,
      "source_article": "Labour law",
      "x": 1.2592142820358276,
      "y": 0.8333000540733337,
      "level": 2,
      "concepts_tested": [
        "The mediation of relationships among multiple actors (workers, employers, unions, government) by labour law (collective vs. individual frameworks)",
        "The distinction between collective labour law (tripartite relationships) and individual labour law (rights at work via contracts)",
        "The enforcement mechanism of labour law by government agencies (legislature, regulatory, or judicial)"
      ],
      "original_question_hash": "f9407745"
    },
    {
      "question": "Why is empowerment more effective when individual agency and enabling support structures act together, rather than when only one is present?",
      "options": {
        "A": "Because individuals acting alone can rely solely on personal will and initiative to change their circumstances without external supports.",
        "B": "Because providing external resources and services by themselves automatically transforms people’s sense of control and produces lasting empowerment.",
        "C": "Because empowerment is a reciprocal process: individuals actively use their agency to identify and mobilize resources from supportive structures, and those resources build skills, autonomy, and reinforce further agency, producing more sustainable change.",
        "D": "Because empowerment is achieved only through large-scale policy or structural reforms; individual actions and immediate supports are unnecessary for genuine empowerment."
      },
      "correct_answer": "C",
      "simplification_notes": "Question rephrased to be concise and direct for undergraduates; kept core idea of reciprocity between self-agency and supportive structures. Options were tightened to be plausible alternatives reflecting points in the article.",
      "content_preserved": true,
      "source_article": "Empowerment",
      "x": 1.2288916110992432,
      "y": 0.9524823427200317,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Empowerment as a process of gaining mastery and control over one’s life and resources (mechanism and outcome).",
        "Concept 2: The synergy of individual agency and professional/support structures in empowerment (self-empowerment plus enabling support).",
        "Concept 3: The shift from deficit-oriented to strength-oriented perspectives and its link to social justice, discrimination, and civic engagement."
      ],
      "original_question_hash": "299d685a"
    },
    {
      "question": "How does the absence of a single global enforcer in international law affect states' incentives to comply and the mechanisms used to enforce obligations?",
      "options": {
        "A": "Because there is no universally empowered enforcer, states must assess the costs and benefits of compliance themselves; enforcement therefore relies primarily on diplomacy, targeted sanctions, or coalitions using coercion, so compliance depends on relative power, interests, and reputational costs.",
        "B": "Because international law is supported by a universal police force that automatically punishes violations, states comply to avoid compulsory penalties imposed by that global authority.",
        "C": "Because treaties are automatically self-executing and become domestic law in every state, enforcement occurs universally through national courts without regard to state consent.",
        "D": "Because international law operates only as moral obligation with no practical enforcement mechanisms, state compliance is essentially random and unpredictable."
      },
      "correct_answer": "A",
      "simplification_notes": "Question rephrased to be concise and targeted at undergraduates; retained core idea that international law lacks a central enforcer and that enforcement uses diplomacy, sanctions, and force, while distractors present common but incorrect alternatives (universal police, automatic domestic enforcement, pure moral obligation).",
      "content_preserved": true,
      "source_article": "International law",
      "x": 1.2259379625320435,
      "y": 0.8324073553085327,
      "level": 2,
      "concepts_tested": [
        "The consent-based nature and lack of a single enforcing authority in international law, with enforcement through diplomacy, sanctions, or war.",
        "The sources and formation of international obligations (custom, treaties, general principles) and how they establish binding rules and norms.",
        "The relationship between international law and domestic legal systems, including incorporation of international obligations into national law and the role of supranational tribunals."
      ],
      "original_question_hash": "22891bcb"
    },
    {
      "question": "How does logos operate as a persuasive tactic, and under what audience condition is it most effective?",
      "options": {
        "A": "By appealing mainly to emotions to shift beliefs; most effective when the audience is strongly motivated by emotion.",
        "B": "By presenting evidence-linked premises that lead to a conclusion through valid reasoning; most effective when the audience is motivated and able to process information carefully.",
        "C": "By invoking the speaker's authority or credibility to endorse the conclusion; most effective when the audience doubts or lacks access to the evidence.",
        "D": "By repeating claims until they seem true without requiring verification; most effective when the audience is distracted or mentally fatigued."
      },
      "correct_answer": "B",
      "simplification_notes": "Question shortened and clarified; removed extra historical detail; options rewritten to contrast logos with pathos, ethos, and repetition-based tactics while keeping the original correct answer.",
      "content_preserved": true,
      "source_article": "Persuasion",
      "x": 1.266478180885315,
      "y": 1.007375955581665,
      "level": 2,
      "concepts_tested": [
        "Ethos (credibility) as a mechanism for persuasive influence",
        "Logos (reason/logic) as a mechanism for persuasive influence",
        "The taxonomy of persuasion forms (propaganda, coercion, systematic persuasion, heuristic persuasion) and how they leverage different appeals"
      ],
      "original_question_hash": "3fcfa2f9"
    },
    {
      "question": "Why can increasing debt up to an optimal leverage raise a firm's value even when profitable projects could be funded with equity, and how is that optimal debt level determined?",
      "options": {
        "A": "Because debt automatically reduces a firm's taxes on every project, so after‑tax cash flows are always higher regardless of project risk.",
        "B": "Because debt absorbs all cash‑flow volatility and therefore lowers the risk borne by equity holders.",
        "C": "Because interest is tax‑deductible, debt creates a tax shield that lowers the firm's after‑tax cost of capital and increases value; however, higher leverage raises expected bankruptcy and agency costs, so the optimal debt level is reached where the marginal benefit of the tax shield equals the marginal expected cost of financial distress and agency problems (marginal benefit = marginal cost).",
        "D": "Because issuing debt fixes financing costs and thus guarantees more stable earnings, which always increases firm value."
      },
      "correct_answer": "C",
      "simplification_notes": "Question phrasing shortened and clarified; kept technical terms (tax shield, leverage, bankruptcy and agency costs); added explicit marginal benefit = marginal cost statement in inline math; options made plausible but distinct.",
      "content_preserved": true,
      "source_article": "Financial management",
      "x": 1.3635969161987305,
      "y": 0.9840576648712158,
      "level": 2,
      "concepts_tested": [
        "Value maximization through efficient acquisition and deployment of financial resources (short- and long-term) to maximize firm value.",
        "Cash flow management and liquidity as a foundational short-term objective enabling day-to-day operations and survival.",
        "Long-term financing decisions and capital structure (debt vs. equity) and capital budgeting as core mechanisms linking funding decisions to value creation, with ties to corporate finance."
      ],
      "original_question_hash": "8bea7ade"
    },
    {
      "question": "How does building a large reservoir change the distribution of water within a watershed, and why can it increase local surface storage while potentially lowering groundwater recharge downstream?",
      "options": {
        "A": "It increases local storage and evaporation but completely stops all downstream flows, leaving downstream ecosystems without water.",
        "B": "It provides controllable surface storage that smooths and delays runoff—increasing surface-water availability locally—while holding water aboveground can reduce downstream infiltration and therefore groundwater recharge.",
        "C": "It causes all captured surface water to seep immediately into the subsurface, converting surface stores into groundwater and increasing recharge everywhere downstream.",
        "D": "It speeds up runoff and reduces storage capacity, producing faster downstream flows and making floods occur earlier."
      },
      "correct_answer": "B",
      "simplification_notes": "Removed extraneous background, phrased the question concisely for undergraduates, clarified mechanisms (storage, runoff smoothing, reduced infiltration), and kept all options plausible and technically framed.",
      "content_preserved": true,
      "source_article": "Water resources",
      "x": 1.7129371166229248,
      "y": 0.9246028065681458,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Surface water quantity is governed by the hydrological cycle within a watershed, where precipitation is the input and storage capacity, soil permeability, runoff characteristics, timing, and evaporation determine the resulting water availability.",
        "Concept 2: Human interventions (reservoir construction, land-use changes that increase runoff, and canal/pipeline imports) actively modify storage, runoff, and the distribution of water resources.",
        "Concept 3: Groundwater is a renewable resource in principle, but overdrafting leads to depletion, illustrating a sustainability dynamic where extraction outpaces recharge and affects regional water security."
      ],
      "original_question_hash": "ee14cd00"
    },
    {
      "question": "Why do drinking-water utilities typically set tariffs with a large fixed charge and a relatively small per-unit (variable) charge?",
      "options": {
        "A": "Because capital investments (pipes, treatment plants, reservoirs) and staffing dominate costs and do not vary with short-term consumption; a fixed charge helps recover those costs and provides stable revenue to maintain treatment (purification, disinfection) and reliable supply, while the small variable charge covers incremental energy and chemical costs — but this weakens price signals for conservation.",
        "B": "Because variable (marginal) costs dominate the cost base, so the fixed charge acts mainly as a household subsidy to promote equity and keep basic access affordable.",
        "C": "Because high fixed charges are intended to encourage conservation by making the bill large regardless of usage; variable charges are kept low so users are not discouraged from minimal consumption.",
        "D": "Because tariff structures are primarily historical or political conventions rather than responses to the underlying cost structure or reliability needs; governments simply choose a fixed-heavy design for other reasons."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording shortened and clarified for undergraduates; explicit link added between dominant fixed costs (capital, personnel), marginal variable costs (energy, chemicals), and the role of tariffs in revenue stability and reliability; kept treatment (purification, disinfection) as an example of reliability requirements. All options made plausible.",
      "content_preserved": true,
      "source_article": "Water supply",
      "x": 1.6674835681915283,
      "y": 0.9083197116851807,
      "level": 2,
      "concepts_tested": [
        "Water treatment chain as a mechanism for drinking-water safety (purification, disinfection, fluoridation)",
        "Cost structure and tariff design (dominance of fixed costs vs. variable costs related to consumption)",
        "Infrastructure strategy and governance (centralized vs. decentralized approaches, including desalination, rainwater harvesting, and fit-for-purpose concepts)"
      ],
      "original_question_hash": "b6d892ce"
    },
    {
      "question": "How does strengthening institutions and information flows improve management of water quantity (e.g., flood and drought response) and water quality (e.g., pollution control)?",
      "options": {
        "A": "It centralizes authority in a single actor who can override market incentives and make unilateral decisions.",
        "B": "It produces rigid top-down rules that limit local adaptation and hinder effective response to diverse conditions.",
        "C": "It aligns incentives across agencies and stakeholders, enables coordinated planning and timely risk responses, and provides transparent data sharing that supports both supply reliability and pollution control.",
        "D": "It directs resources mainly to physical infrastructure, thereby improving hardware but often neglecting governance and information systems."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording shortened and clarified; kept the flood/drought and pollution examples; option texts rephrased for clarity while maintaining plausibility; correct answer unchanged (C).",
      "content_preserved": true,
      "source_article": "Water security",
      "x": 1.3242228031158447,
      "y": 0.8437861800193787,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3"
      ],
      "original_question_hash": "b4f2f9a9"
    },
    {
      "question": "Why does opportunity cost include non-monetary factors (for example, time and personal satisfaction), and how does including them affect choices when resources are scarce?",
      "options": {
        "A": "Opportunity cost is defined only by monetary outlays; adding time or utility would double-count costs and lead to misleading decisions.",
        "B": "Because the value of the best forgone alternative includes both monetary and non-monetary benefits (e.g., time, satisfaction); including them provides a fuller measure of net benefit and can change which option is preferred under scarcity.",
        "C": "Non-monetary factors are too uncertain or subjective to be useful, so they should be ignored; focusing on explicit monetary costs ensures objective choices.",
        "D": "Non-monetary factors are typically negligible compared with monetary costs, so including them rarely affects the decision."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the question to clear undergraduate language; kept economic terms (opportunity cost, forgone alternative, explicit/implicit). Made all four options plausible and preserved the original correct answer and core reasoning.",
      "content_preserved": true,
      "source_article": "Opportunity cost",
      "x": 1.3637984991073608,
      "y": 0.9714651703834534,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Opportunity cost as the value of the best alternative forgone and its link to scarcity and choosing among mutually exclusive options.",
        "Concept 2: Explicit vs. implicit costs as components of the full opportunity cost and how they differ in measurability and recognition.",
        "Concept 3: Opportunity cost includes non-monetary factors (time, utility, pleasure) and thus informs decisions beyond monetary accounting."
      ],
      "original_question_hash": "ae2a68f9"
    },
    {
      "question": "During live improvisation performers create unrehearsed material while responding to audience input. Which cognitive mechanism best explains how this interactive process preserves coherence and supports creative, on-the-spot generation without a pre-planned script?",
      "options": {
        "A": "Retrieving fixed scripts or rehearsed material from long-term memory and reproducing them with small adjustments.",
        "B": "Continuously integrating audience cues to update an evolving internal representation, which narrows options and guides moment-to-moment generation.",
        "C": "Generating ideas in isolation and only trying to fit them to audience reactions after they have been produced.",
        "D": "Randomly combining elements from memory without using audience feedback or contextual information."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the stem; replaced longer phrasing with concise academic language; kept the core cognitive alternatives but tightened wording so all options remain plausible.",
      "content_preserved": true,
      "source_article": "Improvisation",
      "x": 0.43744948506355286,
      "y": 1.2741775512695312,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Improvisation as a stop-gap/problem-solving mechanism that uses available resources and may involve brainstorming and thinking outside the box.",
        "Concept 2: The cognitive process of improvisation—real-time generation of ideas and unrehearsed delivery, with interplay between the performer and audience.",
        "Concept 3: Transferability and training of improvisation skills across disciplines (e.g., performing arts training) and its applicability to diverse domains."
      ],
      "original_question_hash": "94dc3155"
    },
    {
      "question": "How do protected areas increase resilience to climate change by reducing the main drivers of ecological disruption?",
      "options": {
        "A": "They encourage diversified land uses aimed at maximizing short-term economic returns within conserved landscapes.",
        "B": "They restrict extractive and destructive activities, preserving intact habitats and ecological processes that reduce habitat loss, limit rapid shifts in species distributions, and buffer systems against storm impacts and sea-level rise.",
        "C": "They substitute natural ecosystems with engineered or artificial landscapes specifically built to withstand climate stressors.",
        "D": "They remove all human settlements from within protected boundaries to eliminate social pressures on ecosystems."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was tightened and technical detail focused on how protection reduces habitat loss, species-range shifts, and buffers climate impacts; extraneous historical and administrative context was removed while keeping the tested mechanisms and governance tensions implicit in the options.",
      "content_preserved": true,
      "source_article": "Protected area",
      "x": 1.4294112920761108,
      "y": 0.8102908730506897,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Protected areas function by restricting human exploitation to conserve biodiversity and associated ecosystem services.",
        "Concept 2: Protected areas contribute to climate change resilience by reducing drivers of ecological disruption (e.g., limiting habitat loss, controlling species distribution shifts, and mitigating storm and sea-level rise impacts).",
        "Concept 3: Governance and social dimensions, including criticisms of fortress conservation and impacts on Indigenous peoples and local communities, affect the legitimacy and effectiveness of protected areas."
      ],
      "original_question_hash": "678b45a2"
    },
    {
      "question": "Why does a moral panic develop and spread when media outlets and political actors present a condition as a serious threat, even though objective evidence of harm may be weak or uncertain?",
      "options": {
        "A": "Because accurate, comprehensive data are always collected and presented before public reaction, so responses are proportionate to the real harm.",
        "B": "Because sensational media coverage together with political framing and moral entrepreneurs label the condition as a threat and amplify its severity and immediacy, producing a self-reinforcing cycle of public concern and policy response.",
        "C": "Because people inherently distrust official sources and independently seek out sensational stories that confirm their pre-existing biases, so the panic spreads regardless of media framing.",
        "D": "Because once political measures or laws are proposed, public fear naturally subsides and the issue stops amplifying, leaving no lasting public alarm."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; preserved technical terms (moral entrepreneurs, political framing) in option B; options rewritten to be concise and all plausible alternatives.",
      "content_preserved": true,
      "source_article": "Moral panic",
      "x": 1.251758337020874,
      "y": 0.9947526454925537,
      "level": 2,
      "concepts_tested": [
        "Concept 1: How a condition or event is constructed as a threat and amplified by media and political actors (the mechanism of moral panic).",
        "Concept 2: The roles of moral entrepreneurs and folk devils in shaping public concern and the definition of the threat.",
        "Concept 3: The link between moral panics and lawmaking/policy changes, including how claims may be exaggerated and still drive social responses."
      ],
      "original_question_hash": "10145132"
    },
    {
      "question": "Why does the presence of oxygen usually speed up biodegradation and produce different final products than oxygen-free (anoxic) conditions?",
      "options": {
        "A": "Oxygen allows aerobic respiration, which generates more ATP per substrate than anaerobic pathways; this supports faster microbial growth and enzyme-driven breakdown, so aerobic degradation proceeds more quickly and mainly yields CO2 and H2O, whereas anaerobic degradation is slower and commonly produces CO2 and CH4 (plus biomass and residues).",
        "B": "Oxygen itself chemically cleaves bonds in the substrate without microbial involvement, directly accelerating breakdown and changing the end products.",
        "C": "Oxygen has no effect on either the rate or the end products of biodegradation; those outcomes depend only on temperature.",
        "D": "Oxygen is toxic to microbes and therefore inhibits biodegradation, preventing formation of CO2 and H2O."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified to focus on aerobic vs anaerobic pathways; retained explanation about ATP yield, microbial growth, rate differences, and end products; distractors made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Biodegradation",
      "x": 1.7766237258911133,
      "y": 0.9650440216064453,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The three-stage model of biodegradation (biodeterioration → biofragmentation → assimilation) and how each stage contributes to material breakdown and incorporation into new biomass.",
        "Concept 2: The role of oxygen in biodegradation pathways (aerobic versus anaerobic digestion), including how oxygen presence affects rate, end products (CO2, water, minerals, biomass vs methane), and overall material reduction.",
        "Concept 3: The time- and condition-dependent nature of biodegradation (why rate varies by material and environment; the idea that biodegradation can occur across a wide range of substances but is governed by time and conditions)."
      ],
      "original_question_hash": "f64fec2d"
    },
    {
      "question": "How does niche partitioning allow ecologically similar species to coexist in the same community?",
      "options": {
        "A": "It increases competition by forcing species to use identical resources, making exclusion more likely.",
        "B": "It reduces overlap in resource use (for example different foods or activity times), so each species limits its own population more than it limits others (intraspecific > interspecific competition), which lowers interspecific competition and permits coexistence.",
        "C": "It requires all species to occupy the exact same ecological role, producing stability because species tolerate one another.",
        "D": "It removes the need for distinct functional roles, producing a uniform community with lower diversity."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the question into a concise undergraduate-level sentence; clarified 'self-limitation' as intraspecific competition and contrasted it with interspecific competition in option B; kept all distractors plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Community (ecology)",
      "x": 1.6729423999786377,
      "y": 1.012893557548523,
      "level": 2,
      "concepts_tested": [
        "Niche partitioning as a mechanism for coexistence and reduced interspecific competition",
        "Abiotic factors (e.g., temperature, soil pH) governing species distributions and community composition",
        "Scale and methodological frameworks in community ecology (local vs. regional focus; shift toward experiments and models)"
      ],
      "original_question_hash": "0df69393"
    },
    {
      "question": "Why does allocating health resources according to need, rather than giving everyone identical resources, better advance health equity?",
      "options": {
        "A": "Because directing resources to those with the greatest need yields the largest marginal health gains, narrowing disparities by raising deprived groups toward the health levels of less-deprived groups.",
        "B": "Because it guarantees everyone receives identical health inputs, which ensures parity in treatment.",
        "C": "Because it assumes health outcomes are determined only by genetics and therefore cannot be changed by redistributing resources.",
        "D": "Because need-based allocation necessarily reduces overall population health in order to maximize fairness."
      },
      "correct_answer": "A",
      "simplification_notes": "Question language was tightened for clarity and concision; options were rephrased to be academically clear while preserving original distractor logic. Technical terms (e.g., 'marginal health gains', 'deprived groups') were retained to match undergraduate level.",
      "content_preserved": true,
      "source_article": "Health equity",
      "x": 1.2336901426315308,
      "y": 0.9079639315605164,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Equity vs. equality and the need-based allocation principle to achieve health equity (principle about how resources should be distributed to reduce disparities).",
        "Concept 2: Social determinants of health and SES as mechanisms linking access to resources with health outcomes (relationship between wealth/power/prestige and health disparities).",
        "Concept 3: Health as a human right and the development/justice framework (principle linking health equity to human rights and to Millennium Development Goals)."
      ],
      "original_question_hash": "8de9c5c7"
    },
    {
      "question": "In two-dimensional Euclidean space, why does the existence of a line (axis) of symmetry guarantee a figure is achiral (its mirror image can be mapped onto the original by a rigid motion)?",
      "options": {
        "A": "Because reflection across that axis is an orientation-reversing isometry that leaves the figure invariant, so the mirror image is congruent to the original.",
        "B": "Because any symmetry of a figure implies its mirror image can be obtained by a rotation (an orientation-preserving rigid motion) alone.",
        "C": "Because a line of symmetry fixes a handedness for the figure, preventing any orientation reversal; therefore it must be achiral.",
        "D": "Because chirality can only exist in three dimensions, so two-dimensional figures with any symmetry are necessarily achiral."
      },
      "correct_answer": "A",
      "simplification_notes": "Rephrased the prompt using precise geometric language (\"axis of symmetry\", \"orientation-reversing isometry\") and removed extraneous examples; options were kept plausible but concise. Preserved the original logical point that reflection across the symmetry line maps the mirror image to the figure.",
      "content_preserved": true,
      "source_article": "Chirality",
      "x": 1.7998831272125244,
      "y": 1.0971193313598633,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Chirality as non-superposability of an object with its mirror image, producing enantiomorphs.",
        "Concept 2: The relationship between symmetry operations and chirality/achirality (e.g., an object is chiral if rotations/translations cannot map it to its mirror image; the presence of orientation-reversing isometries implies achirality; 2D axis of symmetry yields achirality).",
        "Concept 3: Examples and manifestations of handedness and symmetry across objects and dimensions (hands; helices; Möbius strip; S/Z/Tetris shapes) to illustrate how the concept appears in real-world and mathematical contexts."
      ],
      "original_question_hash": "bce21441"
    },
    {
      "question": "According to parental investment theory, why is the sex that invests more in offspring generally more selective in choosing mates, while the less-investing sex shows greater intra-sex competition for mates?",
      "options": {
        "A": "Because higher parental investment raises the opportunity cost of mating for that sex, so choosing higher-quality mates yields greater fitness returns, whereas the lower-investing sex gains more by mating multiply and therefore competes more with same-sex rivals.",
        "B": "Because the higher-investing sex has inherently higher fertility, forcing it to choose mates more carefully and to avoid competing with conspecifics.",
        "C": "Because parental investment is effectively random and does not shape mating strategies, so both sexes show similar levels of selectivity and competition.",
        "D": "Because the lower-investing sex biologically suppresses its own reproduction, which leaves mating opportunities scarce and forces the higher-investing sex into competition for mates."
      },
      "correct_answer": "A",
      "simplification_notes": "Shortened and clarified the original question while keeping technical terms (e.g., opportunity cost, fitness returns). Removed detailed examples and empirical caveats to focus on the core causal logic of parental investment theory. Options kept plausible but only A preserves the correct mechanism.",
      "content_preserved": true,
      "source_article": "Parental investment",
      "x": 1.8114243745803833,
      "y": 1.111283540725708,
      "level": 2,
      "concepts_tested": [
        "Parental Investment Theory: the relation between which sex invests more in offspring and its resulting mate selectivity versus the other sex’s intra-sexual competition.",
        "Modes of Parental Care and Mating Systems: how exclusive maternal/paternal care vs. biparental care influence sexual selection and offspring survival across life stages.",
        "Parent–Offspring Conflict: the evolutionary tension between the parent’s optimal level of investment and the offspring’s optimal level."
      ],
      "original_question_hash": "35f2b87c"
    },
    {
      "question": "How did the move from historical materialism toward the cultural and linguistic turns change social historians' methods?",
      "options": {
        "A": "It reduced analysis to economic determinism, denying cultural or discursive factors any causal role.",
        "B": "It enforced a single, uniform methodology across subfields, restricting methodological diversity.",
        "C": "It expanded the methodological toolkit to integrate material conditions, culture, and discourse, producing new subfields and approaches.",
        "D": "It abandoned study of everyday people and social groups, returning focus to elite political narratives."
      },
      "correct_answer": "C",
      "simplification_notes": "Reworded the original question into a concise undergraduate-level prompt; tightened language while keeping the contrast between historical materialism and the cultural/linguistic turns; preserved the original answer and ensured all distractors remain plausible.",
      "content_preserved": true,
      "source_article": "Social history",
      "x": 1.234275460243225,
      "y": 0.9714922904968262,
      "level": 2,
      "concepts_tested": [
        "Concept 1: History from below / internal social change — emphasizes lived experience and societal forces over the actions of great individuals.",
        "Concept 2: Theoretical turns and their influence — historical materialism and later cultural/linguistic turns expand methods and sub-fields within social history.",
        "Concept 3: Old vs. new social history / central concerns — contrasts hodgepodge early topics with a focus on labor history and critiques of Whiggish narratives."
      ],
      "original_question_hash": "76942d3d"
    },
    {
      "question": "Why would a firm selling a high-end, service‑intensive product choose an exclusive distribution arrangement (a small, carefully chosen set of channel partners) instead of mass distribution?",
      "options": {
        "A": "To expand market coverage quickly by putting the product into many outlets, thereby reducing the firm's need to manage the brand actively.",
        "B": "To concentrate distribution through a few selected partners who can be trained and motivated to deliver the required high service levels, helping maintain prices and tighter control of the brand.",
        "C": "Because exclusive channels remove the need for promotional support—the partners will automatically generate sufficient consumer demand.",
        "D": "Because exclusive arrangements lower margins by forcing partners to compete on price, which can lead to a damaging price war for the brand."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was condensed and focused on strategic rationale; jargon preserved (e.g., \"exclusive distribution\", \"channel partners\"); distractor options kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Distribution (marketing)",
      "x": 1.359237551689148,
      "y": 0.9866669178009033,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Channel strategy must align with product characteristics, market scope, and organizational mission (mass vs selective vs exclusive distribution). How/why choices affect coverage, cost, and service levels.",
        "Concept 2: Intermediaries and logistics add value and influence performance; channel member selection and rewards shape distribution outcomes. How/why the distribution network is designed and incentivized.",
        "Concept 3: Distribution is a core element of the marketing mix and interacts with product, price, and promotion within strategic planning. How/why distribution decisions influence overall value proposition and strategic objectives."
      ],
      "original_question_hash": "1c972e16"
    },
    {
      "question": "How do division of labour and comparative advantage produce the gains from trade that underpin commerce?",
      "options": {
        "A": "By having every producer try to make every good themselves, eliminating trade and reducing the need for coordination.",
        "B": "By encouraging each producer to specialise in activities where they have the lowest opportunity cost, which raises total output and lets trade reallocate goods to those who value them most.",
        "C": "By making absolute productivity equal across all producers so that trade becomes only a matter of individual preference rather than efficiency.",
        "D": "By automatically equalising incomes across regions through central planning and fixed prices, removing incentives for exchange."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; key economic terms (division of labour, comparative advantage, opportunity cost, specialisation, reallocation) retained; distractors made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Commerce",
      "x": 1.3247265815734863,
      "y": 0.9190332293510437,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Aids to trade and intermediaries reduce frictions (time/place, payments, knowledge, risk) to enable smoother transactions.",
        "Concept 2: Division of labor and comparative advantage drive specialization and exchange, forming the basis for commerce.",
        "Concept 3: Commerce influences macroeconomic outcomes (growth, development, employment, living standards) and interdependence between regions/nations."
      ],
      "original_question_hash": "e9618294"
    },
    {
      "question": "In the data→information→knowledge pipeline, raw observations are summarized into information and then interpreted as domain knowledge. Why can the same piece of information (for example, the average regional temperature over the past hour) count as knowledge in meteorology but not in retail, without changing the information itself?",
      "options": {
        "A": "Because knowledge requires data to be validated against a single universal truth that applies across all domains.",
        "B": "Because interpretation depends on domain-specific models, goals, relevance criteria, and prior assumptions that map patterns to actionable understanding.",
        "C": "Because information becomes knowledge only when it completely eliminates all uncertainty about the domain phenomenon.",
        "D": "Because the format of the information (numeric versus textual) determines whether a domain can treat it as knowledge."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem to be concise and explicit about the data→information→knowledge pipeline, kept the temperature example, and shortened option texts while preserving the original ideas. Technical language retained where relevant (e.g., 'domain-specific models').",
      "content_preserved": true,
      "source_article": "Information",
      "x": 1.3828034400939941,
      "y": 1.0694944858551025,
      "level": 2,
      "concepts_tested": [
        "The data-information-knowledge pipeline and interpretation: data is processed into information, which is then interpreted to become knowledge; understanding depends on context and domain.",
        "Information vs. knowledge and meaning: information is the content that can inform through interpretation, but it is not itself knowledge; meaning emerges via interpretation.",
        "Encoding, transmission, and compression as mechanisms: information can be encoded into signs or signals, transmitted/stored, and compressed by exploiting redundancy up to a theoretical limit; interpretation hinges on decoding and handling ambiguity."
      ],
      "original_question_hash": "cea6d893"
    },
    {
      "question": "Why can a firm's net income for a period differ from its cash flow from operations? How does net income affect the balance sheet via retained earnings, given the accounting equation $Assets = Liabilities + Equity$?",
      "options": {
        "A": "Net income is added directly to cash, so it always equals cash flow from operations.",
        "B": "Net income increases retained earnings (part of equity) on the balance sheet, while cash flow from operations records actual cash receipts and payments; non‑cash items (e.g., depreciation) and working capital changes can make net income and cash flow diverge.",
        "C": "The income statement determines cash on hand, so net income and cash flow are always identical at period end.",
        "D": "Changes in long‑term debt determine net income, and those debt changes appear in the cash flow statement but not in net income, explaining any differences."
      },
      "correct_answer": "B",
      "simplification_notes": "Question shortened and clarified: explicitly links net income to retained earnings and the accounting equation $Assets = Liabilities + Equity$. Distractors kept plausible but concise; technical terms (retained earnings, working capital, non‑cash items) retained for undergraduate level.",
      "content_preserved": true,
      "source_article": "Financial accounting",
      "x": 1.3478952646255493,
      "y": 0.9319583177566528,
      "level": 2,
      "concepts_tested": [
        "The objective of financial reporting: providing useful information to investors, lenders, and other creditors.",
        "The accounting equation as a fundamental relationship: Assets = Liabilities + Equity.",
        "Interconnections among financial statements: how net income affects equity and how the cash flow, income, and balance sheet statements relate through time (opening/closing balances, cash flows, and profit)."
      ],
      "original_question_hash": "0951fed5"
    },
    {
      "question": "Why does a purely mathematical reliability model typically fail to give an accurate \"true\" reliability for a real-world system?",
      "options": {
        "A": "Because real systems operate in varying environments and usage profiles and include interdependent, often unknown factors (human actions, manufacturing variation, interactions, etc.), so a single equation yields only a conditional probability with large uncertainty.",
        "B": "Because mathematical models assume time can be extrapolated exactly; with enough historical data they will always predict reliability exactly.",
        "C": "Because true reliability depends entirely on operator skill and behavior, which mathematics cannot describe.",
        "D": "Because the physics-of-failure are inherently unmeasurable, so mathematical models provide no useful reliability information."
      },
      "correct_answer": "A",
      "simplification_notes": "Condensed the original long explanation to a single, clear question emphasizing multivariate uncertainty, operational context, and limitations of quantitative prediction. Distractors were kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Reliability engineering",
      "x": 1.4771002531051636,
      "y": 1.0515681505203247,
      "level": 2,
      "concepts_tested": [
        "The reliability function as a probabilistic measure of performing a function over time and its relationship to availability and environment.",
        "The limits of quantitative reliability predictions due to real-world uncertainty and multivariate factors, emphasizing that math alone cannot capture true reliability.",
        "The interdisciplinary, cost-driven nature of reliability engineering, including its links to quality and safety engineering and the focus on costs associated with failures (downtime, spares, warranty)."
      ],
      "original_question_hash": "52aa9d57"
    },
    {
      "question": "In hard real-time systems, why is it insufficient to use only average response times to guarantee deadlines, and how does increasing system load affect the likelihood of meeting those deadlines?",
      "options": {
        "A": "Hard deadlines require worst-case execution-time (WCET) analysis and a scheduler that can bound each task's worst-case completion time; guarantees exist only if the task set is provably schedulable given WCETs and known interference. As system load approaches the schedulable utilization limit, some deadlines will begin to be missed.",
        "B": "Real-time guarantees can be based on average response times; higher load lowers average delays, so deadlines will still be met under heavier load.",
        "C": "Deadlines can be assured simply by using a faster processor; system load does not influence the ability to meet hard deadlines.",
        "D": "Real-time systems always meet deadlines regardless of load if they use fixed-priority scheduling, so no worst-case timing analysis is required."
      },
      "correct_answer": "A",
      "simplification_notes": "Clarified and condensed the original wording, introduced technical terms (WCET, schedulable) for precision, and preserved the core idea that worst-case bounds and schedulability—not averages—are required and that increasing load reduces margin for meeting deadlines.",
      "content_preserved": true,
      "source_article": "Real-time computing",
      "x": 1.5147453546524048,
      "y": 1.1079939603805542,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Real-time constraint and hard deadlines (deterministic timing) — why deadlines must be met and how system load relates to guarantee of response.",
        "Concept 2: Mechanisms enabling real-time behavior (RTOS, synchronous programming languages, real-time networks) — how these frameworks support predictable timing and meeting deadlines.",
        "Concept 3: Real-time computing in safety-critical control contexts — how timely data processing and action affect environmental control and safety outcomes."
      ],
      "original_question_hash": "a3a13bb5"
    },
    {
      "question": "How do informal (internalized) norms and formal (external) sanctions work together to maintain social order, and why does a society need both across different situations?",
      "options": {
        "A": "They operate in parallel but independently; only one pathway is effective at a time, so societies rely on either internalized norms or external sanctions depending on the situation.",
        "B": "Informal internalized norms create self-regulation through conscience and socialization, aligning behavior without constant surveillance; formal external sanctions supply deterrence and legitimate authority when norms are not internalized or enforcement is required; together they produce compliance in everyday and borderline cases, sustaining order.",
        "C": "They are in continual conflict; formal sanctions undermine internalization, so social order emerges only when formal power is dominant over informal norms.",
        "D": "Informal norms are timeless and universal while formal sanctions are arbitrary; social order results only when formal institutions impose rules regardless of individuals' internal beliefs."
      },
      "correct_answer": "B",
      "simplification_notes": "Question phrasing was shortened and clarified for undergraduate readers. Technical distinction between informal (internalized) and formal (external) control was preserved; historical examples and secondary details were removed. Options were rewritten to be concise and plausible while keeping the original correct choice.",
      "content_preserved": true,
      "source_article": "Social control",
      "x": 1.2521878480911255,
      "y": 0.9973604679107666,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Dual pathways of social control — informal/internalized norms versus formal/external sanctions, and how each contributes to regulating behavior and sustaining social order.",
        "Concept 2: The paradox of deviance and social control — how deviance both signals normative boundaries and motivates the application of social control to maintain order.",
        "Concept 3: Evolution of the concept — how the meaning and focus of social control shifted over time (from collective regulation to emphasis on individual conformity and theoretical frameworks across disciplines)."
      ],
      "original_question_hash": "2edc004e"
    },
    {
      "question": "According to the elastic-rebound theory, by what physical process is stored elastic strain energy converted into the seismic waves measured during an earthquake?",
      "options": {
        "A": "Most energy is dissipated as frictional heat on the fault, with only negligible mechanical radiation of seismic waves.",
        "B": "A rupture front propagates rapidly along the fault, producing abrupt slip and high strain-rate rock deformation that radiates seismic waves.",
        "C": "Stress is relieved slowly by ductile flow over long timescales, producing continuous low‑amplitude seismic signals rather than impulsive waves.",
        "D": "The elastic energy stays locked in minerals and only radiates if magma or fluid intrudes and perturbs the fault zone."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and made more direct; retained technical terms (elastic‑rebound, rupture front, strain‑rate). Distractors rewritten to be plausible but incorrect (frictional heat, ductile flow, fluid/magma trigger).",
      "content_preserved": true,
      "source_article": "Earthquake",
      "x": 1.0212949514389038,
      "y": 0.15678457915782928,
      "level": 2,
      "concepts_tested": [
        "The mechanism of energy release and the generation of seismic waves in the lithosphere, including the elastic rebound theory that governs rupture dynamics.",
        "The relationship between fault geometry/motion (normal, reverse/thrust, strike-slip) and the characteristics of earthquakes, including how tectonic stresses accumulate and are released.",
        "The connections between earthquakes and secondary hazards (tsunamis, landslides, soil liquefaction) and how epicenter location influences the type and extent of hazard."
      ],
      "original_question_hash": "1a8cbd42"
    },
    {
      "question": "During an earthquake, sudden slip on a fault releases stored strain energy as waves. Why are those waves elastic (mechanical) waves rather than other energy forms, and how does the elastic-wave behavior allow seismologists to infer subsurface structure?",
      "options": {
        "A": "Because rocks behave approximately as elastic solids until failure and store recoverable strain energy; when slip occurs that energy is released as coherent mechanical oscillations (P- and S-waves) whose propagation speeds depend on density and elastic moduli. By measuring wave types and arrival times at multiple stations, seismologists map variations in material properties with depth.",
        "B": "Because most earthquake energy is converted to heat, and seismologists infer interior properties from temperature changes or thermal anomalies produced by rupture and measured at the surface.",
        "C": "Because only the Earth's surface matters for recorded signals; the interior cannot support transmitted waves, so seismology relies on surface roughness and surface scattering to infer structure.",
        "D": "Because earthquakes emit electromagnetic pulses that are picked up by sensors, and variations in those EM signals are used to deduce the structure of the Earth's interior."
      },
      "correct_answer": "A",
      "simplification_notes": "Removed historical context and extraneous detail; phrased mechanism and inference more directly for undergraduates; used $P$- and $S$-wave notation and emphasized dependence of wave speed on density and elastic moduli.",
      "content_preserved": true,
      "source_article": "Seismology",
      "x": 1.0357593297958374,
      "y": 0.16967184841632843,
      "level": 2,
      "concepts_tested": [
        "Elastic-wave generation and propagation as the mechanism by which earthquakes release energy and are studied.",
        "Origin of earthquakes within the Earth's interior (rock movement, plate tectonics) and the resulting secondary phenomena (e.g., tsunamis) — causal relationship.",
        "Role of seismographs/seismograms in recording Earth's motion and enabling interpretation of seismic events (measurement and data interpretation)."
      ],
      "original_question_hash": "8741930f"
    },
    {
      "question": "How can sensationalist reporting distort audience understanding and misrepresent the truth even when it includes some accurate facts?",
      "options": {
        "A": "By supplying a large volume of neutral facts that overwhelms readers and makes them overly cautious in judgment.",
        "B": "By focusing attention on emotionally charged elements, selectively citing facts, and framing causality in a sensational way, which skews interpretation and exaggerates significance.",
        "C": "By strictly sticking to verifiable data and presenting multiple perspectives without emotive or narrative emphasis.",
        "D": "By omitting some facts but always including a clear, opposing viewpoint intended to balance the impact."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the question in concise undergraduate-level language, preserved the core mechanism (attention bias, selective facts, sensational framing). Options were kept plausible and aligned to the article; historical/institutional context was omitted to focus the item on causal mechanisms. Correct answer retained as B.",
      "content_preserved": true,
      "source_article": "Sensationalism",
      "x": 1.2303802967071533,
      "y": 0.9853569865226746,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Sensationalism as an attention-maximizing editorial tactic that uses emotional appeal, controversy, omission, loudness, and self-centered presentation to influence audience perception.",
        "Concept 2: The causal relationship between sensationalist tactics and outcomes, including biased/impaired impressions and misrepresentation of truth.",
        "Concept 3: Historical and institutional factors shaping sensationalism (e.g., print culture expansion, sensational literature influence, policy changes like the repeal of the Fairness Doctrine, and censorship)."
      ],
      "original_question_hash": "d076ebea"
    },
    {
      "question": "Why does combining true facts, half‑truths, and value judgments in a disinformation campaign increase its persuasive power in identity‑driven controversies?",
      "options": {
        "A": "Because it depends on repeating only obvious falsehoods so frequently that audiences stop analyzing them critically.",
        "B": "Because mixing partial truths with value judgments aligns the message with people’s preexisting beliefs and trusted sources, creating plausible narratives that are difficult to refute quickly.",
        "C": "Because it seeks to eliminate all credible information channels so audiences cannot verify claims independently.",
        "D": "Because it uses a single, uniform false claim across all channels to achieve broad and consistent acceptance."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the question language, clarified 'forms of knowing' as 'true facts, half‑truths, and value judgments', and kept options concise and plausible while preserving the original intent.",
      "content_preserved": true,
      "source_article": "Disinformation",
      "x": 1.2205588817596436,
      "y": 0.9622453451156616,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Intentionality and adversarial goals of disinformation (deliberate deception for political, military, or commercial gain and public harm)",
        "Concept 2: Mechanisms of disinformation (coordinated campaigns, strategic deception, media manipulation, weaponizing multiple forms of knowing including truths and half-truths to exploit controversies)",
        "Concept 3: Relationship to misinformation and terminology (how disinformation relates to misinformation and why scholars avoid interchangeable use with terms like \"fake news\")"
      ],
      "original_question_hash": "9256f7d8"
    },
    {
      "question": "According to the availability heuristic, how does the subjective ease of recalling examples affect judgments of how common or likely an event is, and why does this lead to biased estimates?",
      "options": {
        "A": "Because people ignore how easily examples come to mind and instead base judgments on full, objective statistics, producing unbiased estimates.",
        "B": "Because retrieval ease is used as a cue to frequency or likelihood: examples that come to mind more readily are judged as more common or more impactful, which skews judgments toward vivid, recent, or well-publicized cases.",
        "C": "Because memory failures make people remember only rare events, causing them to underestimate the frequency of typical events.",
        "D": "Because the ease of recall is converted into precise objective probabilities through deliberate statistical reasoning, so judgments are properly calibrated."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the stem in concise academic language; preserved the core idea that ease of recall is used as a cue for frequency and that this misleads judgments. Options rephrased to be clear and plausible distractors while keeping the original correct choice (B).",
      "content_preserved": true,
      "source_article": "Availability heuristic",
      "x": 1.3260338306427002,
      "y": 1.0424000024795532,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Availability heuristic as a mental shortcut that ties memory accessibility to evaluation/decision making.",
        "Concept 2: The relationship between ease of recall and perceived magnitude or likelihood of outcomes.",
        "Concept 3: Recency bias and the methodological debate about whether judgments reflect lived experience or biased sampling of recalled information."
      ],
      "original_question_hash": "57e7b94d"
    },
    {
      "question": "An artist working along the figurative–abstract continuum is depicting a familiar subject. Why might they choose partial abstraction rather than complete non‑representational depiction?",
      "options": {
        "A": "It retains enough recognizable cues to anchor the subject’s identity and meaning for the viewer while altering form, color, or composition to emphasize perceptual or emotional qualities.",
        "B": "It removes all recognizable cues so the image becomes completely non‑referential and no longer read as the original subject.",
        "C": "It depends on highly detailed, photographic rendering to convince the viewer that the work is an exact depiction of reality.",
        "D": "It signals a failure of representational skill, using color and shape mainly as decorative elements without conceptual purpose."
      },
      "correct_answer": "A",
      "simplification_notes": "Clarified the figurative–abstract 'continuum', tightened phrasing, removed historical examples and extra context, and focused the question on the perceptual/semantic trade-off between partial and total abstraction.",
      "content_preserved": true,
      "source_article": "Abstract art",
      "x": 0.430957168340683,
      "y": 1.0646328926086426,
      "level": 2,
      "concepts_tested": [
        "Abstraction as a continuum: partial versus total abstraction and departure from recognizable imagery",
        "Relationship to figurative art: how abstract and representational art relate, overlap, or diverge",
        "Historical/mechanistic drivers: how 19th-century shifts and prior movements (Romanticism, Impressionism, Expressionism) contributed to the development of abstract art and its theoretical underpinnings"
      ],
      "original_question_hash": "6e4f7152"
    },
    {
      "question": "In Synthetic Cubism, how did artists extend representation beyond classical single-point perspective, and what function did collage and varied textures serve?",
      "options": {
        "A": "They abandoned pictorial form in favor of symbolic signs and relied on flat color as the sole carrier of meaning.",
        "B": "They returned to careful single-point perspective to create a photographic record of reality, using texture only as ornament.",
        "C": "They incorporated collage elements and diverse textures to combine different materials and images on one planar surface, disrupting conventional viewpoint and opening new interpretive possibilities.",
        "D": "They enforced geometric simplification with uniform, rigid surfaces, making perspective a strictly mathematical device."
      },
      "correct_answer": "C",
      "simplification_notes": "Rewrote the multipart question into a single clear sentence, used concise academic wording, preserved the emphasis on collage, textures, and the break with traditional perspective; kept original answer key.",
      "content_preserved": true,
      "source_article": "Modern art",
      "x": 0.38391342759132385,
      "y": 1.04499351978302,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Abstraction as a defining principle driven by experimentation with seeing and materials, and a move away from narrative in modern art.",
        "Concept 2: Evolution of movements as a causal chain (Post-Impressionists → Fauvism → Cubism) with each stage introducing new ideas about form, color, and representation.",
        "Concept 3: Cubism’s technical progression (Analytic Cubism followed by Synthetic Cubism) and the introduction of collage and varied textures as mechanisms for expanding depiction beyond traditional perspective."
      ],
      "original_question_hash": "f05fd1d5"
    },
    {
      "question": "How do automated systems transform large surveillance data streams into actionable intelligence, and which principle explains their main drawback?",
      "options": {
        "A": "They eliminate the need for human interpretation and produce perfectly accurate results by applying algorithmic processing to all data.",
        "B": "They use predefined criteria or triggers to filter and prioritize huge data streams, enabling scalable detection but creating risks of bias and false positives when those criteria are imperfect.",
        "C": "They only aggregate and store data without influencing analysis, leaving all interpretation and prioritization to human analysts.",
        "D": "They ensure comprehensive detection of every possible signal, thereby removing blind spots and guaranteeing no missed targets."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified to focus on automated filtering of surveillance data and its trade-off; options rewritten in concise academic language but kept technically plausible. Core idea of predefined triggers, scalability, and bias/false positives preserved.",
      "content_preserved": true,
      "source_article": "Surveillance",
      "x": 1.2366764545440674,
      "y": 0.8668490648269653,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Automation and data processing as the mechanism that turns data streams into actionable surveillance intelligence (e.g., automated analysis, trigger words, wide-scale monitoring)",
        "Concept 2: Legal and political contexts shape and constrain surveillance, highlighting tensions between information gathering, civil liberties, and governance",
        "Concept 3: Distinction between overt surveillance and espionage, including how legitimacy and legality differ across actors and regimes"
      ],
      "original_question_hash": "0c575338"
    },
    {
      "question": "How does the type–token distinction show that abstraction generalizes from concrete instances?",
      "options": {
        "A": "A token represents the universal features common to all instances, while a type retains the unique attributes of one particular instance.",
        "B": "A type represents the common structure shared by many tokens, omitting the instance-specific details present in any single token.",
        "C": "A token encodes the detailed general rules that define a category, whereas a type is just a fixed single exemplar.",
        "D": "The distinction is only a matter of terminology and does not affect how generalization or abstraction works."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording streamlined, clarified that the task is to link the type–token distinction to abstraction and generalization; distractor options kept plausible but made concise. Technical terms (type, token, abstraction) retained.",
      "content_preserved": true,
      "source_article": "Abstraction",
      "x": 1.272323727607727,
      "y": 1.1116310358047485,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Abstraction as generalization by filtering information to essential aspects, producing a common concept that unifies subordinate concepts.",
        "Concept 2: Type-token distinction and levels of abstraction (types being more abstract than their tokens), illustrating the relationship between general categories and concrete instances.",
        "Concept 3: Abstraction as an inductive reasoning process (synthesizing particular facts into general ideas) and its historical role, including contrasts with deduction."
      ],
      "original_question_hash": "16438833"
    },
    {
      "question": "How do build tools and compilers help port software to a new platform, and why does that usually reduce the amount of source-code changes required?",
      "options": {
        "A": "They automatically translate the program into a single universal bytecode that runs unmodified on any operating system without recompilation.",
        "B": "They detect differences in the target environment during configuration and generate portable build scripts or configuration headers (e.g., via Autotools or cross-compilers), so the same source can be built on multiple platforms with little or no source changes.",
        "C": "They automatically rewrite platform-specific API calls in the source code into standardized equivalents, eliminating the need for manual code edits.",
        "D": "They provide one universal runtime that completely hides all hardware and OS differences at runtime, so the source never needs to be changed."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and focused on the role of build tools/compilers; retained examples (Autotools/GCC) and the key mechanism (environment detection + generated build/config), removed historical background.",
      "content_preserved": true,
      "source_article": "Porting",
      "x": 1.471754550933838,
      "y": 1.1488744020462036,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Porting aims to minimize source-code changes when moving software to a new context, with portability assessed by the cost of porting relative to rewriting.",
        "Concept 2: International standards reduce platform differences, making porting easier, though gray areas can still cause subtle variations.",
        "Concept 3: Build tools and compilers (e.g., GCC, Autotools) act as mechanisms to detect and adapt environment differences, thereby enabling smoother porting."
      ],
      "original_question_hash": "064a6ee7"
    },
    {
      "question": "Why does a deterministic finite automaton (DFA) have exactly one computation path for any given input word, and what does this imply about how it recognizes a language compared to a nondeterministic finite automaton (NFA)? (Use the transition function $\\delta:Q\\times\\Sigma\\to Q$ if helpful.)",
      "options": {
        "A": "Because its transition function $\\delta:Q\\times\\Sigma\\to Q$ maps each (state, symbol) pair to exactly one next state, the DFA therefore produces a unique sequence of states for any input; acceptance is determined by that single path, whereas an NFA can follow many possible paths and accepts if any path ends in an accepting state.",
        "B": "Because a DFA evaluates all possible next states in parallel and requires every resulting path to lead to acceptance for the input to be accepted.",
        "C": "Because a DFA's finite set of states can encode the entire input history, so the path is fixed by that stored history rather than by nondeterministic branching.",
        "D": "Because only the initial state matters for acceptance, making the rest of the input irrelevant to whether the automaton accepts the word."
      },
      "correct_answer": "A",
      "simplification_notes": "Focused the question on the core mechanism (the transition function $\\delta$) and the direct implication for acceptance; removed broader historical and application context and reworded options as plausible distractors.",
      "content_preserved": true,
      "source_article": "Automata theory",
      "x": 1.5355063676834106,
      "y": 1.1599791049957275,
      "level": 2,
      "concepts_tested": [
        "Mechanism of a finite automaton: states, transitions, and a transition function that moves based on input symbols.",
        "Automata-language relationship: automata as finite representations of formal languages and how different automata classify languages within the Chomsky hierarchy.",
        "Applications and theoretical roles: the use of automata in computation, parsing, compiler construction, AI, and formal verification."
      ],
      "original_question_hash": "43036bbf"
    },
    {
      "question": "Why does delegating authority from a central body to local subunits typically improve the quality of decisions in a decentralized system?",
      "options": {
        "A": "Because it preserves central micromanagement, ensuring all local choices are vetted by the central authority.",
        "B": "Local units have better access to proximate information and stakeholder preferences, enabling faster, context-appropriate decisions; coordination mechanisms are still needed to maintain overall coherence.",
        "C": "Because it guarantees uniform, conflict-free policies across all units without any tradeoffs or need for coordination.",
        "D": "Because it transfers all responsibility to local actors and removes the need for any centralized oversight or accountability."
      },
      "correct_answer": "B",
      "simplification_notes": "Rephrased the stem to be concise and academic; retained the core mechanism (delegation), the role of local information and participation, and the caveat about coordination. Options were made plausible distractors but kept the original correct choice.",
      "content_preserved": true,
      "source_article": "Decentralization",
      "x": 1.2882428169250488,
      "y": 0.9905441403388977,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Delegation of authority from a central location to smaller subunits as the core mechanism of decentralization.",
        "Concept 2: Decentralization as a means to enhance civic participation and local autonomy (the civic dimension).",
        "Concept 3: The historical and ideological relationship between decentralization and centralization, including how decentralization can counter central power or be driven by various ideological currents (e.g., anarchism, libertarianism) with different outcomes."
      ],
      "original_question_hash": "75b5eb96"
    },
    {
      "question": "Why are fundamental structural decisions in software architecture costly to change later in a system's life?",
      "options": {
        "A": "Because early architectural choices intentionally isolate functionality into well-encapsulated modules, so changing one area is inexpensive.",
        "B": "Because they establish module boundaries and coupling patterns; modifying those structures forces changes to many components, increasing effort and risk.",
        "C": "Because architectural choices concern only performance, so changing them has little to do with long-term maintenance or evolution.",
        "D": "Because once architectural decisions are documented they become effectively immutable, and altering them requires extensive bureaucracy that raises costs."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the question for clarity and concision; removed references to specific practices (e.g., C4, Architectural Kata) while preserving the core idea that early architectural choices create boundaries and coupling that make later changes expensive. Options were made concise and kept plausible.",
      "content_preserved": true,
      "source_article": "Software architecture",
      "x": 1.428821086883545,
      "y": 1.0726255178451538,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Architecture encodes fundamental structural decisions that are costly to change, requiring trade-offs and early commitments.",
        "Concept 2: Non-functional requirements (quality attributes) drive architectural modeling and evaluation, using practices like Architectural Kata and the C4 Model to realize desired attributes.",
        "Concept 3: Architecture serves as a communication framework and supports reuse, distinct from application design, with documentation facilitating stakeholder alignment."
      ],
      "original_question_hash": "6da087d3"
    },
    {
      "question": "Data mining produces patterns (for example, clusters, associations, sequential patterns) that act as compact summaries of the input data. How does treating these patterns as summaries help decision making, and what fundamental limitation must be recognized when relying on them?",
      "options": {
        "A": "Because patterns provide a compressed, lossless representation of the original records, allowing exact reconstruction of every case for decisions; limitation: this reconstruction is always computationally expensive.",
        "B": "Because patterns reveal regularities that often persist into new data, supporting prediction and more informed decisions; limitation: those regularities can be spurious or fail to generalize if the sample is not representative.",
        "C": "Because patterns identify all possible relationships in the data, ensuring no information is lost for later analyses; limitation: discovering every relationship is typically computationally intractable.",
        "D": "Because once patterns are discovered they are immediately valid and can be deployed without further testing; limitation: patterns then never need updating as new data arrive."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was condensed and clarified for undergraduate readers; technical examples (clusters, associations, sequential patterns) were retained; distractors made plausible but incorrect. The correct option and core concept (patterns support prediction but may not generalize) were preserved.",
      "content_preserved": true,
      "source_article": "Data mining",
      "x": 1.510722041130066,
      "y": 1.0983244180679321,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Data mining is the analysis step in the Knowledge Discovery in Databases (KDD) process and is distinct from data collection/pre-processing and post-processing.",
        "Concept 2: The goal of data mining is to discover patterns (e.g., clusters, anomalies, associations, sequential patterns) in large data sets, and these patterns summarize the input data to support further analysis or decision making.",
        "Concept 3: Data mining is interdisciplinary, drawing on machine learning, statistics, and database systems, and it is related to but distinct from broader data analysis and analytics (including common misperceptions about the term)."
      ],
      "original_question_hash": "6c4349d4"
    },
    {
      "question": "Why is the data analysis process typically described as iterative, where findings in later phases can require returning to earlier steps?",
      "options": {
        "A": "Because once a phase is completed its outputs are final and cannot provide information relevant to earlier work.",
        "B": "Because results from later phases often expose data-quality issues, flawed assumptions, or new patterns that force re-cleaning, redefining variables/features, or even re-collecting data.",
        "C": "Because iteration is mainly a project-management convenience and does not change the validity or content of the analysis.",
        "D": "Because early steps (requirements, collection, cleaning) are independent of later analytical results and should be done only once before analysis begins."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the question in concise academic language for undergraduates; preserved the original rationale about feedback from later phases requiring earlier-phase revisions; kept all four options plausible and retained the correct choice.",
      "content_preserved": true,
      "source_article": "Data analysis",
      "x": 1.4512525796890259,
      "y": 1.0481709241867065,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The data analysis process is iterative, with feedback across phases that can loop back to earlier steps.",
        "Concept 2: Different analytical approaches (descriptive, exploratory, confirmatory, predictive, text analytics, data mining, BI) have distinct goals and relate to one another within a broader framework of data analysis.",
        "Concept 3: Data requirements and data collection constitute the input side of the data-to-information pipeline, shaping how raw data are transformed into information for decision-making."
      ],
      "original_question_hash": "dae9f924"
    },
    {
      "question": "How do mechanisms such as increasing returns and network effects produce path dependence and possible lock-in of a technology or institution?",
      "options": {
        "A": "They create positive feedback loops: as more users adopt a standard, its usefulness and complementary goods increase, drawing still more users and raising switching costs, which pushes the system toward a dominant, self-reinforcing path.",
        "B": "They guarantee the market instantly selects the objectively best design, so no suboptimal path can persist.",
        "C": "They make later entrants automatically compatible with the incumbent standard, eliminating switching costs or biases toward earlier choices.",
        "D": "They make historical choices irrelevant by ensuring future outcomes depend only on current price signals and not on past adoption patterns."
      },
      "correct_answer": "A",
      "simplification_notes": "Question and options were tightened to undergraduate-level academic language, clarified terms (positive feedback, switching costs, dominant standard), and distractors made plausible yet incorrect. Technical examples from the article were omitted to focus on the general mechanism.",
      "content_preserved": true,
      "source_article": "Path dependence",
      "x": 1.3515815734863281,
      "y": 0.9932183027267456,
      "level": 2,
      "concepts_tested": [
        "Concept 1: History matters; past events/decisions constrain future events and can shape long-run equilibria.",
        "Concept 2: Mechanisms that generate path dependence (e.g., increasing returns, positive feedback, network effects) leading to lock-in.",
        "Concept 3: The role of interpretation and context (examples can support or challenge path dependence; outcomes may reflect demand fit or other factors, not just mechanism)."
      ],
      "original_question_hash": "b3c5cc49"
    },
    {
      "question": "How does local context influence curriculum effectiveness, and how should that influence selecting a curriculum?",
      "options": {
        "A": "Context only affects logistical details (e.g., scheduling, classroom size); it does not change what or how students should learn.",
        "B": "A curriculum is effective when its goals, content, teaching methods and assessments fit local needs, resources, culture and the program’s scope; selection should follow a systematic development cycle (analysis → design → selecting → formation → review) and be guided by inclusive, participatory policy principles.",
        "C": "There is a single universally best curriculum that will work equally well in every setting, so context is irrelevant to choice.",
        "D": "Curriculum policy and selection should be driven solely by experts or central authorities, without adapting to local demographic, cultural or resource differences."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified: 'context' specified as local needs, resources, culture and program scope. Correct option expanded to mention the staged development cycle and participatory policy; distractors made concise and reflect common misconceptions.",
      "content_preserved": true,
      "source_article": "Curriculum development",
      "x": 1.2505768537521362,
      "y": 0.9417315125465393,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Systematic, staged curriculum development process (analysis → design → selecting → formation → review) as a mechanism for continuous improvement.",
        "Concept 2: Context-dependency in curriculum choice (no universal best curriculum; effectiveness depends on location, population, and program scope).",
        "Concept 3: Participatory, humanistic policy approach (inclusive policy dialogue, social/economic justice, environmental responsibility) shaping legitimacy and outcomes of curriculum frameworks."
      ],
      "original_question_hash": "59e983cb"
    },
    {
      "question": "In professional development that emphasizes intensive collaboration (e.g., coaching, mentoring, communities of practice) and includes an evaluative stage, which mechanism best explains why this design tends to change daily professional practice rather than only produce knowledge gain?",
      "options": {
        "A": "Increased content density ensures more material is learned, which drives behavior change.",
        "B": "Reflective feedback from peers and mentors during authentic work cycles creates social accountability and directly links learning to everyday practice.",
        "C": "The evaluative stage primarily measures compliance with regulations, which accounts for observed changes.",
        "D": "Short, discrete modules minimize cognitive load and therefore make it easier to apply new knowledge to tasks."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the stem to emphasize collaborative PD forms and the evaluative stage; replaced \"real-work cycles\" with \"authentic work cycles\"; kept original distractors but phrased them more concisely and academically.",
      "content_preserved": true,
      "source_article": "Professional development",
      "x": 1.2789651155471802,
      "y": 0.9751086235046387,
      "level": 2,
      "concepts_tested": [
        "Concept 1: PD serves to earn/maintain professional credentials and maintain professional competence, linking learning outcomes to certification and ongoing qualifications.",
        "Concept 2: PD is characterized by intensive, collaborative practice and an evaluative component, delivered through diverse approaches (consultation, coaching, communities of practice, mentoring, etc.).",
        "Concept 3: Participation in PD is motivated by lifelong learning, moral obligation, career advancement, keeping up with new practices, and regulatory requirements that vary by jurisdiction."
      ],
      "original_question_hash": "a08cf9a3"
    },
    {
      "question": "Why can education serve both as a pathway for social mobility and as a mechanism that reproduces social inequality in practice?",
      "options": {
        "A": "Because educational outcomes are determined only by innate ability, so mobility is equally available regardless of background.",
        "B": "Because schools systematically redesign society to remove preexisting hierarchies, guaranteeing mobility for everyone.",
        "C": "Because education operates within existing social structures and, via processes such as tracking, differential resource allocation, the hidden curriculum, and privileging of dominant cultural capital, it can expand opportunities for some while reinforcing class, race, and gender disparities for others.",
        "D": "Because curricula are identical across all schools and therefore produce the same outcomes for students from different social backgrounds."
      },
      "correct_answer": "C",
      "simplification_notes": "Wording was made more concise and direct; retained key theoretical terms (tracking, resource allocation, cultural capital, hidden curriculum) and gave concrete mechanisms from the article while preserving the original correct option.",
      "content_preserved": true,
      "source_article": "Sociology of education",
      "x": 1.2378402948379517,
      "y": 0.9653224945068359,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Education as both a vehicle for social mobility and a mechanism that reproduces inequality (class, race, gender) in practice.",
        "Concept 2: The empirical relationship between social class and educational achievement, showing that education reflects existing stratification and yields limited mobility.",
        "Concept 3: Competing theoretical perspectives (functionalism, human-capital theory, neo-Marxism) and their proposed mechanisms for how education functions in society (e.g., preparing a skilled workforce, maintaining class relations, or enabling equal opportunity)."
      ],
      "original_question_hash": "de4f98fd"
    },
    {
      "question": "Why do axioms together with deductive proofs serve as the foundation of mathematical truth, and what happens to a theorem if one adopts a different but consistent set of axioms?",
      "options": {
        "A": "Because axioms make truth inherent: once a statement is proved it is universally valid no matter what starting assumptions are used.",
        "B": "Because proofs derive the logical consequences of a chosen set of axioms; changing to a different consistent axiom set can change which statements are theorems, and one compares theories by constructing models or mappings to see which statements hold in both systems.",
        "C": "Because axioms are empirical assumptions and proofs verify experimental outcomes, so different axioms produce different empirical predictions.",
        "D": "Because theorems are determined only by formal symbol‑manipulation rules and axioms add no substantive mathematical content."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the two-part question into clear academic wording; removed historical and applied-math context while keeping technical terms (axioms, proofs, theorems, models); preserved original correct option B.",
      "content_preserved": true,
      "source_article": "Mathematics",
      "x": 1.6330658197402954,
      "y": 1.181069016456604,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The role of axioms and deductive proofs as the foundation for mathematical truth (how proofs derive theorems from axioms).",
        "Concept 2: The relationship between pure mathematics and applied mathematics, including why some areas are developed independently and later find practical applications.",
        "Concept 3: The bidirectional relationship between mathematics and empirical sciences (mathematics models phenomena and advances in mathematics reflect and influence scientific discovery)."
      ],
      "original_question_hash": "e6a129d6"
    },
    {
      "question": "Using Noether's theorem, why does invariance of a system's Lagrangian under time translations imply energy conservation, and what is the consequence if the Lagrangian has explicit time dependence?",
      "options": {
        "A": "Time-translation symmetry gives a conserved quantity via Noether's theorem: the Hamiltonian $H=\\sum_i p_i\\dot q_i- L$, which corresponds to the system's total energy. If $L$ has no explicit time dependence then $\\frac{dH}{dt}=0$ and energy is conserved. If $L$ depends explicitly on time the symmetry is broken and $H$ is not generally conserved.",
        "B": "Time-translation invariance implies conservation of momentum rather than energy, and allowing explicit time dependence in $L$ does not alter energy conservation.",
        "C": "Time invariance forces the kinetic energy alone to remain constant; if $L$ has explicit time dependence the energy of the system increases linearly with time.",
        "D": "Noether's theorem does not apply to time translations, so energy conservation is merely empirical and can be violated when the Lagrangian (or laws) depend explicitly on time."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and focused on the Noether link between time-translation symmetry and the Hamiltonian. Mathematical expressions converted to inline LaTeX ($H=\\sum_i p_i\\dot q_i- L$, $dH/dt=0$). Historical/contextual material removed while keeping the core physics concept.",
      "content_preserved": true,
      "source_article": "Conservation of energy",
      "x": 1.7185547351837158,
      "y": 1.1070404052734375,
      "level": 2,
      "concepts_tested": [
        "Energy conservation principle: in isolated/closed systems, total energy remains constant while individual forms can transform or transfer.",
        "Mass-energy equivalence and its impact on conservation: E = mc^2 implies mass and energy are interchangeable and conserved together in relativistic contexts.",
        "Noetherian mechanism linking symmetry to conservation: time-translation invariance leads to conservation of energy via Noether's theorem."
      ],
      "original_question_hash": "2de9dd2c"
    },
    {
      "question": "A block slides on a rough surface and its kinetic energy decreases because of friction. Explain why the total energy remains constant if we include the heat produced in the block and the surface. (You may write the total energy as $E_{\\text{total}}=K+U_{\\text{int}}$.)",
      "options": {
        "A": "Energy is not conserved; the kinetic energy simply vanishes and is lost as heat.",
        "B": "The kinetic energy lost is converted into internal (thermal) energy of the block and surface, so $E_{\\text{total}}$ stays the same.",
        "C": "Friction adds energy to the block from the environment, which compensates for the drop in kinetic energy.",
        "D": "As the block slows it gains gravitational potential energy, which offsets the decrease in kinetic energy."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified scenario and question, added a compact expression $E_{\\text{total}}=K+U_{\\text{int}}$, shortened option text and kept all four options plausible while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Energy",
      "x": 1.750222086906433,
      "y": 1.0681270360946655,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Conservation of energy — energy can be converted between forms but is not created or destroyed.",
        "Concept 2: Mechanical energy as the sum of kinetic and potential energy, and energy exchange between these forms within a system.",
        "Concept 3: Diversity and transformability of energy forms (kinetic, potential, elastic, chemical, radiant, internal, rest) and their interrelations within the single conserved energy framework."
      ],
      "original_question_hash": "03d332a7"
    },
    {
      "question": "Which mechanism best explains how nation-states can shape archaeological interpretations of past lifeways?",
      "options": {
        "A": "They guarantee that interpretations are scientifically objective and free from political or cultural bias.",
        "B": "By controlling access to sites and artifacts, directing funding and heritage laws, and shaping public narratives, states influence which data are collected and how findings are interpreted and presented.",
        "C": "They require archaeologists to study only the oldest or most spectacular artifacts, excluding mundane or recent materials.",
        "D": "They cause archaeologists to depend exclusively on written records and to neglect material culture and excavation evidence."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording shortened and clarified; mechanism specified (access, funding, legislation, public narratives) while preserving the original idea that state power steers data collection and interpretation.",
      "content_preserved": true,
      "source_article": "Archaeology",
      "x": 1.392753005027771,
      "y": 0.9030779004096985,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The link between archaeological methods (survey, excavation, analysis) and its goals (understanding culture history, reconstructing past lifeways, explaining changes in human societies)",
        "Concept 2: The interdisciplinary nature of archaeology and its relationships with other fields (social science, humanities, anthropology, history, geography)",
        "Concept 3: The relationship between archaeology and society/power (how nation-states shape past visions; issues like pseudoarchaeology, looting, ethical concerns)"
      ],
      "original_question_hash": "b59e8250"
    },
    {
      "question": "Environmental psychology frames human–environment interactions as a two-way feedback loop: environments offer affordances that guide behavior, and behaviors modify environments. Which statement best explains how this loop sustains adaptation over time?",
      "options": {
        "A": "Environments are fixed constraints; people adjust their behavior but do not meaningfully change the environment.",
        "B": "Perceived affordances direct action; repeated actions transform the physical and social environment, which changes future affordances and thus alters subsequent behavior.",
        "C": "Internal values and motivations override environmental cues, so environmental change is driven primarily by individual intentions rather than reciprocal interaction.",
        "D": "Environmental changes always occur first and people respond passively, providing no feedback that modifies the environment."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem to clarify the two-way feedback loop and affordances; shortened and used precise academic language; kept core concept that behaviors change environments which then alter future affordances.",
      "content_preserved": true,
      "source_article": "Environmental psychology",
      "x": 1.3750916719436646,
      "y": 0.9915102124214172,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The bidirectional human–environment relationship (how environments shape individuals and how humans shape environments)",
        "Concept 2: A predictive model of human-environment interactions used to guide design, management, protection, and restoration",
        "Concept 3: Pro-environmental behavior and sustainability as outcomes shaped by psychological factors and interventions"
      ],
      "original_question_hash": "13f34eff"
    },
    {
      "question": "How does a decline in fertility during the demographic transition tend to raise capital per worker and encourage investment in human capital?",
      "options": {
        "A": "Because having fewer children increases the number of dependents per worker, which pressures households and the economy to accumulate more physical capital per worker.",
        "B": "Because fewer dependents per working-age adult frees household and public resources for saving and for investing more in each child's education and health, thereby increasing capital per worker and human capital.",
        "C": "Because lower fertility directly increases the rate of capital depreciation, which forces firms and households to invest more in machinery and equipment.",
        "D": "Because higher incomes associated with demographic change reduce the incentive to save, which paradoxically accelerates capital accumulation through stronger consumption-led growth."
      },
      "correct_answer": "B",
      "simplification_notes": "Removed extended historical background and jargon; focused the question on the economic mechanism (fewer dependents → more resources per worker → higher physical and human capital). Wording made concise and targeted to undergraduate readers.",
      "content_preserved": true,
      "source_article": "Demographic transition",
      "x": 1.2236140966415405,
      "y": 0.9124980568885803,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The demographic transition as a theory linking demographic shifts (fertility and mortality) with broader economic and social development.",
        "Concept 2: Mechanisms that drive the transition and its economic impact, notably reduced dilution of capital/land, increased investment in human capital, and changes in labor-force size and age structure.",
        "Concept 3: Debates about causality and factors (does industrialization/income lead to lower fertility, or vice versa, and what roles do mortality, human capital, and old-age security play?)."
      ],
      "original_question_hash": "376f3354"
    },
    {
      "question": "In procedural justice, giving people a chance to voice their concerns during a decision often makes the process feel fairer even when the final outcome is unchanged. Why does allowing a voice alter perceptions of fairness and the legitimacy of the process?",
      "options": {
        "A": "Because speaking increases the likelihood that the outcome will change in the speaker's favor, making the result more equitable.",
        "B": "Because being allowed to speak signals respect and that participants are valued, improving perceptions of interpersonal treatment, transparency, and legitimacy of the decision procedure.",
        "C": "Because permitting everyone to speak removes all potential biases from the final decision, guaranteeing neutrality.",
        "D": "Because allowing voice shortens or streamlines decision-making, making the process seem more efficient and thus more legitimate."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording tightened for undergraduate readers; question framed concisely about why voice affects perceived fairness; distractor options rephrased to remain plausible but incorrect; correct answer preserved.",
      "content_preserved": true,
      "source_article": "Procedural justice",
      "x": 1.2519243955612183,
      "y": 0.8996846079826355,
      "level": 2,
      "concepts_tested": [
        "The impact of giving individuals a voice on perceptions of fairness and legitimacy of decisions",
        "The distinction between procedural justice and distributive justice, including scenarios where fair procedures yield acceptance even with unequal outcomes",
        "The role of interpersonal treatment (respect/value) in procedural justice and its link to employee satisfaction and performance"
      ],
      "original_question_hash": "9ae07201"
    },
    {
      "question": "Collateral estoppel (issue preclusion) bars relitigation in a civil case of an issue that was actually litigated and essential to a final criminal judgment. What justifies this doctrine as a rule of civil procedure?",
      "options": {
        "A": "To avoid imposing multiple punishments on the defendant for the same conduct.",
        "B": "To ensure criminal and civil trials use identical standards of proof and admissibility of evidence.",
        "C": "To promote judicial efficiency, finality, and consistency by preventing duplicative litigation and conflicting rulings on issues already decided.",
        "D": "To allow a civil plaintiff to rely automatically on the criminal trial's findings regardless of the civil forum's procedures."
      },
      "correct_answer": "C",
      "simplification_notes": "Condensed the stem to define collateral estoppel clearly and succinctly; preserved core rationale. Options were reworded to be concise and all remain plausible distractors.",
      "content_preserved": true,
      "source_article": "Civil procedure",
      "x": 1.2416820526123047,
      "y": 0.796687662601471,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The lifecycle of a civil case (beginning, service, pleadings, discovery, trial, judgment, post-trial procedures) as a coherent mechanism for adjudicating disputes.",
        "Concept 2: The civil–criminal divide (who initiates actions, who bears responsibility, and why separate procedural systems exist).",
        "Concept 3: Cross-procedural evidentiary principles (evidence from criminal trials in civil actions and the collateral estoppel doctrine) and their rationale (efficiency, consistency, finality)."
      ],
      "original_question_hash": "1a1258f3"
    },
    {
      "question": "Why is feedback-driven reflection central to a continual improvement process, and how does it enable sustainable, incremental improvement?",
      "options": {
        "A": "Because it reveals gaps between actual performance and stated objectives, enabling small, evidence-based adjustments and organizational learning that accumulate over time.",
        "B": "Because it ensures improvements are large, radical jumps led by external experts that rapidly overhaul the system.",
        "C": "Because it prioritizes data collection so much that it delays timely action, increasing the risk of stagnation.",
        "D": "Because it concentrates solely on efficiency and ignores effectiveness or customer needs."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the question to be clearer and more concise for undergraduates; removed extraneous phrasing while keeping the core idea that feedback drives small, cumulative adjustments and learning. Options were made equally plausible but only A preserves the correct mechanism.",
      "content_preserved": true,
      "source_article": "Continual improvement process",
      "x": 1.4067622423171997,
      "y": 1.027764081954956,
      "level": 2,
      "concepts_tested": [
        "Feedback-driven reflection and adjustment as the core mechanism of continual improvement",
        "Incremental/evolutionary change (Kaizen) as the preferred mode of improvement",
        "Worker-driven, participative approaches (ownership, teamwork, low capital requirements) enabling feasible and sustainable improvements"
      ],
      "original_question_hash": "d5466d91"
    },
    {
      "question": "Why does the doctrine of \"piercing the corporate veil\" exist, and which fact pattern most strongly supports applying it?",
      "options": {
        "A": "To remove the protection of limited liability when owners use a corporation to perpetrate fraud or evade obligations; it is most appropriate where there is a unity of interest between owner and company and treating them as separate would produce an inequitable result.",
        "B": "To punish shareholders generally; it should be applied whenever a corporation fails to pay its debts, regardless of the circumstances.",
        "C": "To make parent companies automatically liable for subsidiary debts whenever the parent controls the subsidiary, without regard to undercapitalization, commingling of assets, or fraud.",
        "D": "To hold employees personally liable for all actions taken on behalf of the corporation; it applies whenever someone acts as an agent of the company."
      },
      "correct_answer": "A",
      "simplification_notes": "Shortened and clarified the question; used precise legal language (e.g., \"unity of interest\", \"inequitable result\"); kept original legal rationale and the strongest supporting scenario as option A. Distractors were made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Limited liability",
      "x": 1.3087708950042725,
      "y": 0.854521632194519,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Limited liability as a protective mechanism with exceptions (piercing the corporate veil) that link corporate form to liability exposure.",
        "Concept 2: Personal liability exceptions (co-signing, director guarantees, employee actions) that undermine or constrain limited liability in specific contexts.",
        "Concept 3: Relationship and liability allocation within corporate groups (parent-subsidiary) and how veil-piercing can extend or limit liability across entities; plus the contract vs tort distinction and its policy implications."
      ],
      "original_question_hash": "bc85e0a7"
    },
    {
      "question": "In a mechatronic manufacturing system, how does a hybrid control architecture balance global production objectives with autonomous local modules?",
      "options": {
        "A": "It depends entirely on a single central controller that computes and enforces all decisions to achieve global optimality, sacrificing local responsiveness.",
        "B": "It removes centralized coordination and uses fully distributed local controllers only, maximizing local responsiveness but reducing system-wide coordination.",
        "C": "It uses centralized coordination to set global objectives and decentralized local controllers that react to local disturbances, combining alignment with adaptability.",
        "D": "It sequences control so local controllers act only after a global controller finishes planning, preventing conflicts but introducing scheduling delays."
      },
      "correct_answer": "C",
      "simplification_notes": "Clarified 'reconcile' as 'balance', shortened phrasing, retained control-architecture types and trade-offs (centralized, distributed, hybrid, sequential) and their implications for global objectives and local autonomy.",
      "content_preserved": true,
      "source_article": "Mechatronics",
      "x": 1.528062343597412,
      "y": 1.0490907430648804,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Synergistic integration across mechanical, electrical/electronic, and computing domains to create simpler, more economical, and reliable systems.",
        "Concept 2: The role of control theory and control architectures (hierarchy, polyarchy, heterarchy, hybrid) in coordinating mechatronic modules to achieve production goals and flexible manufacturing.",
        "Concept 3: The broadening scope of mechatronics from a mechanics-plus-electronics idea to a cross-disciplinary design approach that encompasses automation, robotics, and related subfields."
      ],
      "original_question_hash": "d0787d1f"
    },
    {
      "question": "How does the implicit (hidden) curriculum affect student learning, and why can it either reinforce or undermine the explicit (planned) curriculum's stated objectives?",
      "options": {
        "A": "It prescribes the sequence of topics and thus guarantees that classroom instruction aligns exactly with the stated objectives of the explicit syllabus.",
        "B": "It transmits norms, expectations, and everyday behaviors through school culture and daily classroom interactions; when these align with official goals they reinforce them, but when they conflict they can undermine the explicit curriculum.",
        "C": "It is identical to the explicit curriculum, so there is no difference between what is formally planned and what students learn.",
        "D": "It comes only from extracurricular activities and therefore has no effect on formal classroom learning outcomes."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was shortened and clarified for undergraduate students; removed historical and peripheral details and focused on the distinction between implicit (hidden) and explicit curricula and their mechanisms of influence. Options were made concise and plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Curriculum",
      "x": 1.2577377557754517,
      "y": 0.9699963927268982,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Explicit vs implicit (including hidden) curriculum — how formal content interacts with school culture, behaviors, and unintended lessons to influence learning.",
        "Concept 2: Curriculum as the totality of student experiences — the coupling of planned instruction, materials/resources, and evaluation in achieving educational objectives.",
        "Concept 3: Variability and interpretation of curricula — the tension between standardized and autonomous curricula and the existence of multiple theoretical definitions/frameworks (e.g., Dewey, Eisner) shaping why/how curricula are designed and implemented."
      ],
      "original_question_hash": "92c5c9a2"
    },
    {
      "question": "How did early 19th-century, state-imposed mass compulsory schooling (for example, Prussia c.1800) illustrate the way government policy can shape education and broader social outcomes?",
      "options": {
        "A": "By delegating schooling to private religious institutions, thereby preserving local loyalties and reducing the state's ability to form a unified citizenry.",
        "B": "By enforcing compulsory attendance with standardized curricula and assessments that served explicit state goals (e.g., producing soldiers and obedient citizens), thus shaping social behavior and political allegiance.",
        "C": "By directing funds mainly to elite universities, concentrating resources on producing a scholarly ruling class relatively disconnected from mass civic formation.",
        "D": "By removing state oversight and letting market forces govern schooling, increasing curricular diversity but weakening coordinated civic instruction and national cohesion."
      },
      "correct_answer": "B",
      "simplification_notes": "Question language was tightened and an explicit historical example (Prussia c.1800) was added; options were kept plausible and aligned with the original concepts about state influence, religious institutions, elite funding, and market-driven schooling.",
      "content_preserved": true,
      "source_article": "History of education",
      "x": 0.3645920157432556,
      "y": 0.3051500618457794,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Education as a function of religious/clerical institutions (monasteries, mosques, church-led schooling) that preserve and transmit knowledge.",
        "Concept 2: State policy as a driver of education (compulsory schooling, aims to produce soldiers/obedient citizens, modernization efforts) and its social/political outcomes.",
        "Concept 3: The evolution of higher education from church-centered to secular or state-supported universities, reflecting changing power structures and funding mechanisms."
      ],
      "original_question_hash": "bdb53b86"
    },
    {
      "question": "Why can a landscape both anchor a community's local identity and also change to express wider national narratives?",
      "options": {
        "A": "Because landscapes change only through climate and geological processes, so cultural influences do not shape their character or meaning.",
        "B": "Because landscapes are determined entirely by human intention; natural landforms are incidental and do not contribute to identity.",
        "C": "Because landscapes are a dynamic synthesis: enduring natural landforms provide continuity, while cultural overlays (buildings, land use, and collective memory) are added over time, allowing landscapes to reflect evolving local and national identities.",
        "D": "Because landscapes are purely aesthetic constructs that do not influence or reflect social or national senses of self."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; technical phrase 'dynamic synthesis' retained but explained; options reworded to be concise and plausible; correct answer letter unchanged.",
      "content_preserved": true,
      "source_article": "Landscape",
      "x": 1.5198497772216797,
      "y": 0.9614195823669434,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Landscape as a dynamic synthesis of natural landforms, ecosystems, and human/cultural overlays, shaping and reflecting local and national identity.",
        "Concept 2: Aesthetic design of landscapes (e.g., color landscapes) and its impact on perception, emotion, and social meaning, influenced by geography, climate, materials, culture, and socioeconomic factors.",
        "Concept 3: Landscaping as deliberate modification of visible landscape features and the implications for what counts as a landscape in different definitions and contexts."
      ],
      "original_question_hash": "b092c6fb"
    },
    {
      "question": "In statistical mechanics, why does the second law produce an arrow of time and predict that the entropy of an isolated system typically increases? How does this relate to the number of microscopic states (microstates) corresponding to different macrostates?",
      "options": {
        "A": "Because the fundamental microscopic dynamical equations are themselves time‑irreversible, so microscopic trajectories cannot be run backward and the system cannot evolve toward lower entropy.",
        "B": "Because macrostates with higher entropy have the same number of microstates as low‑entropy macrostates, so there is no statistical preference and the direction of evolution is indifferent.",
        "C": "Because macrostates with higher entropy correspond to vastly greater numbers of microstates than low‑entropy macrostates, so random microscopic dynamics overwhelmingly lead the system into higher‑entropy macrostates, producing an emergent arrow of time.",
        "D": "Because entropy increase only occurs when a system continuously receives energy from its surroundings; that external input enforces a preferred time direction."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording shortened and focused on the statistical‑mechanics explanation; retained the link between entropy increase, arrow of time, and microstate counting. Distractors rephrased to be plausible alternatives. Correct answer letter preserved (C).",
      "content_preserved": true,
      "source_article": "Second law of thermodynamics",
      "x": 1.750938057899475,
      "y": 1.0759633779525757,
      "level": 2,
      "concepts_tested": [
        "Entropy, equilibrium, and the arrow of time: entropy tends to increase in isolated systems, driving processes toward thermodynamic equilibrium and giving a directional sense to time.",
        "Irreversibility and limits on energy conversion: not all heat can be converted into work in a cyclic process; the second law imposes fundamental constraints (e.g., Carnot efficiency) beyond the first law.",
        "Statistical mechanics interpretation: the second law emerges from microscopic probability distributions, linking macroscopic irreversibility to the behavior of large assemblages of atoms/molecules."
      ],
      "original_question_hash": "830607cc"
    },
    {
      "question": "How can the mere possibility of financial distress impose costs on a firm even if it never files for bankruptcy?",
      "options": {
        "A": "It cannot; indirect costs only occur if the firm actually goes through bankruptcy.",
        "B": "Stakeholders respond to higher perceived risk — lenders charge higher interest and impose tighter covenants, suppliers demand stricter terms, and management diverts attention from operations — creating higher financing costs and reduced operational performance.",
        "C": "It reduces the firm's need for external financing because managers become more risk‑averse, lowering overall financing costs.",
        "D": "It lowers the firm's cost of capital because creditors and suppliers become more lenient to avoid forcing bankruptcy."
      },
      "correct_answer": "B",
      "simplification_notes": "Question rephrased for clarity and concision; emphasized stakeholder behavioral responses (higher rates, covenants, diverted managerial attention) as sources of indirect costs; options adjusted to remain plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Financial distress",
      "x": 1.3389837741851807,
      "y": 0.8848656415939331,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3"
      ],
      "original_question_hash": "e06e9de4"
    },
    {
      "question": "Why are latitude and longitude given with respect to reference lines (the equator and the Prime Meridian), and how does that show that an “absolute” location is still relative to those references?",
      "options": {
        "A": "Because a fixed, shared reference frame is required to make the numeric coordinates meaningful; latitude and longitude are angular distances measured from chosen reference lines (equator, Prime Meridian), so changing the reference frame would change the numbers — therefore an “absolute” location is defined relative to the chosen references.",
        "B": "Because latitude and longitude are intrinsic, frame‑independent properties of a place that exist regardless of any chosen reference lines, so the reference lines do not alter their numeric values.",
        "C": "Because coordinates are invariant under changing reference lines; you can pick a different Prime Meridian or equator and keep the same latitude/longitude numbers.",
        "D": "Because absolute position is correctly defined only by the distance from Earth’s center, so surface reference lines like the equator or Prime Meridian are irrelevant to whether a location is absolute."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording tightened and focused on why coordinates use reference lines and what that implies about the relativity of 'absolute' locations. Extraneous examples removed; options kept plausible and aligned with the original concepts.",
      "content_preserved": true,
      "source_article": "Location",
      "x": 1.6057097911834717,
      "y": 1.0688748359680176,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The distinction between location and place, including how boundary precision vs. ambiguity affects identification of a region.",
        "Concept 2: The relationship between absolute and relative location, including why a coordinate pair is described using reference lines (Prime Meridian, equator) and how absolute locations are still relative to those references.",
        "Concept 3: The role of geographic coordinate systems (Cartesian/spherical grids, latitude/longitude, WGS) in specifying position and how different systems provide different frameworks for measuring and describing location."
      ],
      "original_question_hash": "eb206ba2"
    },
    {
      "question": "In a platform with direct network effects, which explanation best describes how adding users increases the platform’s value and can produce multiple adoption equilibria?",
      "options": {
        "A": "Each new user raises per-user production costs, making the platform less attractive and reducing incentives to join.",
        "B": "Each additional user increases the number of potential connections, so the platform becomes more valuable to current and prospective users; this positive-feedback loop (critical mass) raises perceived value as adoption grows and can produce low- and high-adoption equilibria depending on expectations.",
        "C": "The platform’s value is determined solely by its core features and does not depend on the number of users.",
        "D": "A larger user base only lowers price per user through economies of scale, so adoption increases because of cost savings rather than increased network value."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed phrasing, emphasized direct network effects, positive feedback/critical mass, and multiple equilibria; clarified contrast with cost-based (economies of scale) explanations; retained technical terms.",
      "content_preserved": true,
      "source_article": "Network effect",
      "x": 1.3072279691696167,
      "y": 0.9839702844619751,
      "level": 2,
      "concepts_tested": [
        "Direct vs indirect network effects as mechanisms that change user value and adoption decisions (why/how the number and composition of users affects utility).",
        "Positive feedback, critical mass, and the potential for multiple equilibria or monopoly (why/how increasing adopters can accelerate further adoption and create different market outcomes).",
        "Relationship to economies of scale and the role of expectations (why/how network effects differ from cost-based explanations and how expectations shape outcomes)."
      ],
      "original_question_hash": "5231e583"
    },
    {
      "question": "Why does a single empirical finding rarely prove a scientific theory, and how does this influence how scientists apply the theory–hypothesis–data cycle?",
      "options": {
        "A": "Because empirical findings are always due to chance; a single result proves nothing and should not be expected to align with a theory.",
        "B": "Because data are interpreted within theoretical and methodological frameworks: a single result can support, refute, or prompt revision of a theory, especially given measurement choices, auxiliary assumptions, and sampling variability; therefore scientists rely on repeated tests, varied designs, and iterative use of the theory–hypothesis–data cycle.",
        "C": "Because theories are axioms that cannot be tested empirically, so empirical findings are irrelevant to proving or disproving them.",
        "D": "Because once data align with a theory, no further data can contradict it, so one confirming result is sufficient to settle the theory."
      },
      "correct_answer": "B",
      "simplification_notes": "I shortened and clarified the stem, emphasized that interpretation depends on theoretical and methodological choices (measurement, assumptions, sampling), and highlighted that scientists use replication and iterative testing. Kept original correct answer and made distractors plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Empirical research",
      "x": 1.288522481918335,
      "y": 1.0547943115234375,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The theory–hypothesis–data cycle and how empirical results support, modify, or falsify underlying theory.",
        "Concept 2: The complementary roles of qualitative and quantitative evidence and why integrating both can better answer empirical questions.",
        "Concept 3: The role of research design and context (including when experiments are used and when field settings are necessary) in establishing what can be inferred from data and how causal relationships are approached."
      ],
      "original_question_hash": "73119af2"
    },
    {
      "question": "Why explicitly include learning in the objective function (i.e., adopt active adaptive management), and what common trade-off does this create?",
      "options": {
        "A": "It focuses on maximizing immediate outcomes and reduces the need to collect new information; the trade-off is reduced long-term learning and worse future decisions.",
        "B": "It assigns value to actions that improve knowledge even if they temporarily lower current performance, so decisions become better over time; the trade-off is greater computational and operational complexity to optimize for both outcomes and information gains.",
        "C": "It eliminates the role of new observations and keeps a fixed strategy, avoiding updates to models; the trade-off is consistent but increasingly outdated decisions.",
        "D": "It treats learning only as a passive byproduct of actions rather than an objective, avoiding deliberate experiments; the trade-off is slower accumulation of useful information and slower adaptation."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the stem to a single clear question, retained the technical phrase 'objective function' and 'active adaptive management', clarified the conceptual reason (value of information) and the typical trade-off (computational/operational difficulty). Distractors were adjusted to remain plausible and distinct.",
      "content_preserved": true,
      "source_article": "Adaptive management",
      "x": 1.3789597749710083,
      "y": 0.8806410431861877,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3"
      ],
      "original_question_hash": "44ed1124"
    },
    {
      "question": "In air-quality monitoring, what additional role do air-dispersion models play when interpreting sensor measurements, beyond the raw concentration data?",
      "options": {
        "A": "They act as a full replacement for sensors by predicting pollutant concentrations everywhere, removing the need to collect measurements.",
        "B": "They combine emissions, topography, and meteorological data to help attribute observed concentrations to likely sources, explain spatial patterns, and test hypothetical scenarios.",
        "C": "They only apply regulatory thresholds to measured values to check compliance with standards.",
        "D": "They mainly reduce data volume by averaging or smoothing measurements over time, losing temporal detail in the process."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording tightened for clarity; emphasized the models' integrative role (emissions, terrain, weather) and kept four plausible distractors.",
      "content_preserved": true,
      "source_article": "Environmental monitoring",
      "x": 1.746496319770813,
      "y": 0.9913662672042847,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3"
      ],
      "original_question_hash": "2462eae3"
    },
    {
      "question": "Why does relocating a site-specific artwork to a different place effectively destroy the work?",
      "options": {
        "A": "Because the physical object itself is the primary essence of the piece, so moving it preserves that essence regardless of location.",
        "B": "Because the audience’s interpretation is fixed and independent of place, so changing the site cannot alter the work’s meaning.",
        "C": "Because the artwork’s form, meaning, and even its ability to exist were created in response to the site’s unique combination of physical and contextual conditions, so moving it breaks that essential relationship.",
        "D": "Because the materials and techniques used are universal and unaffected by context, so relocating the object would not change what the work means."
      },
      "correct_answer": "C",
      "simplification_notes": "Wording shortened and clarified that \"site\" includes both physical and contextual conditions; preserved the original idea that site-specific works depend on place for form and meaning.",
      "content_preserved": true,
      "source_article": "Site-specific art",
      "x": 0.45566853880882263,
      "y": 1.0797972679138184,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The site determines the artwork—its form, existence, and limits (e.g., the work “can only exist under such circumstances,” and “to move the work is to destroy the work”).",
        "Concept 2: Artworks derive meaning from their context and audience, opposing the Modernist emphasis on transportable, central museum objects and critiquing the museum as an institution.",
        "Concept 3: Public space and site-specific works generate unique social and cultural dynamics (e.g., debates like Tilted Arc) that influence reception, legitimacy, and the relationship between art, space, and spectators."
      ],
      "original_question_hash": "c6c356a1"
    },
    {
      "question": "Why does a site-specific installation typically aim to transform how a viewer experiences a place rather than just present discrete objects, and how does this relate to the artist's conceptual intent?",
      "options": {
        "A": "Because integrating with a particular site and directing viewers' movement lets meaning emerge from the interaction of space, materials, and observer—echoing conceptual art's emphasis on idea over standalone form.",
        "B": "Because the primary aim is to select very large or dominant materials to physically and emotionally overwhelm the viewer, using the site's size mainly for impact.",
        "C": "Because designing for a specific location ensures the work can be reproduced identically in any other venue, preserving a fixed interpretation.",
        "D": "Because site-specific installations try to avoid audience participation so the artist can maintain complete control over the work's meaning."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened to undergraduate-level academic language; extraneous historical detail was removed. Options were rewritten to remain plausible while preserving the original correct rationale linking site integration, viewer interaction, and conceptual intent.",
      "content_preserved": true,
      "source_article": "Installation art",
      "x": 0.4396032691001892,
      "y": 1.0709307193756104,
      "level": 2,
      "concepts_tested": [
        "Site-specificity and transformation of perception: installations are designed for particular spaces to alter how they are experienced.",
        "Artistic intention and conceptual roots: emphasis on the artist’s intention and ties to conceptual art and readymades.",
        "Multisensory, media-rich immersion: use of varied materials, video, sound, performance, VR, and other media to create immersive experiences."
      ],
      "original_question_hash": "37974a44"
    },
    {
      "question": "In a product design process that cycles through Analysis, Concept, and Synthesis, why is iterating between these stages important for producing a successful product?",
      "options": {
        "A": "It ensures the team strictly follows the original constraints and prevents altering requirements.",
        "B": "It primarily delays final decisions so the team can collect more data before committing to a direction.",
        "C": "It creates a feedback loop in which testing and evaluation update understanding of user needs and technical feasibility, refining concepts and reducing the risk of late-stage failures.",
        "D": "It lets marketing priorities drive technical decisions early so the product aligns with current market trends."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording was made more concise and direct for undergraduate readers; extraneous details about tools and related disciplines were removed while preserving the core idea that iteration provides feedback to refine designs and reduce risk.",
      "content_preserved": true,
      "source_article": "Product design",
      "x": 1.410264015197754,
      "y": 1.0454624891281128,
      "level": 2,
      "concepts_tested": [
        "The product design process is a structured, iterative framework (e.g., analysis, concept, synthesis) that spans from idea generation to commercialization.",
        "The integration of art, science, and technology, and the role of digital tools (communication, visualization, analysis, 3D modeling, prototyping) in enabling design work.",
        "The relationships and boundaries between product design and related disciplines (industrial design, service/software design) and how boundaries are blurred or overlapping."
      ],
      "original_question_hash": "c5ccab59"
    },
    {
      "question": "How do international organizations make states' commitments more credible so cooperation is sustained?",
      "options": {
        "A": "By applying uniform, automatic sanctions regardless of situation, which forces compliance.",
        "B": "By creating credible enforcement mechanisms—monitoring and reporting plus predictable consequences—that connect present behaviour to future rewards or penalties.",
        "C": "By taking over all policy decisions from member states so governments can avoid domestic political costs of cooperation.",
        "D": "By relying only on moral persuasion and appeals to norms to secure guaranteed future cooperation."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the question in clearer undergraduate academic language and condensed the stem. Kept the original mechanism in the correct option (monitoring, reporting, predictable consequences linking present actions to future benefits/penalties). Made all distractors plausible but incorrect.",
      "content_preserved": true,
      "source_article": "International organization",
      "x": 1.193347692489624,
      "y": 0.8566879034042358,
      "level": 2,
      "concepts_tested": [
        "Mechanisms by which IOs facilitate cooperation (e.g., reducing transaction costs, providing information, credible commitments, focal points, reciprocity, shadow of the future, issue linkage)",
        "Compliance dynamics with IO decisions (rational cost-benefit calculations and normative socialization/social learning)",
        "IO design space and its impact on effectiveness (membership scope, rule precision/flexibility, hard vs soft obligations, delegation of power)"
      ],
      "original_question_hash": "21c07238"
    },
    {
      "question": "Which mechanism best explains how a newly synthesized polypeptide chain adopts its specific three-dimensional (3D) structure in an aqueous environment?",
      "options": {
        "A": "The amino acid sequence alone rigidly determines the final 3D structure solely through backbone hydrogen bonds, independent of the solvent and side-chain interactions.",
        "B": "Folding is driven primarily by covalent disulfide bonds that lock residues into a final conformation, so non-covalent interactions are largely irrelevant.",
        "C": "A cooperative, spontaneous process in which many non-covalent interactions — hydrogen bonds, ionic interactions, van der Waals forces and the hydrophobic effect — between side chains and the backbone drive the chain into a thermodynamically favorable native structure.",
        "D": "The polypeptide remains unfolded until it encounters its substrate; binding to the substrate provides the structural information that folds the chain into its functional conformation."
      },
      "correct_answer": "C",
      "simplification_notes": "Reworded the stem for clarity and concision; preserved technical terms (non-covalent interactions, hydrophobic effect, disulfide bonds). Distractors were kept plausible but made concise to match undergraduate level.",
      "content_preserved": true,
      "source_article": "Protein structure",
      "x": 1.975854754447937,
      "y": 1.1050727367401123,
      "level": 2,
      "concepts_tested": [
        "The sequence of amino acids (primary structure) determines higher-order structure and function.",
        "Protein folding is driven by non-covalent interactions (hydrogen bonds, ionic interactions, Van der Waals forces, hydrophobic effects) and peptide-bond formation underpins polymerization.",
        "Proteins adopt and interconvert between multiple conformations (conformational changes) that enable their biological roles."
      ],
      "original_question_hash": "dd8aa1a3"
    },
    {
      "question": "Why can an allosteric activator that binds to a site distinct from the active (orthosteric) site increase an enzyme's catalytic activity even when substrate binding at the active site is unchanged?",
      "options": {
        "A": "It stabilizes the enzyme in a high-activity conformation, improving active-site geometry and dynamics to favor catalysis.",
        "B": "It increases the effective local concentration of substrate at the active site (substrate presentation), raising the reaction rate.",
        "C": "The activator directly participates in the chemical transformation at the active site as a co-substrate.",
        "D": "It prevents an inhibitory regulator from binding to the enzyme, thereby indirectly increasing activity."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened and made more direct for undergraduates; terminology clarified (active/orthosteric site). Distractor choices were rewritten to be plausible alternative mechanisms (substrate presentation, direct chemical role, blocking an inhibitor).",
      "content_preserved": true,
      "source_article": "Allosteric regulation",
      "x": 2.069342613220215,
      "y": 1.1056973934173584,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Binding at an allosteric site induces conformational changes and/or dynamic changes that modulate the activity of the protein (mechanism of action).",
        "Concept 2: Distinction between allosteric and orthosteric regulation, including how allosteric inhibitors often produce non-competitive effects (activity changes independent of active-site substrate concentration).",
        "Concept 3: Role of allostery in cellular regulation and signaling, including long-range effects and involvement in feedback/feedforward control loops."
      ],
      "original_question_hash": "c07221b4"
    },
    {
      "question": "Holism holds that emergent properties are system-level features not predictable from the properties of individual parts. Which mechanism best explains how such emergent properties arise?",
      "options": {
        "A": "A simple linear sum of the properties of individual components (an additive model).",
        "B": "Random environmental or conditional variations that create new properties independently of interactions among parts.",
        "C": "Nonlinear interactions, specific internal organization, and feedback among parts that produce new system-level constraints and behaviors.",
        "D": "Each part carrying identical properties that simply scale up when many parts are combined into a whole."
      },
      "correct_answer": "C",
      "simplification_notes": "Rephrased the prompt for clarity and concision, removed background detail, and kept four plausible distractors while preserving the original correct choice (interactions/organization/feedback).",
      "content_preserved": true,
      "source_article": "Holism",
      "x": 1.2364118099212646,
      "y": 1.0784887075424194,
      "level": 2,
      "concepts_tested": [
        "Emergent properties: The idea that a system’s properties emerge at the level of the whole and are not reducible to its components; how such properties arise and are identified.",
        "Whole–part relationship/non-reducibility: The claim that wholes have properties and behaviors that cannot be fully explained by examining parts alone; why reductionist analyses fail to predict system-level phenomena.",
        "Holism as a framework for understanding processes (evolution and universal order): The notion that holism explains evolution as a creative, whole-system process and posits a fundamental ordering of the universe; how this contrasts with reductionist or purely mechanistic accounts."
      ],
      "original_question_hash": "6b5892e7"
    },
    {
      "question": "In a structure-centered social theory, why must explanations state clear scope conditions and historical contingencies to be convincing?",
      "options": {
        "A": "Because these theories emphasize individual choices and momentary actions, so broad generalizations are needed to accommodate short-term variation.",
        "B": "Because they treat social structures as fixed and universally applicable, so any explanation will hold across all times and places without specifying conditions.",
        "C": "Because enduring structures produce effects only under particular conditions and historical moments; without explicit scope conditions and contingencies the theory risks overgeneralization and misattributing causes.",
        "D": "Because they assume human agency alone drives social change, making structural qualifications irrelevant to convincing explanation."
      },
      "correct_answer": "C",
      "simplification_notes": "Wording shortened and clarified for undergraduate readers; retained technical terms (\"scope conditions\", \"historical contingencies\", \"structure-centered\"). Distractors were kept plausible by reflecting common misunderstandings about structure vs. agency.",
      "content_preserved": true,
      "source_article": "Social theory",
      "x": 1.2021814584732056,
      "y": 0.9764661192893982,
      "level": 2,
      "concepts_tested": [
        "Structure vs. agency: the central tension about whether social outcomes are best explained by social structures or by human agency, and how this shapes theoretical analysis.",
        "Methodological orientation (positivism vs antipositivism): how debates about research methods influence what counts as valid theory and evidence in social science.",
        "Criteria for robust social theory: features such as clear terms, well-defined statements, scope conditions, absence of contradictions, generality, parsimony, and conditionality that enable theories to explain and apply broadly."
      ],
      "original_question_hash": "86fe8038"
    },
    {
      "question": "Why does avant-garde art act as a vehicle for social or political reform, and by what mechanism does its experimental practice affect society?",
      "options": {
        "A": "By embedding explicit political messages into familiar, traditional forms so audiences read them as concrete policy proposals.",
        "B": "By destabilizing dominant norms through unexpected forms and ideas, prompting critical reflection and creating space for new political interpretations and collective action.",
        "C": "By relying on the charisma or public persona of the artist to persuade audiences to adopt reforms, rather than on the artwork's form or content.",
        "D": "By using sensationalism and emotional shock that automatically produces swift legislative change without further public discourse."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened and academic phrasing retained; removed extended historical examples and jargon while preserving the idea that avant-garde challenges norms and effects change via destabilization and public reflection. Options were made concise and kept plausible.",
      "content_preserved": true,
      "source_article": "Avant-garde",
      "x": 0.4905645251274109,
      "y": 1.1011345386505127,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Avant-garde as a vanguard that challenges established art forms and norms through experimentation.",
        "Concept 2: Art as a vehicle for social/political reform (art-as-politics) and the mechanism by which avant-garde influences society.",
        "Concept 3: Historical relationships and trajectories among movements (e.g., Dada, Situationist International, Language poets) showing how avant-garde practices evolve in response to cultural contexts."
      ],
      "original_question_hash": "f913a7e8"
    },
    {
      "question": "Given that contemporary art often lacks a single organizing principle and combines diverse materials, methods, concepts, and subjects, how does this shape how viewers derive meaning from such works?",
      "options": {
        "A": "It makes interpretation depend entirely on the artist's stated intent.",
        "B": "It encourages plural interpretations: different viewers use their cultural, personal, and contextual knowledge, and the varied materials and choices provide multiple entry points to meaning.",
        "C": "It requires museums or institutions to impose a fixed, codified interpretation so audiences have a standardized experience.",
        "D": "It forces artists to choose one dominant medium or unifying device so viewers can arrive at a single clear meaning."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened and clarified; emphasized link between absence of a unifying principle and plural interpretation; options rephrased to be concise and plausible while preserving original intent.",
      "content_preserved": true,
      "source_article": "Contemporary art",
      "x": 0.49468809366226196,
      "y": 1.092286467552185,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Contemporary art as a dynamic, boundary-challenging practice characterized by a lack of a single organizing principle and a blend of materials, methods, concepts, and subjects.",
        "Concept 2: The relationship between contemporary art and broader contextual frameworks (personal/cultural identity, family, community, nationality) shaping meaning and interpretation.",
        "Concept 3: The evolving boundary between modern and contemporary art, including changing start dates, shifting classifications, and the role of institutions like museums in defining scope."
      ],
      "original_question_hash": "2a7e8435"
    },
    {
      "question": "Why do applied researchers typically rely on established theories and existing empirical methods when solving concrete real-world problems, and how does that choice support commercial or practical goals?",
      "options": {
        "A": "Because seeking theoretical novelty requires discarding proven theories, which is necessary to solve practical problems and to pursue market disruption.",
        "B": "Because established theories and empirical methods provide interpretable, actionable solutions that can be deployed efficiently and are aligned with commercial objectives.",
        "C": "Because applied research benefits from ignoring data and relying solely on intuition to quickly craft new frameworks that may not be testable.",
        "D": "Because relying only on randomized experiments in all contexts guarantees generalizable results, even when context and timelines constrain data collection."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was shortened and clarified for undergraduate readers; jargon reduced while keeping technical terms. Options were kept plausible and concise; the correct answer and core distinctions between applied and basic research, empirical methods, and practical/commercial aims were preserved.",
      "content_preserved": true,
      "source_article": "Applied science",
      "x": 1.3792310953140259,
      "y": 1.0550122261047363,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The distinction between applied science/research and basic/pure science, focusing on practical application of knowledge versus theory-building.",
        "Concept 2: The role of empirical methods and existing theories in applied research to solve concrete, real-world problems (and the link to commercial objectives).",
        "Concept 3: Methodological flexibility in applied research (relaxation of strict protocols in messy real-world contexts) while emphasizing transparency and interpretive considerations."
      ],
      "original_question_hash": "5a82c443"
    },
    {
      "question": "How does replicating parts of a prior study test instruments, procedures, or experiments, and why does this increase confidence in the findings?",
      "options": {
        "A": "By applying the instrument to different subject areas to create novel results and expand its use.",
        "B": "By re-running the core procedures in new samples and settings to determine whether the results are robust or depend on particular conditions.",
        "C": "By re-publishing the original study so the same results get more attention and perceived credibility.",
        "D": "By analyzing the original dataset again and claiming broader validity without introducing new controls or samples."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; 'replicating elements' explained as re-running procedures in new samples/settings; distractors kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Research",
      "x": 1.3136860132217407,
      "y": 1.0313538312911987,
      "level": 2,
      "concepts_tested": [
        "Systematic method and bias control as essential mechanisms to increase knowledge",
        "Replication and validation as methods to test instruments, procedures, or experiments",
        "Epistemology-driven diversity of research forms and the basic vs. applied research relationship"
      ],
      "original_question_hash": "860b88ca"
    },
    {
      "question": "Why must forecasts report the degree of uncertainty, and how does that affect planning and risk management?",
      "options": {
        "A": "Reporting uncertainty is unnecessary; it encourages stakeholders to treat the forecast as a precise target to hit, which simplifies decisions by removing the need to consider outcome ranges.",
        "B": "Showing uncertainty lets decision-makers weigh the likelihood of alternative outcomes, support risk-aware choices, enable contingency plans and flexible allocation of resources; omitting uncertainty tends to produce overconfident plans that fail when reality deviates.",
        "C": "Reporting uncertainty guarantees forecast accuracy because it exposes data errors and therefore removes uncertainty from future estimates.",
        "D": "Reporting uncertainty is mainly cosmetic and does not meaningfully affect the quality of planning or risk management."
      },
      "correct_answer": "B",
      "simplification_notes": "Compressed the original multi-paragraph rationale into a single clear question; kept technical idea that forecasts must convey uncertainty and how that enables contingency planning and flexible resource use; shortened distractors while keeping them plausible.",
      "content_preserved": true,
      "source_article": "Forecasting",
      "x": 1.423122763633728,
      "y": 0.9977219700813293,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Forecasts should acknowledge and convey uncertainty, since risk and uncertainty are central to forecasting.",
        "Concept 2: Forecast accuracy hinges on data quality, timeliness, and the nature of input data (time series, cross-sectional, longitudinal), including cases where inputs themselves are forecast.",
        "Concept 3: Forecasts and budgets serve different purposes in planning and decision-making, with forecasts providing flexible estimates to adapt to changing circumstances."
      ],
      "original_question_hash": "7c57ce5a"
    },
    {
      "question": "Why does increasing temperature usually speed up a chemical reaction, if the activation energy barrier $E_{a}$ is unchanged?",
      "options": {
        "A": "Because raising temperature increases the average kinetic energy of molecules, so a larger fraction of collisions have energy ≥ $E_{a}$ and are successful.",
        "B": "Because increasing temperature lowers the activation energy $E_{a}$, so collisions need less energy to produce products.",
        "C": "Because heating shifts the chemical equilibrium toward products, which makes the observable rate of product formation faster.",
        "D": "Because higher temperature reduces the total number of collisions but makes each collision more energetic, which somehow increases the overall rate."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording shortened and made more direct for undergraduates; activation energy written as $E_{a}$ in LaTeX; distractors kept plausible common misconceptions (Ea change, equilibrium shift, collision-frequency confusion).",
      "content_preserved": true,
      "source_article": "Chemical reaction",
      "x": 1.8754562139511108,
      "y": 1.038576602935791,
      "level": 2,
      "concepts_tested": [
        "Activation energy and its role in determining reaction rate and the effect of temperature on rates",
        "Reaction mechanism: sequence of elementary steps, and the idea of a rate-determining step",
        "Catalysis and enzymes: how catalysts accelerate reactions and enable biological pathways"
      ],
      "original_question_hash": "052b0500"
    },
    {
      "question": "Why does the Fundamental Theorem of Calculus relate the accumulation function \\(A(x)=\\int_{a}^{x} f(t)\\,dt\\) to the original rate function \\(f(x)\\)?",
      "options": {
        "A": "Because increasing the upper limit by an infinitesimal \\(dx\\) adds approximately \\(f(x)\\,dx\\) to \\(A\\), and taking the limit \\(dx\\to 0\\) gives \\(dA/dx=f(x)\\).",
        "B": "Because the derivative of \\(A\\) gives the average rate of accumulation over the interval \\([a,x]\\), which equals the average value of \\(f\\) on that interval.",
        "C": "Because the theorem holds for any function \\(f\\), regardless of continuity, so \\(\\frac{d}{dx}\\int_{a}^{x}f(t)\\,dt\\) always equals \\(f(x)\\).",
        "D": "Because integration and differentiation are completely unrelated; the equality is a surprising coincidence that occurs only for some functions."
      },
      "correct_answer": "A",
      "simplification_notes": "Question language shortened and focused on the infinitesimal/limit intuition; mathematical expressions converted to inline LaTeX; distractor options kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Calculus",
      "x": 1.6500887870788574,
      "y": 1.1712244749069214,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The fundamental relationship between differential and integral calculus via the Fundamental Theorem of Calculus (linking rates of change to accumulation).",
        "Concept 2: Calculus as the study of continuous change and accumulation (differential calculus for instantaneous rates/slopes; integral calculus for areas/quantities).",
        "Concept 3: The role of limits and convergence in defining and grounding the calculus (limits of sequences/series underpin derivatives and integrals)."
      ],
      "original_question_hash": "8d714fe3"
    },
    {
      "question": "Under mercantilist \"bullionism\", why was keeping gold and silver circulating through the economy prioritized over simply hoarding it?",
      "options": {
        "A": "Because circulating specie increases exchanges, which stimulates demand and production; hoarding immobilizes capital and reduces economic activity, slowing wealth creation.",
        "B": "Because circulation weakens state authority and reduces the government's capacity to regulate trade, which mercantilists sought to avoid.",
        "C": "Because hoarding precious metals more reliably finances military expansion and colonial ventures than allowing money to circulate.",
        "D": "Because circulating money directly raises a country's gold reserves and therefore immediately improves the balance of payments."
      },
      "correct_answer": "A",
      "simplification_notes": "Question phrasing simplified for clarity; retained mercantilist bullionism concept. Options reworded to be concise and plausible distractors while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Mercantilism",
      "x": 1.2145466804504395,
      "y": 0.9289906620979309,
      "level": 2,
      "concepts_tested": [
        "Bullionism and the movement of money: wealth is tied to precious metals and money should circulate to spur trade and growth, not merely be hoarded.",
        "Trade balance and reserves: policies aim for a positive balance of trade and accumulating monetary reserves, linking current account surpluses to economic strength.",
        "State regulation and tariffs as instruments of power: government intervention (tariffs, regulation) is used to augment state power and national wealth, often with implications for expansion and competition with rivals."
      ],
      "original_question_hash": "d742411c"
    },
    {
      "question": "Under the biopsychosocial model, which mechanism best explains how social factors can change the health effects of chronic stress?",
      "options": {
        "A": "By directly altering DNA sequences to cause disease, independently of stress, coping, or behavior.",
        "B": "By shaping coping strategies and perceived control, which modifies physiological stress responses (e.g., HPA‑axis activity) and health-related behaviours, thereby altering health outcomes.",
        "C": "By determining fixed biological characteristics such as age and sex, which alone set an individual's disease risk regardless of social or psychological factors.",
        "D": "By eliminating biological influences on health entirely through social programs and policies."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording tightened and made more concise for undergraduates; retained the biopsychosocial framework and HPA‑axis example; distractors made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Health psychology",
      "x": 1.2825392484664917,
      "y": 1.0050292015075684,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Biopsychosocial model as the framework for health and illness, integrating biological, psychological, and social factors.",
        "Concept 2: Mechanisms by which psychological and behavioral factors affect health (e.g., stress impacting the HPA axis; health behaviors such as smoking and exercise influencing health outcomes).",
        "Concept 3: Translation of theory into practice in health psychology (clinical care, public health programs, professional training, and collaborative healthcare)."
      ],
      "original_question_hash": "7c495194"
    },
    {
      "question": "Why is assuming self-interested behavior by voters, politicians, and bureaucrats necessary in public-choice models to explain phenomena like logrolling and pork-barrel spending? How would the model's predictions change if agents were instead altruistic (seeking to maximize collective welfare)?",
      "options": {
        "A": "Because self-interest generates strategic interactions, trade-offs, and exchange-like bargains that produce coalitions, logrolling, and targeted spending; if agents were altruistic, incentives would align more with total welfare, reducing rent-seeking and changing coalition formation.",
        "B": "Because self-interest guarantees voters will pick policies that maximize social welfare, so observed outcomes don't require strategic explanations; assuming altruism would leave predictions essentially unchanged.",
        "C": "Because self-interested political agents make government action efficient through competitive political markets; if agents were altruistic, political incentives would weaken and inefficiencies would increase.",
        "D": "Because the label 'self-interested' is mainly historical and adds little predictive power; replacing it with altruism would simply make the model unpredictable and untestable."
      },
      "correct_answer": "A",
      "simplification_notes": "Question shortened and clarified for undergraduates; preserved the core contrast between self-interested and altruistic assumptions and their implications for logrolling, pork-barrel spending, coalitions, and rent-seeking. Options rewritten to be concise and plausible.",
      "content_preserved": true,
      "source_article": "Public choice",
      "x": 1.2383577823638916,
      "y": 0.9544733166694641,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Decisions in political systems are made by combined individual choices rather than by an aggregate whole.",
        "Concept 2: Markets (or market-like mechanisms) are used within the political system to allocate political resources and influence outcomes.",
        "Concept 3: The self-interested behavior of voters, politicians, and bureaucrats is a core assumption, subject to empirical testing and used to explain political phenomena."
      ],
      "original_question_hash": "a62690a7"
    },
    {
      "question": "Why does adding a second drug typically make it much less likely that a pathogen or tumor will be resistant to both drugs?",
      "options": {
        "A": "Because resistance to each drug usually requires independent genetic changes, so the probability of resistance to both is roughly the product $p_{1}\\times p_{2}$ of the individual probabilities, which is far smaller than resistance to a single drug.",
        "B": "Because the second drug universally raises cell killing to 100%, leaving no surviving cells that could acquire resistance.",
        "C": "Because two drugs always target the same cellular pathway, so a single mutation will confer resistance to both drugs.",
        "D": "Because using two drugs reduces the total treatment duration, thereby limiting the time available for resistance to develop."
      },
      "correct_answer": "A",
      "simplification_notes": "Question phrasing made concise for undergraduates; preserved technical concept that independent mutations multiply probabilities and expressed it using inline LaTeX $p_{1}\\times p_{2}$. Distractors kept plausible but incorrect. Kept original correct answer.",
      "content_preserved": true,
      "source_article": "Combination therapy",
      "x": 2.077543020248413,
      "y": 1.0951193571090698,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Resistance mitigation as a rationale for combination therapy (why multi-drug regimens reduce the likelihood of simultaneous resistance).",
        "Concept 2: Systems biology and precision medicine as approaches to design effective combinations (how targeting multiple biomarkers and using data-driven methods can yield better outcomes in cancer).",
        "Concept 3: Practical dynamics and trade-offs of combination therapy (how complexity and large possible combination spaces affect feasibility, regulatory approval, and cost-benefit considerations)."
      ],
      "original_question_hash": "ea4885bf"
    },
    {
      "question": "Modern (phylogeny-driven) taxonomy prefers grouping organisms into monophyletic taxa rather than grouping them by superficial similarity alone. Why is monophyly preferred?",
      "options": {
        "A": "Because monophyletic groups guarantee that all members have identical physical appearances.",
        "B": "Because monophyletic groups include all descendants of a common ancestor, preserving actual evolutionary history and allowing more reliable inference about shared traits and relationships.",
        "C": "Because monophyletic groups always occur in equal numbers across different evolutionary lineages, producing balanced classifications.",
        "D": "Because monophyletic groups prevent any overlap between taxa and geographic regions, avoiding biogeographic mixing."
      },
      "correct_answer": "B",
      "simplification_notes": "Removed lengthy background and condensed to a single clear question; explicitly defined the benefit of monophyly and made distractors plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Taxonomy (biology)",
      "x": 3.1570982933044434,
      "y": 1.0603840351104736,
      "level": 2,
      "concepts_tested": [
        "Taxonomy aims to reflect evolutionary relationships in modern classification (phylogeny-driven taxonomy).",
        "The relationship between taxonomy and systematics, including debates about whether nomenclature is part of taxonomy.",
        "Hierarchical organization of classification (domain to species) and the nesting of lower-rank groups within higher-rank groups."
      ],
      "original_question_hash": "430ac951"
    },
    {
      "question": "Why does convergent evidence from independent methods or datasets strengthen a scientific consensus, and why is independence important?",
      "options": {
        "A": "Independent lines of evidence are unlikely to share the same systematic bias or methodological flaw; when several independent approaches converge on the same conclusion, that conclusion is more robust and credible, which strengthens the consensus.",
        "B": "Combining data from the same method increases precision and so produces consensus regardless of whether the lines are truly independent.",
        "C": "Independence is irrelevant: a single large or definitive study can create consensus if it reports a strong effect.",
        "D": "Convergent evidence tends to produce deadlock when methods disagree, so having independent lines actually makes consensus harder to form."
      },
      "correct_answer": "A",
      "simplification_notes": "Shortened and clarified the question; defined 'independent lines' as different methods or datasets; removed historical/philosophical details (Popper/Kuhn) while preserving the core reasoning that independent convergence reduces the likelihood of shared systematic error and increases robustness of conclusions.",
      "content_preserved": true,
      "source_article": "Scientific consensus",
      "x": 1.2484101057052612,
      "y": 0.9959985017776489,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Mechanisms of consensus formation in science (publication, replication, peer review, conferences, debate) and how these processes build shared judgments.",
        "Concept 2: The relationship between convergent evidence and consensus, including how independent lines of evidence that converge support consensus.",
        "Concept 3: Dynamics of consensus over time and competing theories about scientific change (e.g., Popperian falsification vs. Kuhnian paradigm shifts) and the complexity of modeling such change."
      ],
      "original_question_hash": "26dc2766"
    },
    {
      "question": "How does the 'Room to Roam' hypothesis modify the prediction of the competitive exclusion principle about whether multiple species can persist together in a finite habitat?",
      "options": {
        "A": "It implies that coexistence still requires strict niche differentiation and resource specialization for species to persist.",
        "B": "It proposes that persistence can occur because species colonize and occupy newly available, previously unoccupied livable space, reducing direct encounters and allowing multiple species to be maintained even with limited resources.",
        "C": "It claims that competitive exclusion never happens in nature and therefore all species will persist indefinitely regardless of resource overlap.",
        "D": "It argues that predation pressure alone determines biodiversity, making interspecific competition irrelevant to species persistence."
      },
      "correct_answer": "B",
      "simplification_notes": "Removed extended background and examples, tightened wording to contrast the competitive exclusion principle with the Room to Roam idea; emphasized colonization of unoccupied livable space as the key mechanism.",
      "content_preserved": true,
      "source_article": "Competition (biology)",
      "x": 1.6898878812789917,
      "y": 1.0484143495559692,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Resource limitation and fitness outcomes — competition reduces the available resources and lowers the fitness of both competitors, linking resource supply to population dynamics.",
        "Concept 2: Mechanisms of competition — interference, exploitation, and apparent competition as distinct pathways with different directness and effects, illustrated by examples.",
        "Concept 3: Theoretical frameworks for competition outcomes — the competitive exclusion principle and its caveats, plus alternative explanations (e.g., Room to Roam) that shape how competition influences biodiversity and species persistence."
      ],
      "original_question_hash": "5a2a0ec6"
    },
    {
      "question": "Pinar describes curriculum as a 'symbolic representation'. Using this idea, why can a single top-down policy lead to different classroom meanings and practices across schools?",
      "options": {
        "A": "Because policies encode fixed symbols that necessarily produce identical classroom practices in every context.",
        "B": "Because teachers' and students' interpretive processes—shaped by local cultures, histories, institutional constraints, and classroom interactions—reconstruct the policy's symbols into different meanings and practices.",
        "C": "Because the symbolic nature of curriculum prevents teachers from adapting policies, forcing uniform implementation across schools.",
        "D": "Because policy directives automatically and uniformly reorganize school power relations, causing the same structural outcomes and classroom practices everywhere."
      },
      "correct_answer": "B",
      "simplification_notes": "Removed extraneous historical detail and condensed theoretical points. Kept Pinar's 'symbolic representation' framing and emphasized teacher/student interpretation and local context as the explanation. Options made mutually plausible but only B matches the article's interpretation.",
      "content_preserved": true,
      "source_article": "Curriculum theory",
      "x": 1.1964598894119263,
      "y": 0.9693094491958618,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The unit of curriculum as a fundamental, contested concept that shapes theoretical systems and practice (e.g., whether the unit is rational decisions, action processes, language patterns, etc.).",
        "Concept 2: Curriculum as symbolic representation (Pinar’s framing) and its implications for interpreting curricula, meaning-making, and policy.",
        "Concept 3: Historical-mechanistic influences on curriculum (faculty psychology) and their impact on teaching methods, memorization-focused pedagogy, and policy decisions (e.g., NEA committees)."
      ],
      "original_question_hash": "d9f1cfe5"
    },
    {
      "question": "Why does using an ex ante welfare criterion together with a veil-of-ignorance thought experiment promote an impartial assessment of the public interest?",
      "options": {
        "A": "It removes knowledge of one's own identity so the policy is judged by its expected welfare averaged across all possible individuals, reducing bias toward any particular group.",
        "B": "It guarantees the chosen policy will maximize the welfare of the currently worst-off person in every possible outcome.",
        "C": "It forces evaluation to rely only on outcomes observed after implementation rather than on anticipated effects.",
        "D": "It requires only that the total sum of welfare increases, without regard to how benefits are distributed among people."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the stem for clarity and concision, clarified 'ex ante' and 'veil of ignorance' roles, preserved technical terms and made all four options plausible while keeping the original correct answer.",
      "content_preserved": true,
      "source_article": "Public interest",
      "x": 1.2614396810531616,
      "y": 0.9583941698074341,
      "level": 2,
      "concepts_tested": [
        "Ex ante welfare and the veil of ignorance as a method for defining the public interest, aiming for impartial policy evaluation before outcomes are known.",
        "Impartiality as a mechanism for potential consensus in public-interest assessment, connected to Rawls/Harsanyi and the idea that policy should be judged without knowing personal benefits.",
        "Social welfare functions and the aggregation of individual welfare into a social measure, including debates over whether to use ranked states or cardinal utilities and the lack of consensus on the appropriate function."
      ],
      "original_question_hash": "ed5c864d"
    },
    {
      "question": "In quality assurance, a \"shift-left\" approach moves QA activities earlier in development to emphasize defect prevention rather than defect detection. Why does this primarily produce better quality and lower total cost?",
      "options": {
        "A": "Defects found earlier are cheaper to fix and removing them early prevents cascading failures, which reduces rework and delivery delays.",
        "B": "Performing QA earlier guarantees a flawless product and eliminates the need for any later testing.",
        "C": "Shifting left simply moves all quality-related costs into the design phase without reducing downstream defects or work.",
        "D": "Early QA makes downstream teams unnecessary and removes the need for feedback loops during development."
      },
      "correct_answer": "A",
      "simplification_notes": "Condensed the stem to focus on 'shift-left', defect prevention vs detection, and cost-quality outcome; options rewritten to be concise and plausible distractors while preserving the correct rationale about cheaper fixes and preventing cascading failures.",
      "content_preserved": true,
      "source_article": "Quality assurance",
      "x": 1.4518365859985352,
      "y": 1.0070383548736572,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Defect prevention vs defect detection and the shift-left approach (why QA emphasizes preventing defects early in development/production).",
        "Concept 2: The core QA principles \"fit for purpose\" and \"right first time\" (how these criteria shape quality planning and execution).",
        "Concept 3: QA as a feedback-driven system with measurement against standards (how monitoring, comparison to standards, and feedback enable ongoing improvement)."
      ],
      "original_question_hash": "eb8e3aa8"
    },
    {
      "question": "Why can material objects serve as mediators of social relationships across time and space?",
      "options": {
        "A": "Because objects carry fixed, intrinsic meanings that automatically transmit social messages without social interaction.",
        "B": "Because objects accumulate histories of use, ownership, and memory, allowing messages and relationships to be conveyed across time and distance.",
        "C": "Because objects are purely aesthetic artifacts with no role in social communication or relationships.",
        "D": "Because objects force others to accept the owner's intended meaning, eliminating ambiguity in interpersonal communication."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; retained original concepts about objects mediating relationships via histories, memory, and social meaning. Distractors were rephrased to remain plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Material culture",
      "x": 1.2971140146255493,
      "y": 0.943255603313446,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Objects mediate social relationships and communication across time and space; material culture is about the relationships between people and their things.",
        "Concept 2: Meaning and value of objects arise from social/psychological processes (monetary value, sentimental value, endowment effect, memory, identity, social standing).",
        "Concept 3: Gift-giving functions as a vehicle of social obligation and political maneuver within material culture, influencing relationships and social dynamics."
      ],
      "original_question_hash": "7a07259b"
    },
    {
      "question": "How does the sorites paradox show that some vague predicates cannot be given a simple true/false (bivalent) assignment?",
      "options": {
        "A": "Because if we assign each case true or false and accept a tolerance principle (that a single small change doesn't alter truth), repeatedly applying that step from a clearly true instance to a clearly false one yields a contradiction, showing no fixed bivalent assignment can capture the predicate.",
        "B": "Because vagueness necessarily requires more than two truth-values for statements, so one must adopt a multi-valued logic (e.g., fuzzy logic) instead of bivalence.",
        "C": "Because borderline cases reflect multiple distinct meanings of the predicate (ambiguity), so a single truth value cannot apply to all interpretations.",
        "D": "Because legal and normative concepts must be precise, so natural-language vagueness implies that simple true/false assignments are inappropriate for such predicates."
      },
      "correct_answer": "A",
      "simplification_notes": "Question was shortened and rewritten in clear academic language; emphasized 'tolerance' and repeated steps causing contradiction. Distractors were kept plausible (multi-valued logic, ambiguity, legal precision) but the correct logical consequence (A) was preserved.",
      "content_preserved": true,
      "source_article": "Vagueness",
      "x": 1.3298447132110596,
      "y": 1.120988130569458,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Borderline cases and the sorites paradox as central to vagueness, illustrating why some predicates resist bivalent truth valuations.",
        "Concept 2: The distinction between vagueness and ambiguity and its implications for linguistic meaning and interpretation.",
        "Concept 3: The role of vagueness in law and governance (e.g., void for vagueness, handling borderline cases in legal standards) and its relation to formal semantic modeling."
      ],
      "original_question_hash": "1ce13bda"
    },
    {
      "question": "Why would a theory that grants fundamental rights to many nonhuman animals insist those rights cannot be overridden by considerations of aggregate welfare?",
      "options": {
        "A": "Because fundamental rights attribute intrinsic moral worth to individuals, creating inviolable protections (e.g., life, liberty, freedom from torture) that cannot be sacrificed even to increase total welfare.",
        "B": "Because rights are merely instrumental devices for maximizing overall welfare, so they can be overridden when doing so raises aggregate welfare.",
        "C": "Because rights are grounded in species membership, and only humans qualify; hence animals cannot have rights regardless of welfare considerations.",
        "D": "Because legal personhood and rights require complex mental capacities (e.g., advanced self-awareness), so animals lack rights and welfare trade-offs remain decisive."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the stem for clarity (\"overall welfare\" → \"aggregate welfare\"), tightened phrasing, removed extraneous examples, and kept each option concise and plausible while preserving original meaning.",
      "content_preserved": true,
      "source_article": "Animal rights",
      "x": 1.2921438217163086,
      "y": 0.8914391398429871,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Moral status and equal consideration of interests (including the argument from marginal cases as a reasoning mechanism)",
        "Concept 2: Rights vs aggregate welfare vs property (fundamental rights such as life, liberty, freedom from torture that may not be overridden by welfare considerations; anti-speciesism)",
        "Concept 3: Basis of moral value and legal personhood (sentience vs mental complexity; debates over primates/hominids; how law treats animals)"
      ],
      "original_question_hash": "6c831ba3"
    },
    {
      "question": "How do open-source licensing and redistribution rules enable decentralized peer production?",
      "options": {
        "A": "They require that recipients get the source code and the legal right to modify and redistribute their changes, so community improvements remain available to everyone.",
        "B": "They assign exclusive ownership to the original author, letting that author veto forks and restrict downstream access.",
        "C": "They concentrate control in a small governing body that must approve all contributions before they can be shared.",
        "D": "They eliminate copyright entirely so anyone can use the work without any obligations or attribution."
      },
      "correct_answer": "A",
      "simplification_notes": "Question phrasing tightened for clarity; 'licensing and redistribution mechanisms' simplified to 'licensing and redistribution rules'. Options rewritten to be concise and maintain plausibility while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Open source",
      "x": 1.3764030933380127,
      "y": 1.0763158798217773,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Open source as a decentralized, collaborative development model driven by peer production.",
        "Concept 2: Licensing and redistribution as mechanisms that guarantee universal access to designs/code and allow modification/publication of forks.",
        "Concept 3: The historical and institutional context—why the movement emerged to address copyright/licensing issues and how organizations like the Apache Software Foundation support open-source development."
      ],
      "original_question_hash": "d0e9e397"
    },
    {
      "question": "In an axiomatic system, which feature of a proof guarantees that a derived statement is a logical consequence of the axioms?",
      "options": {
        "A": "Every step in the proof is either an axiom or follows from earlier steps by an allowed inference rule, so the whole sequence is a justified deductive chain from the axioms to the conclusion.",
        "B": "Each axiom represents an absolute truth about the external world, so any statement derived from them automatically inherits that worldly truth.",
        "C": "Proofs depend on checking semantic truth across all possible interpretations (models), ensuring the derived statement is true in every model.",
        "D": "The system fixes precise meanings for all symbols, preventing ambiguity so any derivation must be a consequence of those fixed meanings."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording tightened to an undergraduate level; historical/contextual material removed; emphasized the deductive-chain and inference-rule mechanism. Options kept plausible alternatives but preserved the original correct choice.",
      "content_preserved": true,
      "source_article": "Axiomatic system",
      "x": 1.5930230617523193,
      "y": 1.1930724382400513,
      "level": 2,
      "concepts_tested": [
        "The deductive mechanism: axioms plus deductive steps produce theorems; a proof is a sequence of justified steps.",
        "The syntactic nature of formal systems: axioms are placeholders in a formal language, and proofs are carried out within a formal (syntactic) framework such as predicate calculus, minimizing semantic content.",
        "The axiomatic method as foundational practice and its historical/philosophical context: reduction of propositions to axioms, and debates like formalism vs. deductivism and their impact on foundations of mathematics."
      ],
      "original_question_hash": "6146cb77"
    },
    {
      "question": "Barnett and Duvall define power as \"the production, in and through social relations, of effects that shape the capacities of actors to determine their circumstances and fate.\" Why does this definition imply that persuasion is not constitutive of power?",
      "options": {
        "A": "Persuasion changes actors' beliefs or preferences but does not reconfigure the underlying social relations or the structural capacities that generate power effects; therefore it does not produce the relational, enduring changes that constitute power.",
        "B": "Persuasion typically involves implicit coercive threats, so it should be counted as power rather than excluded.",
        "C": "Persuasion directly increases an actor's material resources, and material resources are the sole determinant of power in this framework.",
        "D": "Persuasion is a private cognitive process unrelated to social structures, and thus cannot be analyzed as a form of power produced through social relations."
      },
      "correct_answer": "A",
      "simplification_notes": "Kept the Barnett & Duvall quote and core idea; clarified that power is about effects produced through social relations and that persuasion alters beliefs but not structural relations. Reduced historical framing and extraneous examples; made distractors plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Power (international relations)",
      "x": 1.1892515420913696,
      "y": 0.9295883774757385,
      "level": 2,
      "concepts_tested": [
        "Material power vs. relational power and their interplay in shaping outcomes",
        "Power as production in social relations (Barnett & Duvall) and why persuasion is not constitutive of power",
        "The taxonomy of political power (power as a goal, influence/control, victory/security, resources, status) and implications for state behavior"
      ],
      "original_question_hash": "78724e33"
    },
    {
      "question": "How does daily light synchronize (entrain) an endogenous circadian clock that would otherwise run with a different intrinsic period, so that the organism's rhythms align with the 24-hour day?",
      "options": {
        "A": "Light shifts the phase of the internal circadian oscillator by modifying the timing of clock gene/protein feedback loops and related signaling, causing the clock to adjust its cycle toward a 24-hour rhythm.",
        "B": "Light permanently alters the clock's biochemical rate constants so the intrinsic period becomes exactly 24 hours, fixing the clock at that period.",
        "C": "Light only turns external behaviors on and off without changing the internal timing mechanism, so any apparent entrainment is purely behavioral.",
        "D": "Light acts only on peripheral tissue processes while leaving the central pacemaker unchanged; behavior is aligned indirectly through these peripheral effects."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording made more concise and explicit about 'synchronize' and 'entrain'; option A clarified to mention clock gene/protein feedback loops; distractors kept plausible but concise.",
      "content_preserved": true,
      "source_article": "Chronobiology",
      "x": 2.0535500049591064,
      "y": 1.1507866382598877,
      "level": 2,
      "concepts_tested": [
        "Endogenous circadian clocks generate ~24-hour rhythms that regulate behavior and physiology.",
        "Entrainment: external cues (e.g., light) modulate and synchronize internal clocks, linking environment to timing.",
        "Multiscale rhythmic organization: circadian, infradian, ultradian, tidal, and lunar rhythms across diverse organisms, illustrating broader temporal regulation in biology."
      ],
      "original_question_hash": "660f17f6"
    },
    {
      "question": "How does an endogenous circadian clock produce an approximately 24-hour rhythm and remain synchronized to the 24-hour day, given that most biochemical reactions act on much shorter timescales?",
      "options": {
        "A": "By a simple positive feedback loop that continuously increases clock gene expression until a 24-hour cycle emerges and then stops without requiring external timing cues.",
        "B": "By a delayed negative transcription–translation feedback loop in which clock gene products inhibit their own transcription, combined with post-translational modifications that set the pace; light (and other cues) then shifts the clock phase to keep it aligned with the 24-hour day.",
        "C": "By an intrinsic 24-hour metabolic timer that functions independently of gene transcription/translation and does not respond to external signals.",
        "D": "By behavior and physiology fixed by ecological demands (diurnal/nocturnal patterns) with no internal oscillator generating the timing."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; preserved technical term 'delayed negative transcription–translation feedback' and added explicit mention of post-translational adjustments and light entrainment; options made succinct and all plausible.",
      "content_preserved": true,
      "source_article": "Chronobiology",
      "x": 2.035095691680908,
      "y": 1.1526248455047607,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Endogenous circadian clocks generate roughly 24-hour rhythms that regulate timing of behavior and physiology.",
        "Concept 2: Entrainment/ regulation by exogenous cues (e.g., light) shows interaction between internal clocks and external signals.",
        "Concept 3: Diversity of biological rhythms (circadian, infradian, ultradian, tidal, lunar) and their different regulatory contexts and ecological/behavioral roles."
      ],
      "original_question_hash": "3da4386b"
    },
    {
      "question": "How can a keystone predator have a disproportionately large effect on community structure despite being relatively rare?",
      "options": {
        "A": "By directly suppressing most species equally through generalized, nonselective predation.",
        "B": "By producing cascading indirect effects (trophic cascades) via targeted interactions—for example, by preying on a dominant competitor—thereby shifting the balance among many other species.",
        "C": "By increasing primary production through nutrient recycling and thus directly boosting plant biomass.",
        "D": "By keeping abiotic conditions (e.g., temperature, hydrology) uniformly constant across the habitat."
      },
      "correct_answer": "B",
      "simplification_notes": "Rephrased the question for clarity and concision; clarified the mechanism in the correct option as trophic cascades and indirect effects. Distractors were made plausible and concise (generalized predation, nutrient recycling, abiotic stabilization).",
      "content_preserved": true,
      "source_article": "Keystone species",
      "x": 1.6609508991241455,
      "y": 0.9694290161132812,
      "level": 2,
      "concepts_tested": [
        "Keystone species exert a disproportionately large effect on ecosystem structure relative to their abundance.",
        "Removal or decline of a keystone species causes cascading changes that alter community composition and ecosystem function.",
        "Interspecific interactions (e.g., predation, competition) are the mechanisms by which keystone species regulate other species, shaping biodiversity and community balance."
      ],
      "original_question_hash": "7d7b43d6"
    },
    {
      "question": "How does classifying civil and political rights as negative (first‑generation) rights affect the way a government must realize them, and what does this imply about the state's resource commitments and constraints?",
      "options": {
        "A": "Because negative rights require governments to finance extensive social programs and redistribution to secure equal outcomes, their realization depends heavily on available budgets.",
        "B": "Because negative rights stress non‑interference and procedural protections, realization depends more on rule‑of‑law institutions and legal guarantees than on immediate material provisioning, allowing states to guarantee them without large direct resource outlays.",
        "C": "Because negative rights rely mainly on international enforcement, responsibility for realization shifts to supranational bodies, reducing domestic budgetary burdens.",
        "D": "Because negative rights are managed chiefly through civil society self‑regulation, the state has reduced formal obligations and therefore faces lower resource demands to realize them."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the original multipart question into a single clear sentence; used precise terms (\"negative rights\", \"non‑interference\", \"rule‑of‑law\"); rephrased all options to be plausible distractors while preserving the original correct answer and core concepts.",
      "content_preserved": true,
      "source_article": "Civil and political rights",
      "x": 1.1215425729751587,
      "y": 0.8465885519981384,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Civil and political rights protect individuals from government/other entities and enable participation in society, implemented through legal norms and due process, shaping state-citizen relationships.",
        "Concept 2: The distinction between negative vs. positive rights and the classification of civil and political rights as “first-generation” rights, including implications for how they are realized or limited.",
        "Concept 3: The placement of civil and political rights within the broader human rights framework (e.g., as core content of the UDHR) and their historical development and influence on constitutional law."
      ],
      "original_question_hash": "51ac89b6"
    },
    {
      "question": "In a regulatory system that scales testing by device risk, why does the required evidentiary burden increase as a device's risk level rises?",
      "options": {
        "A": "Because higher-risk devices can cause greater harm, regulators require stronger, more comprehensive evidence that benefits outweigh risks, including robust safety and efficacy data and mechanisms to detect and mitigate rare adverse events.",
        "B": "Because higher-risk devices have more complex manufacturing, regulators therefore demand the same basic evidence as for low-risk devices to keep approval procedures uniform.",
        "C": "Because regulators prefer to reduce premarket requirements for high-risk devices and rely mainly on post-market surveillance to discover safety problems.",
        "D": "Because higher-risk devices are used by larger populations, regulators relax testing requirements to speed access to the market."
      },
      "correct_answer": "A",
      "simplification_notes": "Rewrote the stem in clearer, concise academic language; removed historical/regional detail and focused on the core regulatory principle (risk–benefit and evidence requirement). Options were made plausible but only A preserves the correct rationale.",
      "content_preserved": true,
      "source_article": "Medical device",
      "x": 1.3701649904251099,
      "y": 0.9075825810432434,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Risk-based testing and the risk-benefit trade-off in regulation (as device risk increases, testing requirements and required patient benefits increase).",
        "Concept 2: Global/regional regulatory landscape and harmonization challenges (different bodies and definitions prevent easy global harmonization, affecting how devices are marketed).",
        "Concept 3: The role of device design within biomedical engineering (design is a major component of the field and influences safety, efficacy, and application)."
      ],
      "original_question_hash": "de287601"
    },
    {
      "question": "Form-based zoning regulates the physical form of buildings (height, massing, setbacks, density) rather than prescribing exact land uses. Why does this approach usually create a more predictable urban streetscape while still permitting a mix of uses?",
      "options": {
        "A": "It requires a single use per parcel, so streetscape elements remain uniform but future functional changes are heavily restricted.",
        "B": "It anchors development to physical form standards, producing a consistent street character while allowing different uses to occupy buildings as long as the form rules are met.",
        "C": "It forbids any later modifications to building shapes after approval, making the code highly rigid and preventing adaptive reuse.",
        "D": "It emphasizes aesthetic materials and ornamentation over spatial rules, using visual design to control appearance and indirectly limit uses."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; retained technical terms (massing, setbacks, density); options rephrased to be concise and plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Zoning",
      "x": 1.3720760345458984,
      "y": 0.8449647426605225,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3"
      ],
      "original_question_hash": "273dfdb5"
    },
    {
      "question": "Why do riskier investments typically offer higher expected returns than safer ones?",
      "options": {
        "A": "Because higher risk automatically guarantees a higher actual payoff regardless of market conditions.",
        "B": "Because risk-averse investors demand compensation for bearing uncertainty, so riskier assets carry a risk premium that raises their expected returns.",
        "C": "Because riskier assets always have higher liquidity, and liquidity alone determines the return.",
        "D": "Because returns depend solely on a company’s realized performance, so perceived risk does not affect what investors expect to earn."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the stem to focus on the risk–return trade-off; defined the correct mechanism succinctly as a risk premium demanded by risk-averse investors; made all distractors plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Investment",
      "x": 1.3396589756011963,
      "y": 0.9054341316223145,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Risk-return trade-off — higher risk is generally associated with higher potential returns and higher potential losses.",
        "Concept 2: Diversification as a mechanism to reduce overall portfolio risk.",
        "Concept 3: Composition of returns — returns arise from capital gains (realized/unrealized), income (dividends, interest, rent), and other factors (currency gains/losses)."
      ],
      "original_question_hash": "acbaf3e1"
    },
    {
      "question": "Why does discounting a project's expected cash flows at the firm's cost of capital and accepting projects with $NPV>0$ maximize shareholder value?",
      "options": {
        "A": "Because this procedure effectively ranks projects by their payback period, favoring those that recover investment fastest, which directly increases the firm's market value.",
        "B": "Because discounting future cash inflows at the firm's cost of capital converts them to present value, capturing both the time value of money and the required return for risk; a project with $NPV>0$ yields more than the capital providers require and therefore adds value to the firm.",
        "C": "Because it relies on accounting profits and accounting-based rates of return, which better represent the earnings available to shareholders than cash-based measures.",
        "D": "Because the firm's cost of capital is the risk-free rate, so any project with $NPV>0$ produces riskless profit that must increase firm value."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem for clarity, introduced $NPV>0$ notation, tightened language to emphasize time value of money, required return for risk, and cash flows vs accounting profits; maintained original concepts and correct choice.",
      "content_preserved": true,
      "source_article": "Capital budgeting",
      "x": 1.3610893487930298,
      "y": 0.9131689667701721,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Time value of money and the NPV decision rule as a value-maximizing criterion (discounting cash flows at the cost of capital; accepting positive NPV projects to increase firm value)",
        "Concept 2: Incremental cash flows and the use of cash-based measures over accounting profits (why accounting measures like accounting rate of return are considered improper for investment decisions)",
        "Concept 3: Real options/managerial flexibility in capital budgeting (the notion that investments have optionality and can be valued using real options techniques)"
      ],
      "original_question_hash": "62fdeda5"
    },
    {
      "question": "According to Stuart Hall's encoding/decoding model in reception theory, why do readers from different cultural backgrounds often produce different interpretations of the same text?",
      "options": {
        "A": "Because readers who share a cultural background tend to interpret texts similarly, so differing backgrounds produce different interpretations.",
        "B": "Because the text itself encodes several independent messages that simply replace one another for different readers.",
        "C": "Because producers encode meanings into a text, and readers decode those meanings through their own cultural histories and life experiences, producing negotiated or divergent readings.",
        "D": "Because authorial intent alone determines a single correct meaning, so readers should reconstruct that same meaning regardless of background."
      },
      "correct_answer": "C",
      "simplification_notes": "Question rephrased to explicitly reference Hall's encoding/decoding model and clarify the roles of encoding and decoding. Wording tightened; technical terms (encode/decode, negotiated) retained. Distractor options made concise and plausible.",
      "content_preserved": true,
      "source_article": "Reception theory",
      "x": 1.0836237668991089,
      "y": 1.0313482284545898,
      "level": 2,
      "concepts_tested": [
        "Meaning is generated by the interaction between text and reader, not inherent in the text itself.",
        "Encoding/Decoding model and audience negotiation: producers encode meanings; diverse audiences decode them differently, leading to multiple readings.",
        "Aberrant decoding and cultural influence: readers’ interpretations may diverge from authorial intent due to their cultural/background context."
      ],
      "original_question_hash": "a0c48b84"
    },
    {
      "question": "How does treating public participation as a form of empowerment strengthen democratic governance and increase legitimacy?",
      "options": {
        "A": "By speeding decision-making through restricting input to a small group of experts, reducing delays from broad consultation.",
        "B": "By ensuring policies reflect the preferences of those in power, even if participatory processes are only nominal.",
        "C": "By giving affected people a meaningful voice and stake, integrating diverse knowledge and values into decisions, thereby increasing legitimacy and public trust.",
        "D": "By transferring decision authority to external private actors, which can bypass public accountability while appearing participatory."
      },
      "correct_answer": "C",
      "simplification_notes": "Reworded the original question into concise academic language appropriate for undergraduates; preserved the core concept that empowerment through participation enhances legitimacy and trust. Distractor options were kept plausible but clarified.",
      "content_preserved": true,
      "source_article": "Public participation",
      "x": 1.263586401939392,
      "y": 0.9529864192008972,
      "level": 2,
      "concepts_tested": [
        "Public participation as empowerment and a pillar of democratic governance (how involvement of those affected influences decision-making and legitimacy)",
        "The relationship between public participation and governance outcomes such as trust, legitimacy, and social justice (how direct citizen involvement can address governance legitimacy and public trust)",
        "Rights-based/human-centric framing of participation (how recognizing the right to participate shapes inclusive processes and participation itself)"
      ],
      "original_question_hash": "165249fd"
    },
    {
      "question": "How do networks of trust and reciprocity make the collective provision of public goods more likely than when individuals act alone?",
      "options": {
        "A": "Because formal contracts and external enforcement are always sufficient to secure contributions, making social networks unnecessary.",
        "B": "Because trust and reciprocity lower monitoring and enforcement costs and enable conditional cooperation, aligning individual incentives to contribute to the public good.",
        "C": "Because such networks necessarily centralize information and decision-making, removing information asymmetries and eliminating coordination problems.",
        "D": "Because shared norms and group identity completely eliminate private incentives to free-ride, guaranteeing that everyone will cooperate."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; emphasized the causal mechanism (monitoring costs, conditional cooperation). Distractors made plausible and aligned with themes from the article but remain incorrect. Preserved original correct choice.",
      "content_preserved": true,
      "source_article": "Social capital",
      "x": 1.2561315298080444,
      "y": 0.9844259023666382,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Social capital as a network of relationships that enables cooperation and the production of public goods through trust and reciprocity.",
        "Concept 2: The role of shared norms, identity, and values in sustaining coordination and collective action within groups.",
        "Concept 3: The link between social capital and organizational outcomes (e.g., performance, entrepreneurship, alliances) along with recognition of measurement challenges."
      ],
      "original_question_hash": "7707a36a"
    },
    {
      "question": "Why should metadata, classification schemes, and user interfaces in library and information science be designed to match the actual tasks and goals of different user groups?",
      "options": {
        "A": "Because metadata and classification are fixed once created and must be applied uniformly to all users without adaptation.",
        "B": "Because different user groups carry out different search tasks and use different mental models; aligning organization and interfaces with those tasks improves retrieval effectiveness (e.g., recall and precision) and user satisfaction.",
        "C": "Because technological constraints and system capabilities should dictate how resources are described and presented, regardless of user needs.",
        "D": "Because only professional librarians know how to categorize resources correctly, so interfaces should assume expert search behaviour rather than novice users."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the original wording to a clear, undergraduate-level question focused on user-centered information organization; clarified retrieval metrics (recall, precision) and preserved the rationale for alignment with user tasks. Distractors were kept plausible and related to common misconceptions.",
      "content_preserved": true,
      "source_article": "Library and information science",
      "x": 1.117263913154602,
      "y": 0.9100567102432251,
      "level": 2,
      "concepts_tested": [
        "Interdisciplinary foundations: LIS combines management, information technology, education, and other areas to govern the creation, organization, preservation, and dissemination of information resources.",
        "Information organization and user-centered access: How classification, organization, and technology enable retrieval and use of information for different user groups, reflecting the mechanisms by which people interact with systems.",
        "Relationships and distinctions among subfields: The connection between library science and information science, distinctions with library philosophy, and how aims (techniques vs. justification/ends) shape practice."
      ],
      "original_question_hash": "b5cfb831"
    },
    {
      "question": "In data publishing within scholarly communication, why are standardized metadata, persistent identifiers (e.g., DOIs), and clear licensing attached to datasets in a repository essential for long-term reuse and citation?",
      "options": {
        "A": "They guarantee the dataset will remain permanently unchanged and prevent any form of versioning or update.",
        "B": "They enable automated discovery, precise attribution via data citations, lawful reuse under known terms, and reliable preservation, supporting replication and new research across contexts.",
        "C": "They allow the data producer to restrict access and retain exclusive control over who can view or use the data.",
        "D": "They ensure the data are stored only on a single local system so that external access and redistribution are avoided."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the question into a concise undergraduate-level sentence, clarified examples (DOIs), and tightened options to be plausible but distinct; preserved technical terms (metadata, persistent identifiers, licensing) and the tested concept.",
      "content_preserved": true,
      "source_article": "Scholarly communication",
      "x": 1.220964789390564,
      "y": 1.0038976669311523,
      "level": 2,
      "concepts_tested": [
        "The scholarly communication lifecycle and the role of quality assessment (how creation leads to publication/dissemination/preservation with peer review as a mechanism for quality control)",
        "Data publishing as a dedicated mechanism within scholarly communication (how data become accessible, reusable, citable; the infrastructure and incentives that support standardized data publishing)",
        "Influence of open access, repositories, and rights/economics on dissemination and preservation (how publishing models and policy factors affect access, preservation, and future use)"
      ],
      "original_question_hash": "a8310a36"
    },
    {
      "question": "Why is a multimethod research approach valuable in building science when studying occupant comfort and indoor environmental quality (IEQ)?",
      "options": {
        "A": "Because extensive physical measurements alone can fully explain individual differences in comfort without any subjective data.",
        "B": "Because combining objective data (measurements, experiments, simulations) with social-science methods (interviews, surveys, observations) captures both environmental conditions and occupants' subjective responses, explaining why identical conditions can produce different comfort outcomes.",
        "C": "Because interviews and surveys alone can predict all building performance and occupant responses, making sensor measurements unnecessary.",
        "D": "Because simulations by themselves can perfectly predict long-term comfort and IEQ trends without validation from field measurements or occupant feedback."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened and technical components (measurements, simulations, interviews/surveys) were explicitly named; language adjusted for undergraduate readers while preserving core concepts and the correct choice.",
      "content_preserved": true,
      "source_article": "Building science",
      "x": 1.4863215684890747,
      "y": 0.9206429719924927,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The interplay between indoor environmental factors (thermal, acoustic, light, air quality) and occupant comfort/health, including how comfort varies among individuals and depends on both objective measurements and subjective responses.",
        "Concept 2: The value of a multimethod research approach that blends physical measurements, experiments, simulations with social science methods (interviews, surveys) to understand both objective conditions and subjective occupant experiences.",
        "Concept 3: The role of design decisions (building envelope, mechanical/electrical systems) and standards/guidelines in controlling indoor environmental quality and building performance, often aided by computational modeling to optimize outcomes."
      ],
      "original_question_hash": "41c65a75"
    },
    {
      "question": "Organizational architecture can be seen as interacting building blocks (formal structure, informal culture, processes, strategy, and human resources). Why does adopting a new strategy usually require coordinated changes in several blocks instead of changing only the strategy?",
      "options": {
        "A": "Because formal structure fully determines behavior; after a strategy change the existing culture and processes become irrelevant.",
        "B": "Because a change in strategy automatically reconfigures processes and reporting lines, so no additional adjustments are necessary.",
        "C": "Because the blocks influence one another: a new strategy changes incentives, information flows, and capability needs, so structure, processes, and HR must be adjusted to support and sustain the new behavior.",
        "D": "Because culture is the sole driver of organizational outcomes; if culture shifts to endorse the new strategy, all other blocks will realign without further changes."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording was shortened and focused on the interacting building-block model; language clarified that strategy interacts with incentives, information flows, and capabilities. Distractors were kept plausible but made concise.",
      "content_preserved": true,
      "source_article": "Organizational architecture",
      "x": 1.4014722108840942,
      "y": 1.0519828796386719,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Alignment and internal consistency among strategy, organizational architecture (structure and culture), processes, and the external environment to realize core qualities and deliver value.",
        "Concept 2: The organizational architecture as a set of interacting building blocks (formal structure, informal culture, processes, strategy, HR) that together shape the organizational space and outcomes.",
        "Concept 3: A non-linear, milestone-based design process (business case and discovery; strategic grouping; integration) as a mechanism to translate strategy into changes in structure, processes, and cross-boundary collaboration."
      ],
      "original_question_hash": "58763f90"
    },
    {
      "question": "Why does the performance of predictive analytics depend on the quality of historical data and on the plausibility of assumptions about relationships among variables (e.g., stability over time, monotonic effects, independence)?",
      "options": {
        "A": "Because predictive models learn patterns from historical data and estimate probabilities (e.g., $P(Y\\mid X)$); if the data are noisy, biased, or incomplete the learned patterns will reflect those flaws and mislead predictions, and if assumed relationships (such as stationarity, monotonicity, or conditional independence) do not hold the model may fail to generalize to new cases.",
        "B": "Because predictive analytics is inherently robust: with enough data quantity the method overcomes any data quality issues, and assumptions about relationships between variables have negligible effect on outcomes.",
        "C": "Because model performance is driven primarily by the number of features included; even if data are low quality or assumptions are questionable, adding many features will ensure good predictive accuracy.",
        "D": "Because predictive analytics is mainly intended to reveal causal mechanisms, so high data quality guarantees that causal effects are identified without error and assumptions about relationships are secondary."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified; explicitly noted models estimate predictive probabilities $P(Y\\mid X)$; examples of assumptions reduced to concise technical terms (stationarity, monotonicity, conditional independence); distractors rephrased to be plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Predictive analytics",
      "x": 1.4184613227844238,
      "y": 1.0209593772888184,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Predictive analytics relies on identifying and exploiting relationships between explanatory variables and outcomes in historical data to forecast unknown future events.",
        "Concept 2: The technique outputs probabilistic predictive scores that guide decisions across large numbers of individuals or units.",
        "Concept 3: It employs a spectrum of methods (regression, machine learning, time-series models such as ARIMA) and its effectiveness depends on data quality and underlying assumptions."
      ],
      "original_question_hash": "138e02ae"
    },
    {
      "question": "Public historians apply professional standards and methods when presenting scholarly history to general audiences. Which statement best explains how those standards help maintain credibility while remaining accessible to diverse publics?",
      "options": {
        "A": "They require transparent methods and clear documentation so the public can assess and verify historical claims, while allowing engaging interpretation.",
        "B": "They act as strict gatekeeping that excludes non-professionals from interpreting or contributing to historical narratives.",
        "C": "They prioritize scholarly jargon and technical language to protect academic authority, limiting public understanding.",
        "D": "They favor rapid dissemination and simplified content even if it risks accuracy, to reach the widest possible audience."
      },
      "correct_answer": "A",
      "simplification_notes": "Rephrased the original question for clarity and concision; clarified 'standards function' as supporting credibility and accessibility; retained the original distractors and ensured all options are plausible.",
      "content_preserved": true,
      "source_article": "Public history",
      "x": 1.1713643074035645,
      "y": 0.9194557666778564,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Public history aims to make historical knowledge useful and accessible to the general public, not solely to academics.",
        "Concept 2: It relies on professional standards and methodologies to bring scholarly history into public settings (museums, archives, media, government) and to ensure credible practice.",
        "Concept 3: The field embodies ongoing discussions about authority, inclusivity, and the relationship between historians and diverse publics across different national contexts."
      ],
      "original_question_hash": "f711984a"
    },
    {
      "question": "How does integrating diverse disciplines, ways of knowing, skills, and stakeholder values across boundaries enable holistic solutions to complex societal problems?",
      "options": {
        "A": "By imposing standardized methods on all participants so variation is minimized and outcomes become uniform",
        "B": "By blending multiple epistemologies, disciplinary skills, and stakeholder perspectives to jointly frame problems, question assumptions, and co‑create adaptive strategies that integrate system knowledge, target knowledge, and transformation knowledge for interdependent social, ecological, and institutional dimensions",
        "C": "By deferring decision-making to a single dominant discipline so goals and methods remain coherent within one worldview",
        "D": "By keeping insights from different fields isolated so evaluation and implementation remain simple and narrowly contained"
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the question in clear academic language suitable for undergraduates; retained the original concepts (co-creation with stakeholders, boundary-spanning integration, and the three knowledge types) and made distractors plausible alternatives.",
      "content_preserved": true,
      "source_article": "Transdisciplinarity",
      "x": 1.2453117370605469,
      "y": 0.9755812287330627,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Co-creation and inclusion of societal partners in knowledge production, bridging academia and community to address real-world problems.",
        "Concept 2: Boundary-spanning, integration of diverse approaches, disciplines, and values to form holistic solutions for complex societal challenges.",
        "Concept 3: Knowledge typology (system knowledge, target knowledge, transformation knowledge) guiding problem framing, goals, and transformative action."
      ],
      "original_question_hash": "0abeeb62"
    },
    {
      "question": "In irregular warfare, how do indirect and asymmetric methods translate into strategic impact compared with direct, symmetric battles?",
      "options": {
        "A": "They distribute costs and risks across multiple domains (military, political, economic, social) and signal weakness to the adversary’s population and elites, eroding legitimacy, will, and capability without relying on a single decisive military victory.",
        "B": "They concentrate all resources into a single decisive battle to rapidly defeat the opponent’s main army and end the conflict quickly.",
        "C": "They avoid any engagement with civilian populations altogether to prevent political backlash and keep the conflict purely military.",
        "D": "They guarantee immediate victory by applying superior conventional firepower alone, making prolonged political efforts unnecessary."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; maintained emphasis on multi-domain effects, legitimacy, and protracted nature; distractor options made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Irregular warfare",
      "x": 1.2639130353927612,
      "y": 0.7480185627937317,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The objective of irregular warfare is to gain legitimacy and influence over relevant populations, shaping political and social dynamics rather than only achieving military victory.",
        "Concept 2: Indirect and asymmetric warfare are primary mechanisms used to erode an adversary’s power, influence, and will, employing a broad spectrum of capabilities.",
        "Concept 3: It is a protracted, multi-domain struggle in which control of institutions and infrastructure, along with sustained strategic signaling, determines outcomes and tests the resolve of states and partners."
      ],
      "original_question_hash": "8de908e4"
    },
    {
      "question": "How does the four-principle (principlism) approach function as a decision-making framework in applied bioethics?",
      "options": {
        "A": "It enforces a fixed ranking in which autonomy always overrides non-maleficence, beneficence, and justice in every case.",
        "B": "It treats autonomy, non-maleficence, beneficence, and justice as prima facie (generally binding but defeasible) principles that must be weighed and justified against one another in context when they conflict.",
        "C": "It abandons principled balancing and substitutes a simple utilitarian calculation that totals overall happiness to decide the morally right act.",
        "D": "It replaces multiple principles with a single, universal rule grounded in justice that must be followed regardless of circumstances."
      },
      "correct_answer": "B",
      "simplification_notes": "Question language was shortened and clarified; the term 'principlism' and 'prima facie' were retained but briefly explained; options kept plausible and faithful to the article while wording made more direct.",
      "content_preserved": true,
      "source_article": "Applied ethics",
      "x": 1.1947567462921143,
      "y": 1.0125995874404907,
      "level": 2,
      "concepts_tested": [
        "The four-principle approach (autonomy, non-maleficence, beneficence, justice) as a decision-making framework in bioethics",
        "The relationship of applied ethics to normative ethics and meta-ethics (interdisciplinary use of normative theories in practical contexts)",
        "The contrast and interaction between consequentialist (e.g., utilitarian) and deontological foundations in evaluating moral actions within applied ethics"
      ],
      "original_question_hash": "7a9371b5"
    },
    {
      "question": "Horizontal concentration means a few companies own many outlets in the same media market. Why does this concentration tend to reduce the diversity of viewpoints, and what mechanism best explains that effect?",
      "options": {
        "A": "By increasing the fixed cost of original content production, firms encourage each outlet to pursue different niche viewpoints to attract distinct audiences, which incidentally can reduce overlap between outlets.",
        "B": "It reduces the number of independent editorial voices; the common owner can standardize or align content across outlets to reflect its strategic or political preferences, decreasing exposure to alternative viewpoints.",
        "C": "It increases the number of outlets competing for audience attention, which naturally leads to a wider range of editorial stances across the market.",
        "D": "It triggers regulatory intervention that enforces diversity requirements, making editorial independence largely immune to ownership concentration."
      },
      "correct_answer": "B",
      "simplification_notes": "Defined 'horizontal concentration' and used clearer language; kept the original mechanisms but rephrased options so each is concise and plausible at undergraduate level.",
      "content_preserved": true,
      "source_article": "Concentration of media ownership",
      "x": 1.2891845703125,
      "y": 0.9592753052711487,
      "level": 2,
      "concepts_tested": [
        "Ownership concentration and media pluralism/diversity: how horizontal/vertical concentration can reduce or threaten diversity of viewpoints.",
        "Mergers/oligopoly as mechanisms: how mergers create market power, entry barriers, and potential bias in content.",
        "Media integrity and editorial independence as vulnerabilities to ownership concentration (and the link to political influence and net neutrality)."
      ],
      "original_question_hash": "dedfa2f1"
    },
    {
      "question": "How does audience participation distinguish entertainment from private recreation, i.e., what separates passive reception (watching) from active engagement (playing or interacting) when deciding that an activity is entertainment?",
      "options": {
        "A": "It does not; whether something is entertainment is determined solely by the medium or format (for example, a film or a game), not by how the audience participates.",
        "B": "Only emotional effect matters: an activity becomes entertainment when the audience is emotionally moved, regardless of whether they are actively involved or merely watching.",
        "C": "By turning private recreation into a shared, interactive event: attention and engagement become mutual, active participation can change the roles of performer and spectator, and that alteration of structure and experience is what makes it entertainment.",
        "D": "An activity counts as entertainment only if the audience entertains themselves (for example by producing laughter); without the audience generating amusement, it is not entertainment."
      },
      "correct_answer": "C",
      "simplification_notes": "Clarified question to contrast passive reception vs active engagement, emphasized that audience interaction can transform private recreation into shared entertainment; options rewritten to be concise and plausible while preserving the original correct choice (C).",
      "content_preserved": true,
      "source_article": "Entertainment",
      "x": 1.3270094394683838,
      "y": 0.9869691729545593,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The audience as a central determinant of what counts as entertainment, including passive vs. active participation.",
        "Concept 2: Evolution and adaptation of entertainment forms over time due to cultural, technological, and fashion changes, enabling longevity and cross-media remix.",
        "Concept 3: The dual aims of entertainment (amusement and insight) and the ethical/relational variability of what is entertaining across groups and contexts."
      ],
      "original_question_hash": "fa8603bf"
    },
    {
      "question": "How does cultural hegemony preserve the social and political status quo by shaping norms and worldviews, and why does that produce durable power without continuous coercion?",
      "options": {
        "A": "By presenting the ruling group's worldview as natural, inevitable, and beneficial, it secures popular consent and voluntary compliance, reducing resistance so power persists even when formal rules or institutions change.",
        "B": "By relying on a permanent security apparatus (police, military) to punish dissent, maintaining control primarily through fear and coercion.",
        "C": "By repeatedly renegotiating treaties and formal agreements to extract resources from subordinate groups through external domination and legal imposition.",
        "D": "By randomly selecting leaders with no coherent cultural or institutional strategy, so control persists by chance rather than design."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording tightened for clarity and concision; removed historical examples and jargon; preserved the core mechanism (shaping norms/worldview to secure consent) and contrasted it with coercive, imperial, and random alternatives.",
      "content_preserved": true,
      "source_article": "Hegemony",
      "x": 1.1733380556106567,
      "y": 0.951704740524292,
      "level": 2,
      "concepts_tested": [
        "Cultural Hegemony as a mechanism of power: the ruling class uses consent and shaping of social norms/worldview to maintain the status quo.",
        "Distinction between hegemony and empire: hegemony involves external predominance with limited or no internal control, differing from direct imperial rule.",
        "Hegemonic order’s effect on subordinate states: it dictates internal politics or installs governments, shaping societal character and political arrangements."
      ],
      "original_question_hash": "9fb68808"
    },
    {
      "question": "Which statement best explains why clinicians individualize pharmacotherapy using a patient’s genetics and liver/kidney function?",
      "options": {
        "A": "Individualization is achieved only by scaling dose to body weight, assuming all patients metabolize drugs the same way.",
        "B": "Inherited variation in drug‑metabolizing enzymes and differences in liver or kidney function change a drug’s pharmacokinetics and systemic exposure; clinicians therefore adjust dose or select a different drug to keep exposure within a therapeutic window that balances efficacy and toxicity.",
        "C": "Individualization simply means choosing the drug that is most commonly used by other clinicians, regardless of the patient’s genetic or organ‑function differences.",
        "D": "Organ function affects only the timing of a drug’s peak effect, not total systemic exposure, so the dose amount does not need to be changed."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduate readers; retained technical terms (pharmacokinetics, systemic exposure, therapeutic window). Options rewritten to be concise and plausible; core concept (genetics and organ function alter exposure, so clinicians tailor drug/dose) preserved.",
      "content_preserved": true,
      "source_article": "Pharmacotherapy",
      "x": 1.457688570022583,
      "y": 0.9944278001785278,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The aims of pharmacotherapy and the role of guidelines in guiding treatment decisions (symptomatic relief, treating underlying conditions, prophylaxis; decisions guided by evidence-based guidelines).",
        "Concept 2: Personalization/precision medicine in pharmacotherapy (tailoring drug choice and dosing to individual patient factors such as genetics, liver/kidney function).",
        "Concept 3: Medication adherence and the pharmacist’s role as determinants of pharmacotherapy effectiveness (importance of adherence and considering compliance)."
      ],
      "original_question_hash": "1931297c"
    },
    {
      "question": "When moving a gene from species X into species Y, the inserted gene is usually paired with a promoter that works in species Y. Why must the promoter be compatible with species Y to produce the desired trait?",
      "options": {
        "A": "Because promoters determine the exact location in the host genome where the inserted DNA will integrate.",
        "B": "Because promoters control transcription of the inserted gene in species Y, determining whether the gene is expressed and at what level.",
        "C": "Because promoters encode the protein product produced by the inserted gene.",
        "D": "Because promoters catalyze replication of the plasmid or vector that carries the gene."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and focused on promoter compatibility and its role in expression; kept four plausible distractors and the original correct answer.",
      "content_preserved": true,
      "source_article": "Biotechnology",
      "x": 1.9013535976409912,
      "y": 1.0441044569015503,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Biotechnology as the integration of natural sciences and engineering to harness living systems for products and services.",
        "Concept 2: Genetic engineering as a mechanism to modify organisms by inserting genes to create new traits or modify existing ones.",
        "Concept 3: Practical techniques (tissue culture, fermentation) that enable manipulation and production, connecting fundamental biology to real-world applications."
      ],
      "original_question_hash": "ee00e790"
    },
    {
      "question": "According to a rule-based definition of musical genre (for example, Fabbri's), why can two pieces that have the same musical form still be placed in different genres?",
      "options": {
        "A": "Because genre is determined solely by tempo and meter, so differences in speed or meter place pieces into different genres.",
        "B": "Because genre depends only on geographic origin, so pieces from different places are assigned different genres.",
        "C": "Because genre is defined by a set of socially accepted rules governing musical events — including techniques, cultural context, subject matter and themes — so identical form can conform to different rule sets and be classified into different genres.",
        "D": "Because genres are fixed and never change over time, so two pieces with the same form must belong to different immutable genre labels."
      },
      "correct_answer": "C",
      "simplification_notes": "Question rephrased to reference Fabbri's rule-based account; language tightened and academic terms retained (techniques, cultural context, themes). All options made plausible but only C matches the article's definition.",
      "content_preserved": true,
      "source_article": "Music genre",
      "x": 0.29672926664352417,
      "y": 1.6032884120941162,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3"
      ],
      "original_question_hash": "a69723e1"
    },
    {
      "question": "How do humor, storytelling, and metaphors function in science communication to increase engagement and understanding, and what role do audience values play in that mechanism?",
      "options": {
        "A": "They mainly boost entertainment value, which can distract audiences from scientific content and does not reliably improve comprehension or retention.",
        "B": "They build relatable mental models by mapping scientific concepts onto familiar narratives and audience values, thereby increasing attention, encoding, memory, and the ability to transfer understanding to new situations.",
        "C": "They substitute empirical evidence with vivid anecdotes and emotional appeals, which simply strengthens belief without improving critical understanding.",
        "D": "They standardize messages into uniform stories that try to fit every audience, thereby neglecting audience values and reducing engagement."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed language to a single clear question; retained technical terms (mental models, encoding, transfer); clarified the role of audience values; made all four options plausible contrasts.",
      "content_preserved": true,
      "source_article": "Science communication",
      "x": 1.1933436393737793,
      "y": 0.999553918838501,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Science communication as a bridge between science and society with aims to inform policy, influence public attitudes, and engage diverse communities.",
        "Concept 2: Use of communication mechanisms (humor, storytelling, metaphors) to connect with audience values and improve engagement and understanding.",
        "Concept 3: The relationship between research on science communication and actual practice, including interdisciplinarity and efforts to bridge theory and practice, as well as the influence of systemic inequalities on outreach/inreach."
      ],
      "original_question_hash": "acf89b3b"
    },
    {
      "question": "How does replacing the information-deficit model with dialogue and participation models alter how the public engages with science?",
      "options": {
        "A": "It increases the amount of information delivered to the public through lectures and other one-way messages.",
        "B": "It establishes ongoing, bidirectional interfaces in which lay experiences and values help shape how questions are framed, how risks are interpreted, and which trade-offs are considered, thereby improving relevance and trust.",
        "C": "It ensures policy decisions are made solely by scientists to maximize technical accuracy.",
        "D": "It reduces public input to avoid conflicting viewpoints and public confusion."
      },
      "correct_answer": "B",
      "simplification_notes": "Compressed the original prompt into a concise question, clarified 'information-deficit model', preserved key contrast with dialogue/participation models and kept the concepts of bidirectional engagement, framing, risk interpretation, trade-offs, and trust.",
      "content_preserved": true,
      "source_article": "Public awareness of science",
      "x": 1.2333869934082031,
      "y": 0.9971883296966553,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The shift from the information deficit model to dialogue and participation models in science communication, and how these models alter the way the public engages with science.",
        "Concept 2: The mediating interfaces between expert and lay knowledge and how developing such interfaces facilitates understanding and decision-making.",
        "Concept 3: Trust in science as a relational outcome shaped by socioeconomic factors, country, and political leanings, and how different engagement approaches influence trust."
      ],
      "original_question_hash": "0640ed70"
    },
    {
      "question": "Why is data veracity (data quality) essential for extracting value from big data even when you already have large volume, high variety, and high velocity?",
      "options": {
        "A": "Because very large volume automatically compensates for poor-quality records: more data will eventually reveal true patterns despite inaccuracies.",
        "B": "Because low-quality data creates noise and systematic biases that lead to spurious correlations and costly wrong decisions; therefore value depends on trustworthy data and strong data governance.",
        "C": "Because high velocity keeps data current, veracity only matters for historical analyses and can be ignored for real-time decision making.",
        "D": "Because high variety ensures all contexts are represented, so individual errors are diluted and data quality is less important."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem to be concise and clear for undergraduates, made veracity explicit as data quality, and kept technical terms (noise, biases, data governance). Distractors were kept plausible misconceptions.",
      "content_preserved": true,
      "source_article": "Big data",
      "x": 1.4238322973251343,
      "y": 1.0665291547775269,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The 4Vs framework (volume, variety, velocity, and veracity) and the central role of data quality (veracity) in determining value from big data.",
        "Concept 2: The relationship between data characteristics (large volume, high variety/velocity) and the need for expertise, infrastructure, and practices to avoid costs and risks that outweigh value.",
        "Concept 3: The shift in meaning and usage of \"big data\" from a focus on dataset size to value extraction through analytics (predictive analytics, user behavior analytics) and the implication that value comes from insights, not merely data size."
      ],
      "original_question_hash": "740fa60d"
    },
    {
      "question": "Why can science diplomacy both enable cross-border problem-solving and also reinforce power imbalances or become misaligned with policy?",
      "options": {
        "A": "Because scientific exchanges always equalize participants by sharing resources and data access equally, eliminating political context.",
        "B": "Because science diplomacy depends only on formal treaties that impose uniform agendas regardless of existing power relations.",
        "C": "Because cross-border scientific collaboration can produce practical solutions when participants share needs and capabilities, but access to resources, agenda-setting, and influence over policy often reflect existing power hierarchies, especially during periods of conflict.",
        "D": "Because science diplomacy operates independently of political interests, so it cannot be affected by power or policy agendas."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; retained core idea of dual nature. Options were rephrased to be concise and plausible while preserving the original correct choice (C). Removed extended historical/contextual details.",
      "content_preserved": true,
      "source_article": "Science diplomacy",
      "x": 1.1178582906723022,
      "y": 0.8640181422233582,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Science diplomacy as a mechanism for building international relations through cross-border scientific cooperation.",
        "Concept 2: The actors and formats of science diplomacy (states, international organizations, non-state actors; formal and informal exchanges; interactions between scientists and officials) as the practical means by which it operates.",
        "Concept 3: The dual nature of science diplomacy—its potential to address common problems and strengthen cooperation, alongside risks of reinforcing power asymmetries or misalignment with policy, especially in times of conflict."
      ],
      "original_question_hash": "be24c8be"
    },
    {
      "question": "How does horizontal gene transfer (HGT) change how we interpret universal common descent, and why can phylogenies built from single genes disagree with the species tree?",
      "options": {
        "A": "HGT makes all genes share the same evolutionary history, which would strengthen the idea of a single universal ancestor.",
        "B": "HGT can move genes between distantly related organisms, so some genes trace back to different ancestral sources than the species' primary lineage, producing gene trees that conflict with the species tree.",
        "C": "HGT only affects nonfunctional or neutral DNA, so it has no real effect on inferences about common descent.",
        "D": "HGT happens mainly under laboratory conditions and is negligible in nature, so it does not meaningfully affect whether life is monophyletic."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording shortened and clarified for undergraduates; emphasized that HGT transfers genes between distant lineages and can make individual gene trees disagree with the species tree; kept original correct option (B) and made all alternatives plausible misconceptions.",
      "content_preserved": true,
      "source_article": "Common descent",
      "x": 1.847443699836731,
      "y": 1.1175358295440674,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Common descent as a mechanism/relationship that explains relatedness among species, where closer common ancestors mean closer relatedness.",
        "Concept 2: LUCA and potential non-monophyly due to horizontal gene transfer, highlighting how genetic heritage across life informs and complicates the single-ancestry idea.",
        "Concept 3: Speciation as the process generating descendant lineages from an ancestral population, with genetic evidence (shared gene groups) used to infer a common ancestry across life."
      ],
      "original_question_hash": "60158539"
    },
    {
      "question": "In comparative genomics, how does synteny support inference of common ancestry, and what specifically defines a synteny block?",
      "options": {
        "A": "Synteny depends on identical DNA sequence identity between species to demonstrate descent; a synteny block is any region that contains the same genes in two genomes regardless of their order or orientation.",
        "B": "Synteny relies on conservation of gene order and orientation to infer shared ancestry; a synteny block is a chromosomal region where a set of homologous genes preserve the same relative order (collinearity) across species, even if other rearrangements occur elsewhere.",
        "C": "Synteny is determined by identical chromosome numbers or matching overall gene counts; a synteny block is defined as a chromosome-scale segment with the same number of genes in both species.",
        "D": "Synteny is inferred from conserved gene expression patterns across species; a synteny block is defined by transcriptomic features that are preserved between species."
      },
      "correct_answer": "B",
      "simplification_notes": "Rephrased the question to be concise and academic; clarified that synteny refers to conserved gene order/orientation and defined synteny block as a region of homologous genes with conserved relative order. Distractor options were made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Comparative genomics",
      "x": 1.97976553440094,
      "y": 1.1279584169387817,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Evolutionary conservation of DNA sequences as a basis for cross-species comparison; how conserved regions indicate shared ancestry and potential functional importance.",
        "Concept 2: Synteny/collinearity as a framework for identifying conserved gene order and inferring descent from a common ancestor; how synteny blocks are defined and used in comparative genomics.",
        "Concept 3: Genome size does not reliably reflect gene number or evolutionary status; how this observation shapes interpretation of genomic data in comparative analyses."
      ],
      "original_question_hash": "33e7768d"
    },
    {
      "question": "If a church holds that the Holy Spirit guides its decisions through communal discernment by ordained leaders, how does that theological commitment typically produce a presbyteral/conciliar (council-of-elders) form of governance rather than rule by a single bishop? What mechanism translates the belief in Spirit-led communal discernment into shared institutional authority?",
      "options": {
        "A": "The conviction that truth and legitimate church order are discovered through collective, Spirit-led discernment among ordained elders creates shared authority, mutual accountability, and institutional checks, which institutionalize presbyteral/conciliar governance.",
        "B": "A strong doctrine of apostolic succession implies continuity of singular decision-making authority in one successor (a single bishop), so the theology mandates centralized episcopal rule rather than conciliar leadership.",
        "C": "A theology that insists salvation is mediated only through the local congregation requires that all issues be decided by congregational voting, producing a congregational polity where the assembly is ultimate authority.",
        "D": "A view that church and state must mirror one another forces ecclesial structures to be centralized under civil authority, which concentrates governance in secular hands and discourages shared ecclesial leadership."
      },
      "correct_answer": "A",
      "simplification_notes": "Rewrote the stem in clear academic language, defined 'presbyteral/conciliar' as council-of-elders, and made the mechanism explicit (collective discernment → shared authority/checks). Kept all four original answer themes plausible and retained the correct option.",
      "content_preserved": true,
      "source_article": "Ecclesiology",
      "x": 0.15570904314517975,
      "y": 0.5734883546829224,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Church polity and authority as reflections of ecclesiological commitments (e.g., presbyteral councils vs. a single bishop; authority of the bishop of Rome). Why/how governance structures arise from theological understandings of church leadership.",
        "Concept 2: The church’s relationship to salvation and inclusion (e.g., whether salvation is possible outside the church; Gentile inclusion in the early church). Why/how ecclesiology defines membership, soteriology, and universality.",
        "Concept 3: The church’s role in the world and in relation to the State, including ecumenical considerations and the scope of ecclesiology across denominations. Why/how the church engages with civil authority and broader society, and how this shapes inter-denominational identity."
      ],
      "original_question_hash": "9ccf1a15"
    },
    {
      "question": "Within the social-cognition framework, why are biases and heuristics treated as functional shortcuts for interpreting social information, and how do social and affective factors (e.g., mood, emotions, situational cues) interact with these processes to influence behavior?",
      "options": {
        "A": "They provide efficient shortcuts that draw on prior knowledge (schemas, stereotypes, attributions) to generate quick interpretations under uncertainty; current affective states and social cues prime these cognitive elements, biasing perception and judgment and thereby guiding subsequent behavior.",
        "B": "They always produce perfectly accurate social interpretations because affective states enhance memory for social facts and remove uncertainty from judgment.",
        "C": "They arise only from memory retrieval errors and therefore have no effect on online judgments or immediate behavior.",
        "D": "They require slow, explicit, logically reasoned processing and are never modulated by mood, emotion, or social context."
      },
      "correct_answer": "A",
      "simplification_notes": "Condensed the original multi-part question into a single clear sentence at undergraduate level; emphasized schemas, heuristics, and affective priming; removed historical and developmental examples while keeping the causal link from cognitive shortcuts and affect to behavior. Wording made more concise and technical terms retained.",
      "content_preserved": true,
      "source_article": "Social cognition",
      "x": 1.3101680278778076,
      "y": 1.0163654088974,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Encoding, storage, retrieval, and processing as a cognitive pipeline for social information and how each stage influences perception, judgment, and memory of social stimuli.",
        "Concept 2: Cognitive elements (schemas, attributions, stereotypes) as mental representations that guide interpretation and processing of social information and affect subsequent behavior.",
        "Concept 3: The social cognition framework as an application of cognitive psychology to social phenomena, including the role of biases/heuristics and the interaction of cognitive processes with social and affective factors to produce behavioral outcomes."
      ],
      "original_question_hash": "4d6a4e9e"
    },
    {
      "question": "How do equity (stock) and debt (bonds) differ in investor rights and risk if a company becomes financially distressed or is liquidated?",
      "options": {
        "A": "Equity holders receive fixed payments and have priority in asset liquidation, while debt holders own the company and usually lack influence over major decisions.",
        "B": "Debt holders have contractual, priority claims on assets and receive fixed payments; equity holders have residual claims with upside potential and voting rights but face higher risk of losing their investment.",
        "C": "Both equity and debt holders have identical claims on assets in liquidation, so their risk comes only from market price changes, not from different legal rights.",
        "D": "The distinction only affects how quickly an investment can be traded (liquidity); it does not change the likelihood of recovery in distress or governance influence."
      },
      "correct_answer": "B",
      "simplification_notes": "Question phrasing condensed and clarified for undergraduates; retained terms 'equity' and 'debt' and focused on liquidation priority, fixed vs residual claims, voting rights, and relative risk. Extraneous background on markets and issuance was removed.",
      "content_preserved": true,
      "source_article": "Capital market",
      "x": 1.3433036804199219,
      "y": 0.9005430340766907,
      "level": 2,
      "concepts_tested": [
        "Primary vs secondary markets and the role of liquidity in enabling investment exits",
        "Mechanism of channeling savers’ funds into long-term productive use via debt and equity instruments, and the roles of issuers and investors",
        "Distinction between stock markets (equity) and bond markets (debt) and how ownership versus creditor relationships shape investor rights and risk"
      ],
      "original_question_hash": "59ce5408"
    },
    {
      "question": "According to the transactional view of records, why do context and the process of creation/use matter when deciding whether a document or item should be retained as a record?",
      "options": {
        "A": "The document's content alone determines it is a record because the information it contains proves what happened.",
        "B": "The same item can be evidence of a transaction in one context and non-record material in another, depending on how it was created, used, and retained to document actions.",
        "C": "Only items formally designated as \"records\" by organizational policy qualify as records, regardless of how they were used.",
        "D": "Any document that is legally required to be kept is a record, so legal requirement alone decides record status."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the question in concise academic language for undergraduates; preserved the transactional perspective by emphasizing context and process. Options were made shorter and clearer while keeping all four plausible and the original correct answer unchanged.",
      "content_preserved": true,
      "source_article": "Records management",
      "x": 1.3989343643188477,
      "y": 1.0089765787124634,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Records serve as evidence of organizational actions and as organizational assets, underpinning accountability and governance.",
        "Concept 2: Life cycle management of records (from creation/receipt to disposition) as a mechanism to capture, maintain, store, retrieve, and eventually retain or destroy records in a way that supports risk management and compliance.",
        "Concept 3: Different conceptual definitions of what constitutes a record (transactional view vs. evidential/informational properties) and the importance of context and process in determining records."
      ],
      "original_question_hash": "81d9009e"
    },
    {
      "question": "According to symbolic interactionism, how are meanings created and stabilized in social life?",
      "options": {
        "A": "By inherent, objective properties of objects that people universally recognize and transmit across generations.",
        "B": "By ongoing interaction with shared symbols: individuals interpret cues, negotiate meanings, and anticipate others' reactions, which stabilizes those meanings over time.",
        "C": "By top-down imposition of meanings from powerful institutions or authorities, independent of everyday interactions.",
        "D": "By random, individual interpretations that rarely converge, making durable shared meanings unlikely."
      },
      "correct_answer": "B",
      "simplification_notes": "Question rephrased in clearer academic language; options shortened and made more direct while retaining alternatives contrasting interactionist, essentialist, structural, and chaotic accounts; core interactionist elements (symbols, interpretation, negotiation, anticipation, stabilization) preserved in option B.",
      "content_preserved": true,
      "source_article": "Symbolic interactionism",
      "x": 1.2291316986083984,
      "y": 1.0167697668075562,
      "level": 2,
      "concepts_tested": [
        "Meaning construction through ongoing verbal/nonverbal interactions and symbol interpretation.",
        "Society as the product of everyday, micro-level interactions that are repeated and maintained over time.",
        "The self and behavior as socially produced, shaped by shared meanings and the anticipation of others' reactions."
      ],
      "original_question_hash": "4b2366df"
    },
    {
      "question": "Why does the holistic (system-wide) comparative method describe and analyze foreign legal systems as integrated wholes, and how does that clarify why legal systems differ from one another?",
      "options": {
        "A": "Because analyzing isolated provisions uncovers universal legal principles that operate independently of social or institutional context.",
        "B": "Because treating a legal system as an integrated whole highlights the interdependence of institutions, norms, and practices; this reveals how a system's institutional configuration shapes legal outcomes and explains variation between systems.",
        "C": "Because the method ignores practical consequences and concentrates only on doctrinal texts and formal rules, comparing abstract ideals rather than actual practice.",
        "D": "Because it assumes all legal systems share a single historical origin, so differences are seen as minor deviations from a common source."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the original wording; made the question explicitly about 'system-wide' analysis and its explanatory role; retained concepts of institutional interdependence, law families/context-dependence, and the purpose of revealing reasons for variation; removed lengthy historical examples and quotes.",
      "content_preserved": true,
      "source_article": "Comparative law",
      "x": 1.2294342517852783,
      "y": 0.8006993532180786,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3",
        "Concept 1: Classification by law families based on core features (sources of law, role of precedents, origin/development) and how this framework enables cross-system understanding.",
        "Concept 2: Holistic comparative method (describing and analyzing foreign legal systems and comparing systems as wholes) and its purpose in revealing similarities, differences, and reasons for variation.",
        "Concept 3: Context-dependence of legal systems (law should fit the people and environment, including climate, culture, economy, religion) and the implication that legal design varies by societal context."
      ],
      "original_question_hash": "05a81051"
    },
    {
      "question": "How does the principle of subsidiarity foster civic participation and act as a check on centralised power?",
      "options": {
        "A": "By shifting decision-making to the central government to create uniform policies across regions, which is claimed to increase citizen engagement nationally.",
        "B": "By assigning decisions to the most local level that can resolve them, enabling direct citizen input and local accountability, thereby counterbalancing central authority.",
        "C": "By reserving policy initiation exclusively to national authorities, ensuring local voices are excluded from policymaking.",
        "D": "By replacing local institutions with faster national decision-making to prioritise administrative efficiency over local involvement."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was condensed and clarified for undergraduates; historical and treaty references were removed while preserving the core idea that subsidiarity delegates authority to local levels to increase participation and check central power.",
      "content_preserved": true,
      "source_article": "Subsidiarity",
      "x": 1.2453885078430176,
      "y": 0.8076418042182922,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Local-first authority allocation — The principle that issues should be handled at the most immediate level capable of resolution, with central authority intervening only when needed.",
        "Concept 2: Decentralization and civic engagement — How subsidiarity fosters citizen participation and acts as a counterweight to centralized power.",
        "Concept 3: Theoretical foundations and cross-domain application — The origins in natural-law thought (Aquinas, Taparelli) and its articulation in EU law and political theory (e.g., Tocqueville’s analysis)."
      ],
      "original_question_hash": "2de56b0d"
    },
    {
      "question": "Why do governments impose nuisance laws and zoning regulations that limit how private property can be used, and what is the typical effect on social welfare?",
      "options": {
        "A": "They prioritize individual autonomy by allowing owners to use property however they choose, regardless of costs imposed on others.",
        "B": "They correct negative externalities by constraining private uses that would impose costs on others, thereby better aligning private incentives with social costs and improving overall welfare.",
        "C": "They abolish private property rights entirely to guarantee equal access to resources for everyone.",
        "D": "They increase market concentration by enabling owners to appropriate neighboring land or uses for private profit, concentrating benefits among a few."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; key concept 'negative externalities' was retained and highlighted; options rephrased to be concise and academically precise while keeping all four plausible and preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Property law",
      "x": 1.2665871381759644,
      "y": 0.8277018070220947,
      "level": 2,
      "concepts_tested": [
        "Private property as an efficient, decentralized mechanism for managing resources, enabling specialization, investment, exchange, and individual autonomy.",
        "Limits on property rights arising from social costs (negative externalities) and the need for regulation (nuisance laws, zoning, antitrust) to balance private interests with social welfare.",
        "Property as a bundle of rights embedded in a social/political context, including the distinction between informal and formal property rights and their implications for autonomy and identity."
      ],
      "original_question_hash": "b4c46ede"
    },
    {
      "question": "Why do autologous (patient-derived) cell therapies reduce the risk of immune rejection compared with donor-derived cells?",
      "options": {
        "A": "Because they display the patient's own $MHC$ molecules and other self-antigens, so the host immune system recognizes them as 'self' and host T cells are less likely to attack.",
        "B": "Because autologous cells lack any surface antigens, making them invisible to the immune system.",
        "C": "Because autologous cells secrete immunosuppressive molecules that globally shut down the recipient's immune responses.",
        "D": "Because autologous cells are always genetically identical to the host's entire genome, so there can be no mismatch."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; explicitly mentioned $MHC$/self-antigens in the correct option; distractors kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Regenerative medicine",
      "x": 1.888237476348877,
      "y": 1.0999481678009033,
      "level": 2,
      "concepts_tested": [
        "Regenerative medicine strategies (cell therapies, immunomodulation, tissue engineering) as mechanisms to restore function",
        "Autologous/source compatibility and transplant rejection (how patient-derived cells can circumvent immunological mismatch)",
        "Lab-based tissue/organ growth and implantation as a method to replace damaged tissues and address organ shortages"
      ],
      "original_question_hash": "0e54bca4"
    },
    {
      "question": "How does confirmation bias allow pseudoscientific claims to persist even when evidence contradicts them?",
      "options": {
        "A": "It encourages proponents to seek and perform rigorous experiments designed to falsify their own claims.",
        "B": "It causes supporters to overweight confirming evidence and to dismiss, reinterpret, or ignore contradictory data.",
        "C": "It promotes open, critical debate among experts that quickly leads to corrective revisions of the claims.",
        "D": "It establishes a standardized, transparent methodology for hypothesis testing that reveals errors."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the original question to a concise undergraduate-level formulation while keeping the tested concept (confirmation bias sustaining pseudoscience). Options were rewritten for clarity but kept plausible contrasts; correct answer letter unchanged.",
      "content_preserved": true,
      "source_article": "Pseudoscience",
      "x": 1.1720249652862549,
      "y": 1.0641895532608032,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Demarcation criteria between science and pseudoscience (falsifiability, openness to refutation, systematic development of hypotheses)",
        "Concept 2: Cognitive and methodological mechanisms sustaining pseudoscience (confirmation bias, unfalsifiability, reliance on confirmation over refutation)",
        "Concept 3: Real-world relationships and impacts (health consequences, public safety, policy/education implications)"
      ],
      "original_question_hash": "5d3a4782"
    },
    {
      "question": "How can epigenetic regulation change gene expression without altering the DNA sequence, and how can such expression states be copied when a cell divides?",
      "options": {
        "A": "Because epigenetic regulation changes the DNA sequence by chemically modifying nucleotides (e.g., methylating cytosines) to create a different base sequence that DNA polymerase then copies during replication.",
        "B": "Because chemical marks such as DNA methylation and histone modifications alter chromatin compaction and DNA accessibility to transcriptional machinery, and maintenance enzymes or re‑establishment mechanisms copy or rebuild these marks on daughter strands or new histones after replication, allowing the expression state to be inherited.",
        "C": "Because epigenetic regulation acts only at transcription factor binding sites and is randomly erased and reset after each cell division, so it cannot produce stable inherited states.",
        "D": "Because epigenetic marks are always erased during meiosis and therefore cannot influence the expression state of descendant cells or be transmitted through cell divisions."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was shortened and clarified for undergraduates. Technical terms (chromatin, DNA methylation, histone modifications, maintenance enzymes) were retained. Distractors were made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Regulation of gene expression",
      "x": 2.071276903152466,
      "y": 1.1499937772750854,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Regulation can modulate gene expression at multiple stages, not only transcription initiation, enabling nuanced control of gene product levels.",
        "Concept 2: Epigenetic regulation—chromatin structure, DNA methylation, histone modifications, and ncRNA—modulates gene accessibility and expression, and can be inherited.",
        "Concept 3: Gene regulation drives development and cell differentiation through networks of regulators and signaling, producing diverse cell types from the same genome."
      ],
      "original_question_hash": "87650397"
    },
    {
      "question": "How are DNA methylation patterns maintained through cell division so that daughter cells preserve cell-type–specific gene expression without altering the DNA sequence?",
      "options": {
        "A": "A maintenance DNA methyltransferase (e.g., DNMT1) recognizes hemimethylated CpG sites after replication and copies the methylation pattern onto the new strand, preserving the methylation state.",
        "B": "By creating permanent mutations in regulatory DNA sequences so daughter cells inherit the same expression pattern genetically.",
        "C": "By globally erasing and resetting all histone modifications to a neutral state each cell cycle so gene expression is reestablished de novo in each daughter cell.",
        "D": "By exporting epigenetic information (such as RNAs or proteins) into the cytoplasm that then directs transcriptional states in daughter cells after mitosis."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was shortened and focused on the specific mechanism for maintaining DNA methylation through replication; technical terms (maintenance methyltransferase, hemimethylated CpG, DNMT1) were kept for precision. Distractors were kept plausible and aligned with mechanisms discussed in the article but remain incorrect.",
      "content_preserved": true,
      "source_article": "Epigenetics",
      "x": 2.0640792846679688,
      "y": 1.15952730178833,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Mechanisms of epigenetic regulation—how DNA methylation, histone modification, and non-coding RNAs alter gene expression without changing the DNA sequence.",
        "Concept 2: Persistence and inheritance of epigenetic states—how epigenetic marks can be maintained through cell division and, in some cases, across generations.",
        "Concept 3: Epigenetics in development and differentiation—how epigenetic regulation shapes cell fate by activating or repressing gene expression to produce diverse cell types."
      ],
      "original_question_hash": "4c316b0d"
    },
    {
      "question": "A morphogen is produced at one boundary of a developing tissue, diffuses through the tissue, and is degraded, creating a spatial concentration gradient. How does this gradient encode positional information that leads to distinct cell fates?",
      "options": {
        "A": "Cells read local morphogen concentration thresholds and, via activation or repression of transcription factors and gene regulatory networks, initiate different transcriptional programs—often stabilized by feedback and epigenetic mechanisms—producing distinct cell fates at different positions.",
        "B": "Because diffusion rapidly equalizes the morphogen, every cell experiences essentially the same concentration, so the gradient cannot provide positional cues for fate specification.",
        "C": "Cells sense the gradient’s slope and use it only to modulate proliferation rate (how fast they divide), without changing gene expression or activating different regulatory networks.",
        "D": "The morphogen only changes cell adhesion or mechanical properties, and does not influence transcriptional programs or gene regulatory cascades that determine fate."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording was made more concise and formal for undergraduates; technical concepts (threshold reading, transcription factors, GRNs, feedback, epigenetics) were preserved. Distractors were kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Gene regulatory network",
      "x": 2.0556728839874268,
      "y": 1.1466721296310425,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Transcription factor binding to promoter regions drives regulatory cascades that control gene expression (activation and/or inhibition; direct and indirect regulation).",
        "Concept 2: Feedback loops and epigenetic/chromatin modifications contribute to cellular memory and stable cell identity during development.",
        "Concept 3: Morphogen gradients and diffusion of gene products provide positional information that guides tissue patterning and cell fate decisions."
      ],
      "original_question_hash": "06e87b18"
    },
    {
      "question": "Why must an event manager coordinate creative, technical, and logistical tasks to ensure smooth on-site operations and effective risk management?",
      "options": {
        "A": "It speeds up production by giving the sponsor final authority over all operational decisions.",
        "B": "It ensures the audience experience is aligned with venue capabilities, technical systems, and safety/risk controls, reducing conflicts and last-minute changes.",
        "C": "It allows designers to ignore budget constraints and focus solely on aesthetics.",
        "D": "It guarantees media coverage because an integrated design will automatically attract publicity."
      },
      "correct_answer": "B",
      "simplification_notes": "Made the question more direct and concise; removed parenthetical phrasing; preserved the three domains (creative, technical, logistical). Options were reworded for clarity and plausibility while keeping the original correct choice.",
      "content_preserved": true,
      "source_article": "Event management",
      "x": 1.3733668327331543,
      "y": 1.0079363584518433,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Event management as the application of project management to plan, execute, and deliver events.",
        "Concept 2: Events as strategic marketing and communication tools to build brand, engage audiences, and generate media coverage.",
        "Concept 3: The integrated role of the event manager across creative, technical, and logistical domains, including coordination with venues and vendors and managing on-site operations and risk."
      ],
      "original_question_hash": "b5892d7f"
    },
    {
      "question": "Why is analyzing the audience essential to effective public speaking, and what does practical adaptability look like when addressing a diverse audience?",
      "options": {
        "A": "Because audience analysis identifies a single set of facts that will persuade everyone, and adaptability simply means speaking louder so everyone can hear.",
        "B": "Because audience analysis aligns content with the audience's knowledge, values, and interests, and adaptability involves tailoring examples, language, structure, and delivery (tone, pace, and media) to fit diverse listeners.",
        "C": "Because audience analysis removes the need for humor, and adaptability means abruptly changing topics mid-speech to chase immediate reactions.",
        "D": "Because audience analysis is unnecessary; adaptability means relying more on slides and spending extra time on visual aids regardless of the audience."
      },
      "correct_answer": "B",
      "simplification_notes": "Question shortened and clarified; technical terms retained (e.g., \"delivery, tone, pace, media\"). Distractors reworded to remain plausible but incorrect. Core concept preserved.",
      "content_preserved": true,
      "source_article": "Public speaking",
      "x": 1.2546875476837158,
      "y": 1.0123107433319092,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The relationship between speech purpose (inform vs. influence) and how content/delivery are designed, including how audience affects outcomes.",
        "Concept 2: The centrality of audience analysis and adaptability—recognizing audience diversity and tailoring the speech accordingly.",
        "Concept 3: The role of rhetorical mechanisms (storytelling, informational approaches, visual aids) in achieving aims and how their use depends on context and audience."
      ],
      "original_question_hash": "5f4e2684"
    },
    {
      "question": "In common-law systems, requiring consideration (a bargained-for exchange) is said to make promises legally enforceable. Explain why consideration performs that role, and how shifting to a mutual-assent-only rule (i.e. enforceability based solely on a \"meeting of the minds\" as in many civil-law systems) would change the legal basis for enforcing promises.",
      "options": {
        "A": "Because consideration links the promise to a reciprocal exchange, showing the promisor incurred a legal detriment or received a legal benefit in return and thus supports the inference of a binding obligation; eliminating consideration shifts enforceability to mutual assent alone, making it harder to separate gratuitous promises from enforceable contracts and potentially increasing disputes over whether a promise was intended to be binding.",
        "B": "Because consideration primarily makes contracts easier to prove in writing, whereas a mutual-assent rule would permit no evidential requirements and therefore produce many more informal, unverifiable contracts.",
        "C": "Because consideration requires the parties to obtain equal economic value from the deal, and replacing it with mutual assent would allow courts to enforce clearly unconscionable bargains where one side receives almost nothing.",
        "D": "Because consideration functions as a formal ceremony (like a signed deed) required to create a contract in common law, while mutual assent would remove any need for signature or formality altogether."
      },
      "correct_answer": "A",
      "simplification_notes": "Reduced length and removed historical and jurisdictional detail; clarified technical terms (consideration, mutual assent, gratuitous promises) and stated the comparative legal effect succinctly. Kept the original correct conclusion (A) and made other options plausible misconceptions.",
      "content_preserved": true,
      "source_article": "Contract",
      "x": 1.295190691947937,
      "y": 0.824543297290802,
      "level": 2,
      "concepts_tested": [
        "The core principle: contracts create legally enforceable obligations that must be honoured, with breaches leading to remedies (damages, specific performance, rescission).",
        "Jurisdictional mechanisms for validity: in common law, consideration is typically required; in civil/mixed systems, a meeting of the minds may suffice, illustrating how different legal frameworks govern contract formation.",
        "International framework and rationales: UNIDROIT Principles aim to harmonize international contracts and reject certain doctrines (e.g., consideration, abstraction) to reduce uncertainty and litigation in cross-border trade."
      ],
      "original_question_hash": "29dba094"
    },
    {
      "question": "A vertical rectangular plate is fully submerged in a static fluid where pressure varies with depth according to $p=\\rho g y$. Why does the resultant hydrostatic force on the plate act below the plate's geometric centroid?",
      "options": {
        "A": "Because the pressure is uniform with depth across the plate, so the centroid is the center of action.",
        "B": "Because pressure increases with depth ($p=\\rho g y$), so the lower portion contributes more force and the resultant shifts below the centroid.",
        "C": "Because the effective area 'seen' by the pressure increases with depth, pulling the line of action downward.",
        "D": "Because viscous shear near the plate produces unequal forces that shift the resultant downward."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording made concise for undergraduates; included the hydrostatic relation $p=\\rho g y$; distractors kept plausible but incorrect; core physics (pressure increases with depth causing greater lower contribution) preserved.",
      "content_preserved": true,
      "source_article": "Hydraulic engineering",
      "x": 1.742276668548584,
      "y": 0.9635974764823914,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Pressure in a fluid at rest increases with depth, described by p = ρ g y, and the concept of pressure head p/(ρ g) = y.",
        "Concept 2: Pressure acts on submerged surfaces and contributes to equilibrium; understanding how distributed pressures result in resultant forces on bodies and structures.",
        "Concept 3: Measurement of fluid pressure using devices (piezometer, manometer, Bourdon gauge, etc.) and how these devices translate pressure into observable readings."
      ],
      "original_question_hash": "638b0693"
    },
    {
      "question": "Why does inter-modal freight transport—using high-capacity modes (rail or sea) for long-haul legs and road for the short \"first/last mile\"—often reduce total logistics cost compared with using a single mode for the entire trip?",
      "options": {
        "A": "Because long-haul modes provide a high-capacity, low-cost-per-distance core while road offers flexible short-haul delivery; although transfer and handling incur costs, overall unit costs fall due to economies of scale and route optimization.",
        "B": "Because road transport always has lower variable costs than rail or sea, so moving everything by road is cheaper and inter-modal simply adds complexity and extra handling costs.",
        "C": "Because inter-modal transport eliminates the need for any fixed infrastructure investment by carriers or shippers, transferring all infrastructure costs away from the logistics chain.",
        "D": "Because inter-modal transport inherently increases transit time, so it is used primarily for reliability and scheduling rather than for lowering total cost."
      },
      "correct_answer": "A",
      "simplification_notes": "Rewrote the question concisely, introduced the term \"first/last mile,\" and tightened option wording while preserving the original reasoning and distractors.",
      "content_preserved": true,
      "source_article": "Transport",
      "x": 0.7111909985542297,
      "y": 0.3129604756832123,
      "level": 2,
      "concepts_tested": [
        "Inter-modal/multi-modal transport: combining multiple modes to optimize routes and efficiency, with implications for logistics and planning.",
        "Mode choice and trade-offs: each mode has advantages and disadvantages; decisions are based on cost, capability, and route, shaping outcomes like efficiency and accessibility.",
        "Economic and policy impacts of transport: transport enables trade and globalization, influences urban planning, regulation, ownership models, subsidies, and environmental effects."
      ],
      "original_question_hash": "57a4c785"
    },
    {
      "question": "Why can increasing road capacity fail to produce sustained reductions in congestion and sometimes worsen environmental outcomes?",
      "options": {
        "A": "Because it permanently lowers travel times and costs for all trips, so congestion declines indefinitely.",
        "B": "Induced demand: by lowering the generalised cost of travel (time + money), more people make trips and take longer trips, raising total vehicle‑kilometres traveled (VKT) and often increasing congestion and emissions.",
        "C": "Because increased capacity instantly shifts travellers to cleaner modes (public transit, walking, cycling), eliminating additional emissions.",
        "D": "Because expanding capacity removes external costs, so environmental impacts do not increase."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified the question, defined 'generalised cost' as time + money, used the term VKT, shortened options while keeping each plausible. Preserved the original focus on induced demand and externalities.",
      "content_preserved": true,
      "source_article": "Transport economics",
      "x": 1.3106670379638672,
      "y": 0.9391632676124573,
      "level": 2,
      "concepts_tested": [
        "Generalized cost as a determinant of travel demand (money and time components)",
        "Induced demand and the environmental/congestion implications of capacity expansion",
        "Externalities of transport (positive and negative), including congestion and climate change impacts"
      ],
      "original_question_hash": "28ff48cb"
    },
    {
      "question": "Why does non-excludability tend to cause private markets to underprovide public goods, and how does government provision correct this failure?",
      "options": {
        "A": "Non-excludability allows individuals to benefit without paying (the free-rider problem), so private firms cannot capture enough of the good’s value to cover costs and therefore underprovide it; government provision funded by taxation or compulsory charges pools costs and ensures supply.",
        "B": "Non-excludability makes it straightforward to charge each user separately, so private firms can recover costs and will provide the good at the efficient level; government intervention is therefore unnecessary.",
        "C": "Because public goods are non-rivalrous, one person’s consumption reduces others’ consumption, so private markets stop at the point where marginal social benefit equals price; government then ration forces to prevent overuse.",
        "D": "If a good is non-excludable, private providers will overproduce since consumption cannot be limited; governments respond by restricting supply through public–private partnerships and caps."
      },
      "correct_answer": "A",
      "simplification_notes": "Condensed wording, highlighted the free-rider mechanism and taxation remedy; kept technical terms (non-excludability, free-rider) and retained original correct answer and distractors.",
      "content_preserved": true,
      "source_article": "Public good",
      "x": 1.2836058139801025,
      "y": 0.9581193923950195,
      "level": 2,
      "concepts_tested": [
        "Non-excludability and non-rivalry as defining properties of public goods and their link to the free-rider problem and potential under-provision.",
        "The role of government/public provision (versus private provision) due to profitability/scale considerations and market failure.",
        "Relationships and distinctions among related goods (public vs common vs club vs global public goods) and how excludability and rivalry shape access and provision mechanisms."
      ],
      "original_question_hash": "569b703d"
    },
    {
      "question": "In the case of a disputed territory, why does recognition by other states matter for a claimant's sovereignty even if military or administrative control on the ground is contested?",
      "options": {
        "A": "Because recognition by other states automatically and legally transfers sovereignty to the claimant.",
        "B": "Because recognition ensures the claimant will have permanent physical control of the territory under international law.",
        "C": "Because recognition reduces ambiguity about legitimate authority by signaling acceptance of the claim, enables the claimant to access international forums and cooperation, and can help stabilize governance even if physical control is contested.",
        "D": "Because recognition permits the recognizing states to replace or supersede local governance with their own administration."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording shortened and clarified for undergraduate readers; removed lengthy examples and kept the core causal point about recognition affecting legitimacy, access to international institutions, and governance stability. Options were made concise and plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Territory",
      "x": 0.6429135203361511,
      "y": 0.47319984436035156,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Territory as a political unit under the jurisdiction of a sovereign state, and its relationship to self-government and governance.",
        "Concept 2: Different territorial regimes (capital/federal territory, dependent territory, federal territory) and how those governance arrangements alter political control and status.",
        "Concept 3: Disputed and occupied territories and the role of international recognition in legitimizing sovereignty and territorial claims."
      ],
      "original_question_hash": "88682fb2"
    },
    {
      "question": "Why does autobiographical writing, which depends on the author's memory, tend to be subjective and to contain embellishments?",
      "options": {
        "A": "Because memory is reconstructive and filtered by the author's current self-conceptions and emotions; the writer arranges events into a coherent life narrative, emphasizing salient episodes and omitting or reshaping details that do not fit, which can produce embellishment.",
        "B": "Because autobiographers typically have complete, accurate records of all events, so any embellishment reflects deliberate literary flair rather than memory processes.",
        "C": "Because writing in the first person automatically makes recalled events more accurate through self-affirmation, so autobiographies are generally more reliable.",
        "D": "Because autobiographies must strictly follow an objective, chronological ledger of facts, which prevents the inclusion of embellished or altered details."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the question into concise academic language, introduced the term 'reconstructive memory' and 'self-conceptions', and tightened option wording to keep all alternatives plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Autobiography",
      "x": 0.9539355039596558,
      "y": 1.1120482683181763,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Autobiography as memory-based, subjective narrative and its impact on accuracy (embellishments, personal interpretation).",
        "Concept 2: Distinction between autobiography and other narrative forms (biography, diary) and what perspective/timeframe each emphasizes.",
        "Concept 3: Variation in forms of autobiography (memoirs, spiritual autobiographies, fictional autobiographies) and how form influences scope, purpose, and narrative approach."
      ],
      "original_question_hash": "d9923dfb"
    },
    {
      "question": "How do interconnections among financial institutions enable distress at one firm to trigger cascading failures in the broader financial system?",
      "options": {
        "A": "Interconnections create multiple transmission channels—credit exposures, funding liquidity links, and information spillovers—so distress at one node produces losses and forced asset sales (fire-sales) that propagate to others.",
        "B": "Interconnections concentrate contagion so that only the single most stressed institution absorbs the shock, keeping the rest of the system unaffected.",
        "C": "Interconnections automatically diversify and evenly distribute exposures across firms, thereby preventing any cascading failures.",
        "D": "Interconnections remove feedback loops between firms, which stabilizes prices and prevents systemic propagation of stress."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified for undergraduate level; key mechanisms (credit exposure, funding liquidity, information spillovers, fire-sales) were preserved; distractors made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Systemic risk",
      "x": 1.3391326665878296,
      "y": 0.8833150267601013,
      "level": 2,
      "concepts_tested": [
        "Interconnections and cascading failures as the mechanism by which systemic risk propagates through a financial system.",
        "Endogenous risk arising from interactions among market participants, rather than external shocks.",
        "Policy and risk-management emphasis on system resiliency and the notion of entities being “too interconnected to fail.”"
      ],
      "original_question_hash": "c8a85839"
    },
    {
      "question": "Iconography is a classificatory and interpretive framework for reading images. Which statement best describes how iconography operates across disciplines and why it differs from analyses of artistic style?",
      "options": {
        "A": "It reads recurring subjects, figures, symbols, and compositional devices as signs that encode culturally specific meanings, so interpretation emphasizes content and arrangement rather than technique (e.g., brushwork, pigments) or stylistic manner.",
        "B": "It concentrates primarily on mechanical production techniques (brush strokes, pigment types, printing methods) to classify works by workshop, school, or hand.",
        "C": "It depends on applying a single universal code of meanings to motifs, ignoring cultural variation and historical context.",
        "D": "It insists that interpretations should be identical across all media and historical periods, regardless of cultural or contextual differences."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording tightened for undergraduates; emphasized content-vs-style distinction; kept core concept and answer. Distractors made plausible and distinct.",
      "content_preserved": true,
      "source_article": "Iconography",
      "x": -0.007041251752525568,
      "y": 0.4542129933834076,
      "level": 2,
      "concepts_tested": [
        "Iconography analyzes image content, composition, and detailing to interpret meaning, distinct from artistic style.",
        "Iconography functions as a classificatory/interpretive framework (with relationships to iconology and other fields) guiding how images are read and understood.",
        "Historical development and cross-disciplinary application of iconography (from 19th-century scholarship to its use in fields like semiotics, media studies, archaeology) illustrate how conceptual approaches to images evolve and apply beyond pure art history."
      ],
      "original_question_hash": "9397a6d5"
    },
    {
      "question": "Why do the lines (phase boundaries) on a pressure–temperature phase diagram separate single-phase regions from regions where two phases coexist, and what thermodynamic condition defines those lines?",
      "options": {
        "A": "Because along the boundary viscous dissipation between phases goes to zero; the line is defined by equal viscosities (or zero relative viscous work) of the two phases.",
        "B": "Because along the boundary the molar Gibbs free energies (chemical potentials) of the two coexisting phases are equal, so the system can lower its overall $G$ by forming both phases; crossing the line changes which phase has the lower $G$, causing a transition.",
        "C": "Because along the boundary the internal energies per mole of the phases become identical; the line is defined by equal internal energy $U$ of the phases.",
        "D": "Because along the boundary the entropies of the phases reach a maximum and are equal; the line is defined by equal entropy $S$ of the phases."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was condensed and made more direct for undergraduate readers; technical term 'molar Gibbs free energy (chemical potential)' was clarified as the defining equilibrium condition. Distractors were kept plausible but incorrect (viscosity, internal energy, entropy).",
      "content_preserved": true,
      "source_article": "Phase diagram",
      "x": 1.8191466331481934,
      "y": 1.0316097736358643,
      "level": 2,
      "concepts_tested": [
        "Phase boundaries represent conditions where multiple phases are in equilibrium; crossing a boundary causes a phase transition due to thermodynamic stability changes.",
        "Triple point and critical point as fundamental features: the triple point is where three phases coexist, and the critical point marks the end of the liquid–gas boundary and the limit of distinct liquid/gas phases.",
        "Solidus and liquidus delineate temperatures (and regions) where the substance is solid, liquid, or a mixture (mushy region), illustrating how phase composition changes within phase regions."
      ],
      "original_question_hash": "6fd48289"
    },
    {
      "question": "In a perfectly competitive market, why do prices tend to move to the market-clearing level where $Q_{d}=Q_{s}$, and how does price adjustment operate when either the demand or supply curve shifts?",
      "options": {
        "A": "A central planner or authority sets the price so that $Q_{d}=Q_{s}$, deliberately balancing the two quantities.",
        "B": "If $Q_{d}>Q_{s}$ (excess demand) this puts upward pressure on price, and if $Q_{s}>Q_{d}$ (excess supply) this puts downward pressure on price; those price changes change quantities demanded and supplied until $Q_{d}=Q_{s}$.",
        "C": "Prices change only because production costs change, so demand-side shifts do not drive the adjustment that eliminates imbalances.",
        "D": "The market-clearing price is fixed by tradition or policy, and when demand or supply shifts the traded quantities (not the price) adjust to that fixed price."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the stem concisely for undergraduate readers, used $Q_{d}$ and $Q_{s}$ for clarity, and kept the core mechanism (excess demand/supply → price changes → quantities adjust). Options were made plausible distractors but preserved the original correct answer letter.",
      "content_preserved": true,
      "source_article": "Supply and demand",
      "x": 1.3058043718338013,
      "y": 0.9401770830154419,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Market-clearing equilibrium as the mechanism for price determination in a competitive market (price adjusts so Qd = Qs).",
        "Concept 2: Supply is driven by marginal cost; producers supply up to the point where price equals marginal cost, and production costs shift the supply curve (left/right).",
        "Concept 3: The relevance of market structure (perfect competition vs oligopoly or monopsony) and the need for different models to describe outcomes when firms or buyers have market power."
      ],
      "original_question_hash": "30299526"
    },
    {
      "question": "Two cells that contain the same DNA can develop into different functional cell types. Which mechanism best explains how these cells express different sets of genes?",
      "options": {
        "A": "By accumulating different DNA mutations in each cell over time so each cell has a distinct set of genes.",
        "B": "By selectively degrading most RNA transcripts in each cell so that only a single gene's transcript remains and is expressed.",
        "C": "By differential regulation of chromatin accessibility and transcription via chromatin remodeling and transcription factor networks, allowing different gene sets to be transcribed in each cell type.",
        "D": "By translating identical mRNA transcripts at different rates in each cell, producing different protein abundances without changing transcription."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording shortened and focused on gene expression; technical terms (chromatin remodeling, transcription factor networks) retained; distractors made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Biology",
      "x": 1.9082965850830078,
      "y": 1.089647650718689,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Evolution by natural selection as the driver of biological diversity and adaptation (how changes in populations over time lead to new traits and species).",
        "Concept 2: The cell as the basic unit of life and genes/heredity as the basis of inheritance (how cellular organization and DNA encode, transmit, and express biological information).",
        "Concept 3: Energy transformation and homeostasis as unifying mechanisms (how organisms obtain, transform, and use energy and maintain internal stability, and how energy flow/nutrient cycling support life across ecosystems)."
      ],
      "original_question_hash": "4a9defe7"
    },
    {
      "question": "Pontryagin's Maximum Principle introduces a costate \\(\\lambda(t)\\) and the Hamiltonian \\(H(x,u,\\lambda,t)=L(x,u,t)+\\lambda^{T}f(x,u,t)\\). Why does minimizing \\(H\\) with respect to \\(u\\) at each time, given \\(x\\) and \\(\\lambda\\), produce a control law that is (globally) optimal under the principle’s assumptions? Explain conceptually how the term \\(\\lambda^{T}f\\) lets an instantaneous choice reflect future costs.",
      "options": {
        "A": "Because \\(\\lambda\\) represents the shadow price (costate) of the state variables, the term \\(\\lambda^{T}f\\) measures how a small change in the current control changes future states and hence future costs. Minimizing \\(H\\) therefore balances the immediate running cost \\(L\\) against the marginal future cost encoded by \\(\\lambda^{T}f\\); when combined with the state and costate dynamics and boundary conditions required by Pontryagin’s principle, this pointwise minimization yields the globally optimal trajectory (under the principle’s hypotheses).",
        "B": "Because the Hamiltonian depends only on the current state and control, future consequences are absent; therefore choosing \\(u\\) to minimize \\(H\\) at each instant automatically makes the whole trajectory optimal.",
        "C": "Because the costate \\(\\lambda(t)\\) is treated as a fixed parameter determined before solving the control problem, minimizing \\(H\\) becomes a static optimization at each time and thus trivially provides the optimal control for all times.",
        "D": "Because the dynamics \\(f(x,u,t)\\) are assumed linear, the Hamiltonian is linear in \\(u\\), and instantaneous minimization of a linear Hamiltonian therefore always yields a globally optimal solution regardless of the running cost or terminal cost."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified; mathematical notation converted to inline LaTeX; emphasized conceptual role of \\(\\lambda^{T}f\\) and balancing immediate vs future costs. Distractors kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Optimal control",
      "x": 1.6653294563293457,
      "y": 1.1518604755401611,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The optimization problem structure in optimal control — cost functional, state and control variables, dynamics, and constraints that define the objective and feasible solutions.",
        "Concept 2: Core optimality principles — Pontryagin's maximum (minimum) principle as a necessary condition and the Hamilton–Jacobi–Bellman equation as a sufficient condition, and what each implies for deriving the optimal control.",
        "Concept 3: The control policy–system relationship — how a control law maps states to controls to steer the system toward minimizing the objective (e.g., minimizing travel time, fuel, or monetary cost), including trade-offs and constraints illustrated by examples."
      ],
      "original_question_hash": "937021c0"
    },
    {
      "question": "Under GMP, why must manufacturing processes be clearly defined, controlled, and validated, and why must changes to those processes be evaluated and revalidated?",
      "options": {
        "A": "Because validation demonstrates the process can reliably and consistently produce product within specifications; changes can modify critical parameters or variability, so revalidation confirms continued process control, product quality, and safety.",
        "B": "Because once a process is defined, its output is guaranteed to be identical under all conditions, so no revalidation is ever required after changes.",
        "C": "Because validation is mainly required at initial start‑up; ongoing quality can be ensured only by testing the final product, not by revalidating the process after changes.",
        "D": "Because process control is a theoretical goal and, in practice, manufacturers may implement changes without assessment or revalidation."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was made more concise and direct for undergraduates; technical terms (validation, process control, specifications) retained. Distractors were rewritten to be plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Good manufacturing practice",
      "x": 1.4326318502426147,
      "y": 0.9092447757720947,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Manufacturing processes must be clearly defined, controlled, and validated to ensure consistency and compliance (and changes require validation).",
        "Concept 2: Environmental controls and hygiene prevent contamination, thereby safeguarding product safety.",
        "Concept 3: Documentation, record-keeping, and traceability enable verification of process steps, investigation of deviations, and accountability within a quality management system."
      ],
      "original_question_hash": "3a63e567"
    },
    {
      "question": "How does the WTO's consensus-based decision-making affect negotiation dynamics and outcomes, and what is a typical consequence of using consensus?",
      "options": {
        "A": "Decisions are taken by majority vote, which speeds up outcomes when large coalitions form.",
        "B": "Decisions require consensus, so any single member can effectively veto proposals, often blocking proposals and slowing progress.",
        "C": "Decisions are delegated to a rotating subset of members (e.g., regional blocs), which can prioritise those members' interests over full membership approval.",
        "D": "Expert technical committees pre-clear proposals, allowing them to bypass individual member objections and reducing direct member oversight."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the question to undergrad level language; explicitly defined 'consensus' as unanimity and highlighted the main consequence (veto/blocking and slow progress). All four options kept plausible.",
      "content_preserved": true,
      "source_article": "World Trade Organization",
      "x": 1.2751284837722778,
      "y": 0.8449960947036743,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Most-Favoured-Nation and national treatment principles with exceptions (how they govern non-discriminatory trade and why exceptions exist)",
        "Concept 2: Dispute settlement and negotiation framework (how the WTO resolves disputes and negotiates agreements)",
        "Concept 3: Governance and decision-making structure (the roles of the Ministerial Conference, General Council, and Secretariat, and how consensus-based decisions influence outcomes)"
      ],
      "original_question_hash": "bc4cead6"
    },
    {
      "question": "Why does a surface with lower albedo (i.e., it reflects less sunlight and absorbs more solar energy) typically produce stronger daytime convection and more vigorous weather?",
      "options": {
        "A": "Because greater absorbed solar energy warms the surface, increasing near-surface air buoyancy, evaporation, and moisture transport, which promotes vertical lifting, convection, and cloud development.",
        "B": "Because absorbing more solar energy paradoxically cools the surface, stabilizing the lower atmosphere and thus reducing convection and weather activity.",
        "C": "Because higher absorption significantly changes the spectral composition of incident sunlight, driving photochemical reactions that directly generate storms independent of surface heating.",
        "D": "Because increased absorption reduces the net outgoing longwave radiation to space, which directly suppresses buoyant overturning and convective motions."
      },
      "correct_answer": "A",
      "simplification_notes": "Question phrasing simplified and albedo defined; kept technical terms (albedo, buoyancy, convection). Options reworded to be concise and all remain plausible distractors; core physical mechanism preserved.",
      "content_preserved": true,
      "source_article": "Solar energy",
      "x": 1.5914021730422974,
      "y": 0.8701194524765015,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Solar radiation balance and its climate implications (input, reflection, absorption; driving convection, weather, and climate).",
        "Concept 2: Passive versus active solar technologies (design and materials vs. devices like photovoltaics, CSP, and solar water heating) and how each captures/converts solar energy.",
        "Concept 3: Solar energy as the source for chemical energy via photosynthesis and its link to biomass and fossil fuels."
      ],
      "original_question_hash": "84e4597f"
    },
    {
      "question": "How do soil microorganisms make phosphorus available to plants, and why do soil factors like pH and moisture change that availability?",
      "options": {
        "A": "Microbes sequester P in fungal spores or storage structures and plants access it only when those propagules germinate, so availability is largely independent of soil pH and moisture.",
        "B": "Microorganisms instantly convert all soil phosphorus into inorganic phosphate, so plant-available phosphate equals total soil P and does not depend on pH or moisture.",
        "C": "Microorganisms mineralize organic phosphorus and solubilize mineral phosphates, releasing inorganic phosphate in plant-available forms; the rate depends on microbial activity, which is governed by soil moisture, available carbon, and pH.",
        "D": "Microbes immobilize phosphorus into stable organo-mineral compounds that remain effectively unavailable to plants, so fertility depends solely on external phosphorus inputs."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; used ‘microbes’/‘microorganisms’ interchangeably; emphasized key microbial processes (mineralization, solubilization) and explicitly linked pH, moisture, and carbon supply to microbial activity and chemical solubility. Distractors were made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Soil fertility",
      "x": 1.7893953323364258,
      "y": 0.9712038636207581,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Fertility arises from providing essential nutrients and water in the right amounts/proportions, which depend on soil properties (depth, drainage, organic matter, pH) and absence of toxins.",
        "Concept 2: Soil fertility is maintained or degraded through management practices (e.g., erosion control, conservation), illustrating cause-effect relationships between land management and soil quality.",
        "Concept 3: Biological and chemical forms of nutrients (microorganisms, plant-available nutrient forms, and fertilizer composition like N-P-K) govern nutrient availability and uptake, linking soil biology with nutrient supply."
      ],
      "original_question_hash": "dce40365"
    },
    {
      "question": "Why are normative ethics, metaethics, and applied ethics taught as separate but connected branches, and how do they typically influence one another in practice?",
      "options": {
        "A": "They address the same moral questions only in different historical eras, so each branch is independent and does not affect the others.",
        "B": "Normative ethics develops general principles of right action; metaethics investigates the status and justification of those principles (for example, whether moral claims are objective or relative); applied ethics applies normative principles to concrete cases — and changes in metaethical views can alter which normative principles are regarded as credible, thereby affecting applied judgments.",
        "C": "Applied ethics decides what is right across all situations, normative ethics only evaluates consequences, and metaethics is concerned exclusively with the semantics of moral language.",
        "D": "Metaethics directly prescribes what people should do, normative ethics is mainly about factual claims, and applied ethics proceeds without regard to metaethical positions."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified roles: normative = principles, metaethics = status/justification, applied = application; removed historical/extra examples while keeping the causal influence of metaethics on normative and applied ethics.",
      "content_preserved": true,
      "source_article": "Ethics",
      "x": 1.2051546573638916,
      "y": 1.0139809846878052,
      "level": 2,
      "concepts_tested": [
        "The relationship and distinctions among normative ethics, applied ethics, and metaethics, including their aims and interconnections",
        "The central normative theories (consequentialism, deontology, virtue ethics) and their mechanisms for determining right action",
        "Metaethics and value theory: questions about moral objectivity and the nature of value (intrinsic vs. instrumental) and their connection to moral judgment and psychology"
      ],
      "original_question_hash": "894aaab1"
    },
    {
      "question": "In autoregulation, when local tissue activity rises and produces metabolic byproducts (e.g., CO2 and H+), how does the microvasculature respond, and why does that response help maintain tissue perfusion despite changes in systemic arterial pressure?",
      "options": {
        "A": "Local arteriolar smooth muscle relaxes in response to metabolic and endothelial signals (elevated CO2 and H+), causing vasodilation and reduced local resistance; this increases flow to match demand and helps keep tissue perfusion relatively constant across a range of arterial pressures.",
        "B": "Arterioles constrict in response to CO2 and H+, diverting blood away from the active tissue and thereby reducing local perfusion.",
        "C": "The body preserves perfusion by increasing heart rate and cardiac output so more blood is forced through the same local resistance, without local vasomotor change.",
        "D": "Autoregulation is mediated solely by central neural reflexes; local chemical signals like CO2 and H+ do not change arteriolar tone."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified: emphasized metabolic vasodilation (CO2/H+ → smooth muscle relaxation → lower local resistance) and its role in stabilizing perfusion; distractors rewritten to remain plausible. Removed unrelated rheology/math detail while keeping core hemodynamic concept.",
      "content_preserved": true,
      "source_article": "Hemodynamics",
      "x": 1.9883413314819336,
      "y": 1.143000841140747,
      "level": 2,
      "concepts_tested": [
        "Autoregulation and control mechanisms in hemodynamics: how homeostatic control adjusts blood flow and pressure in response to changing conditions.",
        "Non-Newtonian behavior of blood and the role of vessel compliance: why classical hydrodynamics alone is insufficient and how rheology and vessel elasticity influence flow.",
        "Osmotic pressure, hematocrit, and red blood cell deformability as determinants of circulatory mechanics: how osmotic shifts affect water distribution, cell concentration, and the mechanical properties of blood under flow."
      ],
      "original_question_hash": "46b66208"
    },
    {
      "question": "In diffusion of innovations, why does reaching a critical mass make diffusion self‑sustaining, and which mechanism best explains the shift from early adopters to broad, widespread uptake?",
      "options": {
        "A": "Critical mass mainly lowers unit production and distribution costs so the innovator can flood the market and coerce adoption.",
        "B": "Critical mass generates social proof and network effects that reduce uncertainty for remaining potential adopters and produce a positive feedback loop.",
        "C": "Critical mass eliminates the information needs of late adopters so they adopt automatically without further evaluation.",
        "D": "Critical mass forces a recategorization of individuals into new adopter types, making the original adopter categories obsolete and accelerating diffusion."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the original stem to a single clear question; kept key terms (critical mass, self‑sustaining, early adopters, widespread uptake); clarified mechanisms in options using academic terminology (social proof, network effects, uncertainty, positive feedback).",
      "content_preserved": true,
      "source_article": "Diffusion of innovations",
      "x": 1.3068954944610596,
      "y": 1.001360297203064,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Diffusion is a time-extended, channel-mediated process that occurs within a social system.",
        "Concept 2: Adopter categories and innovativeness shape when and how quickly individuals adopt an innovation (the diffusion curve and the innovation-decision process).",
        "Concept 3: Dynamic thresholds (critical mass and the marketing chasm) determine when diffusion becomes self-sustaining and widely adopted."
      ],
      "original_question_hash": "85947739"
    },
    {
      "question": "By what mechanism do social determinants of health (SDOH) produce health inequities between population groups?",
      "options": {
        "A": "They primarily determine genetic predispositions that differ between groups, which would explain all observed disparities.",
        "B": "They influence who is exposed to health risks and who has access to resources (material, social, institutional) that prevent, buffer, or help people adapt to those risks, thereby changing the distribution of health outcomes across groups.",
        "C": "They affect only access to advanced medical technologies and treatments, without altering exposure to risks or resilience.",
        "D": "They impact health only during early life stages, producing no lasting effects on health across the lifespan."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified to focus on mechanism; answer B rephrased to explicitly state exposure + access to buffering resources; distractors made concise but still plausible.",
      "content_preserved": true,
      "source_article": "Population health",
      "x": 1.22960364818573,
      "y": 0.9115819931030273,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Social determinants of health (SDOH) shape health outcomes and drive inequities across populations.",
        "Concept 2: Population health as a group-level approach (outcomes, determinants, and policies/interventions) and how interventions can alter the distribution of health across populations.",
        "Concept 3: A broadened definition of health (capacity to adapt/cope) and its implications for measuring and improving population health beyond the absence of disease."
      ],
      "original_question_hash": "c276afa7"
    },
    {
      "question": "Which statement best explains why forgiveness is described as an intentional, voluntary process that transforms one’s feelings toward an offender?",
      "options": {
        "A": "Because it depends on external punishment or legal redress to ‘reset’ a person’s emotions so the offense no longer matters.",
        "B": "Because it requires a conscious reappraisal and internal work that reduces resentment and cultivates greater tolerance or inner peace, and can occur regardless of whether the offender apologizes.",
        "C": "Because it is simply forgetting the offense and pretending it had no impact.",
        "D": "Because it inevitably guarantees full reconciliation with the offender and complete restoration of the relationship."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem to be concise and undergraduate-appropriate; clarified the contrast between internal, voluntary change and alternatives (punishment, forgetting, forced reconciliation); kept the original distinctions from the article.",
      "content_preserved": true,
      "source_article": "Forgiveness",
      "x": 1.280590295791626,
      "y": 0.9940819144248962,
      "level": 2,
      "concepts_tested": [
        "Forgiveness as a voluntary, transformative process that changes feelings/attitudes to overcome the offense.",
        "Conceptual relationships and distinctions between forgiveness and related ideas (condoning, excusing, forgetting) and implications for outcomes like peace or reconciliation.",
        "Existence and role of theoretical frameworks/models of forgiveness (e.g., Enright’s 20-Step Process) across disciplines."
      ],
      "original_question_hash": "cf201c04"
    },
    {
      "question": "Why does independence of a central bank typically increase its policy credibility and improve macroeconomic outcomes?",
      "options": {
        "A": "Because independence lets the central bank focus on long-run objectives (e.g. low inflation) without being forced to respond to short-term political pressures or noisy short-run data.",
        "B": "Because independence reduces the time-inconsistency problem: the bank can make a credible commitment to a rule or objective (for example price stability), which anchors expectations and helps stabilize inflation and output.",
        "C": "Because an independent central bank will always produce better macroeconomic outcomes under every type of shock simply because it is insulated from political influence.",
        "D": "Because independence automatically causes fiscal and monetary authorities to coordinate policy effectively, even without explicit communication or formal agreement."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording shortened and clarified; retained technical term 'time-inconsistency' and phrase 'anchors expectations'; removed extraneous background detail while keeping each option plausible.",
      "content_preserved": true,
      "source_article": "Central bank",
      "x": 0.6897728443145752,
      "y": 0.4410378634929657,
      "level": 2,
      "concepts_tested": [
        "Central bank independence and governance: how independence from political interference affects policy credibility and outcomes.",
        "Mechanisms of monetary and financial stability: monopoly on extending the monetary base plus regulatory/supervisory powers to prevent bank runs and maintain stability.",
        "Role of macroeconomic forecasting in policy decisions: how forecasting informs monetary policy, especially during economic turbulence."
      ],
      "original_question_hash": "13d01193"
    },
    {
      "question": "Why does tort law's principal aim—compensating victims for harm—lead courts to use monetary damages as the usual remedy, while injunctions are reserved for continuing or threatened harms?",
      "options": {
        "A": "Because damages compensate actual losses and restore the victim's position, whereas injunctions are used to prevent or stop future or ongoing harm that monetary compensation alone cannot adequately remedy.",
        "B": "Because the primary objective of tort law is to punish wrongdoers, so injunctions are the main remedy and damages are mostly symbolic.",
        "C": "Because tort and contract law use an identical set of remedies, meaning damages in tort function exactly the same as damages in contract.",
        "D": "Because civil law jurisdictions codify every remedy in a single civil code, making injunctive relief unnecessary in tort."
      },
      "correct_answer": "A",
      "simplification_notes": "Shortened and clarified the question into a single sentence, emphasised the remedial rationale (compensation vs prevention), removed historical and jurisdictional detail while keeping the core legal contrast between damages and injunctions; distractors were kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Tort",
      "x": 1.2646006345748901,
      "y": 0.8066885471343994,
      "level": 2,
      "concepts_tested": [
        "The primary objective of tort law is to compensate victims for harm, influencing what counts as liability and the choice of remedies.",
        "Remedies in tort (damages as the main remedy; injunctions for ongoing harms; specific performance in certain nuisance cases) and when each remedy is appropriate.",
        "The relationship and distinctions between tort, contract, and criminal law, including how obligations can arise independently of contracts and how different legal traditions shape tort liability (common law vs civil code, precedent vs codification)."
      ],
      "original_question_hash": "2db68b0d"
    },
    {
      "question": "Why is foreseeability essential to establishing a duty of care in negligence, and how does it shape what counts as reasonable care?",
      "options": {
        "A": "It decides whether a defendant owed any duty by demanding absolute protection against all possible harms, regardless of their likelihood.",
        "B": "It confines the duty to harms a reasonable person could have foreseen in the same circumstances, thereby defining the standard of reasonable care.",
        "C": "It requires that the particular damages be foreseeable before any duty can be recognized; if damages were not foreseeable, no duty exists.",
        "D": "It turns the duty into a purely contractual obligation between the defendant and the plaintiff, dependent on express agreements."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording condensed and clarified; legal examples and case citations removed; technical idea (foreseeability limits duty and defines reasonable-person standard) preserved; distractors made plausible.",
      "content_preserved": true,
      "source_article": "Negligence",
      "x": 1.281967282295227,
      "y": 0.804893434047699,
      "level": 2,
      "concepts_tested": [
        "Duty of care as the foundational obligation and its link to reasonable care and foreseeability",
        "Causation (actual and proximate) and the connection between breach and damages",
        "The four-element framework (duty, breach, causation, damages) and how it varies across jurisdictions, including landmark principles shaping the doctrine"
      ],
      "original_question_hash": "bdc7328d"
    },
    {
      "question": "According to the perception-to-meaning pathway in visual communication, why can the same image be interpreted differently by viewers from different backgrounds?",
      "options": {
        "A": "Because the image contains a single, universal meaning that everyone decodes identically from its fixed perceptual features.",
        "B": "Because viewers' brains map visual features (color, shape, tone, texture, figure-ground, etc.) onto concepts using each person's prior knowledge, experiences, and cultural context, producing different interpretations.",
        "C": "Because viewers physically misperceive colors or shapes in unpredictable ways, so sensory errors create different meanings.",
        "D": "Because the designer's intended message always dominates perception, ensuring a constant meaning regardless of viewer background."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened for clarity and concision; technical phrase 'perception-to-meaning pathway' was retained and explained by mentioning mapping of visual features to concepts and the role of prior knowledge and culture. Options were rephrased to remain plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Visual communication",
      "x": 1.3478341102600098,
      "y": 1.0408433675765991,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Perception-to-meaning pathway in visual communication and how interpretation varies with viewer experience.",
        "Concept 2: The role of core visual design components (color, shape, tones, texture, figure-ground, balance, hierarchy) in shaping message effectiveness and comprehension.",
        "Concept 3: Evaluation of visual communication based on audience comprehension rather than aesthetic preference, and the absence of universal aesthetics principles."
      ],
      "original_question_hash": "8ec3708e"
    },
    {
      "question": "Why must information be organized at multiple scales, and what do large-, medium-, and small-scale design decisions each contribute to user understanding?",
      "options": {
        "A": "Large-scale decisions select relevant content for a given audience and purpose; medium-scale decisions structure each document or manual with an organizing principle (including overviews, concepts, examples, references, and definitions); small-scale decisions ensure logical development of topics, emphasize what is important, and apply clear writing, navigational cues, typography, and use of white space.",
        "B": "Large-scale decisions focus only on color and visual style; medium-scale decisions determine physical page size and layout dimensions; small-scale decisions handle punctuation and grammar only.",
        "C": "Large-scale decisions are limited to selecting platform constraints; medium-scale decisions merely arrange topics hierarchically without an overarching organizing principle; small-scale decisions are confined to choosing fonts.",
        "D": "The three scales operate independently; decisions at large, medium, and small scales do not interact and changing one scale has no effect on the others."
      },
      "correct_answer": "A",
      "simplification_notes": "Rephrased the question to be concise for undergraduates and clarified that scales work together; preserved the original hierarchical descriptions. Removed extra background wording while keeping precise role descriptions for each scale.",
      "content_preserved": true,
      "source_article": "Information design",
      "x": 1.3723323345184326,
      "y": 1.05388343334198,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Explanation design — information design as a practice that explains facts to lead to knowledge and informed action.",
        "Concept 2: Multi-scale organization mechanics — at large scale (content selection by audience/purpose), medium scale (structuring within manuals), and small scale (clear writing, navigational cues, typography, white space).",
        "Concept 3: Relationships with related disciplines — overlaps and connections with data visualization, information architecture, and communication/design practices, shaping how information is designed for digital and print contexts."
      ],
      "original_question_hash": "d57805fb"
    },
    {
      "question": "How does typography communicate ideas, feelings, and attitudes beyond the literal words, and what mechanism enables that effect?",
      "options": {
        "A": "By changing the core semantic content: altering letterforms so the words themselves have different meanings.",
        "B": "By establishing mood and emphasis through the personality of the typeface, spacing (tracking/kerning/leading), and contrast; these visual cues guide the reader’s interpretation beyond the literal words.",
        "C": "By eliminating ambiguity: presenting text in a neutral, standard font so every reader interprets the message uniformly.",
        "D": "By serving only a decorative function: affecting aesthetics but not the meaning or emotional tone of the text."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the stem in clear academic language for undergraduates; kept technical typographic terms (tracking, kerning, leading) and emphasized visual cues as the mechanism. Options rephrased to be concise and plausible.",
      "content_preserved": true,
      "source_article": "Graphic design",
      "x": 1.2754758596420288,
      "y": 1.0417457818984985,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The graphic designer as encoder/interpreter of messages, transforming linguistic content into visual manifestations to convey meaning.",
        "Concept 2: The role of typography, composition, ornamentation, and imagery in conveying ideas, feelings, and attitudes beyond language.",
        "Concept 3: Graphic design as an interdisciplinary field with broad applications (not limited to sketching), integrating arts, communication, and human factors across domains."
      ],
      "original_question_hash": "4bd6f07f"
    },
    {
      "question": "In non-equilibrium thermodynamics the macroscopic entropy at time $t$ is written as $S(t)=\\int s(x,t)\\,dV$, where $s(x,t)$ is a locally defined entropy density. Why is it valid to reconstruct the total entropy by this spatial integral, and what essential assumption does this rely on?",
      "options": {
        "A": "Because the system is in global equilibrium everywhere, so the local entropy density equals the global entropy and the integral is trivial.",
        "B": "Because each infinitesimal volume element is assumed to be in local thermodynamic equilibrium, so $s(x,t)$ is a function only of the local state variables; integrating $s(x,t)$ over the volume gives the total entropy, whose time change is determined by local entropy production and transport.",
        "C": "Because entropy is a purely global quantity that does not vary with position, so integrating any local quantity over space always returns the same global entropy regardless of gradients or fluxes.",
        "D": "Because integrating local densities yields an average entropy per unit volume; this reconstruction is valid only when there are no spatial gradients or fluxes so that the density is uniform."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the question concisely, introduced the integral $S(t)=\\int s(x,t)\\,dV$ in inline LaTeX, and clarified that the key assumption is local thermodynamic equilibrium; options were rephrased to be concise and plausible.",
      "content_preserved": true,
      "source_article": "Non-equilibrium thermodynamics",
      "x": 1.7576830387115479,
      "y": 1.0724408626556396,
      "level": 2,
      "concepts_tested": [
        "Local equilibrium and its role in connecting non-equilibrium behavior to equilibrium thermodynamics",
        "Time-courses, transport processes, and reaction rates as central to non-equilibrium dynamics",
        "Local entropy density and its integral to obtain macroscopic entropy in non-equilibrium systems"
      ],
      "original_question_hash": "53273e7b"
    },
    {
      "question": "How does dividing labor on an assembly line primarily increase production throughput and consistency?",
      "options": {
        "A": "By assigning each worker a narrow, repetitive task so they can perform it rapidly and accurately, reducing task-switching and allowing precise timing between steps, which lowers cycle time and variation.",
        "B": "By having every worker trained to perform every step of production so they can switch tasks as needed, maximizing flexibility and overall speed.",
        "C": "By centralizing control and real-time decision-making to continuously optimize the entire process and eliminate bottlenecks.",
        "D": "By increasing the variety of tasks each worker performs to keep them engaged and reduce fatigue, which speeds up the line."
      },
      "correct_answer": "A",
      "simplification_notes": "Question language was tightened to focus on the core mechanism (division of labor on assembly lines). Extraneous historical context was removed; options rephrased to be concise and plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Mass production",
      "x": 1.444091558456421,
      "y": 0.987823486328125,
      "level": 2,
      "concepts_tested": [
        "Standardization and interchangeable parts enable rapid, repeatable production and easier maintenance.",
        "Assembly line and division of labor as a mechanism to increase throughput, consistency, and efficiency.",
        "Economies of scale and market impact: how mass production shifts supply-demand dynamics and competitiveness in markets."
      ],
      "original_question_hash": "c22f6350"
    },
    {
      "question": "How does peer review function as a form of self-regulation that maintains quality standards and professional credibility within a field?",
      "options": {
        "A": "Practitioners with relevant expertise evaluate each other’s work against shared professional standards, give corrective feedback, enable iterative improvement and credentialing decisions, and signal competence to peers and the public.",
        "B": "It replaces formal standards with individual reviewers’ personal opinions, reducing consistency and institutional control.",
        "C": "It concentrates decision-making among well‑connected individuals who shape standards to increase prestige, which can introduce bias into outcomes.",
        "D": "It mainly serves marketing, reputation management, or branding rather than rigorously evaluating methods, results, or competence."
      },
      "correct_answer": "A",
      "simplification_notes": "Condensed the question to focus on the mechanism of self-regulation; removed historical and peripheral details; rephrased options into concise, plausible positive and critical alternatives while preserving the tested concept.",
      "content_preserved": true,
      "source_article": "Peer review",
      "x": 1.2635505199432373,
      "y": 0.990993857383728,
      "level": 2,
      "concepts_tested": [
        "Peer review as a mechanism of self-regulation to uphold quality standards and credibility",
        "The relationship between peer review and professional outcomes (credentialing, licensure, tenure, advancement)",
        "Variations of peer review across domains (clinical vs scholarly vs educational) and fields, and their common underlying purposes (quality improvement and learning)"
      ],
      "original_question_hash": "956961ae"
    },
    {
      "question": "Why is it advantageous to separate the technical authority that manages core Internet identifiers (e.g., DNS root, IP address allocations) from the separate policy-making processes that determine how those identifiers are used across borders?",
      "options": {
        "A": "It preserves technical stability and global interoperability by keeping a stable, operational authority for identifiers while allowing diverse stakeholders to shape cross-border usage policies.",
        "B": "It removes centralized decision-making and lets market forces determine policies, which can speed decisions but increases the risk of fragmentation and inconsistent practices.",
        "C": "It requires every country to run its own root zone and address registries to protect sovereignty, preserving national control but undermining global consistency.",
        "D": "It turns technical identifier management into a primarily political process to increase legitimacy, which can compromise operational reliability."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording was shortened and focused on the separation of operational technical functions (DNS/IP) from cross-border policy-making; options were made more concise and kept plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Internet governance",
      "x": 1.3621091842651367,
      "y": 1.0061968564987183,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Multistakeholder governance model (how diverse actors collaborate to create shared policies and standards)",
        "Concept 2: Role and transition of technical governance institutions (ICANN/IANA/NTIA) and the balance between centralized control of core identifiers and distributed policy-making",
        "Concept 3: Interoperability and public-good orientation (why shared principles, norms, and rules are needed to maintain global reach and functioning of the Internet)"
      ],
      "original_question_hash": "184e8971"
    },
    {
      "question": "How do randomization and replication together enable valid causal inference and precise estimation in an experiment?",
      "options": {
        "A": "Randomization balances both known and unknown confounding factors across treatment groups on average, so observed treatment differences reflect causal effects; replication provides multiple independent estimates of the effect, allowing estimation of sampling variability and improving precision and statistical power.",
        "B": "Randomization guarantees there will be no measurement error, and replication guarantees that repeated trials will produce identical results.",
        "C": "Randomization increases the magnitude of the observed effect, while replication reduces the observed effect size.",
        "D": "Randomization affects only external validity (generalizability), and replication affects only internal validity (the ability to make causal claims)."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the stem for clarity and concision; kept technical terms (randomization, replication, confounding, sampling variability, precision, power). Options B–D were made plausible but incorrect distractors.",
      "content_preserved": true,
      "source_article": "Design of experiments",
      "x": 1.5757631063461304,
      "y": 1.1095703840255737,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Independent, dependent, and control variables and their roles in isolating causal effects; why controlling extraneous factors is necessary.",
        "Concept 2: Randomization and replication as mechanisms to enable valid inference and adequate statistical power; how these reduce bias and variability.",
        "Concept 3: Validity, reliability, and replicability as core aims of experimental design; how design choices and documentation influence them."
      ],
      "original_question_hash": "68ed8651"
    },
    {
      "question": "When a state is much stronger than a non-state actor, why does the weaker actor typically adopt asymmetric tactics (e.g., guerrilla warfare, terrorism) instead of trying conventional, open-field battles? Describe the causal mechanism by which the power imbalance shapes the choice of tactics.",
      "options": {
        "A": "Because asymmetric warfare guarantees immediate battlefield victories against the stronger state.",
        "B": "Because direct conventional confrontation is costly and unlikely to produce political concessions; the weaker actor uses mobility, local knowledge, clandestine operations, and attacks on the state's legitimacy to impose costs and coerce concessions, thereby partially offsetting the power gap.",
        "C": "Because the stronger state can quickly adapt to guerrilla tactics, so the weaker actor usually prefers to surrender to minimize losses.",
        "D": "Because the power asymmetry makes large-scale conventional offensives by the non-state actor trivially effective at overwhelming the state's forces."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was tightened and academic phrasing applied; the question now asks directly for the causal mechanism linking power imbalance to tactic choice. Technical examples (guerrilla warfare, terrorism) retained. Distractors were kept plausible but incorrect. Correct answer letter preserved.",
      "content_preserved": true,
      "source_article": "Political violence",
      "x": 1.1864755153656006,
      "y": 0.8680983781814575,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Power asymmetry between actors leads to asymmetric warfare (e.g., guerrilla warfare, terrorism) as a strategic response.",
        "Concept 2: Violence is used as a strategic instrument to achieve political objectives, motivated by beliefs about necessity and legitimacy.",
        "Concept 3: The relational framework of political violence (state vs. state, state vs. civilians, non-state actors vs. states/civilians) influences the choice of tactics and targets."
      ],
      "original_question_hash": "8042885e"
    },
    {
      "question": "Why does the Basel framework place discretionary supervisory powers under Pillar 2 instead of relying only on Pillar 1's standardized capital and liquidity rules?",
      "options": {
        "A": "Because Pillar 2 lets supervisors use judgment to require additional capital and liquidity buffers tailored to a bank's specific risk profile, governance and internal-control weaknesses, so requirements reflect actual risk beyond the standardized rules.",
        "B": "Because Pillar 2 replaces Pillar 1 during periods of stress, enabling regulators to swap in different rules on the fly to prevent panic.",
        "C": "Because Pillar 2 is mainly about market disclosure and market discipline, reducing the need for fixed regulatory standards.",
        "D": "Because Pillar 2 applies only to very large, systemically important banks, whereas Pillar 1 imposes uniform rules on all banks."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened and technical distinctions clarified (Pillar 1 = standardized rules; Pillar 2 = supervisory judgment; Pillar 3 = disclosure). Distractors were kept plausible by reflecting common misunderstandings.",
      "content_preserved": true,
      "source_article": "Banking regulation and supervision",
      "x": 1.3108012676239014,
      "y": 0.8483273983001709,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Prudential regulation and supervision aim to ensure banks are viable and resilient by imposing risk controls, capital and liquidity requirements, large-exposure limits, and disclosure, to reduce bank failures and systemic risk.",
        "Concept 2: The distinction between regulation (rule-setting) and supervision (oversight with discretionary judgment), exemplified by the Basel Pillars: Pillar 1 regulation, Pillar 2 supervisory discretion, and Pillar 3 market discipline (disclosure).",
        "Concept 3: Microprudential versus macroprudential perspectives, i.e., regulation targeting individual banks versus the broader financial system and systemic risk."
      ],
      "original_question_hash": "4d5cf3f7"
    },
    {
      "question": "Which explanation best reconciles that dielectric strength is an intrinsic bulk property of a material, yet measured breakdown voltages depend on electrode geometry and how the field is applied?",
      "options": {
        "A": "The intrinsic dielectric strength is a bulk property (the field $E$ at which bound electrons become mobile), but real devices have local field enhancements at electrode edges, grain boundaries, surface defects or interfaces. Those local increases in $E$ can trigger avalanche ionization and breakdown earlier, so geometry and field application determine where and when breakdown occurs.",
        "B": "The intrinsic dielectric strength is determined only by the electrode materials and their geometry; the bulk properties of the dielectric do not influence breakdown behavior.",
        "C": "Practical breakdown voltage is always directly proportional to electrode separation and therefore independent of how the electric field is distributed inside the material.",
        "D": "Dielectric strength is only well defined at absolute zero; at room temperature breakdown is dominated by thermal effects and is unaffected by local field concentration or electrode configuration."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording tightened to undergrad level, emphasized intrinsic bulk $E$ vs local field enhancements and avalanche initiation; removed excess detail but kept technical terms (avalanche, grain boundaries, field $E$).",
      "content_preserved": true,
      "source_article": "Dielectric strength",
      "x": 1.7709332704544067,
      "y": 1.0386046171188354,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Dielectric strength as an intrinsic bulk property versus practical breakdown dependent on electrode configuration and field application.",
        "Concept 2: Avalanche breakdown mechanism in solids—how high electric fields liberate bound electrons, increase free charge carriers, and rapidly form a conductive path.",
        "Concept 3: Relationship between electric field, charge carriers, and insulating behavior—how a rise in field increases carriers and lowers resistivity until breakdown occurs."
      ],
      "original_question_hash": "dfc1343d"
    },
    {
      "question": "In STS, why is the development and deployment of a technology described as socially embedded rather than determined only by its technical merits?",
      "options": {
        "A": "Because market forces always favour the most technically efficient designs, so social factors do not affect which technologies succeed.",
        "B": "Because every technology follows the same, universal route from invention to widespread use that is independent of cultural or institutional context.",
        "C": "Because technical design decisions, user practices, regulatory and policy contexts, and cultural meanings interact as a network that jointly shapes which technologies persist and how they are used.",
        "D": "Because scientists and engineers can fully predict all social responses to a technology before it is developed, so social outcomes are simply planned technical consequences."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording tightened for clarity; removed extended historical examples and jargon; retained core contrast between social embedding and technological determinism and preserved key causal factors (design, users, policy, culture).",
      "content_preserved": true,
      "source_article": "Science and technology studies",
      "x": 1.2155511379241943,
      "y": 0.9846143126487732,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Social embeddedness of science and technology",
        "Why/how contexts (historical, cultural, social) shape the creation, development, and consequences of science and technology.",
        "Concept 2: Interdisciplinary critique of technodeterminism",
        "Why/how STS draws from multiple disciplines to study science/tech and challenge the idea that technology deterministically drives society.",
        "Concept 3: Frameworks shaping inquiry (historical/philosophical/feminist perspectives)",
        "Why/how theoretical lenses (e.g., Kuhn’s paradigm shifts; feminist critiques) affect what is studied in STS and how findings are interpreted."
      ],
      "original_question_hash": "d5f9bf01"
    },
    {
      "question": "Why does increasing confining pressure with depth favor plastic/ductile flow over brittle fracture in rocks, and how does this connect to the decomposition of the stress tensor into an isotropic pressure and a deviatoric (shear) part under gravity (i.e. $\\sigma=-pI+\\sigma'$ where $p=-\\tfrac{1}{3}\\mathrm{tr}(\\sigma)$)?",
      "options": {
        "A": "Because the isotropic pressure $p$ grows with depth under gravity, the stress state becomes more hydrostatic and suppresses crack initiation and propagation; with high confining pressure fractures close and rocks accommodate strain by ductile/plastic flow controlled by the deviatoric stress $\\sigma'$, so gravity produces a depth-dependent shift from brittle to ductile behaviour.",
        "B": "Because the shear (deviatoric) component $\\sigma'$ necessarily increases with depth, leading to more brittle fracturing regardless of confining pressure.",
        "C": "Because temperature alone determines whether rocks deform ductilely or brittly, so gravity and confining pressure are irrelevant to the deformation mode.",
        "D": "Because hydrostatic equilibrium implies no deformation can occur, so depth and the change in confining pressure do not influence whether deformation is brittle or ductile."
      },
      "correct_answer": "A",
      "simplification_notes": "Question shortened and clarified; stress decomposition $\\sigma=-pI+\\sigma'$ added in-line; language made concise and aimed at undergraduates while preserving the original reasoning about confining pressure, hydrostatic stress, and deformation mode.",
      "content_preserved": true,
      "source_article": "Geodynamics",
      "x": 1.7601706981658936,
      "y": 0.9551627039909363,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Deformation modes of rocks (elastic, plastic, brittle) and how stress magnitude, material properties, and gravity influence which mode dominates; distinction between pressure (volume change) and shear (shape change) and the idea of hydrostatic equilibrium when shear is absent.",
        "Concept 2: Mantle convection as the driving mechanism behind plate tectonics and associated geologic phenomena (seafloor spreading, mountain building, earthquakes).",
        "Concept 3: Use of the continuous medium approximation and equilibrium stress fields to model Earth's interior, enabling an average or large-scale response despite heterogeneity and complex local behavior."
      ],
      "original_question_hash": "d540020e"
    },
    {
      "question": "What physical mechanism explains how heat is transported from Earth's deep mantle to the surface, producing upwelling at mid-ocean ridges and downwelling at subduction zones?",
      "options": {
        "A": "Heat is transported mainly by solid-state diffusion through mantle minerals, which somehow organizes into rising and sinking flow beneath ridges and trenches.",
        "B": "Buoyancy-driven convection: temperature- and sometimes composition-dependent density contrasts cause hot, less-dense mantle to rise beneath spreading centers and cold, dense lithosphere to sink at subduction zones, producing a continuous convective circulation.",
        "C": "Uniform gravity means convection is driven primarily by surface tidal friction and external forcing, which creates ridges and trenches rather than internal buoyancy.",
        "D": "The lithosphere moves independently while the underlying mantle is essentially static; plate motions are driven by forces within the plates themselves rather than mantle convection."
      },
      "correct_answer": "B",
      "simplification_notes": "Question was made more concise and focused on the core mechanism (heat transport by mantle flow). Extraneous details about tomographic evidence, phase changes, and plume debates were removed; options were rewritten to remain plausible at undergraduate level while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Mantle convection",
      "x": 1.8571827411651611,
      "y": 0.9443387985229492,
      "level": 2,
      "concepts_tested": [
        "Mantle convection as the heat-transport mechanism driving plate tectonics, via upwelling at spreading centers and downwelling/subduction at plate boundaries",
        "The whole mantle convection vs layered mantle convection debate and the evidence (seismic tomography, gravity) that informs it",
        "The connection between subduction, lower-mantle interactions (e.g., phase transitions, D\" layer heterogeneities), and surface volcanism/plume dynamics"
      ],
      "original_question_hash": "4cdfc247"
    },
    {
      "question": "If barriers to entry in a market are lowered, how do competitive dynamics change and why does that occur?",
      "options": {
        "A": "It increases aggregate demand by raising consumers' willingness to pay, which forces incumbents to cut prices to retain market share.",
        "B": "It increases the number of potential rivals, intensifying competition; firms must compete on price and productive efficiency so prices move closer to marginal cost and innovation is incentivized.",
        "C": "It decreases information symmetry between buyers and sellers, producing wider price dispersion as firms exploit asymmetric information.",
        "D": "It shifts resources and market power toward established incumbents who can pre-empt new entrants, reinforcing entry barriers and stabilizing prices."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording shortened and focused on 'lowered barriers to entry'; preserved technical terms (marginal cost, information symmetry, incumbents); options kept plausible distractors; correct answer unchanged.",
      "content_preserved": true,
      "source_article": "Competition (economics)",
      "x": 1.3064930438995361,
      "y": 0.9445412755012512,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Competition as a driver of prices and innovation; higher competition tends to lower prices and spur development of new products and technologies.",
        "Concept 2: Structural determinants of competition; the number of firms, barriers to entry, information availability, and resource access shape the level of competition, with buyer demand (willingness to pay) also influencing outcomes.",
        "Concept 3: Inter-firm networks, geographic clustering, and externalities as mechanisms that create competitive advantages and affect market dynamics."
      ],
      "original_question_hash": "b1536af0"
    },
    {
      "question": "According to role theory, why do changing external conditions often lead to changes in a social role?",
      "options": {
        "A": "Because individuals' preference for stability causes them to resist shifts in legitimacy or incentives, so roles remain unchanged despite external changes.",
        "B": "Because altered external conditions change the perceived legitimacy and the expected rewards and costs of the role, which reshapes conformity costs and sanctions and can make an alternative role more advantageous.",
        "C": "Because sanctions become universal and purely punitive under new conditions, which enforces role maintenance regardless of changes in legitimacy or rewards.",
        "D": "Because fixed biological constraints prevent people from adapting their roles, so external changes cannot produce role shifts."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened for clarity and concision; technical terms from the article (legitimacy, rewards/costs, conformity costs, sanctions) were retained; distractors were kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Role",
      "x": 1.2509902715682983,
      "y": 0.9987645149230957,
      "level": 2,
      "concepts_tested": [
        "Concept 1: A social role is a bundle of connected behaviors, rights, obligations, beliefs, and norms that guide expectations and actions in a social situation.",
        "Concept 2: Mechanisms that sustain or alter roles, including conformity costs, sanctions, rewards, legitimacy, and pressures from changing conditions leading to role change.",
        "Concept 3: Distinctions among role types (achieved vs. ascribed; semi-permanent vs. transitory) and the implications these distinctions have for role adoption and boundaries (e.g., biological or sociological constraints)."
      ],
      "original_question_hash": "377cdd1e"
    },
    {
      "question": "In distance education, why is timely instructor feedback considered essential for effective learning?",
      "options": {
        "A": "Because it shortens the delay between a learner's action and its evaluation, allowing quick correction of misunderstandings and supporting self-regulated learning when teacher and student are separated in time or space.",
        "B": "Because feedback primarily functions as a motivational reward that increases engagement regardless of whether it targets specific learning goals.",
        "C": "Because timely feedback mainly lengthens courses and increases instructor input, and that extra time on task is the primary reason students learn better.",
        "D": "Because feedback is relevant only for summative assessment (final grades) and does not affect students' ongoing study or correction of errors."
      },
      "correct_answer": "A",
      "simplification_notes": "Question phrasing shortened and focused on the core mechanism: rapid correction and self-regulation when instructor and student are separated; distractors reworded to be plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Distance education",
      "x": 1.2630804777145386,
      "y": 0.9937998652458191,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Feedback as a crucial mechanism that enables effective learning in distance education (e.g., timely feedback compensating for lack of in-person interaction).",
        "Concept 2: Technology- and infrastructure-driven scalability and access (e.g., how mailing systems enabled early distance education and how later technologies (video, TV, Internet) expanded reach).",
        "Concept 3: Interactivity and multiple learning modalities as core elements of effective distance/e-learning (e.g., the need for interactive processes and varied learning modes to support learners)."
      ],
      "original_question_hash": "11040335"
    },
    {
      "question": "How do symbols act as \"vehicles of conception\" for knowledge, and why does that let people make judgments across different domains?",
      "options": {
        "A": "They encode universal truths as fixed, context-independent representations that remain unchanged by new evidence.",
        "B": "They provide a shared representational scaffold that links diverse experiences into common structures, enabling abstraction and inference across domains.",
        "C": "They store raw sensory inputs that each person must reinterpret every time, so judgments become highly subjective and unpredictable.",
        "D": "They confine meaning to a single domain, forcing people to learn separate rules for each situation instead of using broader patterns."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was made clearer and more concise for undergraduates; distractor options were paraphrased to remain plausible while preserving the original correct answer and core concepts.",
      "content_preserved": true,
      "source_article": "Symbol",
      "x": 1.2572916746139526,
      "y": 1.0852711200714111,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Symbols carry multiple levels of meaning and link otherwise different concepts/experiences, enabling complex communication.",
        "Concept 2: Symbols underpin human knowledge and judgments, serving as vehicles of conception for all knowledge.",
        "Concept 3: Symbols facilitate social coordination and identity through constitutive rhetoric, with interpretation and study framed by semiotics."
      ],
      "original_question_hash": "f201482b"
    },
    {
      "question": "How does culture affect the production (encoding) and interpretation (decoding) of nonverbal signals such as gestures, facial expressions, proxemics, and paralanguage (tone, pitch, rate)?",
      "options": {
        "A": "Culture makes nonverbal cues fully universal, so people across cultures encode and decode the same signals in the same way.",
        "B": "Culture shapes the conventional meanings of gestures, norms for using space, eye contact, and paralanguage, so it influences both how people produce signals (encoding) and how others interpret them (decoding).",
        "C": "Culture only affects decoding: receivers interpret signals differently, while senders encode nonverbal signals the same way everywhere.",
        "D": "Culture removes ambiguity by standardizing nonverbal channels within a society, so everyone in that culture uses identical nonverbal signals."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was tightened, technical terms (encoding/decoding, proxemics, paralanguage) were kept and briefly defined; examples added (gestures, eye contact, tone) to make options concrete. The original meaning and correct choice were preserved.",
      "content_preserved": true,
      "source_article": "Nonverbal communication",
      "x": 1.2810440063476562,
      "y": 1.0237635374069214,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Encoding and decoding of nonverbal signals and the influence of culture on how meanings are conveyed and interpreted.",
        "Concept 2: Paralanguage (tone, pitch, rate, loudness, speaking style) as a nonverbal mechanism that adds meaning to messages.",
        "Concept 3: The contextual and relational factors (environmental conditions, physical characteristics, interaction behaviors; relationship dynamics) that shape production and interpretation of nonverbal cues."
      ],
      "original_question_hash": "7b4a6214"
    },
    {
      "question": "Why do legislatures typically enact only a small fraction of the bills that are proposed?",
      "options": {
        "A": "Because the executive’s veto power alone decides outcomes, so only bills the executive supports become law.",
        "B": "Because bill passage requires debate, committee review, and amendments, and political incentives and leadership priorities mean only a limited subset are advanced to final passage, so many bills die or stall.",
        "C": "Because the judiciary reviews and blocks every proposed bill before it can be passed, preventing most from becoming law.",
        "D": "Because any introduced bill automatically becomes law unless defeated within a very short, fixed time window."
      },
      "correct_answer": "B",
      "simplification_notes": "Shortened and clarified the question; emphasized procedural causes (debate, committees, amendments, leadership priorities) while keeping separation-of-powers alternatives as distractors.",
      "content_preserved": true,
      "source_article": "Legislation",
      "x": 1.2123219966888428,
      "y": 0.7826802134513855,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Separation of powers as it relates to legislation (legislature drafts and enacts laws; judiciary interprets; executive acts within legal limits)",
        "Concept 2: The legislative process (from bill to law, including debate, amendments, and the reality that most bills are not enacted)",
        "Concept 3: Forms and legitimacy of law-making (primary vs. secondary legislation, referendums, orders in council, and the role of popular sovereignty)"
      ],
      "original_question_hash": "4332388d"
    },
    {
      "question": "Why do legal positivists insist there is no necessary connection between what the law is (the 'is') and what the law ought to be (the 'ought'), and how does that view shape their method for studying legal systems?",
      "options": {
        "A": "Because they ground legal validity in moral content, so they treat normative judgments as essential to any descriptive account of law.",
        "B": "Because they accept an is–ought separation, permitting descriptive and empirical study of law without assuming moral prescriptions, which enables more objective analysis.",
        "C": "Because they hold that law's authority derives from universal moral truths, making analytic and normative questions inseparable.",
        "D": "Because they view law primarily as a social practice founded on rights-based ideals, so they adopt cross-disciplinary empirical methods to assess legal function."
      },
      "correct_answer": "B",
      "simplification_notes": "Question phrasing shortened and clarified; introduced 'is–ought' terminology; emphasized methodological consequence (descriptive/empirical study) while preserving contrast with natural law and other schools.",
      "content_preserved": true,
      "source_article": "Jurisprudence",
      "x": 1.2245198488235474,
      "y": 0.8810527324676514,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Descriptive vs normative dimensions of law (analytic/positivism vs natural law) and their implications for whether law’s nature is independent of what law ought to be.",
        "Concept 2: The major jurisprudential schools as frameworks (natural law, legal positivism, sociological jurisprudence, experimental jurisprudence) and their core claims about the purpose and function of law.",
        "Concept 3: Law as a social institution that intersects with economics, ethics, history, sociology, and political philosophy, and the methodological implications of studying law through multiple disciplines."
      ],
      "original_question_hash": "d6f54356"
    },
    {
      "question": "If a market requires a fixed upfront cost that new firms must pay but incumbents do not, how does this barrier give incumbents market power?",
      "options": {
        "A": "It increases incumbents' marginal costs, forcing them to lower prices to stay competitive.",
        "B": "It creates a fixed entry hurdle that deters potential competitors and reduces rivalry, allowing incumbents to sustain higher prices.",
        "C": "It pushes new entrants to pursue rapid innovation or disruptive strategies, increasing competition and driving prices down.",
        "D": "It equalizes profitability between entrants and incumbents, eliminating any persistent price differences."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem to 'fixed upfront cost' and 'new firms' for clarity; shortened options while keeping each plausible and preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Barriers to entry",
      "x": 1.3081109523773193,
      "y": 0.925746738910675,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Barriers to entry create market power by imposing costs on new entrants that incumbents do not bear, thereby reducing competition and potentially raising prices.",
        "Concept 2: Barriers arise from both natural factors (e.g., brand loyalty, scarce public resources) and artificially created factors (e.g., government policies) and these sources influence how competition is affected and how policy responses differ.",
        "Concept 3: The existence and nature of barriers to entry have implications for antitrust policy and welfare, linking barriers to outcomes like monopolies/oligopolies and distortive pricing."
      ],
      "original_question_hash": "7f94455e"
    },
    {
      "question": "How does the requirement that state legislative districts have roughly equal populations implement the principle of \"one person, one vote\" in practice?",
      "options": {
        "A": "By making each representative accountable to a similar number of people so individual votes carry roughly the same weight and preventing vote dilution when district populations differ.",
        "B": "By requiring districts to be equal in geographic area so that rural and urban votes cover the same physical territory.",
        "C": "By guaranteeing that equal numbers of residents in each district produce identical voter turnout in every election.",
        "D": "By directing that district boundaries be drawn to maximize the chances of the majority party winning elections."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording was shortened and clarified; removed specific court case names and historical detail while keeping the core mechanism (equal-population districts prevent vote dilution). Options were rephrased to remain plausible distractors.",
      "content_preserved": true,
      "source_article": "One man, one vote",
      "x": 1.15899658203125,
      "y": 0.8498741984367371,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The principle of one person, one vote as a mechanism to ensure equal representation across districts through roughly equal populations.",
        "Concept 2: The legal framework enforcing equal voting weight (Equal Protection Clause; Reynolds v. Sims; Wesberry v. Sanders) and how court decisions shape districting practice.",
        "Concept 3: The relationship between population distribution (urban vs. rural, demographic shifts) and political power, and how redistricting responds to these changes to maintain equality."
      ],
      "original_question_hash": "a9625bd5"
    },
    {
      "question": "In an information retrieval system each candidate document receives a numeric relevance score for a query and results are shown in decreasing score order. Why can a small change in how the score is computed (for example reweighting features) change which documents appear in the top-$k$ results, even though many documents remain potentially relevant?",
      "options": {
        "A": "Because ranking depends on relative scores: changing feature weights alters document scores by different amounts, which can reorder documents and cause different items to cross into the top-$k$.",
        "B": "Because a small adjustment changes the estimated probability that each document is relevant, so previously lower-ranked documents may become top results.",
        "C": "Because IR systems store only metadata rather than full content, so changing the scoring on metadata fields will swap which documents are shown at the top.",
        "D": "Because user feedback or personalization automatically updates scores after any scoring tweak, which can produce a different top-$k$ independent of the original query."
      },
      "correct_answer": "A",
      "simplification_notes": "Simplified wording, introduced top-$k$ notation, kept the mechanism-focused core (score-based ranking) and made distractors plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Information retrieval",
      "x": 1.357558250427246,
      "y": 1.0854706764221191,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Query-to-object matching and ranking as a core mechanism; how relevance scores determine the order of results.",
        "Concept 2: Representation of documents by surrogates or metadata (instead of storing full content) and its role in enabling efficient retrieval.",
        "Concept 3: Iterative query refinement by users and how feedback loops influence subsequent results."
      ],
      "original_question_hash": "e8e6e2e8"
    },
    {
      "question": "Why does a spacecraft with the correct horizontal (tangential) velocity $v_{t}$ remain in a circular orbit around Earth instead of simply falling to the surface or escaping into space?",
      "options": {
        "A": "Because the spacecraft's inertia tends to carry it in a straight line while Earth's gravity continuously pulls it inward; at the right $v_{t}$ those effects provide the centripetal acceleration needed for a stable circular orbit.",
        "B": "Because orbital velocity cancels gravity completely, so there is no net force acting on the spacecraft.",
        "C": "Because the thin atmosphere at orbital altitude provides aerodynamic lift that counteracts gravity and keeps the spacecraft aloft.",
        "D": "Because Earth's magnetic field confines the spacecraft and forces it to move in a closed loop around the planet."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the question for clarity, introduced $v_{t}$ for tangential velocity, and made each distractor concise but plausible; preserved original physics explanation.",
      "content_preserved": true,
      "source_article": "Spaceflight",
      "x": 1.8095296621322632,
      "y": 0.8788947463035583,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Orbital mechanics — to achieve and maintain orbit, a spacecraft must reach sufficient horizontal velocity, so it \"falls around\" the Earth rather than back to the surface.",
        "Concept 2: Propulsion architecture — multistage rockets are used to overcome gravity and incrementally increase speed to reach orbital velocity.",
        "Concept 3: Space debris dynamics — uncontrolled objects and debris can lead to Kessler syndrome; mitigation via deorbiting or graveyard orbits."
      ],
      "original_question_hash": "bf80c550"
    },
    {
      "question": "Why does shotgun metagenomic sequencing let researchers infer both which microbes are present (community composition) and what biochemical functions they could carry out (functional potential) from environmental DNA, whereas targeted amplicon surveys (e.g., 16S rRNA) cannot?",
      "options": {
        "A": "Because shotgun metagenomics sequences essentially all DNA in the sample, capturing taxonomic marker genes plus the coding sequences for enzymes and metabolic pathways, so it enables both taxonomic profiling and functional annotation.",
        "B": "Because targeted amplicon sequencing amplifies only one or a few marker genes (for example 16S rRNA), so it mainly provides taxonomic profiles and not the full complement of functional genes.",
        "C": "Because shotgun metagenomics requires culturing each organism first to obtain complete genomes, which then reveal both identity and function.",
        "D": "Because shotgun approaches sequence only RNA transcripts from the community, directly showing gene expression and therefore function."
      },
      "correct_answer": "A",
      "simplification_notes": "Removed historical detail and excessive technical history; focused the stem on the core methodological contrast between shotgun metagenomics and amplicon sequencing; made options concise and each plausibly incorrect except the correct one.",
      "content_preserved": true,
      "source_article": "Metagenomics",
      "x": 2.0461418628692627,
      "y": 1.1267280578613281,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Metagenomics as a culture-independent approach that profiles both microbial composition and functional potential from environmental DNA.",
        "Concept 2: The methodological distinction and relational outcomes between metagenomics and amplicon sequencing (data types, ecological insights).",
        "Concept 3: The role of technology and economics (e.g., sequencing costs, shotgun vs long-read sequencing) in enabling large-scale, ecosystem- and health-relevant metagenomic research."
      ],
      "original_question_hash": "9358e549"
    },
    {
      "question": "Why does philosophy subject its own methods and assumptions to critical scrutiny?",
      "options": {
        "A": "To ensure philosophical conclusions rest exclusively on empirical evidence.",
        "B": "To check that justificatory standards, core concepts, and tools (e.g. thought experiments) are appropriate, preventing arguments from depending on hidden or biased methodological choices.",
        "C": "To imitate the experimental procedures of the natural sciences in every respect.",
        "D": "To impose a single, unchanging method that all philosophers must follow."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording made more concise and direct for undergraduates; retained technical terms (e.g. 'justificatory standards', 'thought experiments'); removed extended background material and kept four plausible options.",
      "content_preserved": true,
      "source_article": "Philosophy",
      "x": 1.1751656532287598,
      "y": 1.0729577541351318,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Philosophy as a self-reflective, method-critical rational inquiry that examines its own methods and assumptions.",
        "Concept 2: The major branches (epistemology, ethics, logic, metaphysics) and the kinds of questions they address (knowledge, right conduct, reasoning, reality).",
        "Concept 3: The methodological toolkit of philosophy (conceptual analysis, thought experiments, analysis of ordinary language, description of experience) as mechanisms for testing and developing philosophical ideas."
      ],
      "original_question_hash": "a819705d"
    },
    {
      "question": "How do hidden transnational financial networks enable corruption to persist across borders?",
      "options": {
        "A": "They layer and route illicit funds through multiple jurisdictions and corporate entities, obscure beneficial ownership, and create opaque audit trails that hinder detection and raise enforcement costs.",
        "B": "They concentrate all transactions in a single, well‑regulated jurisdiction with transparent audits, which actually makes illicit flows easier to detect.",
        "C": "They rely on straightforward, fully transparent transactions that leave clear records and thus reduce risk for corrupt actors.",
        "D": "They affect only purely domestic corruption and therefore do not contribute to cross‑border corrupt activity."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified; kept technical terms (e.g. layering, beneficial ownership, opaque audit trails); distractor options rewritten as plausible misconceptions; core concept unchanged.",
      "content_preserved": true,
      "source_article": "Corruption",
      "x": 0.7051912546157837,
      "y": 0.5899397730827332,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Corruption as the abuse of entrusted power for private gain, encompassing a spectrum from illegal acts to legally ambiguous practices like lobbying.",
        "Concept 2: Mechanisms and enabling structures of corruption, including illegal acts (bribery, embezzlement) and hidden/transnational financial networks that facilitate corruption.",
        "Concept 3: The relationship between corruption and governance/anti-corruption efforts, including policy responses, international initiatives, and debates about measurement and perception (e.g., CPI)."
      ],
      "original_question_hash": "88d0bf9a"
    },
    {
      "question": "According to elite theory, why do a small, privileged group retain influence even when democratic institutions like elections exist?",
      "options": {
        "A": "Regular elections produce frequent elite turnover, preventing any single group from dominating.",
        "B": "Interlocked elite networks build self-reinforcing power and resources, so their influence stays durable and pluralist ideals appear unrealistic.",
        "C": "Ordinary citizens use informal channels and social pressure to exert influence equal to elites, balancing power.",
        "D": "Strict meritocratic selection ensures influence is redistributed according to public-interest competence, removing elite persistence."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified 'formal democratic procedures' as 'democratic institutions like elections'; condensed phrasing; preserved theoretical contrast between elite theory and pluralism; kept all four options plausible.",
      "content_preserved": true,
      "source_article": "Elitism",
      "x": 1.2292708158493042,
      "y": 0.9678022861480713,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Elites and legitimacy of influence. The idea that a select group with desirable qualities is more likely to contribute constructively and deserve greater influence, shaping governance and social order.",
        "Concept 2: Competing frameworks for power distribution. The contrast between elite theory (which sees elite influence as real and pluralism as utopian) and other views like pluralism or egalitarianism, highlighting mechanisms of political power.",
        "Concept 3: Criteria for elite status and its relation to social structures. How elite status arises (achievement, lineage, wealth) and its connection to social stratification (upper/middle/lower classes) and to governance systems (technocracy, meritocracy, plutocracy)."
      ],
      "original_question_hash": "d1b1b7d8"
    },
    {
      "question": "Why is effective management essential for turning policy decisions into the everyday public services citizens receive?",
      "options": {
        "A": "Because it establishes structured processes, clarifies roles, and uses feedback to align resources and operations with policy goals, enabling consistent service delivery and accountability.",
        "B": "Because it imposes rigid, top-down procedures that reduce flexibility and make adapting services to local conditions difficult.",
        "C": "Because it centralizes decision-making at the highest level, insulating programs from political influence and local oversight.",
        "D": "Because its main effect is improving public relations and communication rather than materially changing service outcomes."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the stem for clarity and concision at undergraduate level; preserved the original idea that management links policy to daily services; options rephrased to remain plausible distractors; correct answer unchanged.",
      "content_preserved": true,
      "source_article": "Public administration",
      "x": 1.2271682024002075,
      "y": 0.9408432841300964,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Translation of politics into public reality through policy implementation (how decisions become everyday government action)",
        "Concept 2: The principle that effective management is essential for the proper functioning of public organizations (management as foundational)",
        "Concept 3: The relationships between public administration and broader society, including cross-sector applicability and ongoing definitional/debates about the scope of the field"
      ],
      "original_question_hash": "97abbbf7"
    },
    {
      "question": "Why do context-specific, bottom-up peacebuilding approaches based on local conceptions of peace tend to produce more effective and lasting outcomes?",
      "options": {
        "A": "Because they allow the fastest possible deployment by bypassing local governance and implementing externally designed programs directly.",
        "B": "Because they align interventions with local incentive structures, norms, and institutions, creating legitimacy, local ownership, and capacity for self-sustaining peace that can endure and adapt over time.",
        "C": "Because they transfer all decision-making to local communities, removing the need to coordinate with national policy or external support.",
        "D": "Because they guarantee that local actors will avoid future conflict entirely without any external influence or resources."
      },
      "correct_answer": "B",
      "simplification_notes": "Rephrased the question for clarity and concision while preserving the focus on local ownership, alignment with local dynamics, and durability of peace; distractors were rewritten to remain plausible.",
      "content_preserved": true,
      "source_article": "Peacebuilding",
      "x": 1.1815104484558105,
      "y": 0.8799445629119873,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Targeting root causes and structural conditions to prevent recurrence of violence (cause–effect relationship; why addressing underlying drivers leads to durable peace)",
        "Concept 2: Emphasizing local conceptions and context-specific, bottom-up approaches (principle of local ownership; how aligning with local dynamics improves effectiveness)",
        "Concept 3: Distinction and sequencing among peace processes (pre-conflict and post-conflict scope; how peacebuilding differs from peacemaking and peacekeeping and why timing matters)"
      ],
      "original_question_hash": "6e68ee20"
    },
    {
      "question": "In state-building theory, security is often treated as a prerequisite for wider political and economic development. In what way does security enable subsequent development?",
      "options": {
        "A": "By ensuring rapid deployment of foreign aid and external resources through coercive enforcement.",
        "B": "By reducing violence and fear, creating predictable rules and credible property rights that allow institutions and markets to function.",
        "C": "By automatically aligning ethnic and religious groups and thereby eliminating internal conflicts.",
        "D": "By guaranteeing that democratic elections and political transitions will occur quickly."
      },
      "correct_answer": "B",
      "simplification_notes": "Question phrasing condensed and focused on mechanism; retained key ideas (security as prerequisite, effect on institutions/markets, property rights). Distractors made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "State-building",
      "x": 1.1941555738449097,
      "y": 0.9153527021408081,
      "level": 2,
      "concepts_tested": [
        "Security as a prerequisite: Without security, other tasks of state-building are not possible, and creating a safe environment is necessary before political/economic development can proceed.",
        "Multicausal, interacting factors: State-building is shaped by geopolitical, economic, social, cultural, ethnic, religious, internal, and external factors and by their mutual interactions.",
        "Theoretical approaches to state-building: Historical-process perspectives vs. externally imposed rebuilding vs. development-oriented state-building, and how these frameworks affect strategies and outcomes in peacebuilding contexts."
      ],
      "original_question_hash": "d946ed26"
    },
    {
      "question": "Why does the set of invariants of a geometric object depend on the chosen transformation group, and how should one identify invariants for a given group?",
      "options": {
        "A": "Invariants are universal constants of the object, so changing the group cannot change which properties are preserved.",
        "B": "A quantity is an invariant only if it is unchanged by every transformation in the chosen group (i.e., unchanged for all $g\\in G$); different groups contain different transformations, so they preserve different quantities.",
        "C": "Invariants are determined only by the object's coordinates, so changing the group merely relabels coordinates and does not change which quantities are preserved.",
        "D": "Invariants depend on coordinate scaling rather than the transformations themselves; therefore the choice of group only affects units or scales, not the preserved properties."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the question in concise academic language, added the formal condition 'unchanged for all $g\\in G$' to clarify how to test invariance, and made all options plausible misconceptions or the correct rationale.",
      "content_preserved": true,
      "source_article": "Invariant (mathematics)",
      "x": 1.661091685295105,
      "y": 1.199671983718872,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Invariants are properties unchanged under a specified class of transformations or equivalence relations.",
        "Concept 2: Invariants enable classification by remaining constant across objects related by those transformations, effectively identifying equivalence classes.",
        "Concept 3: The specific invariants depend on the transformation group; different groups (e.g., isometries, scalings, rotations, conformal maps) preserve different quantities (e.g., distance, angles, area), illustrating cause–effect relationships between transformations and what is preserved."
      ],
      "original_question_hash": "d2d10b7e"
    },
    {
      "question": "Homomorphisms preserve the defining operations of algebraic structures. Why does viewing homomorphisms as arrows (morphisms) $f:G\\to H$ in a category give a unified framework for comparing groups, rings, vector spaces, lattices, and similar structures?",
      "options": {
        "A": "Because the categorical viewpoint forces all structures to share the same underlying elements, so every object can be realized on a common set.",
        "B": "Because categories encode objects and structure-preserving maps plus composition and identities; this lets one express and study notions like kernels ($\\ker f$), limits, products and quotients in the same formal language for all these algebraic systems.",
        "C": "Because it requires different algebraic systems to have identical operation tables, reducing them to a single uniform operation table.",
        "D": "Because treating homomorphisms as arrows replaces many case-by-case algebraic checks with a language of morphisms and universal constructions, allowing comparisons via categorical constructions rather than low-level index computations."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified the role of homomorphisms as morphisms and emphasized composition and categorical constructions (e.g. $f:G\\to H$, $\\ker f$, limits, products). Removed historical detail and kept the conceptual core.",
      "content_preserved": true,
      "source_article": "Abstract algebra",
      "x": 1.6787792444229126,
      "y": 1.20885169506073,
      "level": 2,
      "concepts_tested": [
        "The role of homomorphisms and category theory in providing a unifying framework for different algebraic structures.",
        "The historical shift from concrete problem-solving to abstraction and axiomatic definitions in the development of abstract algebra.",
        "The idea of studying algebraic structures as single objects (universal algebra and varieties) to analyze broad properties across structures."
      ],
      "original_question_hash": "e542ce79"
    },
    {
      "question": "How do decompression melting and buoyancy combine to allow magma generated deep in a planet's mantle to rise and potentially erupt at the surface?",
      "options": {
        "A": "Decompression melting increases ambient pressure and rock density, which suppresses melt formation and prevents magma ascent.",
        "B": "Upward mantle flow reduces pressure (decompression melting), producing partial melt that is less dense and typically lower in viscosity than the surrounding solid; this density contrast (buoyancy) drives the melt upward toward the surface.",
        "C": "Decompression melting requires cooling to lower the melting point, and buoyancy causes melt to sink back into the mantle rather than rise.",
        "D": "Buoyancy forces push surrounding solid rock into the pathway of rising melt, compressing and trapping the melt at depth so that decompression and melting occur but do not produce surface eruptions."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the question in clear academic language, emphasized that decompression melting is caused by pressure drop and that melt's reduced density and viscosity produce buoyant ascent; retained original distractor ideas but made all options plausible.",
      "content_preserved": true,
      "source_article": "Volcanism",
      "x": 1.8779233694076538,
      "y": 0.9492131471633911,
      "level": 2,
      "concepts_tested": [
        "Heat-induced mantle melting and viscosity reduction enable magma transport to the surface",
        "Tidal heating as a key heat source driving volcanism on outer solar system bodies (e.g., Io)",
        "Buoyancy-driven ascent and decompression melting as mechanisms for magma rising and erupting at the surface"
      ],
      "original_question_hash": "5ec9b339"
    },
    {
      "question": "A patient with moderate dementia is asked to consent to a surgical procedure. The clinician explains the risks, benefits, and alternatives. The patient can accurately repeat the factual information but repeatedly insists the decision will not affect them and cannot acknowledge how the facts would impact their own life. Which component of decision‑making capacity is impaired, and why does this impairment undermine informed consent?",
      "options": {
        "A": "Choice — the patient cannot evidence or communicate a decision about treatment.",
        "B": "Understanding — the patient cannot apprehend the relevant facts about the procedure, risks, and alternatives.",
        "C": "Appreciation — the patient cannot apply the information to their own situation or recognize its personal relevance, so consent is not truly informed.",
        "D": "Reasoning — the patient cannot make logical inferences or compare options to reach a decision."
      },
      "correct_answer": "C",
      "simplification_notes": "Wording tightened for clarity; definitions of the four capacity components were kept concise and precise. The clinical scenario and core reasoning were preserved; technical terms (Choice, Understanding, Appreciation, Reasoning) are retained and briefly delineated to keep options plausible.",
      "content_preserved": true,
      "source_article": "Informed consent",
      "x": 1.2535737752914429,
      "y": 0.8588457107543945,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Decision-making capacity and its four components (Choice, Understanding, Appreciation, Reasoning) and how impairments can preclude valid consent.",
        "Concept 2: The relationship between providing adequate information, respecting autonomy, and the scope of informed consent across contexts (clinical care, research, disclosure, high-risk activities).",
        "Concept 3: Mechanisms for situations without informed consent (surrogate/guardian consent, assent, implied consent in emergencies) and the role of ethics review/oversight in safeguarding ethical principles."
      ],
      "original_question_hash": "eb287ec9"
    },
    {
      "question": "Why does it matter whether an artwork is made specifically for digital media versus merely created using digital tools when we classify and interpret it?",
      "options": {
        "A": "Because it shows whether the digital medium is constitutive of the artwork’s meaning (central to interpretation) or only a production tool, which guides which features critics and classifiers should emphasize.",
        "B": "Because it proves that digital tools always increase an artwork's artistic or market value.",
        "C": "Because it means only works created entirely within digital environments qualify as 'digital art.'",
        "D": "Because it determines the single legitimate distribution channel (for example, the internet versus physical galleries) for the work."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified; retained the core distinction between digital-as-medium versus digital-as-tool; distractors made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Digital art",
      "x": 0.5053107142448425,
      "y": 1.0724645853042603,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The distinction between art made for digital media vs. art that uses digital tools, and the implications this has for classification and interpretation.",
        "Concept 2: The challenge of defining digital art as a cohesive field, highlighting that there is no single unifying statement due to diverse processes and dissemination.",
        "Concept 3: The impact of digital technology on expanding creative opportunities for both professional and non-professional artists."
      ],
      "original_question_hash": "96ed69fc"
    },
    {
      "question": "Why do new media and older (traditional) media usually develop through non-linear, feedback-driven loops rather than by simple one-way replacement?",
      "options": {
        "A": "Because audiences retain content preferences and demand uniform formats, driving a linear shift from old media to new media.",
        "B": "Because technologies create new affordances and social practices that alter production and use; practices, technologies, and content continually reshape one another in iterative feedback cycles.",
        "C": "Because legal and policy frameworks rigidly fix adoption paths, preventing cross-influence and mutual adaptation between media types.",
        "D": "Because new media always have lower costs and greater scale, so they automatically outcompete and eliminate older formats."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording condensed and clarified for undergraduate readers; preserved core idea of iterative, reciprocal influence (technologies ⇄ practices ⇄ content). Removed historical/citation details and redundant examples; retained the technical term 'affordances' and the concept of feedback cycles.",
      "content_preserved": true,
      "source_article": "New media",
      "x": 1.2552136182785034,
      "y": 1.0043141841888428,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Non-linear, feedback-driven evolution of media—new media and old media influence one another in interconnected loops rather than a simple replacement.",
        "Concept 2: The medium is the message principle—technology and form shape human experience and society, often more than content.",
        "Concept 3: Distinction of new media based on interactivity and digital generativity—what counts as new media hinges on interactive, computer-enabled processes rather than purely non-interactive formats."
      ],
      "original_question_hash": "cc59a320"
    },
    {
      "question": "In devolution, subnational bodies receive area-specific legislative powers while the central government retains ultimate sovereignty. Why is the central government's ability to repeal or amend devolved powers essential, and how does that ability relate to the fact that subunit powers lack constitutional protection?",
      "options": {
        "A": "It lets the central government reverse or modify devolved powers when national priorities change, preserving sovereign supremacy; because these powers are statutory rather than constitutional, the subunits remain revocable.",
        "B": "It guarantees that devolved powers are permanently written into the constitution, preventing the central government from repealing them.",
        "C": "It limits the central government's repeal power to administrative matters only, so devolved legislative powers become effectively irrevocable.",
        "D": "It makes devolution indistinguishable from federalism, where subunit powers are constitutionally protected and cannot be altered by the central government."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened for clarity and concision; preserved the core idea that devolution is reversible because powers are statutory, and made distractors plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Devolution",
      "x": 1.6351884603500366,
      "y": 0.0427418015897274,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Devolution as a form of administrative decentralization where subnational bodies gain area-specific legislative powers while the central government retains ultimate sovereignty.",
        "Concept 2: Reversibility and central control as core mechanisms of devolution (powers can be repealed or amended by the central government; the subunits’ authority is not constitutionally protected).",
        "Concept 3: Distinction from federalism based on constitutional protection of subunit powers and the possibility of unilateral withdrawal or modification by the central government, illustrating different relationship dynamics between levels of government."
      ],
      "original_question_hash": "694e50ec"
    },
    {
      "question": "Why do effective design patterns define a clear context and describe trade-offs instead of prescribing a single fixed implementation?",
      "options": {
        "A": "Because they are intended to be reused across different situations by balancing competing goals; the same pattern can be implemented differently depending on context.",
        "B": "Because they assume there is one universal best solution that will work identically in every context.",
        "C": "Because context is only a descriptive label and the implementation should remain the same regardless of circumstances.",
        "D": "Because listing guiding values prevents designers from adapting the solution and enforces rigid, uniform implementations."
      },
      "correct_answer": "A",
      "simplification_notes": "Rewrote the question concisely for undergraduate readers, removed extended examples, clarified 'trade-offs' as 'competing goals', and kept all four options plausible while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Design pattern",
      "x": 1.4410467147827148,
      "y": 1.0712041854858398,
      "level": 2,
      "concepts_tested": [
        "Concept 1: A design pattern is a reusable solution to a recurring design problem, intended to be applied across many situations without prescribing exact implementations; emphasis on guiding decisions via balancing forces rather than fixed prescriptions.",
        "Concept 2: Context and forces: The applicability of a pattern depends on its context and the constructive tension between competing goals; documentation should explain when a pattern is applicable and what values guide the choice.",
        "Concept 3: Pattern language and cross-domain applicability: Patterns form a pattern language that provides common terminology and can apply across different disciplines (e.g., architectural, software, interaction design), illustrating generalization and relationships among patterns."
      ],
      "original_question_hash": "40095cbd"
    },
    {
      "question": "The triad that shapes user experience consists of the system, the user, and the context of use. Why might a software update that speeds task completion (a system improvement) fail to improve UX for all users in all contexts?",
      "options": {
        "A": "Because UX is determined only by system performance, so any increase in speed should improve UX for every user regardless of who they are or where they use the software.",
        "B": "Because the user’s goals, emotions, skills, and prior experiences, together with situational factors (environment, interruptions, competing tasks), influence how improvements are perceived; the same speed gain may be valued differently or negated in different contexts.",
        "C": "Because once a task is completed faster, context no longer affects perceived usability; only completion time determines whether UX improved.",
        "D": "Because faster system performance always lowers cognitive load for every user, so speed increases necessarily produce universal UX gains."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was made more concise and direct for undergraduates. The triad (system, user, context) was foregrounded; the question was converted to a clear multiple-choice format. Distractors were kept as common misconceptions about UX and usability.",
      "content_preserved": true,
      "source_article": "User experience",
      "x": 1.3738772869110107,
      "y": 1.042533278465271,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The relationship between usability and user experience (pragmatic vs. hedonic aspects) and how improvements in usability can influence overall UX.",
        "Concept 2: The triad of factors shaping UX — the system, the user, and the context of use — and how each factor can alter the user experience.",
        "Concept 3: The subjective nature of UX versus objective attributes, including how emotions/beliefs influence UX and how business objectives (e.g., profitability) can conflict with ethical UX considerations."
      ],
      "original_question_hash": "4788ad3b"
    },
    {
      "question": "Why is providing immediate, specific feedback that matches a user's mental model important for guiding interaction in a user interface?",
      "options": {
        "A": "It lets users validate and update their mental model of the system, so they can choose correct next actions and recover from errors more quickly.",
        "B": "It primarily provides decorative polish to the interface, increasing perceived quality without affecting task performance.",
        "C": "It intentionally delays responses to avoid interrupting the user's focus, ensuring an uninterrupted workflow.",
        "D": "It exposes every internal system state to the user, improving transparency but risking information overload."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording was shortened and clarified for undergraduate readers while retaining technical terms like 'mental model' and 'feedback'; distractor options were kept plausible and aligned with common misconceptions.",
      "content_preserved": true,
      "source_article": "User interface design",
      "x": 1.4038158655166626,
      "y": 1.0788458585739136,
      "level": 2,
      "concepts_tested": [
        "Concept 1: User-centered design and design thinking as mechanisms to maximize usability and the user experience.",
        "Concept 2: The role of feedback, informing users, and aligning with mental models to guide interaction.",
        "Concept 3: The balance between technical functionality and visual/design elements (typography, aesthetics) and how this balance affects usability and user performance."
      ],
      "original_question_hash": "023c0c9e"
    },
    {
      "question": "Why are third-party professional certifications typically portable across different employers and work settings instead of being tied to a single company?",
      "options": {
        "A": "They are issued by the employer that created the specific job role, so the credential is specific to that company.",
        "B": "They are granted by an independent external authority and use standardized assessments, signaling a consistent level of competence across settings.",
        "C": "They automatically provide legal permission to practice in any jurisdiction, ensuring universal acceptance by employers.",
        "D": "They remove the need for ongoing education or renewal, making the credential permanently valid and transferable."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording shortened and clarified 'third-party' as 'independent external authority'; emphasized portability and standardized assessment. Distractors kept plausible (employer‑issued, legal permission, no renewal).",
      "content_preserved": true,
      "source_article": "Professional certification",
      "x": 1.3059560060501099,
      "y": 0.921441912651062,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Certification as a third-party attestation of knowledge/proficiency and its portability across employers/contexts.",
        "Concept 2: The distinction between certification and licensing, including who issues them, legal implications, and voluntary versus mandatory status.",
        "Concept 3: Renewal and continuing education requirements as mechanisms to maintain certification and ensure ongoing competence."
      ],
      "original_question_hash": "895829b6"
    },
    {
      "question": "According to the principle of compositionality, why can we predict the meaning of a complex linguistic expression from the meanings of its parts and their syntactic arrangement?",
      "options": {
        "A": "Because the total meaning is just the sum of the individual word meanings, regardless of how they are syntactically combined.",
        "B": "Because the meaning of the whole is computed by applying the meanings of functional elements (e.g., determiners, verbs) to the meanings of their arguments in a way that mirrors the expression's syntactic structure.",
        "C": "Because the meaning of the entire expression is determined solely by its final word.",
        "D": "Because each word contributes a fixed, context-free meaning that remains unchanged when words are combined."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording compressed and clarified for undergraduates; explicit examples (determiners, verbs) added to illustrate 'functional elements'; removed broader theory list to focus on compositionality. Options kept plausible and distinct.",
      "content_preserved": true,
      "source_article": "Semantics",
      "x": 1.2770663499832153,
      "y": 1.1026930809020996,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Sense vs. reference as core relations in meaning",
        "Concept 2: Compositionality: how the meaning of a complex expression arises from its parts and their arrangement",
        "Concept 3: Competing theories of meaning (referential, ideational, causal, truth-conditional, verificationist, use, inferentialist) and what each claims about how meaning is determined"
      ],
      "original_question_hash": "60fea15e"
    },
    {
      "question": "Why do scholars generally study myths for their social and cultural roles rather than for whether the events described actually happened?",
      "options": {
        "A": "Because myths shape and legitimize a society’s customs, institutions, rituals, and taboos, so scholars focus on their social effects rather than on literal truth.",
        "B": "Because myths are always factual historical records, so determining their truth is the main task of scholarship.",
        "C": "Because myths are purely imaginary stories with no connection to real social life or institutions, making their social role irrelevant.",
        "D": "Because myths exist only within religious settings and therefore have no bearing on secular social or political institutions."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the stem to ask why scholars focus on social/cultural roles rather than historicity; clarified option A to cite customs, institutions, rituals, and taboos; kept other options plausible distractors.",
      "content_preserved": true,
      "source_article": "Myth",
      "x": 0.5250502228736877,
      "y": 0.1871723234653473,
      "level": 2,
      "concepts_tested": [
        "The social function of myths: myths as foundational narratives that shape and legitimize a society's customs, institutions, and taboos.",
        "The relationship between myth and ritual: the connection between narrative recital and the enactment of rituals in cultural practice.",
        "Scholarly reframing of myth: the idea that myth is a genre studied for its social/cultural function rather than for veracity or truth value."
      ],
      "original_question_hash": "ecda8cf6"
    },
    {
      "question": "In a synchronous digital circuit, a global clock determines when sequential elements (e.g., registers) sample and update their values. Conceptually, how does this clocking prevent race conditions and ensure the correct runtime order of operations?",
      "options": {
        "A": "Registers sample and update simultaneously on a clock edge, so data must meet setup and hold times and is captured within a shared time window; intermediate changes after sampling cannot affect that captured state.",
        "B": "The clock increases the propagation speed of all signal paths so data arrives faster and timing conflicts are avoided.",
        "C": "The clock encodes data onto a higher-frequency carrier, separating parallel signal paths in the frequency domain to prevent contention.",
        "D": "Using a clock removes the need to consider propagation delays and timing constraints during circuit design."
      },
      "correct_answer": "A",
      "simplification_notes": "Rephrased the question for clarity, kept technical terms (registers, setup/hold times, clock edge), shortened options to be concise while keeping plausible distractors.",
      "content_preserved": true,
      "source_article": "Synchronization",
      "x": 1.6313062906265259,
      "y": 1.0990158319473267,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Clock signals in synchronous circuits coordinate the start/end of time periods to ensure correct runtime order and prevent race conditions.",
        "Concept 2: Time transfer technologies (GPS and Network Time Protocol) provide real-time alignment to a common timescale (UTC) to synchronize distributed systems.",
        "Concept 3: Standardization of time (e.g., railway time) as a societal mechanism to enable safe, coordinated operation across transportation networks."
      ],
      "original_question_hash": "5e3c12a5"
    },
    {
      "question": "In a structured impact assessment, why does clearly defining the problem and the policy objectives at the start matter for the monitoring phase that comes later?",
      "options": {
        "A": "Because it guarantees the policy will be implemented on schedule without delays.",
        "B": "Because it identifies which indicators will be meaningful and credible so monitoring measures outcomes that actually reflect the policy objectives.",
        "C": "Because it determines the total budget for the project, including monitoring costs.",
        "D": "Because it requires only quantitative metrics to be used in monitoring and excludes qualitative evidence."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; retained the focus on how early problem/objective definition links to monitoring. Options kept plausible and concise; technical IA context preserved.",
      "content_preserved": true,
      "source_article": "Impact assessment",
      "x": 1.3179806470870972,
      "y": 0.8928144574165344,
      "level": 2,
      "concepts_tested": [
        "Concept 1: IA as an evidence-based decision-support mechanism that informs policy design, improves transparency, and enhances legitimacy through public participation.",
        "Concept 2: The structured IA process (planning, analysis, consultation, coordination, reporting) and how each step contributes to shaping policy options and monitoring.",
        "Concept 3: The relationship between pre-implementation IA and post-implementation impact evaluation, including factors that can cause divergence (politicization, complexity)."
      ],
      "original_question_hash": "1a0e494b"
    },
    {
      "question": "In low-income contexts, why might eliminating direct schooling costs (for example, tuition) still fail to close the gender gap in school enrollment?",
      "options": {
        "A": "Removing tuition should remove the main barrier and therefore equalize enrollment; any remaining difference reflects individual choice rather than persistent barriers.",
        "B": "Even with free tuition, households still face indirect costs and constraints—opportunity costs of girls’ time (household chores or paid work), safety risks, and restrictive social norms—so girls’ enrollment can remain limited unless these factors are addressed.",
        "C": "Free primary education is sufficient to ensure equal access; any observed gender gaps are mainly due to measurement error or poor data.",
        "D": "The gender gap is driven entirely by parental preference for boys over girls, a cultural attitude that cost reductions cannot change."
      },
      "correct_answer": "B",
      "simplification_notes": "Rephrased the original question to clarify 'direct schooling costs' as 'tuition' and condensed options into concise, plausible alternatives while preserving the tested concept that indirect costs and social norms limit girls' enrollment.",
      "content_preserved": true,
      "source_article": "Right to education",
      "x": 0.9381328225135803,
      "y": 0.4820835590362549,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Progressive realization and funding mechanisms for the right to education (free primary, progressively free secondary, equitable access to higher education)",
        "Concept 2: Education as a driver of human development and social cohesion (development of personality, respect for human rights, understanding and tolerance)",
        "Concept 3: Relationship between socio-economic inequality and access to education (out-of-school children, gender disparities, barriers to schooling)"
      ],
      "original_question_hash": "ca06f39a"
    },
    {
      "question": "Why does analyzing language as \"speech events\" (socially situated performances) give a better explanation of how language works in social life than a purely structural description of grammar?",
      "options": {
        "A": "Because it privileges memorized rules and abstract grammatical structures as the main source of meaning in interaction.",
        "B": "Because it treats language as independent of context, emphasizing universal grammatical patterns rather than local use.",
        "C": "Because it treats language as performance embedded in specific social settings, showing how talk accomplishes social actions, constructs roles and identities, and is organized by participant roles, turn-taking, and discourse features.",
        "D": "Because it speeds up data collection by assuming all speech events are equivalent across different contexts."
      },
      "correct_answer": "C",
      "simplification_notes": "Question shortened and clarified: defined 'speech events' as socially situated performances, kept key technical terms (participant roles, turn-taking, discourse features), and made distractors more clearly plausible misconceptions.",
      "content_preserved": true,
      "source_article": "Linguistic anthropology",
      "x": 1.2123243808746338,
      "y": 1.0650192499160767,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Language as a social practice and the ethnography of communication (language use within social situations and communities)",
        "Concept 2: The speech-event unit of analysis (shifting focus from purely structural elements to socially situated linguistic performances)",
        "Concept 3: Paradigm shifts and interdisciplinary relations (how documentation, use-in-context analysis, and cross-field influences shape linguistic anthropology and its theories)"
      ],
      "original_question_hash": "0fbeea74"
    },
    {
      "question": "In qualitative research, reflexivity is treated as essential. How does practicing reflexivity alter what counts as evidence and the way researchers draw conclusions, and why is it considered essential rather than optional?",
      "options": {
        "A": "It standardizes evidence by attempting to remove researchers’ biases so that conclusions become strictly objective and neutral.",
        "B": "It makes explicit how both the researcher’s and participants’ perspectives shape what is treated as evidence, frames interpretations as a co‑constructed process, and increases transparency about how conclusions are reached.",
        "C": "It substitutes participants’ meanings with the researcher’s own interpretations to achieve consistent, researcher‑centered conclusions.",
        "D": "It treats collected data as purely descriptive, neutral facts and therefore ignores contextual interpretation or the researcher’s influence."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was shortened and clarified for undergraduate readers; key terms (reflexivity, evidence, co‑construction, transparency) were retained. Distractor options were kept plausible but concise.",
      "content_preserved": true,
      "source_article": "Qualitative research",
      "x": 1.268173336982727,
      "y": 1.0146145820617676,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Qualitative research aims to understand social reality and meanings through non-numerical, richly contextual data.",
        "Concept 2: Subjectivity and reflexivity are central, with researchers' and participants' perspectives shaping the theory and findings.",
        "Concept 3: Philosophical paradigms (e.g., positivism, constructivism, critical theory) influence qualitative approaches and interpretations, creating relationships between philosophy, method, and conclusions."
      ],
      "original_question_hash": "6de74a4f"
    },
    {
      "question": "A patient-safety prevention loop involves reporting adverse events, analyzing their causes, and implementing system changes. Why should the analysis stage prioritize identifying system-level root causes instead of simply blaming individual clinicians?",
      "options": {
        "A": "Because assigning blame to individuals increases their motivation to avoid future mistakes.",
        "B": "Because finding system-level root causes allows design changes that lower the chance of the same error recurring across many patients.",
        "C": "Because changing individual behavior alone is sufficient to eliminate all safety risks.",
        "D": "Because the analysis stage exists primarily to document events for regulatory or compliance purposes."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed the original historical and contextual material to a single clear question about the reporting→analysis→change loop; emphasized system-level vs individual-blame rationale and made all options plausible.",
      "content_preserved": true,
      "source_article": "Patient safety",
      "x": 1.2818714380264282,
      "y": 0.9550618529319763,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Systematic prevention loop (reporting, analysis, and changes to reduce preventable harm)",
        "Concept 2: Ethical foundation (primum non nocere / do no harm guiding patient safety decisions)",
        "Concept 3: Emergence of patient safety as a transdisciplinary discipline driven by research and enabling technologies"
      ],
      "original_question_hash": "7aa55b76"
    },
    {
      "question": "Why does making apprenticeships and industry collaboration central to a vocational curriculum improve students' readiness for real workplace tasks?",
      "options": {
        "A": "Because it replaces formal classroom teaching with only on-the-job tasks, emphasizing doing over understanding.",
        "B": "Because it anchors learning in authentic work contexts, exposing students to real constraints and feedback from practitioners, which builds tacit knowledge, workplace norms, and adaptability.",
        "C": "Because it shifts responsibility for training mainly to employers, reducing the role and need for school-based pedagogy.",
        "D": "Because it guarantees immediate job offers and fast credentialing for participants regardless of their demonstrated competence."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduate readers; retained technical terms (tacit knowledge, workplace norms, adaptability). Distractors kept plausible but corrected to preserve tested distinctions.",
      "content_preserved": true,
      "source_article": "Vocational education",
      "x": 1.0132120847702026,
      "y": 0.37942978739738464,
      "level": 2,
      "concepts_tested": [
        "Alignment of TVET with labor market needs and economic objectives (industry skills, unemployment reduction, economic growth)",
        "Integrated learning model combining specialized techniques, underlying scientific principles, general knowledge, skills, and values across multiple learning contexts",
        "Industry collaboration and apprenticeship/work-based learning as core mechanisms in curriculum design and delivery"
      ],
      "original_question_hash": "81d710cf"
    },
    {
      "question": "Compatibilism holds that determinism and free will can both be true. Which account best captures the compatibilist view and explains why this still supports meaningful moral responsibility?",
      "options": {
        "A": "Free will requires actions to be uncaused by prior events; therefore, if determinism is true, free will and responsibility are ruled out.",
        "B": "Free will consists in acting in accordance with one’s own reasons, desires, and values without external coercion; those mental states may be causally determined, yet the agent remains responsible.",
        "C": "Free will is the capacity to predict one’s own actions perfectly; determinism therefore guarantees free will by making behavior fully predictable.",
        "D": "Free will depends on indeterministic randomness to generate alternative possibilities; determinism removes that randomness and thus removes free will."
      },
      "correct_answer": "B",
      "simplification_notes": "Question shortened and clarified to focus on compatibilism and moral responsibility; language made more concise and academic; distractors kept plausible versions of incompatibilist or mistaken views; correct answer unchanged.",
      "content_preserved": true,
      "source_article": "Determinism",
      "x": 1.1891217231750488,
      "y": 1.0569127798080444,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Causal determinism vs nomological determinism as different mechanisms for how events are determined and how they relate to predictability.",
        "Concept 2: Compatibilism vs incompatibilism as a relationship between determinism and free will, and whether the two can coexist.",
        "Concept 3: The scope and variety of determinism (universal vs limited systems; domain-specific determinism in physical, biological, psychological, social contexts; and interpretations involving quantum mechanics) as different frameworks that shape how determinism operates."
      ],
      "original_question_hash": "a56a1e58"
    },
    {
      "question": "According to compatibilist theory, which condition best grounds an agent's freedom in a universe governed by deterministic laws?",
      "options": {
        "A": "That the agent could have done otherwise even if the past and the laws of nature were the same",
        "B": "That the agent acted in accordance with their own reasons and desires and was not subject to external coercion",
        "C": "That the agent's decision was produced by random, uncaused events in the brain",
        "D": "That the agent's actions originate from a non-physical, self-causing source (agent/substance causation)"
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and focused on compatibilism; each option reframed to correspond to major positions in the article (libertarian 'could have done otherwise', compatibilism, indeterministic randomness, and agent/substance dualism). Kept original correct answer and core concepts.",
      "content_preserved": true,
      "source_article": "Free will",
      "x": 1.2099781036376953,
      "y": 1.029176950454712,
      "level": 2,
      "concepts_tested": [
        "Compatibilism vs incompatibilism and their sub-positions (libertarianism, hard determinism, illusionism/hard incompatibilism) as different principle-based answers to whether free will can coexist with determinism.",
        "The relationship between free will and moral responsibility (how freedom is tied to credit/blame, desert, and moral judgments).",
        "Different definitional frameworks of free will and how they shape what counts as freedom (e.g., freedom as undetermined by past events vs. freedom requiring agentive control within deterministic or indeterministic settings)."
      ],
      "original_question_hash": "19197155"
    },
    {
      "question": "How do fixed routes and regular headways act as a scheduling mechanism that affects service reliability and rider access in public-transport systems?",
      "options": {
        "A": "They guarantee no waiting and provide instant arrivals for all riders.",
        "B": "They create predictable service intervals and coordinated connections, so riders can plan transfers and anticipate vehicle arrivals, improving reliability and access.",
        "C": "They prevent delays entirely by eliminating interactions with traffic and other external disruptions.",
        "D": "They reduce the number of routes required by imposing the same frequency everywhere regardless of passenger demand."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified (defined role of fixed routes and headways as scheduling tools); removed broader historical/contextual details while keeping multimodal transfer and reliability concepts; options made concise and plausibly misleading.",
      "content_preserved": true,
      "source_article": "Public transport",
      "x": 1.7332321405410767,
      "y": 0.39171040058135986,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Fixed routes and headways as the scheduling mechanism that shapes reliability and rider access.",
        "Concept 2: Multimodal integration (transfers, on-demand options, paratransit) as a design principle to extend coverage and accessibility.",
        "Concept 3: Relationships between external factors (remote work trends, ride-sharing, car affordability) and public transport demand, plus policy and environmental drivers that influence investment and service changes."
      ],
      "original_question_hash": "d4ee1cc4"
    },
    {
      "question": "Which explanation best accounts for a motor deficit caused by a lesion in a peripheral nerve even though central motor circuitry (CNS) is intact?",
      "options": {
        "A": "Central motor programs are generated and executed within the brain alone, so damage to peripheral nerves should not prevent movement.",
        "B": "Motor commands originate in the CNS but must travel via peripheral nerves to reach and activate muscles; a lesion in a peripheral nerve blocks that transmission and produces a motor deficit.",
        "C": "Peripheral nerve lesions affect only sensory pathways, leaving motor output from the CNS intact and therefore not causing motor loss.",
        "D": "The CNS and PNS operate independently for movement control, so damage to the peripheral system does not alter central motor function."
      },
      "correct_answer": "B",
      "simplification_notes": "Question was shortened and phrasing clarified to focus on why peripheral nerve damage produces motor deficits despite intact central circuitry. Historical and comparative context was removed; core concept (CNS→PNS transmission required for muscle activation) was preserved. Options were reworded to be concise and plausible.",
      "content_preserved": true,
      "source_article": "Neuroanatomy",
      "x": 1.855945348739624,
      "y": 1.1540875434875488,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The CNS vs PNS organization as a framework for understanding nervous system connectivity and function.",
        "Concept 2: Lesion studies as a method for inferring the relationship between brain structure and behavior/function.",
        "Concept 3: Comparative neuroanatomy: how bilateral symmetry leads to segregated, better-understood nervous systems compared to radial symmetry."
      ],
      "original_question_hash": "ce7996e7"
    },
    {
      "question": "Why does a highly strained molecule tend to relax to a less strained conformation, and how is this explained by strain energy and thermodynamics (e.g. $ΔG = ΔH - TΔS$ and $K_{eq} = e^{-ΔG/(RT)}$)?",
      "options": {
        "A": "Because strain energy arises from geometric constraints that weaken bonds and store extra internal energy (like a compressed spring); moving to a less strained conformation relieves these unfavorable interactions, lowers enthalpy and internal energy, decreases $ΔG$, and so the lower‑energy form is thermodynamically favored.",
        "B": "Because strain energy strengthens bonds in the strained conformation, locking the molecule into that state and preventing relaxation to other conformations.",
        "C": "Because entropy always dominates and drives every system toward maximum disorder, so the molecule relaxes solely to increase entropy regardless of bond strain or enthalpy changes.",
        "D": "Because external pressure or applied mechanical energy adds stored energy to the molecule, and removing that pressure would spontaneously increase the molecule's internal energy and force it to relax."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording condensed for undergraduate level; retained key thermodynamic relations using $ΔG$ and $K_{eq}$; kept the compressed‑spring analogy and the role of bond weakening in strain energy. Distractors made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Strain (chemistry)",
      "x": 1.8497002124786377,
      "y": 1.0621163845062256,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Strain energy arises from structural constraints that weaken bonds, effectively storing energy in a molecule (like a compressed spring) and influencing which conformations are energetically favorable.",
        "Concept 2: The stability of molecular conformations is governed by Gibbs free energy differences, with ΔG° = ΔH° − TΔS°, and the equilibrium constant Keq = exp(−ΔG°/(RT)), linking structure, enthalpy, entropy, and spontaneity.",
        "Concept 3: Highly strained (high-energy) conformations tend to spontaneously relax to lower-energy conformations, illustrating a fundamental tendency toward thermodynamic stability."
      ],
      "original_question_hash": "59c23a61"
    },
    {
      "question": "Why is the physical site essential for a site-specific sound installation in shaping both perceptual experience and meaning?",
      "options": {
        "A": "It has no effect on perception or meaning; only the recorded or produced sounds themselves determine the work.",
        "B": "Because the site supplies acoustic properties (e.g., reverberation, reflections, spatial layout) and contextual cues (visual, historical, social) that allow the sound to interact with the environment and audience, so meaning emerges from that interaction.",
        "C": "Because the site guarantees that visitors will enjoy the piece; the space simply functions as a venue that presents the sound.",
        "D": "Because the site confines the work to a fixed duration or schedule; the space determines only temporal presentation, not how it is perceived."
      },
      "correct_answer": "B",
      "simplification_notes": "Phrase simplified for clarity; retained technical terms (site-specific, acoustic properties, contextual cues); made each option concise and plausible while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Sound art",
      "x": 0.45216044783592224,
      "y": 1.1084529161453247,
      "level": 2,
      "concepts_tested": [
        "Site-specificity and spatial dialogue: how sound installations interact with surrounding space and context to shape perception and meaning.",
        "Interdisciplinarity and boundary dissolution: how sound art traverses and links visual art, music, and other lineages to create meaning.",
        "Epistemic practice of sound: how practitioners analyze, describe, perform, and interrogate the condition and operation of sound."
      ],
      "original_question_hash": "4b35f347"
    },
    {
      "question": "How does sensorimotor grounding explain how meaning is formed and how higher-level cognition (e.g., reasoning, memory) is supported in embodied theories?",
      "options": {
        "A": "Meaning consists of bodyless, amodal symbols stored independently of perception and action; cognition manipulates these abstract symbols without reactivating sensorimotor systems.",
        "B": "Concepts and meanings are anchored in concrete sensorimotor experiences; learning and thinking partially re‑activate the same perceptual–motor systems, producing embodied simulations that shape meaning and influence reasoning and memory.",
        "C": "Meaning is determined solely by linguistic structure and syntax; bodily states and sensorimotor systems play no role in forming concepts.",
        "D": "Once a concept is learned, the body and environment become irrelevant to cognition; higher cognition proceeds independently of ongoing sensorimotor activity."
      },
      "correct_answer": "B",
      "simplification_notes": "Rephrased the original question into concise undergraduate-level language, kept technical terms (sensorimotor grounding, embodied simulations, perceptual–motor systems), shortened option texts while keeping all four plausible and preserving the original correct choice (B).",
      "content_preserved": true,
      "source_article": "Embodied cognition",
      "x": 1.2578470706939697,
      "y": 1.0512727499008179,
      "level": 2,
      "concepts_tested": [
        "Sensorimotor grounding of cognition: cognition is shaped by the body's motor and perceptual systems and their interactions with the environment, influencing perception, memory, meaning, and higher-level tasks.",
        "Embodiment as embedded in a broader context: embodiment includes biological, psychological, and cultural contexts, making cognitive processes context-sensitive.",
        "Theoretical relationships: embodied cognition challenges disembodied models (like cognitivism and Cartesian dualism) and relates to frameworks such as the extended mind, situated cognition, and enactivism."
      ],
      "original_question_hash": "39d01557"
    },
    {
      "question": "Why does a technological breakthrough in warfare not automatically produce a lasting advantage, and how does its effect usually change over time?",
      "options": {
        "A": "Because initial success from a new weapon or system is locked in by rapid, universal adoption regardless of logistics, training, or doctrine.",
        "B": "Because the effect of new technology propagates through interdependent elements—doctrine, logistics, organization, leadership, tactics and societal support—so a sustained advantage requires coordinated changes across these domains and failure of opponents to adapt; absent that systemic alignment the advantage usually dissipates.",
        "C": "Because technology is the sole determinant of victory; once a belligerent fields a new technology it permanently decides outcomes regardless of other factors.",
        "D": "Because political ideology and the state’s political will matter more than technical change, so battlefield success depends primarily on politics rather than technological innovation."
      },
      "correct_answer": "B",
      "simplification_notes": "Question phrasing was shortened and clarified for undergraduates; technical examples and historical anecdotes were removed; core ideas retained: interdependence of doctrine, logistics, organization, leadership, tactics, societal adaptation, and opponent adaptation.",
      "content_preserved": true,
      "source_article": "Military history",
      "x": 1.2201226949691772,
      "y": 0.9149567484855652,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Interplay and evolution of multiple factors (causes of war, doctrine, logistics, leadership, technology, strategy, tactics) and how they influence each other over time.",
        "Concept 2: The relationship between warfare and society/economy (how wars shape societies and, conversely, social/cultural factors shape military history).",
        "Concept 3: The role of normative/theoretical frameworks (just war theory, military ethics) in interpreting, guiding, or legitimizing warfare."
      ],
      "original_question_hash": "82c77062"
    },
    {
      "question": "According to rule consequentialism, how is the rightness of an action justified, and why can this view accommodate rights-based constraints?",
      "options": {
        "A": "By the action’s immediate outcome in that particular case, with no reference to rules or general practices.",
        "B": "By whether the action conforms to a fixed moral duty or rule regardless of the consequences that follow.",
        "C": "By whether adopting a general rule (and following it) would, overall, produce a better balance of good than adopting alternative rules; rules that protect rights can be included if those rules tend to maximize the overall good.",
        "D": "By whether the action expresses the agent’s intentions or moral character, independent of the action’s consequences."
      },
      "correct_answer": "C",
      "simplification_notes": "Question language shortened and clarified: specified 'rightness' justification and connection to rights-based constraints. Distractors were made clearly plausible (act consequentialism, deontology, virtue/intentionalism) while preserving the original correct explanation.",
      "content_preserved": true,
      "source_article": "Consequentialism",
      "x": 1.20242440700531,
      "y": 1.0222944021224976,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The rightness of an act is determined by its consequences, specifically a greater balance of good over evil.",
        "Concept 2: What counts as \"the good\" can vary (pleasure, absence of pain, satisfaction of preferences, general good), influencing how consequences are valued.",
        "Concept 3: Relationships with other ethical theories (deontology, virtue ethics, pragmatic ethics) and the idea that some theorists propose compatible or overlapping frameworks (e.g., rule consequentialism, incorporation of rights)."
      ],
      "original_question_hash": "6a4ced34"
    },
    {
      "question": "Why might a normative ethicist endorse a hybrid theory that combines virtue ethics, deontological ethics, and consequentialism instead of committing to a single framework?",
      "options": {
        "A": "Because moral evaluation often requires attention to an agent's character (virtue), the correctness of actions in light of duties or rights (deontology), and the consequences of actions (consequentialism); a hybrid can integrate these interacting dimensions in real cases.",
        "B": "Because a hybrid theory claims there is one single universal principle that subsumes virtue, duty, and outcome-based considerations, making it simpler than separate frameworks.",
        "C": "Because a hybrid theory maintains that only outcomes ultimately matter, so considerations of character and duty are irrelevant in moral assessment.",
        "D": "Because a hybrid theory holds that moral judgments are merely social constructions and therefore rejects objective moral facts in favor of culturally determined practices."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording was shortened and made more direct for undergraduates; the three frameworks were labeled by their primary focus (character, duties/rights, outcomes). All original answer choices were preserved conceptually and kept plausible; correct answer unchanged.",
      "content_preserved": true,
      "source_article": "Normative ethics",
      "x": 1.2228553295135498,
      "y": 1.0214073657989502,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The three core normative frameworks (virtue ethics, deontological ethics, consequentialism) and what each considers the locus of ethical evaluation (character, duties/rights, or outcomes).",
        "Concept 2: The role of overarching moral principles and the possibility of hybrid theories that combine elements of multiple frameworks.",
        "Concept 3: The relationships and distinctions between normative ethics and metaethics, applied ethics, and descriptive ethics, and how these relations shape what normative ethics asks about (what one ought to be vs. what one ought to do)."
      ],
      "original_question_hash": "b6fc8611"
    },
    {
      "question": "How do informal institutions (e.g., norms, handshakes) affect transaction costs and the reliability of exchange compared to formal institutions, and under what conditions do informal institutions stop providing these benefits?",
      "options": {
        "A": "They substitute for formal rules in all situations and always reduce enforcement and compliance costs across contexts.",
        "B": "They generate perfect, universal enforcement of agreements regardless of group composition, removing any need for formal institutions.",
        "C": "They can reduce transaction costs and uncertainty by creating shared expectations and reputational enforcement when participants share beliefs or interact repeatedly; they fail in heterogeneous, low‑trust, or one‑off interactions where beliefs diverge, causing coordination failures, higher costs, or the need for formal enforcement.",
        "D": "They only influence non‑economic social interactions and therefore have no effect on production or exchange costs."
      },
      "correct_answer": "C",
      "simplification_notes": "Reduced wording and removed historical/contextual background; clarified contrast between informal and formal mechanisms (shared expectations, reputation vs. formal enforcement) and specified failure conditions (heterogeneity, low trust, one‑off interactions).",
      "content_preserved": true,
      "source_article": "Institution",
      "x": 1.2611522674560547,
      "y": 0.9819320440292358,
      "level": 2,
      "concepts_tested": [
        "Institutions as constraints and enablers: how rules and norms shape behavior through shared expectations and enforcement.",
        "Formal vs informal institutions: how varying inclusivity and complexity of definitions reflect different mechanisms and scopes (laws vs. norms, handshakes, etc.).",
        "Economic impact of institutions: how institutional structures influence transaction/production costs and overall economic performance through rights, obligations, and third-party enforcement."
      ],
      "original_question_hash": "c2fabdf1"
    },
    {
      "question": "Why does betrayal trauma produce PTSD-like symptoms that emphasize anger and the violation of trust, and how does its mechanism differ from conventional PTSD?",
      "options": {
        "A": "Because betrayal trauma is primarily fear-driven by external threats, while conventional PTSD is mainly anger-driven and results from trust violations by someone trusted.",
        "B": "Because betrayal trauma is triggered by a violation of trust by a trusted person or institution, producing anger and moral/relational conflict that yields PTSD-like symptoms; in contrast, conventional PTSD is primarily fear-driven and does not depend on a breach of trust by a trusted source.",
        "C": "Because betrayal trauma and conventional PTSD are the same phenomenon; they differ only in the severity or context of the triggering event.",
        "D": "Because betrayal trauma occurs only within romantic relationships and cannot result from betrayals by organizations or institutions."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem to be more concise and academic, emphasized the contrast (anger/trust violation vs fear), and made each option a plausible, concise alternative without changing the tested concept.",
      "content_preserved": true,
      "source_article": "Betrayal",
      "x": 1.284622311592102,
      "y": 1.0082309246063232,
      "level": 2,
      "concepts_tested": [
        "Betrayal as a breach of trust or presumptive contract that creates moral/psychological conflict in relationships or organizations, with observable consequences for both victims and perpetrators.",
        "The need for a conceptually clear account that differentiates genuine betrayal from perceived betrayal and provides criteria for assessing alleged betrayal in real life.",
        "Betrayal trauma as a distinct mechanism with PTSD-like symptoms, driven by violation of trust by a trusted source, including distinctions from conventional PTSD (emphasis on anger and trust violation)."
      ],
      "original_question_hash": "c2a03b50"
    },
    {
      "question": "Why is jealousy best characterized as a multi-dimensional phenomenon rather than a single emotion?",
      "options": {
        "A": "Because jealousy is mainly a social label for one basic feeling, so it typically manifests as a single emotion across contexts.",
        "B": "Because jealousy arises from multiple cognitive appraisals (for example, perceived threat to a valued relationship, concerns about self-worth, and perceived loss of possessions or status), and those different appraisals can produce diverse emotions (anger, sadness, anxiety) and behaviors depending on context.",
        "C": "Because jealousy is simply equivalent to envy, so all its expressions follow a single psychological mechanism.",
        "D": "Because jealousy is only meaningful in observable behavior; internal subjective states (feelings and appraisals) are irrelevant to its explanation."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote as a concise academic question at undergraduate level; clarified that multiple cognitive appraisals (threat to relationship, self-worth, perceived loss) produce varied emotions and behaviors; kept all four options plausible and preserved the original correct answer (B).",
      "content_preserved": true,
      "source_article": "Jealousy",
      "x": 1.2640433311462402,
      "y": 1.0151299238204956,
      "level": 2,
      "concepts_tested": [
        "Jealousy as a multi-dimensional phenomenon: it involves diverse emotions and behaviors rather than a single emotion, affecting how it arises and is expressed.",
        "Multi-level mechanisms: psychological models, sociocultural influences, and biological factors all contribute to jealousy, illustrating cause-effect relationships across levels.",
        "Relationship to envy and language/culture: historical and linguistic shifts (e.g., jealousy vs. envy, “green-eyed monster”) shape how jealousy is understood and discussed."
      ],
      "original_question_hash": "ab62bbfd"
    },
    {
      "question": "Why does a media message that matches an audience member's preexisting beliefs tend to reinforce those beliefs rather than change them, even when the message is emotionally powerful?",
      "options": {
        "A": "The message's emotional intensity guarantees belief change regardless of prior beliefs.",
        "B": "Audience characteristics shape information processing; congruent content activates selective exposure and confirmation biases and reduces cognitive dissonance, producing reinforcement; the medium's form and context also influence how the message is interpreted.",
        "C": "If a message is emotionally engaging, it will always override prior beliefs and produce attitude change regardless of prior convictions.",
        "D": "Reinforcement only happens when the audience encounters the message in a highly controlled experimental setting, not in ordinary media use."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduate readers; kept technical terms (selective exposure, confirmation bias, cognitive dissonance, medium effects) and emphasized core causal mechanism. Distractors made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Influence of mass media",
      "x": 1.2335935831069946,
      "y": 0.9939398765563965,
      "level": 2,
      "concepts_tested": [
        "Contingency of media effects: effects depend on audience demographics and psychological characteristics; outcomes can be positive/negative, abrupt/gradual, short-term/long-lasting, or reinforcements rather than changes.",
        "Multi-dimensional impact: media influence can affect cognition, belief systems, attitudes, emotions, physiology, and behavior, illustrating that effects span multiple domains.",
        "Theoretical frameworks and role of the medium: concepts like McLuhan’s “The medium is the message” and the evolution of media effects paradigms show how form and context shape impact, not just content."
      ],
      "original_question_hash": "e0790aca"
    },
    {
      "question": "In the expanded view of work design, how does worker-initiated job crafting change the mechanism by which work design affects outcomes, compared with traditional top-down design?",
      "options": {
        "A": "By centralizing control and standardizing tasks so activities are more predictable and uniform.",
        "B": "By enabling workers to modify task boundaries, interpersonal relationships, and the meaning of their work to better match their motives and strengths, which raises intrinsic motivation and can buffer the effects of high job demands.",
        "C": "By reducing employee autonomy and enforcing a fixed set of activities determined by managers.",
        "D": "By making organizational support and formal redesign interventions unnecessary for improving work outcomes."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was shortened and clarified; 'worker-initiated job crafting' was explicitly defined in option B. Distractors were rephrased to remain plausible and contrast top-down vs. bottom-up mechanisms.",
      "content_preserved": true,
      "source_article": "Work design",
      "x": 1.3201090097427368,
      "y": 0.9951004385948181,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Work design influences outcomes across multiple levels (individuals, teams, organizations, society) – a cause-and-effect relationship.",
        "Concept 2: Expansion from traditional job design to work design includes proactive, emergent activities (e.g., job crafting, role innovation) as mechanisms by which workers shape their work.",
        "Concept 3: Deliberate interventions (e.g., job rotation, job enlargement, job enrichment) operate as mechanisms to redesign work and alter outcomes."
      ],
      "original_question_hash": "994570a0"
    },
    {
      "question": "Why does requiring dimensional homogeneity—that both sides of a physical equation have the same base dimensions (e.g. $L, M, T$)—serve as a useful guide and plausibility check when deriving equations for a physical system?",
      "options": {
        "A": "Because it forces all numerical coefficients in the equation to be dimensionless.",
        "B": "Because it requires every term to reduce to the same base dimensions when expressed in $L, M, T,\\dots$, which filters out dimensionally inconsistent dependencies and reveals missing factors or variables.",
        "C": "Because it guarantees the equation uses the smallest possible set of base dimensions for the problem.",
        "D": "Because it ensures the final result of the equation must be a dimensionless number."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; base dimensions shown as $L, M, T$; kept original concepts and logic. Options made concise and plausible, preserving correct choice B.",
      "content_preserved": true,
      "source_article": "Dimensional analysis",
      "x": 1.720185399055481,
      "y": 1.114219069480896,
      "level": 2,
      "concepts_tested": [
        "Dimensional homogeneity: why equations must have the same dimensions on both sides and how this serves as a plausibility check and guide in derivations.",
        "Dimension vs. unit: understanding that dimension is a property of physical quantity (base dimensions) and is distinct from the units used to express it; the implications for modeling and comparisons.",
        "Dimensionless parameters and nondimensionalization: how variables are combined into dimensionless groups (via Buckingham Pi) and why reducing to dimensionless forms clarifies fundamental relationships and scaling."
      ],
      "original_question_hash": "b004ae23"
    },
    {
      "question": "Why do groundwater flow paths in fractured rock often curve and not follow surface topography?",
      "options": {
        "A": "Because flow is driven by hydraulic head gradients, and heterogeneous permeability in fractures and conduits creates preferential, non–surface-aligned pathways so flow can bend around obstacles.",
        "B": "Because groundwater always follows the steepest surface slope, so subsurface flow paths must mirror the terrain above.",
        "C": "Because flow is governed solely by the magnitude of porosity, which forces straight-line movement regardless of fractures or pressure gradients.",
        "D": "Because fractures act as complete barriers to lateral flow, forcing water to move only vertically toward the deepest point."
      },
      "correct_answer": "A",
      "simplification_notes": "Question phrasing was tightened and made more concise for undergraduates; retained technical terms (hydraulic head, permeability, fractures). Distractors were made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Hydrogeology",
      "x": 1.7096248865127563,
      "y": 0.9594600796699524,
      "level": 2,
      "concepts_tested": [
        "Groundwater flow is driven by pressure gradients and can follow complex, non-surface-aligned paths through fractures and conduits.",
        "Flow involves coupled mechanical, chemical, and thermal interactions with the porous solid, including transport of energy and chemical constituents by the flow.",
        "Hydrogeology is interdisciplinary, integrating geology, chemistry, physics, engineering, law, and societal factors to address practical concerns like wells, contamination, and water quality."
      ],
      "original_question_hash": "d11c6a48"
    },
    {
      "question": "Why does punishment require an authorized agent and a breach of a known rule to be legitimate, and what happens if those conditions are relaxed?",
      "options": {
        "A": "Legitimate punishment requires a known rule and an authorized agent; without clear rules or authorization the act looks like revenge or arbitrary harm, which undermines predictability, accountability, and trust and therefore weakens deterrence and social order.",
        "B": "If rules or authority are absent, punishment becomes more effective because it avoids bureaucratic delay and delivers immediate sanctions that better deter offenders.",
        "C": "Legitimacy is irrelevant to punishment’s effects; any harmful consequence deters wrongdoing equally whether or not it follows rules or comes from an authorized agent.",
        "D": "Legitimacy only matters morally for the offender’s desert; formal authority isn’t required if the person clearly deserves the harm."
      },
      "correct_answer": "A",
      "simplification_notes": "Question tightened to a single clear sentence; retained core idea that legitimacy depends on authority and rule-violation and consequences of removing them. Options were made concise and plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Punishment",
      "x": 1.2653459310531616,
      "y": 0.9370011687278748,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Punishment serves multiple, interrelated purposes (retribution, deterrence, rehabilitation, incapacitation) and these goals influence how and why punishment is applied.",
        "Concept 2: The legitimacy of punishment depends on authority and adherence to rules; without rule violation or proper authorization, coercive outcomes are not considered punishment (distinguishing punishment from revenge or arbitrary harm).",
        "Concept 3: The mechanisms by which punishment affects behavior and society (e.g., deterring future acts, rehabilitating the offender, isolating the offender to protect others) and the contextual differences (formal/legal vs. informal/family settings)."
      ],
      "original_question_hash": "f9ced81b"
    },
    {
      "question": "Which statement best explains why making polluters pay can lead to Pareto-efficient pollution outcomes?",
      "options": {
        "A": "It places pollution costs on the polluter so firms face higher private costs; this may reduce their profits but, according to this view, has no necessary effect on overall social welfare.",
        "B": "It internalizes external damages by forcing polluters to bear the social cost of pollution, aligning a polluter’s private marginal cost with the social marginal cost so the equilibrium pollution level where social marginal cost equals social marginal benefit ($SMC = SMB$) is reached, moving toward Pareto efficiency.",
        "C": "It achieves Pareto efficiency by imposing prohibitive charges that eliminate all pollution; zero pollution is assumed to be always Pareto optimal under such strict costs.",
        "D": "It removes regulatory complexity and liability rules so firms operate without pollution-related costs or incentives, simplifying enforcement and market transactions."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; 'internalize externalities' spelled out as 'make polluters bear social costs'; added $SMC = SMB$ notation; options rephrased to be concise and plausible while preserving the tested economic logic. Correct answer unchanged.",
      "content_preserved": true,
      "source_article": "Polluter pays principle",
      "x": 1.3048286437988281,
      "y": 0.8398189544677734,
      "level": 2,
      "concepts_tested": [
        "Internalization of environmental externalities and its link to Pareto efficiency (assigning pollution costs to the polluter to reflect social costs).",
        "Policy mechanisms that implement the principle (ecotaxes, liability regimes) and their impact on pollution prevention and rehabilitation funding.",
        "Cross-jurisdictional legal adoption and normative status (EU law, US law, Canada, Australia) illustrating the principle as a foundational or regional custom."
      ],
      "original_question_hash": "e1fe89a9"
    },
    {
      "question": "When a lender requires collateral for a loan, which statement best describes how this affects the lender's credit risk and why?",
      "options": {
        "A": "Collateral makes borrowers less likely to default, so it primarily lowers the probability of default.",
        "B": "Collateral gives the lender a secured claim on specific assets that can be sold to recover part of the loan if the borrower defaults, thereby reducing loss given default while the borrower's default probability remains driven by their creditworthiness.",
        "C": "Collateral guarantees full repayment regardless of borrower behavior, completely eliminating the lender's credit risk.",
        "D": "Collateral transfers all credit risk to the borrower so the lender has no residual risk."
      },
      "correct_answer": "B",
      "simplification_notes": "Reworded the stem for clarity and brevity; emphasized the distinction between probability of default and loss given default; retained technical terms (default probability, loss given default) and kept the original correct answer.",
      "content_preserved": true,
      "source_article": "Credit risk",
      "x": 1.3589638471603394,
      "y": 0.8714355826377869,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Risk-based pricing (higher credit risk → higher borrowing costs / yield spreads) — how risk levels influence interest rates and pricing decisions.",
        "Concept 2: Mechanisms to mitigate or transfer credit risk (credit checks, collateral, mortgage/insurance, guarantees, securitization, selling debt) — what tools lenders use to reduce exposure and transfer risk.",
        "Concept 3: Types of credit risk (default risk, concentration risk, country risk) — how different risk categories affect lending decisions and overall risk assessment."
      ],
      "original_question_hash": "c0efc1ca"
    },
    {
      "question": "Which account best explains how moral judgments become moral actions, and why people sometimes act contrary to their own judgments?",
      "options": {
        "A": "Kohlberg's six-stage theory claims that people progress through universal levels of moral reasoning and that these stages directly determine action in all contexts, so no systematic gap between judgment and behavior should exist.",
        "B": "Blasi's self-model argues that moral judgments lead to action only when they are integrated into a person's self-concept via moral commitment; weak or absent commitment explains gaps between stated judgments and behavior.",
        "C": "Moral inconsistency occurs because moral judgments are stored in a cognitive module separate from action plans, so only immediate emotional impulses reliably trigger behavior and produce variability.",
        "D": "Moral actions are driven primarily by external social pressures and incentives, so internal moral judgments have little effect unless they are reinforced by the surrounding social environment."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording made concise and undergraduate-appropriate; preserved core concepts (Kohlberg's stages, Blasi's self-model, judgment–behavior gap). Options rewritten for clarity and plausibility without changing the tested content.",
      "content_preserved": true,
      "source_article": "Moral psychology",
      "x": 1.2200653553009033,
      "y": 1.011142611503601,
      "level": 2,
      "concepts_tested": [
        "The relationship between moral judgment/reasoning and moral action, including observed gaps and factors that mediate translation from judgment to behavior",
        "Kohlberg’s stages of moral reasoning and the idea of universal development across cultures",
        "Blasi’s self-model as a mechanism linking moral judgment to action via moral commitment"
      ],
      "original_question_hash": "eae054e1"
    },
    {
      "question": "How do metaethics and normative ethics together shape judgments in applied ethics?",
      "options": {
        "A": "Normative theories decide which outcomes count as morally good, and those conclusions then determine metaethical claims about whether moral facts exist.",
        "B": "Metaethics clarifies the status and justifiability of moral claims (e.g., whether they are objective or subjective), which constrains which normative principles can be defended, and applied ethics evaluates and implements those normative criteria in concrete cases.",
        "C": "Applied ethics first establishes moral facts from practical cases, and normative ethics afterward uses those facts to justify policies, independently of metaethical debate.",
        "D": "Normative ethics, metaethics, and applied ethics are independent domains that do not affect one another when making moral judgments."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was shortened and made more direct; removed historical and disciplinary examples and redundant phrasing while keeping the relational roles: metaethics (status/justification), normative ethics (principles), applied ethics (application). All four options remain plausible but only B preserves the correct directional relation.",
      "content_preserved": true,
      "source_article": "Ethics",
      "x": 1.214773178100586,
      "y": 1.012579321861267,
      "level": 2,
      "concepts_tested": [
        "The branching relationship among normative ethics, applied ethics, and metaethics and how they inform each other in moral inquiry",
        "The main normative theories (consequentialism, deontology, virtue ethics) and the criteria they use to determine moral rightness",
        "Foundational metaethical questions about objective moral facts, the possibility of moral knowledge, and motivational aspects of moral judgments"
      ],
      "original_question_hash": "63098c24"
    },
    {
      "question": "Why does the Modifiable Areal Unit Problem (MAUP) change observed statistical relationships when data are aggregated into different spatial zoning schemes?",
      "options": {
        "A": "Because changing the size and/or partitioning of spatial units alters within-unit homogeneity and between-unit contrasts, which changes variance, correlation, and spatial autocorrelation patterns so that summary statistics can differ even when the underlying raw values are unchanged.",
        "B": "Because aggregation methods automatically correct for sample size, so aggregating into larger units always produces stronger correlations.",
        "C": "Because MAUP is caused solely by measurement error that increases at coarser scales.",
        "D": "Because different zoning only changes the map's visual appearance (e.g., colors or boundaries), making relationships appear different even though the numerical data are identical."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording tightened for clarity; retained technical terms (homogeneity, variance, spatial autocorrelation); removed informal examples; kept the same correct option and core mechanism.",
      "content_preserved": true,
      "source_article": "Scale (geography)",
      "x": 1.6124147176742554,
      "y": 1.0051977634429932,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Scale effect—observations and conclusions depend on the scale/resolution at which a phenomenon is studied.",
        "Concept 2: MAUP and zoning effects—how data aggregation and partitioning can alter statistical results, linking scale to outcomes.",
        "Concept 3: Types and contexts of scale—geographic/spatial scale, cartographic scale, and operational scale, and how their meanings differ by context."
      ],
      "original_question_hash": "2f7aa6ba"
    },
    {
      "question": "Why do economists use real GDP and PPP-adjusted GDP per capita when comparing economies across time or across countries, and what does each adjustment accomplish? (Recall: $\\text{GDP per capita}=\\frac{\\text{GDP}}{\\text{population}}$.)",
      "options": {
        "A": "Real GDP uses constant (base‑year) prices to remove the effect of inflation, isolating changes in real output; PPP‑adjusted GDP per capita converts values using a common set of prices to neutralize cost‑of‑living differences, enabling comparisons of average living standards.",
        "B": "Real GDP uses current market prices to reflect today’s monetary values (i.e., nominal values); PPP‑adjusted GDP per capita ignores exchange rates and therefore preserves each country’s domestic currency values.",
        "C": "Real GDP adjusts for population size to measure per‑person output; PPP‑adjusted GDP per capita corrects for differences in technology and productivity across economies.",
        "D": "Real GDP fully captures welfare by including externalities like environmental damage; PPP‑adjusted GDP per capita expands GDP to include unpaid household work and volunteer services."
      },
      "correct_answer": "A",
      "simplification_notes": "Shortened and clarified the stem; removed historical and procedural detail; added the GDP per capita formula in LaTeX; kept the core distinction that real GDP removes inflation and PPP per capita adjusts for cost‑of‑living across countries.",
      "content_preserved": true,
      "source_article": "Gross domestic product",
      "x": 1.2976824045181274,
      "y": 0.915816068649292,
      "level": 2,
      "concepts_tested": [
        "Concept 1: GDP is the sum of four demand-side components (consumption, investment, government spending, net exports) and changes in these components drive GDP growth.",
        "Concept 2: GDP does not capture well-being or income distribution (externalities and unpaid work are omitted), highlighting a conceptual distinction between economic activity and living standards.",
        "Concept 3: Cross-time and cross-country comparisons require adjustments (real vs nominal, PPP, per-capita measures) to account for inflation and cost of living, reflecting methodological relationships in economic measurement."
      ],
      "original_question_hash": "0b2760ed"
    },
    {
      "question": "Why is feedback especially important in informal learning, given there is no fixed curriculum?",
      "options": {
        "A": "Because it provides immediate, context-specific information that lets learners adjust ongoing actions and refine skills in real situations.",
        "B": "Because it imposes external objectives and standardized tests that measure success in place of a curriculum.",
        "C": "Because it introduces a formal sequencing of topics, converting informal learning into structured instruction.",
        "D": "Because it replaces the need to observe others or participate in communities of practice by providing direct correction."
      },
      "correct_answer": "A",
      "simplification_notes": "Clarified and shortened the stem; preserved core idea that feedback guides adjustment in real contexts; rewrote distractors to be plausible but incorrect contrasts with informal learning.",
      "content_preserved": true,
      "source_article": "Informal learning",
      "x": 1.2809356451034546,
      "y": 1.0206435918807983,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Mechanisms that drive informal learning (trial-and-error, learning-by-doing, modeling, feedback, reflection) and how they support learning without formal objectives.",
        "Concept 2: Learning as participation in social and everyday contexts (socialization, enculturation, communities of practice) and its role in problem solving and knowledge creation.",
        "Concept 3: Distinguishing features and relationships with other learning forms (informal vs. formal, non-formal, self-regulated; learning outside institutions, incidental or spontaneous nature, lack of fixed curriculum)."
      ],
      "original_question_hash": "07b05eb8"
    },
    {
      "question": "Why do nonviolent resistance campaigns commonly spread from one area to neighboring areas?",
      "options": {
        "A": "Because information about successful nonviolent campaigns travels through activist networks and media, reducing uncertainty, increasing perceived legitimacy, and encouraging nearby organizers to imitate those tactics.",
        "B": "Because neighboring regions have identical political institutions that automatically cause the same tactics to be used without influence from information or networks.",
        "C": "Because violent repression in one place primarily causes neighboring groups to copy violent tactics rather than adopt nonviolent methods.",
        "D": "Because international legal mandates require neighboring regions to adopt the same nonviolent tactics regardless of local conditions."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the question to replace 'spatial diffusion' with 'spread from one area to neighboring areas' and made option statements clearer and more concise while keeping technical ideas (information flow, legitimacy, imitation) intact.",
      "content_preserved": true,
      "source_article": "Nonviolent resistance",
      "x": 0.8818705677986145,
      "y": 0.7368941307067871,
      "level": 2,
      "concepts_tested": [
        "Diffusion/Spread of nonviolent campaigns across regions (spatial diffusion and information sharing)",
        "Legitimacy of movements as a function of nonviolence and tactics (how refraining from violence relates to public legitimacy)",
        "Relationship between tactics, participant identity (e.g., gender), and perceived violence (how perceptions are shaped by who leads or participates)"
      ],
      "original_question_hash": "60920729"
    },
    {
      "question": "Which mechanism best explains how DNA methylation at promoter CpG sites represses transcription?",
      "options": {
        "A": "Methylated cytosines sterically block RNA polymerase II from binding the core promoter.",
        "B": "Methyl-CpG-binding proteins recognize methylated CpGs and recruit co-repressor complexes, including histone deacetylases (HDACs), causing chromatin condensation and reduced access of transcription factors and RNA polymerase.",
        "C": "DNA methylation causes the promoter DNA to be degraded or removed from the genome, eliminating transcription initiation.",
        "D": "Methylation directly enhances recruitment of sequence-specific transcriptional repressors that bind the promoter and block transcription."
      },
      "correct_answer": "B",
      "simplification_notes": "Question and options were shortened and clarified for undergraduates; technical terms (CpG, methyl-CpG-binding proteins, HDACs, chromatin condensation) retained. Distractors rewritten to be plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Epigenomics",
      "x": 2.0990607738494873,
      "y": 1.1559691429138184,
      "level": 2,
      "concepts_tested": [
        "DNA methylation at promoter regions represses gene expression, and this process is catalyzed by DNA methyltransferases (DNMTs) and can be reversed by demethylases.",
        "Epigenomic maintenance is dynamic and developmentally regulated (e.g., germ cell demethylation and remethylation after implantation) and contributes to genome stability.",
        "Epigenetic marks are heritable across cell generations and are involved in key processes like differentiation and tumorigenesis, illustrating cause–effect relationships between epigenetic state and cellular outcomes."
      ],
      "original_question_hash": "16b7f4fe"
    },
    {
      "question": "In a lunisolar calendar, occasional intercalary months are inserted into some years. How does adding these extra months keep the calendar aligned with the solar year and prevent long-term drift between lunar months and the seasons?",
      "options": {
        "A": "By increasing the average year length in selected years by about one lunar month (≈ $29.5$ days), so that over many years the total days approximate the solar year (≈ $365.24$ days), preventing seasonal drift.",
        "B": "By anchoring the calendar each year to a solar marker (for example, resetting the year start to the equinox), so months realign with the seasons annually.",
        "C": "By making every lunar month exactly $29.5$ days long, which ensures perfect day-by-day tracking of the Sun's apparent motion and seasons.",
        "D": "By reducing the number of months in leap years so the calendar year becomes shorter than the solar year, which compensates for accumulated lunar excess."
      },
      "correct_answer": "A",
      "simplification_notes": "Clarified the mechanism in concise academic language, added approximate numeric values ($29.5$ days per lunar month, $365.24$ days per solar year), and rewrote distractors to be plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Calendar",
      "x": 0.09322530031204224,
      "y": 0.5196778774261475,
      "level": 2,
      "concepts_tested": [
        "Synchronization of time units with celestial cycles (solar year and lunar months) and the idea of lunisolar calendars",
        "Intercalation/intercalary months as a mechanism to maintain long-term alignment between solar and lunar cycles",
        "Cross-cultural development and evolution of calendars driven by astronomical observation and cultural needs (including reforms like the Roman calendar)"
      ],
      "original_question_hash": "8a8b53d0"
    },
    {
      "question": "How do apostolic succession and papal primacy function to preserve continuity of teaching and governance in the Catholic Church?",
      "options": {
        "A": "They establish an unbroken chain of ordained bishops tracing back to the apostles, with the pope as the recognized center of authority whose magisterial office interprets Scripture and Sacred Tradition to maintain consistent doctrine and unified governance across the Church.",
        "B": "They require each local community to independently reinterpret Scripture according to its own culture, intentionally producing diverse and locally determined doctrines.",
        "C": "They permit lay councils to override bishops and papal decisions whenever there is disagreement, ensuring popular control over doctrine and governance.",
        "D": "They remove or discard historical teachings so the Church can make rapid doctrinal changes without central theological oversight."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording tightened; technical terms retained (apostolic succession, papal primacy, magisterium, Sacred Tradition). Option A expanded to mention the magisterium; distractors made concise but plausible.",
      "content_preserved": true,
      "source_article": "Catholic Church",
      "x": 0.1432236284017563,
      "y": 0.5737811923027039,
      "level": 2,
      "concepts_tested": [
        "Apostolic succession and papal primacy as mechanisms for continuity of teaching and governance",
        "The sacraments (especially the Eucharist) as the central means of grace and primary liturgical practice",
        "The magisterium as the official interpretation source for scripture and sacred tradition"
      ],
      "original_question_hash": "6dd4d2a3"
    },
    {
      "question": "Why does ecumenism—cooperation among Christian denominations through dialogue and shared goals—often succeed as a practical principle for joint action?",
      "options": {
        "A": "Because dialogue removes all doctrinal disagreements and enforces a single uniform set of beliefs across denominations.",
        "B": "Because dialogue reveals overlapping concerns, builds mutual trust, and enables shared initiatives while allowing churches to retain distinct identities.",
        "C": "Because all denominations hold identical beliefs about Jesus and the Bible, so cooperation is inevitable without deliberation.",
        "D": "Because external authorities force separate denominations to merge into one institutional church."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording made more concise and academic; emphasized practical mechanism (dialogue, shared goals) and preserved original alternatives as plausible misconceptions.",
      "content_preserved": true,
      "source_article": "Ecumenism",
      "x": 0.15344451367855072,
      "y": 0.5715968012809753,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Ecumenism as a principle of unity and cooperation across Christian denominations, enabled by dialogue and shared goals.",
        "Concept 2: Institutions and mechanisms (e.g., World Council of Churches, regional councils, Week of Prayer for Christian Unity) that operationalize ecumenism.",
        "Concept 3: Basis for ecumenism in common beliefs (Jesus, the Bible, baptism) and the biblical justification (John 17:20–23) that unity serves as a witness to the world."
      ],
      "original_question_hash": "86e8a21d"
    },
    {
      "question": "Why are literary genre boundaries often fluid and reconfigured over time, even when classification uses technique, tone, content, or length?",
      "options": {
        "A": "Because the criteria (technique, tone, content, length) are mutually exclusive and never overlap, so boundaries shift unpredictably.",
        "B": "Because genres are social constructs tied to cultural context; as reader tastes, publishing practices, and cross-genre experimentation change, the same techniques or tones can be reinterpreted and hybrid forms become more common.",
        "C": "Because once a genre is defined it becomes fixed and never adapts, so boundaries only move when entirely new genres are invented.",
        "D": "Because length is the only valid determinant of genre, making all other criteria irrelevant to boundary changes."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; retained the idea that classification criteria exist but genre boundaries change due to cultural, historical, and practical factors. Options made equally plausible.",
      "content_preserved": true,
      "source_article": "Literary genre",
      "x": 0.7911397218704224,
      "y": 1.1381725072860718,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Genre criteria and boundary fluidity — Genres can be determined by technique, tone, content, or length, and their distinctions are flexible and change over time.",
        "Concept 2: Genre architecture and cross-genre relationships — Large genres contain subgenres, yet mixtures and overlaps (e.g., satire appearing in multiple forms) illustrate non-discrete boundaries and hierarchical organization.",
        "Concept 3: Historical development of genre theory — The classification ideas originate with Aristotle (epic/tragedy/comedy; rhetorical genres) and evolve through Romantic critiques that challenged fixed genre constraints, showing how principles about genre are shaped by historical and cultural forces."
      ],
      "original_question_hash": "2e7a3bf9"
    },
    {
      "question": "Why does building a mathematical model of a decision problem typically improve decision quality compared with relying on intuition alone?",
      "options": {
        "A": "It guarantees a unique optimal solution exists for every real-world problem.",
        "B": "It makes objectives and constraints explicit, clarifies trade-offs, enables systematic sensitivity analysis, and lets you test policies or scenarios without costly real-world experiments.",
        "C": "It removes uncertainty by exactly predicting all random aspects of the system.",
        "D": "It ensures the chosen solution method will always be computationally efficient and scalable."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the stem to be concise and undergraduate-appropriate, retained the original conceptual focus on why modeling helps decisions. Options were made brief and plausible while preserving the original correct choice (B) and keeping distractors close to common misconceptions.",
      "content_preserved": true,
      "source_article": "Operations research",
      "x": 1.6471589803695679,
      "y": 0.759931206703186,
      "level": 2,
      "concepts_tested": [
        "Mathematical modeling as the central tool for understanding systems and guiding decisions",
        "Method selection and method-mixing based on system characteristics, goals, and computational constraints",
        "Optimization-oriented objective (maximizing or minimizing real-world outcomes) and its role in decision support"
      ],
      "original_question_hash": "89760e27"
    },
    {
      "question": "Why can external tactile cues (for example, packaging texture or product weight) increase a consumer's willingness to pay for a product that is functionally identical to a cheaper alternative?",
      "options": {
        "A": "They evoke embodied cognition and positive affect that bias perceived quality upward, thereby increasing willingness to pay.",
        "B": "They convey precise, reliable information about the product's technical performance via touch.",
        "C": "They physically change the product's functionality or performance, making it objectively more valuable.",
        "D": "They do not influence the decision; only objective signals such as price and reviews determine willingness to pay."
      },
      "correct_answer": "A",
      "simplification_notes": "Stem shortened and clarified; retained technical terms ('embodied cognition', 'affect', 'willingness to pay'); options rewritten to be concise and plausible while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Consumer behaviour",
      "x": 1.3301059007644653,
      "y": 0.9811625480651855,
      "level": 2,
      "concepts_tested": [
        "External cues (visual, auditory, tactile) shaping consumer responses and buying behavior",
        "Social influences and reference groups (opinion leaders) affecting decisions and brand loyalty",
        "Data analytics/CRM enabling behavioral segmentation and targeted marketing, influencing loyalty and repurchase/referral behavior"
      ],
      "original_question_hash": "df88c3d7"
    },
    {
      "question": "In occupational safety and health (OSH), adapting job demands to a worker’s physiological and psychological capabilities (person–environment fit) is used to prevent harm. How does achieving person–environment fit reduce the risk of workplace incidents?",
      "options": {
        "A": "By standardizing tasks and requiring workers to operate at maximal speed to remove variability and delays.",
        "B": "By keeping task demands within the worker's capabilities, which lowers physiological strain and cognitive load, reducing fatigue, errors, and incident rates.",
        "C": "By increasing monitoring and punitive enforcement to ensure strict procedural compliance.",
        "D": "By permitting workers to randomly select tasks so they develop broad, generalized resilience to varied demands."
      },
      "correct_answer": "B",
      "simplification_notes": "Question shortened and clarified the term 'person–environment fit'; language made more direct while retaining the OSH prevention context. Distractors were kept plausible and aligned with common but incorrect control strategies.",
      "content_preserved": true,
      "source_article": "Occupational safety and health",
      "x": 1.2642006874084473,
      "y": 0.8959609866142273,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The three-objective framework of occupational health (maintenance and promotion of workers’ health and capacity; improvement of the working environment for safety and health; development of work organization and culture that supports health and safety and may enhance productivity).",
        "Concept 2: Person–environment fit in OSH (adaptation of work to workers’ physiological and psychological capabilities; placing and maintaining workers in environments suited to their capabilities).",
        "Concept 3: Governance and implementation mechanisms (the role of common-law duties and statutory duties, and the use of company-level OSH programs to prevent incidents and diseases)."
      ],
      "original_question_hash": "173a1bcc"
    },
    {
      "question": "How do predictive or planned maintenance strategies change a system's reliability and life‑cycle costs, and through what mechanism do they act?",
      "options": {
        "A": "They time interventions using real‑time condition data or trend analysis to stop fault progression, thereby reducing unplanned downtime and deferring expensive replacements; this increases reliability and lowers life‑cycle costs.",
        "B": "They replace components at fixed calendar or usage intervals regardless of actual wear, which can keep performance consistent but often raises total costs and does not necessarily improve reliability.",
        "C": "They focus mainly on cosmetic or non‑functional upkeep that does not improve functional reliability but still generates maintenance costs.",
        "D": "They eliminate the need for maintenance by making components self‑healing, guaranteeing zero failures."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the original question to be more concise and direct for undergraduates while keeping technical terms (predictive, planned, condition data, life‑cycle costs, reliability). Options were rewritten to be plausible distractors and preserve the original correct choice.",
      "content_preserved": true,
      "source_article": "Maintenance",
      "x": 1.4438178539276123,
      "y": 0.9825975298881531,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The core goal of maintenance is to retain or restore a system’s functional state to meet required performance and serviceability.",
        "Concept 2: Maintainability as a design/operational characteristic that determines how easily a system can be retained or restored to serviceability.",
        "Concept 3: Proactive maintenance strategies (predictive/planned) and their impact on reliability and lifecycle costs by preventing failures."
      ],
      "original_question_hash": "0ecb8245"
    },
    {
      "question": "How does judicial review uphold the separation of powers within a constitutional system?",
      "options": {
        "A": "By allowing the legislature to veto court decisions that limit its policy choices.",
        "B": "By empowering the judiciary to determine whether executive or legislative actions exceed constitutional authority, and to invalidate those that do.",
        "C": "By requiring all branches to adopt identical policies before those policies gain legal legitimacy.",
        "D": "By insulating the judiciary from constraints so it can override the other branches at will."
      },
      "correct_answer": "B",
      "simplification_notes": "Question and answer choices were rewritten in concise academic language; historical/contextual details were removed but the core idea—that judicial review lets courts invalidate actions exceeding constitutional authority as a check on other branches—was preserved. Distractors were made plausible common misconceptions.",
      "content_preserved": true,
      "source_article": "Judicial review",
      "x": 1.2008637189865112,
      "y": 0.7711688280105591,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Judicial review as a mechanism that enforces separation of powers by allowing the judiciary to invalidate actions of the other branches that exceed authority.",
        "Concept 2: The difference between civil-law and common-law systems in how judges interact with law (creating new principles vs. applying existing law).",
        "Concept 3: The historical/theoretical foundations of judicial review (e.g., Montesquieu’s ideas and Marbury v. Madison) and how they institutionalize checks and balances in government."
      ],
      "original_question_hash": "edcaa0d9"
    },
    {
      "question": "Why can an officially produced new film or story change what counts as canon in a long-running fictional universe, even if it contradicts earlier material?",
      "options": {
        "A": "Because canon is a fixed list established at the start and cannot be revised.",
        "B": "Because canon is an official, authoritative framework that is not immutable; rights holders and higher-priority new works can redefine or retcon earlier material.",
        "C": "Because fan consensus and online popularity determine canonical status and can override creators or rights holders.",
        "D": "Because once a work is published, all its events are permanently immutable and cannot be altered by later official works."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was tightened for clarity and concision; franchise-specific examples were removed; emphasized that canon is authoritative but changeable via rights holders and higher-priority works (hierarchy/retcon).",
      "content_preserved": true,
      "source_article": "Canon (fiction)",
      "x": 0.8692342042922974,
      "y": 1.1778311729431152,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Canon as an official, authoritative framework that is not fixed and can be redefined by new material or reversions.",
        "Concept 2: Hierarchical canonicity (tiers) where higher-tier works determine or override the status of lower-tier or past material, shaping what is considered canonical.",
        "Concept 3: The actors and mechanisms that govern canonicity (creators, copyright holders, studios, and guiding bodies like Lucasfilm Story Group) and how their decisions influence what is allowed, retconned, or integrated into the canon."
      ],
      "original_question_hash": "b69373c2"
    },
    {
      "question": "In cochlear hair cells, deflecting a hair bundle toward its tall edge produces an excitatory receptor potential, whereas deflecting it toward the opposite (short) edge produces inhibition. Which explanation best accounts for this directional polarity?",
      "options": {
        "A": "Deflection toward the tall edge increases tension on tip links that gate the mechanotransduction channels, opening them to allow cations to enter and depolarize the cell; deflection toward the short edge relieves tension, closing the channels and causing inhibition.",
        "B": "Deflection toward the tall edge physically forces ions through voltage-gated ion channels, producing excitation independent of any mechanically gated channel.",
        "C": "Deflection toward the tall edge alters the local extracellular pH around the hair bundle, which changes channel conductance and generates an excitatory potential.",
        "D": "Deflection toward the tall edge mechanically damages or tears the membrane, causing uncontrolled ion influx that produces an excitatory response."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the stem concisely for undergraduates, used 'tall/short edge' and explicit terms (tip links, mechanotransduction channels, cation entry, depolarize). Distractor options made plausible yet incorrect.",
      "content_preserved": true,
      "source_article": "Mechanotransduction",
      "x": 1.8944367170333862,
      "y": 1.145397424697876,
      "level": 2,
      "concepts_tested": [
        "Mechanically gated ion channels convert mechanical stimuli into electrical/chemical signals, generating a transduction current and changing membrane potential.",
        "Hair cell transduction in the ear involves deflection of hair bundles caused by shear between the tectorial membrane and the organ of Corti, linking mechanical motion to excitatory/inhibitory electrical responses.",
        "Frequency mapping in the cochlea (base-cilia sensitive to high frequencies, apex to low frequencies) illustrates a conceptual relationship between mechanical stimulation location and sensory encoding."
      ],
      "original_question_hash": "0082c029"
    },
    {
      "question": "How do (i) the prosecution bearing the legal burden of proof and (ii) the requirement to prove guilt \"beyond a reasonable doubt\" together operate to preserve the presumption of innocence in a criminal trial?",
      "options": {
        "A": "Because the defense must produce alternative explanations that definitively prove the accused's innocence, which removes the prosecution's obligation to present evidence.",
        "B": "Because jurors may convict whenever the defendant fails to present any defense, and this practice effectively protects the presumption of innocence.",
        "C": "Because the prosecution must introduce sufficient evidence to eliminate reasonable doubt; if reasonable doubt remains, the accused must be acquitted, thereby maintaining the presumption of innocence.",
        "D": "Because the standard demands absolute certainty, so any unresolved doubt about facts leads to conviction if the defendant offers a possible explanation."
      },
      "correct_answer": "C",
      "simplification_notes": "Reworded the original multi-sentence question into a concise, direct query suitable for undergraduates; retained the two key elements (burden on prosecution and 'beyond reasonable doubt') and clarified the causal link to acquittal. Answer options were kept plausible but corrected for legal accuracy.",
      "content_preserved": true,
      "source_article": "Presumption of innocence",
      "x": 1.2028214931488037,
      "y": 0.7957026958465576,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Burden of proof and standard of proof (prosecution bears the burden to prove guilt; beyond a reasonable doubt as the threshold for conviction).",
        "Concept 2: Presumption of innocence as a universal principle and its cross-system application (how common law, civil law, and international law treat the principle).",
        "Concept 3: Relationship between evidence and verdict (the mechanism by which the presumption leads to acquittal when reasonable doubt remains; contrast with presumption of guilt)."
      ],
      "original_question_hash": "fe1d23ed"
    },
    {
      "question": "Neuroethics often separates issues into two types: those raised by what we can do (new neurotechnologies and interventions) and those raised by what we know (discoveries about neural causes of behavior, identity, etc.). Why is this two-category distinction useful for analysing ethical challenges?",
      "options": {
        "A": "It shifts focus onto current technologies and their risks, thereby sidelining theoretical or explanatory concerns about the brain.",
        "B": "It makes explicit that ethical problems can arise both from our capabilities and from our knowledge, and that advances in one domain can change the moral significance of issues in the other.",
        "C": "It implies that only neuroscientific knowledge about the brain matters ethically, so technological applications are ethically secondary.",
        "D": "It treats ethical responsibility as determined solely by whether an action or technology is technically possible, rather than by understanding, intention, or context."
      },
      "correct_answer": "B",
      "simplification_notes": "Question language was tightened and academic terms preserved; the two categories were restated concisely. Distractors were rephrased to remain plausible while keeping the original correct option (B).",
      "content_preserved": true,
      "source_article": "Neuroethics",
      "x": 1.0910800695419312,
      "y": 0.9911894202232361,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The dual scope of neuroethics — ethics of neuroscience vs neuroscience of ethics and their interrelationship.",
        "Concept 2: The two-category framework — issues arising from what we can do (technologies and actions) versus issues arising from what we know (neural bases and understanding of behavior).",
        "Concept 3: The philosophical implications of neuroscience — how neural findings relate to concepts like free will, moral responsibility, and personal identity."
      ],
      "original_question_hash": "b4dfcc14"
    },
    {
      "question": "Which set of conditions most directly permits density-driven segregation to form a metallic core during planetary formation?",
      "options": {
        "A": "A completely rigid mantle where chemical affinities alone carry dense elements outward or upward without any melting.",
        "B": "A mantle that partially melts and becomes plastic or viscous, allowing dense iron-rich metal and siderophile elements to sink by gravity and segregate toward the center.",
        "C": "Heavy elements becoming less dense when they alloy, so they float to the surface regardless of melting or viscosity.",
        "D": "Very rapid planetary rotation that sorts materials by centrifugal force, producing internal layering independent of melting or changes in mantle viscosity."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the stem in clearer academic language and kept technical terms (e.g., 'partial melt', 'siderophile') where relevant; condensed options to be concise and plausibly incorrect while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Planetary differentiation",
      "x": 1.869110107421875,
      "y": 0.9988036155700684,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Density-driven gravitational differentiation leading to interior layering (core, mantle, crust) and the role of melting/plasticity in allowing heavy materials to sink.",
        "Concept 2: Chemical differentiation via element affinities and partitioning (e.g., siderophile vs lithophile behavior) that can override simple density sorting.",
        "Concept 3: The role of heating (radioactive decay, accretion, and nebular processes) in generating partial melting and enabling differentiation, including loss of volatiles in the early solar system."
      ],
      "original_question_hash": "3280c9e0"
    },
    {
      "question": "Why does keeping a data visualization simple and uncluttered typically improve a viewer's ability to see and interpret data patterns?",
      "options": {
        "A": "Because simplicity lets you display a larger raw volume of data at once, so viewers can inspect more values.",
        "B": "Because it reduces viewers' cognitive load and leverages pre-attentive visual attributes (e.g., color, length, position) so relevant patterns become more salient.",
        "C": "Because simple designs guarantee uniform colour usage across elements, which alone prevents most sources of confusion.",
        "D": "Because simplicity discourages interactive features, forcing viewers to focus on the static image and avoid distraction."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording tightened for clarity; retained technical term 'pre-attentive' and gave examples; answer choices made concise and all plausible; correct option preserved as B.",
      "content_preserved": true,
      "source_article": "Data and information visualization",
      "x": 1.39054536819458,
      "y": 1.0785588026046753,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Visualization as a cognitive tool that enables exploration, interpretation, and insight by revealing patterns, relationships, and trends (often with interactivity).",
        "Concept 2: Core design principles for effective visualization (accuracy and currency of data, simplicity/uncluttered visuals, deliberate use of shapes and colors, appropriate context, and supportive text tailored to the audience).",
        "Concept 3: Conceptual distinctions among data visualization (quantitative data), information visualization (qualitative/abstract data), and scientific visualization (rendering realistic imagery to test hypotheses) and how their goals differ."
      ],
      "original_question_hash": "7f55c605"
    },
    {
      "question": "How does legitimacy based on the consent of the governed reduce the need for coercion, and what mechanism preserves a regime's legitimacy while people accept its exercise of power?",
      "options": {
        "A": "By making the ruler's authority independent of public approval, so leaders can ignore consent and need not respond to popular opinion.",
        "B": "By tying the right to rule to ongoing normative approval from the governed; legitimacy is sustained by a feedback loop in which perceived rightful use of power maintains acceptance, reducing coercion—if consent falls, legitimacy erodes.",
        "C": "By relying on formal religious or sacred endorsement (numinous legitimacy) that substitutes spiritual sanction for coercion and secures compliance.",
        "D": "By cementing a permanent, unchangeable set of institutions whose legally entrenched authority guarantees stability regardless of shifts in public consent."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording shortened and clarified: emphasised 'consent of the governed', reduced coercion, and phrased the sustaining mechanism as a feedback loop linking perceived rightful use of power to ongoing approval.",
      "content_preserved": true,
      "source_article": "Political legitimacy",
      "x": 1.2025092840194702,
      "y": 0.9312201142311096,
      "level": 2,
      "concepts_tested": [
        "Consent-based legitimacy: legitimacy derives from the consent or acceptance of the governed, linking authority to normative approval and reducing coercion.",
        "Weberian types of legitimacy: traditional, charismatic, and rational-legal sources of authority, each with distinct foundations and conditions for legitimacy.",
        "Historical-cultural basis of legitimacy: legitimacy can stem from culturally or historically sanctioned principles (e.g., Mandate of Heaven) and can be lost, prompting regime change or upheaval."
      ],
      "original_question_hash": "740509b8"
    },
    {
      "question": "According to Maxwell's equations, how does a time-varying electric field affect the magnetic field?",
      "options": {
        "A": "Magnetic fields are produced only by moving charges; a changing electric field has no effect on the magnetic field.",
        "B": "A changing electric field produces a displacement-current term that contributes to the curl of the magnetic field via Ampère–Maxwell law: $\\nabla\\times\\mathbf{B}=\\mu_{0}\\left(\\mathbf{J}+\\varepsilon_{0}\\frac{\\partial\\mathbf{E}}{\\partial t}\\right)$, so \\mathbf{B} can exist even where \\mathbf{J}=0.",
        "C": "Magnetic fields are generated independently of electric fields and arise only from permanent magnets or bulk mass currents, not from changing electric fields.",
        "D": "A changing electric field cancels the magnetic field, so the magnetic field becomes zero whenever the electric field varies in time."
      },
      "correct_answer": "B",
      "simplification_notes": "Question rephrased for undergraduate level; retained Maxwell's Ampère–Maxwell law written in inline LaTeX and shortened distractors while keeping them plausible.",
      "content_preserved": true,
      "source_article": "Electricity",
      "x": 1.756689429283142,
      "y": 1.0534063577651978,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Electric charge creates an electric field, and a charge experiences a force within that field (Coulomb’s law and the field concept).",
        "Concept 2: The motion of electric charges (electric current) produces a magnetic field; electric and magnetic fields are interconnected through electromagnetism (Maxwell’s equations).",
        "Concept 3: Electric potential (voltage) is the work required to move a charge between points in an electric field, relating to energy and field strength."
      ],
      "original_question_hash": "f011b25e"
    },
    {
      "question": "How do interparticle forces and particle mobility explain why a solid has a definite shape and volume, while a liquid has a definite volume but takes the shape of its container?",
      "options": {
        "A": "In a solid, strong interparticle forces constrain particles to vibrate about fixed lattice positions, giving a definite shape and volume; in a liquid, weaker forces allow particles to move past one another, so the substance flows and conforms to the container's shape while roughly preserving volume.",
        "B": "In a solid, particles move freely and spread out to form a definite shape; in a liquid, particles are fixed in place in a repeating lattice and cannot move.",
        "C": "The distinction is determined solely by temperature: once temperature is high enough both solids and liquids flow identically and their shapes are the same.",
        "D": "The difference between solid and liquid behavior comes from quantum spin alignment (magnetic ordering) rather than from particle arrangement or mobility."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened for undergraduate level, removing extraneous examples and focusing on interparticle force strength and particle mobility; options were made concise and all kept plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "State of matter",
      "x": 1.8102470636367798,
      "y": 1.0440093278884888,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The linkage between particle arrangement/motion and macroscopic properties (how interparticle forces and mobility determine fixed shape/volume vs. ability to flow or expand).",
        "Concept 2: Phase polymorphism and phase transitions (a single state can contain multiple phases with different microstructures, such as ice forms under different temperature/pressure).",
        "Concept 3: Magnetic ordering as a distinct state-defining mechanism (states determined by spin alignment, not just spatial arrangement, e.g., ferromagnetism vs. antiferromagnetism)."
      ],
      "original_question_hash": "72c512e5"
    },
    {
      "question": "How does integrating richer data collection, machine learning, and simulation tools change how a team makes decisions on and off the field?",
      "options": {
        "A": "It simply increases the amount of data available but leaves decision rules and how uncertainty is handled unchanged.",
        "B": "It lets teams explicitly quantify uncertainty, run simulations of alternative strategies, and compare projected outcomes before choosing actions.",
        "C": "It guarantees the team will pick the globally optimal play or business decision in every circumstance.",
        "D": "It fully replaces a coach's or manager's intuition with automated recommendations from models."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified the role of machine learning and simulations; used concise academic phrasing; kept four plausible options and retained the original correct choice.",
      "content_preserved": true,
      "source_article": "Sports analytics",
      "x": -0.33403027057647705,
      "y": 2.4220259189605713,
      "level": 2,
      "concepts_tested": [
        "Data collection and technology enable analytics (advanced statistics, machine learning, simulations) and drive decision-making.",
        "On-field analytics vs off-field analytics represent distinct yet related mechanisms with different objectives (performance optimization vs business growth).",
        "Analytics influence decision-making and outcomes (competitive advantage as in Moneyball; impact on betting and fan engagement)."
      ],
      "original_question_hash": "e92bb077"
    },
    {
      "question": "Why does religious syncretism typically produce a new, unified belief system rather than simply being a selective patchwork of distinct elements from different traditions?",
      "options": {
        "A": "Because it preserves each borrowed element as a separate, identifiable part—like a labeled collage—without altering their internal logic.",
        "B": "Because it incorporates borrowed elements into a single organizing framework that synthesizes their internal logics into a cohesive whole.",
        "C": "Because it depends on coercive enforcement that eliminates conflicting practices, leaving only a merged system.",
        "D": "Because it only occurs under heavy colonial domination, where the ruling tradition compels conformity and fusion."
      },
      "correct_answer": "B",
      "simplification_notes": "Question rephrased for clarity and concision; removed extended historical detail and examples; preserved the analytical distinction between syncretism (integrative synthesis) and eclecticism (selective patchwork).",
      "content_preserved": true,
      "source_article": "Syncretism",
      "x": 0.47513529658317566,
      "y": 0.4893428385257721,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Merging/assimilation of multiple belief systems into a new, unified religious system (mechanism).",
        "Concept 2: Distinction from eclecticism, emphasizing that syncretism integrates elements into a cohesive whole rather than mere selective adoption (principle of structural outcome).",
        "Concept 3: Contextual drivers and social motivations (proximity of traditions, conquest, and exclusivist vs inclusivist attitudes) that enable or constrain syncretism (causal/contextual relationships)."
      ],
      "original_question_hash": "7f85ac46"
    },
    {
      "question": "In traditions that teach rebirth (samsara), how does karma function to link a person's actions in one life to the conditions of future lives, and why does this mechanism make liberation (moksha or nirvana) possible?",
      "options": {
        "A": "Karma is the cumulative moral causal force produced by actions that \"seed\" future rebirth conditions; liberation is achieved by purifying or exhausting these karmic tendencies so they no longer ripen, ending the cycle.",
        "B": "Karma is a fixed decree assigned by a distant deity at birth and is unrelated to present conduct; liberation is therefore granted only by divine grace, not by ethical change.",
        "C": "Karma functions like a random lottery that sometimes awards better or worse lives; liberation occurs only when chance eventually produces a favourable outcome, independent of moral behaviour.",
        "D": "Karma is simply a tally of past lives; after a predetermined number of incarnations the soul is automatically freed regardless of how one acted in those lives."
      },
      "correct_answer": "A",
      "simplification_notes": "Question condensed to focus on how karma causally links actions to rebirth and how that link enables liberation; extraneous historical and comparative details were removed but technical terms (samsara, moksha/nirvana, karmic seeds) retained for precision.",
      "content_preserved": true,
      "source_article": "Afterlife",
      "x": 1.1434814929962158,
      "y": 1.0411425828933716,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Continuity of identity after death and the form it takes (soul/spirit/essential self) varies by belief system (spiritual realm vs reincarnation).",
        "Concept 2: Reincarnation as a mechanism (samsara) with karma guiding successive lives and the goal of liberation.",
        "Concept 3: Moral causation linking conduct in life to afterlife status or next life, and the variation of this relationship across different religious traditions (paradise/hell in Abrahamic traditions vs rebirth in Indian religions)."
      ],
      "original_question_hash": "fcce1f7f"
    },
    {
      "question": "According to ecological systems theory as applied to positive youth development, how do positive developmental outcomes for young people arise?",
      "options": {
        "A": "By correcting individual deficits alone, independent of social or environmental context.",
        "B": "Through dynamic, reciprocal interactions across multiple contexts (family, school, community), where supportive relationships enable youths to translate social and personal assets into positive contributions.",
        "C": "Solely from the most proximal environment (the family), with little influence from broader social contexts.",
        "D": "Only when external institutions impose structure and control, with minimal youth agency."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; retained term 'ecological systems theory' and PYD concepts; options rephrased for concision while keeping their original meanings; correct answer unchanged.",
      "content_preserved": true,
      "source_article": "Positive youth development",
      "x": 1.2289944887161255,
      "y": 0.9032332897186279,
      "level": 2,
      "concepts_tested": [
        "Asset-based, strength-focused approach to youth development (emphasizing potential and external contexts over problem correction)",
        "Ecological systems theory as the mechanism linking multiple environments and relationships to positive youth development",
        "Youth as assets and agents of change (viewing youth as contributors and the role of engagement, empathy, and productive activity)"
      ],
      "original_question_hash": "23f7ff5c"
    },
    {
      "question": "A Turing-style behavioural criterion says a machine is intelligent if its behaviour across a wide range of tasks is indistinguishable from a human's. Why does this criterion avoid arguments about whether the machine has genuine mental states, and what is a central limitation of relying on behavioural indistinguishability?",
      "options": {
        "A": "Because it defines intelligence by externally observable performance rather than by inner subjective states, thus avoiding ontological commitments about consciousness; however, a central limitation is that behaviourally indistinguishable performance could still occur without genuine understanding (as in the Chinese room critique).",
        "B": "Because it demands direct first-person reports of subjective consciousness from the machine, making the test impractically strict and impossible to apply reliably.",
        "C": "Because it holds that only human-like brain hardware can produce true intelligence, so machines are excluded by definition and the behavioural test therefore denies machine intelligence a priori.",
        "D": "Because it claims that observable behaviour can never indicate intelligence, so behavioural indistinguishability cannot be used to judge whether a machine is intelligent."
      },
      "correct_answer": "A",
      "simplification_notes": "Condensed the original multipart question into a single clear two-part question: (1) why Turing-style criteria sidestep debates about mental states, and (2) what main limitation remains. Removed background detail while keeping the Chinese room objection and the focus on behaviour vs. inner states.",
      "content_preserved": true,
      "source_article": "Philosophy of artificial intelligence",
      "x": 1.2044061422348022,
      "y": 1.0761576890945435,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Turing-style criterion for intelligence (behavioral indistinguishability vs. true minds) and its implications",
        "Concept 2: Physical symbol system hypothesis as the proposed mechanism for general intelligent action",
        "Concept 3: Strong AI versus critiques (e.g., machines having minds/consciousness) and the debate over whether computation entails mental states"
      ],
      "original_question_hash": "868d899e"
    },
    {
      "question": "How does basic (bench) biomedical research inform development of diagnostics and medical devices beyond its role in finding drug targets?",
      "options": {
        "A": "By revealing fundamental biological mechanisms and system interactions that identify measurable biomarkers and show how devices should interface with tissues or signals to produce accurate, safe, and clinically meaningful outcomes.",
        "B": "By primarily identifying molecular drug targets and optimizing pharmacokinetics, therefore providing little useful guidance for non-drug technologies.",
        "C": "By focusing mainly on animal or in vitro models that generally do not translate to human diagnostics or device design.",
        "D": "By only specifying ethical constraints (e.g., Declaration of Helsinki, IRB requirements) without contributing technical guidance on biomarker selection or device function."
      },
      "correct_answer": "A",
      "simplification_notes": "Shortened and clarified wording; emphasized biomarkers, device–biology interfaces, and translational role of basic research while keeping distractors about drugs, models, and ethics plausible.",
      "content_preserved": true,
      "source_article": "Medical research",
      "x": 1.333649754524231,
      "y": 0.957450807094574,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Bench-to-bedside translation – how basic research findings inform pre-clinical and clinical developments, and the purpose of translational research.",
        "Concept 2: Scope and mechanisms beyond pharmaceuticals – why and how basic research applies to diagnostics, medical devices, and non-pharmaceutical therapies, not just drug development.",
        "Concept 3: Ethical governance of human medical research – how the Declaration of Helsinki and IRB requirements shape the conduct of medical research."
      ],
      "original_question_hash": "9ce36f1d"
    },
    {
      "question": "Why do communities often disagree about what should be censored, and how does the fact that censorship standards are normative and change over time affect how censorship is applied across different media?",
      "options": {
        "A": "Because censorship establishes a single, permanent set of rules that every community and medium must follow.",
        "B": "Because moral standards and judgments about harm or obscenity are subjective and evolve over time and between cultures, so authorities and gatekeepers apply context‑dependent criteria that differ by era, society, and medium.",
        "C": "Because censorship decisions are determined only by the loudest or most powerful voices in a society, producing unchanging outcomes across media.",
        "D": "Because once material is suppressed in one medium, suppression automatically and uniformly spreads to all other media without further debate."
      },
      "correct_answer": "B",
      "simplification_notes": "Question was condensed and clarified for undergraduate readers; language simplified while keeping the original core idea that censorship standards are subjective, change over time, and lead to variable, context‑dependent applications across media.",
      "content_preserved": true,
      "source_article": "Censorship",
      "x": 1.2253837585449219,
      "y": 0.9555565118789673,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Censorship as a dual mechanism (external suppression by authorities and self-censorship by individuals) across multiple media with varied justificatory rationales.",
        "Concept 2: Censorship and its potential to hinder progress by limiting open discussion and critical examination of censored topics.",
        "Concept 3: The normative, evolving nature of censorship standards (subjectivity in morality/obscenity and changing values) leading to contested and variable applications."
      ],
      "original_question_hash": "df57ea4a"
    },
    {
      "question": "In the analogy form $A$ is to $B$ as $C$ is to $D$, why does attending to the relational structure ($A\\leftrightarrow B$) rather than to superficial feature matches (e.g. $A$ and $C$ merely look alike) enable transfer of meaning or function across different domains?",
      "options": {
        "A": "Because the cognitive mechanism encodes the relation between $A$ and $B$ and maps that same relation onto $C$, permitting an inference about $D$ from $C$ grounded in the same underlying relation.",
        "B": "Because it depends on matching surface features such as color, size or texture to generalize meaning across domains.",
        "C": "Because it requires $A$ and $C$ to be identical objects so that the inference about $D$ follows directly from their identity.",
        "D": "Because it forms arbitrary pairings between items to produce new predictions without preserving the relation that held between $A$ and $B$.\""
      },
      "correct_answer": "A",
      "simplification_notes": "Rewrote the question in clearer academic language, used inline LaTeX for notation ($A\\leftrightarrow B$, $A$ is to $B$ as $C$ is to $D$), and made distractors plausible while keeping the original correct answer and core concept (structure-mapping vs. surface similarity).",
      "content_preserved": true,
      "source_article": "Analogy",
      "x": 1.2646139860153198,
      "y": 1.0812960863113403,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Analogy as a core cognitive mechanism that transfers information or meaning from a source to a target across domains, enabling wide-ranging cognitive tasks (problem solving, prediction, explanation, generalization, etc.).",
        "Concept 2: Structural mapping in analogy, exemplified by the form A is to B as C is to D, highlighting relational correspondences and inferential reasoning over superficial similarity.",
        "Concept 3: Relationships to related cognitive constructs and theories (e.g., metaphor, homology, isomorphism, conceptual metaphor), and the role of analogy within broader cognitive frameworks and language."
      ],
      "original_question_hash": "bc217279"
    },
    {
      "question": "Let a group $G$ act on a complex manifold $X$ and let an automorphic form satisfy $f(g\\cdot x)=j_g(x)\\,f(x)$ for each $g\\in G$, with nonzero factors of automorphy $j_g(x)$. Why must the family $\\{j_g\\}$ obey the compatibility (cocycle) condition $j_{gh}(x)=j_g(h\\cdot x)\\,j_h(x)$ for all $g,h\\in G$ and $x\\in X$? How does this reflect associativity of the group action on $f$?",
      "options": {
        "A": "Because applying $h$ then $g$ to $x$ must equal applying $gh$ at once: $f(gh\\cdot x)=f(g\\cdot(h\\cdot x))=j_g(h\\cdot x)\\,j_h(x)\\,f(x)$, so this must equal $j_{gh}(x)\\,f(x)$, which forces $j_{gh}(x)=j_g(h\\cdot x)\\,j_h(x)$ and encodes associativity.",
        "B": "Because each $j_g$ must be a group homomorphism $G\\to\\mathbb{C}^\\times$, so the factors multiply like group elements and thus make $f$ linear in the group parameter.",
        "C": "Because to avoid any position dependence one must have $j_g(x)$ independent of $x$, which then automatically yields the stated multiplicative relation.",
        "D": "Because the cocycle condition implies $j_g\\equiv1$ for all $g$, so $f$ becomes strictly invariant under $G$ (no nontrivial factors are allowed)."
      },
      "correct_answer": "A",
      "simplification_notes": "Reduced wording and clarified the derivation using the chain of equalities $f(gh\\cdot x)=f(g\\cdot(h\\cdot x))$, retained the defining equation $f(g\\cdot x)=j_g(x)f(x)$ and the cocycle condition in LaTeX; made wrong options plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Automorphic form",
      "x": 1.6827796697616577,
      "y": 1.2105882167816162,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Invariance under a discrete subgroup generalizes the idea of periodicity and defines automorphic forms as well-behaved functions with symmetry under group actions.",
        "Concept 2: The transformation rule via a factor of automorphy (f(g·x) = j_g(x) f(x)) and the role of j_g in determining how functions transform under group action.",
        "Concept 3: The adelic/global viewpoint and its connections (left invariance under G(F), smoothness/growth conditions, relation to modular forms and Langlands program)."
      ],
      "original_question_hash": "2ec77e8b"
    },
    {
      "question": "Let $f$ be an automorphic form on $X$ for a group $G$ acting on $X$, with $f(g\\cdot x)=j_{g}(x)f(x)$. To ensure consistency when applying two group elements $g$ and $h$ in sequence, what cocycle condition must the factor of automorphy $j$ satisfy for all $g,h\\in G$ and all $x\\in X$?",
      "options": {
        "A": "$j_{gh}(x)=j_{g}(x)\\,j_{h}(x)$ for all $g,h\\in G$ and $x\\in X$.",
        "B": "$j_{gh}(x)=j_{g}(h\\cdot x)\\,j_{h}(x)$ for all $g,h\\in G$ and $x\\in X$.",
        "C": "$j_{g}(x)$ depends only on $x$ (i.e. is independent of $g$).",
        "D": "$j_{g}(x)=1$ for every $g\\in G$ and every $x\\in X$."
      },
      "correct_answer": "B",
      "simplification_notes": "Compressed wording, made the sequential application of $g$ then $h$ explicit, converted all notation to inline LaTeX, and retained four plausible answer choices including the correct cocycle condition.",
      "content_preserved": true,
      "source_article": "Automorphic form",
      "x": 1.6847856044769287,
      "y": 1.2144957780838013,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Transformation law with factor of automorphy (f(g·x) = j_g(x) f(x)) and its role in encoding how automorphic forms respond to group actions.",
        "Concept 2: Generalization from periodic functions to automorphic forms on general topological groups and the adelic framework unifying congruence subgroups.",
        "Concept 3: Equivalence between automorphic property and divisor invariance (divisor invariance under G, and j_g vs identity as the special case)."
      ],
      "original_question_hash": "f9d686d0"
    },
    {
      "question": "Why does a transport system work best when its infrastructure, vehicles, and operating rules are aligned, and how do terminals and interchanges influence overall traffic and transfer times?",
      "options": {
        "A": "Because each component can operate independently to provide service; terminals are mostly storage or holding points and therefore do not affect system flow.",
        "B": "Because overall performance is an emergent property of interacting components; when infrastructure capacity, vehicle capabilities, and operating procedures are coordinated, terminals and interchanges serve as efficient connectors that reduce transfer times and congestion.",
        "C": "Because only the fixed infrastructure determines throughput; vehicle characteristics and operational rules have negligible impact on the system's flow.",
        "D": "Because ownership models and regulation alone decide network outcomes; the physical design of terminals and interchanges is a secondary consideration."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was condensed and clarified for undergraduate readers, preserved technical terms (infrastructure, vehicle capabilities, operations, terminals, interchanges), removed background detail, and emphasized the interactional (emergent) nature of system performance. The correct answer letter and core concept were unchanged.",
      "content_preserved": true,
      "source_article": "Transport",
      "x": 1.6488351821899414,
      "y": 0.830400288105011,
      "level": 2,
      "concepts_tested": [
        "Concept 1: A transport system is a cohesive whole built from infrastructure, vehicles, and operations, and its effectiveness depends on how these three components work together (including how terminals and interchanges enable flow).",
        "Concept 2: Mode choice and inter-modal transport are optimization problems, where selection is based on cost, capability, and route, with multi-modal solutions increasing efficiency and reach.",
        "Concept 3: Regulation and ownership (public, private, or mixed) shape investment, financing, and service provision in transport, influencing performance, access, and sustainability."
      ],
      "original_question_hash": "7547e024"
    },
    {
      "question": "Statistical learning theory models training examples as i.i.d. draws from an unknown distribution $p(x,y)$. Why is this assumption made, and how does it justify using a separate test set to assess a learned predictor's ability to generalize to new data?",
      "options": {
        "A": "Because the objective is to find a function that exactly fits the training examples; if it fits the training data perfectly, a test set is unnecessary.",
        "B": "Because the predictor's true performance is the expected loss under $p(x,y)$, i.e. $E_{(x,y)\\\\sim p}[V(f(x),y)]$, which can differ from the empirical training loss; a test set sampled from the same $p(x,y)$ gives an unbiased estimate of this generalization performance.",
        "C": "Because assuming $p(x,y)$ formalizes the data's randomness and variability; thus one could rely on theoretical properties of $p(x,y)$ alone instead of using a test set to evaluate generalization.",
        "D": "Because $p(x,y)$ implies every input–output pair will eventually occur, so the model can memorize all pairs and generalization follows from memorization."
      },
      "correct_answer": "B",
      "simplification_notes": "Condensed wording; made the role of the unknown distribution explicit as $p(x,y)$ and expressed true performance as the expected loss $E_{(x,y)\\sim p}[V(f(x),y)]$. Kept technical terms (empirical risk, expected loss) and made distractors plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Statistical learning theory",
      "x": 1.612357497215271,
      "y": 1.1356728076934814,
      "level": 2,
      "concepts_tested": [
        "The objective of learning: inferring a predictive function from labeled data that generalizes to future inputs (and the role of training vs. test data in assessing generalization).",
        "The regression vs classification distinction based on the type of output (continuous vs discrete) and its implications for modeling and evaluation.",
        "The formal assumption of an underlying unknown distribution p(x, y) over input-output pairs and its significance for learning and generalization."
      ],
      "original_question_hash": "725deff2"
    },
    {
      "question": "Why can a very high-capacity supervised learning model achieve almost zero training error yet still generalize poorly to new, unseen data? Explain in terms of the bias–variance tradeoff and generalization.",
      "options": {
        "A": "Because very high capacity lowers bias but increases variance: the model can fit training examples and their noise (overfit), producing unstable predictions on new data unless more training data or regularization are applied.",
        "B": "Because very high capacity always improves generalization error: with enough capacity the model can approximate any function and therefore will perform better on unseen data.",
        "C": "Because generalization error depends only on the noise level in the test data, not on the model's tendency to overfit the training data.",
        "D": "Because very high capacity reduces the amount of information the model needs from the training set, making it inherently less sensitive to sampling variability and thus unrelated to overfitting."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording condensed and clarified to target undergraduates; retained technical terms (capacity, bias–variance, overfitting, regularization); removed extraneous procedural details from the original article.",
      "content_preserved": true,
      "source_article": "Supervised learning",
      "x": 1.6020175218582153,
      "y": 1.1427150964736938,
      "level": 2,
      "concepts_tested": [
        "Generalization and generalization error: how a model's performance on training data relates to its performance on unseen data.",
        "Feature representation and curse of dimensionality: how the choice and number of features affect learning accuracy and model efficiency.",
        "Training/validation/test workflow: the purpose of training data, validation (including cross-validation), and test data in building, tuning, and evaluating models."
      ],
      "original_question_hash": "749d843a"
    },
    {
      "question": "Why does bounded rationality predict that people will satisfice (choose a \"good-enough\" option) rather than maximize when facing a complex decision under time pressure?",
      "options": {
        "A": "Because their preferences are fundamentally inconsistent, so no single optimal choice can be identified.",
        "B": "Because cognitive capacity, available information, and time are limited, so decision-makers use an adequacy threshold and stop once an option meets it.",
        "C": "Because they intentionally ignore most information and accept a random outcome to speed up the decision.",
        "D": "Because external incentives always reward speed over accuracy, forcing quick but systematically suboptimal choices."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording shortened and clarified for undergraduate readers; kept the explanation that limited cognition, information, and time lead to satisficing. Distractors made concise and plausibly related to decision constraints.",
      "content_preserved": true,
      "source_article": "Bounded rationality",
      "x": 1.3018852472305298,
      "y": 0.9972769021987915,
      "level": 2,
      "concepts_tested": [
        "Bounded rationality as a departure from perfect rationality due to information, cognitive, and time limits.",
        "Satisficing: choosing a \"good enough\" option that meets adequacy criteria rather than the optimal one.",
        "Relationship to rational-choice theory: bounded rationality complements/contrasts with optimization-based models and explains the discrepancy between assumed rationality and real cognition."
      ],
      "original_question_hash": "872f22e0"
    },
    {
      "question": "Why is art's political influence often conditional—effective in some situations but not others?",
      "options": {
        "A": "Because its persuasive power depends on whether the work resonates with audiences' preexisting beliefs and values, which can amplify or mute its effect.",
        "B": "Because converting awareness or emotion produced by art into concrete political change requires additional mechanisms—organization, resources, institutions, or policy pathways—beyond the artwork itself.",
        "C": "Because contextual factors (timing, medium, dissemination channels, and political environment) can amplify or suppress artistic messages, making outcomes contingent.",
        "D": "All of the above"
      },
      "correct_answer": "D",
      "simplification_notes": "Made the question more direct and concise, clarified 'conditional' as situational effectiveness, tightened each option to a single coherent reason, preserved original meanings and examples implicitly.",
      "content_preserved": true,
      "source_article": "The arts and politics",
      "x": 0.49418073892593384,
      "y": 1.108359456062317,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Art as a force within power dynamics and political life; art responds to contemporaneous politics and can become a site of controversy and change.",
        "Concept 2: Conditional efficacy of art for political change; art can be useless in some contexts but can also be used effectively to promote political aims, depending on context and implementation.",
        "Concept 3: Poetry as a mobilizing political medium; poetry can build emotional unity, attract attention, and influence public discourse, with poets playing a quasi-legislative social role."
      ],
      "original_question_hash": "60b26174"
    },
    {
      "question": "Why do subgenres and fusion genres cause music-genre boundaries to become blurred and shift over time?",
      "options": {
        "A": "Because audiences and institutions resist change and enforce fixed labels, making genre boundaries more rigid.",
        "B": "Because genre classification is a socially negotiated, flexible system: when musicians blend features from different genres, the classification adapts by creating new subgenres and shifting boundaries.",
        "C": "Because genre boundaries are strictly determined by musical form, so new stylistic combinations cannot change established labels.",
        "D": "Because genre labels are determined solely by geographical origin, and since locations remain the same, perceived boundaries do not change."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; preserved the idea that genre boundaries shift due to social negotiation and blending; distractors kept plausible and aligned with article alternatives.",
      "content_preserved": true,
      "source_article": "Music genre",
      "x": 0.25004589557647705,
      "y": 1.591729998588562,
      "level": 2,
      "concepts_tested": [
        "Concept 1: A music genre is defined by a set of socially accepted rules and the thematic/content spirit, not solely by musical form.",
        "Concept 2: Genre classification is dynamic and subjective, with subgenres, fusion, and microgenres arising over time, making boundaries fluid.",
        "Concept 3: The relationship between genre and related terms (form and style) is debated in scholarship, with some treating genre and style as the same and others treating them as distinct concepts."
      ],
      "original_question_hash": "dc0cc573"
    },
    {
      "question": "During meiotic recombination, the SDSA (synthesis-dependent strand annealing) and double-Holliday-junction (DHJ) pathways give different products. Which mechanistic difference explains why SDSA typically yields non-crossovers while DHJ can yield crossovers, and what is the consequence for linkage of nearby loci?",
      "options": {
        "A": "In SDSA the invading 3' end is extended then dissociates and anneals to the other end without forming a stable Holliday junction, producing non-crossover gene-conversion products; in the DHJ pathway stable double Holliday junctions form and can be resolved as crossovers, exchanging distal chromosome segments and altering linkage between nearby loci.",
        "B": "In SDSA double Holliday junctions are formed and maintained, guaranteeing crossovers; in the DHJ pathway no Holliday junctions form, so crossovers do not occur.",
        "C": "SDSA exclusively uses the sister chromatid as the repair template, preserving parental linkage and preventing crossovers; DHJ uses the homologous chromosome and thus can create crossovers that reshuffle linked alleles.",
        "D": "SDSA always causes large-scale chromosomal rearrangements or translocations, whereas DHJ only produces small, local gene conversions and never yields crossovers."
      },
      "correct_answer": "A",
      "simplification_notes": "Question shortened and language clarified: emphasized the key mechanistic distinction (strand extension+dissociation in SDSA vs double Holliday junction formation/resolution in DHJ) and the effect on linkage of nearby loci.",
      "content_preserved": true,
      "source_article": "Genetic recombination",
      "x": 2.023592472076416,
      "y": 1.1485284566879272,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Genetic recombination generates novel allele combinations during meiosis and also participates in DNA repair, linking recombination to genetic diversity and genome maintenance.",
        "Concept 2: There are two mechanistic pathways for recombination in eukaryotes—non-crossover (gene conversion via SDSA) and crossover (via double Holliday junctions)—and each has different implications for genetic material transfer.",
        "Concept 3: The recombination process is driven by specialized enzymes (recombinases such as RecA in bacteria and analogous eukaryotic enzymes) and occurs in multiple cellular contexts (meiosis, mitosis, immune system diversification, and recombination-based DNA repair)."
      ],
      "original_question_hash": "de1a18e1"
    },
    {
      "question": "Why do data-driven (statistical) methods typically generalize better than rule-based systems when language is learned from incremental, positive-only input?",
      "options": {
        "A": "Because data-driven learning infers probabilistic regularities from usage and, given incremental exposure, constructs robust generalizations that reduce brittleness and the need for exhaustive hand-crafted rules.",
        "B": "Because data-driven learning depends on exhaustive negative evidence to prune impossible structures, making learning slower but ultimately more accurate than rule-based approaches.",
        "C": "Because data-driven methods merely memorize exact phrases from corpora and therefore cannot generalize beyond the observed sentences.",
        "D": "Because rule-based systems can be adapted to new languages without data, while data-driven methods require large annotated corpora, so data-driven approaches cannot generalize without extensive datasets."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded the question to be concise for undergraduates, emphasized 'incremental' and 'positive-only' input, and preserved core ideas about probabilistic learning, brittleness of rules, and the role of corpora. Distractors were made plausible but kept incorrect.",
      "content_preserved": true,
      "source_article": "Computational linguistics",
      "x": 1.3136850595474243,
      "y": 1.0991326570510864,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The shift from rule-based approaches to data-driven/statistical learning in computational linguistics, including the idea that explicit rules proved insufficient and learning depends on incremental input and positive evidence.",
        "Concept 2: The role of annotated corpora (e.g., Penn Treebank) as foundational resources that enable systematic linguistic analysis and computational modeling.",
        "Concept 3: Grounding language learning in perception and action via affordance-based models, linking actions, perceptions, and effects to words to achieve word-to-meaning mappings without relying on explicit grammatical structure."
      ],
      "original_question_hash": "12f2dbb5"
    },
    {
      "question": "Why is the Church–Turing thesis regarded as the central link between the informal idea of an effective algorithm and formal models of computation such as Turing machines?",
      "options": {
        "A": "Because it asserts that any function computable by an effective procedure can be computed by a Turing machine (and by other equivalent formal models), thus identifying a single robust class of functions that captures the informal notion of “effectively calculable”.",
        "B": "Because it proves that Turing machines are strictly more powerful than all other computational models, ensuring no other model can compute functions beyond Turing computability.",
        "C": "Because it shows that undecidable problems become decidable after reformulating them in a stronger formal system, collapsing the distinction between decidable and undecidable problems.",
        "D": "Because it claims any physical computation can be simulated by a Turing machine given unlimited time and memory, guaranteeing the practical implementability of any algorithm in the real world."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened to a clear, undergraduate-level sentence; distractors were rephrased to be plausible but incorrect; technical meaning of the thesis preserved.",
      "content_preserved": true,
      "source_article": "Computability theory",
      "x": 1.5750303268432617,
      "y": 1.1806570291519165,
      "level": 2,
      "concepts_tested": [
        "The Church–Turing thesis as the foundational principle linking algorithmic computability to formal computability",
        "Relative computability, reducibility notions, and degree structures as frameworks for classifying problems by their level of noncomputability",
        "Undecidability and the existence of noncomputable problems (e.g., Entscheidungsproblem, word problem for semigroups) to illustrate limits of algorithmic methods"
      ],
      "original_question_hash": "0b02dbe5"
    },
    {
      "question": "How does the forcing method show that a particular mathematical sentence is independent of a theory such as $ZFC$?",
      "options": {
        "A": "By deriving both the sentence and its negation inside the same model, thereby proving the theory inconsistent.",
        "B": "By constructing a larger model (a forcing extension) in which the sentence holds while the original model still satisfies the theory's axioms, showing the axioms do not decide the sentence.",
        "C": "By proving the sentence is true in every model of the theory, so it is a theorem of the theory.",
        "D": "By removing the original axioms and replacing the theory with a different axiomatic system in which the sentence can be proved."
      },
      "correct_answer": "B",
      "simplification_notes": "Rewrote the question concisely for undergraduates, clarified that forcing builds a larger 'forcing extension' model, and made each option a plausible but distinct account of independence.",
      "content_preserved": true,
      "source_article": "Mathematical logic",
      "x": 1.5603944063186646,
      "y": 1.1832008361816406,
      "level": 2,
      "concepts_tested": [
        "The relationship between formal systems and their expressive/deductive power, and what incompleteness theorems imply for goals like Hilbert's program.",
        "The forcing method as a cross-cutting mechanism used across multiple areas (set theory, model theory, recursion theory, intuitionistic mathematics) to study independence and consistency.",
        "The existence of multiple foundational frameworks (e.g., set theory vs. category theory, with toposes as generalized models) and how they relate logic to the foundations of mathematics."
      ],
      "original_question_hash": "484e9a18"
    },
    {
      "question": "How does increasing end fixity (making the column ends more restrained against rotation) change a slender column's critical buckling load, and which fundamental principle explains that change?",
      "options": {
        "A": "The critical buckling load depends only on material yield strength, so changing end fixity does not affect the column's stability.",
        "B": "Increasing end fixity changes the boundary conditions of the buckling eigenvalue problem, which alters the lowest buckling mode (effective length factor $K$) and raises the smallest eigenvalue — thus increasing the Euler critical load $P_{cr}=\\frac{\\pi^{2}EI}{(KL)^{2}}$.",
        "C": "Increasing end fixity reduces the column's cross-sectional area at the supports, creating local stress concentrations that directly lower the buckling load.",
        "D": "Buckling is determined solely by the column length and bending stiffness $EI$, and is independent of how the ends are constrained."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified 'end fixity' as restraint against rotation, referenced Euler buckling formula with effective length factor $K$, and made boundary‑condition dependence explicit. Options were rewritten to be concise and plausible while preserving the tested concepts.",
      "content_preserved": true,
      "source_article": "Strength of materials",
      "x": 1.7692451477050781,
      "y": 1.02682626247406,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The stress-strain relationship is governed by material properties (yield/ultimate strength, Young's modulus, Poisson's ratio) and determines deformation and failure modes (elastic vs. plastic behavior).",
        "Concept 2: Geometry and boundary conditions (dimensions, holes, constraints) govern internal stress distribution and stability (including buckling and stiffness).",
        "Concept 3: Calculated internal states (stresses, strains, deflections, buckling loads) are compared to design criteria to assess load-carrying capacity and reliability, illustrating the constitutive modeling framework."
      ],
      "original_question_hash": "60eef7d2"
    },
    {
      "question": "According to the \"common factors\" perspective, why do different psychotherapies often produce similar outcomes, and what does this imply about how therapeutic change occurs?",
      "options": {
        "A": "Because all therapies use essentially the same techniques, so similar outcomes result from identical procedures.",
        "B": "Because change is driven primarily by the therapist's authority and patients' short-term compliance, independent of therapeutic ingredients.",
        "C": "Because shared elements across modalities—such as the therapeutic alliance, therapist empathy, client expectations, and a supportive context—facilitate improvement, implying change arises mainly from relational and contextual mechanisms rather than technique alone.",
        "D": "Because outcomes are entirely determined by patients' baseline characteristics (e.g., severity, personality), not by the therapy itself."
      },
      "correct_answer": "C",
      "simplification_notes": "Reworded the original multi-part question into a concise undergraduate-level query. Emphasized the core 'common factors' idea (all therapies share relational/contextual elements) and its implication about mechanisms of change. Removed peripheral methodological debate and historical detail but preserved the tested concept and plausible distractors.",
      "content_preserved": true,
      "source_article": "Psychotherapy",
      "x": 1.2543220520019531,
      "y": 0.9972545504570007,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Common factors and the therapeutic relationship as key drivers of psychotherapy efficacy across different modalities",
        "Concept 2: Existence of multiple mechanisms of change and the incomplete understanding of how psychotherapy produces change",
        "Concept 3: Variability in efficacy across individuals and conditions, and ongoing methodological debates about how to evaluate psychotherapy (e.g., RCTs vs individualized approaches)"
      ],
      "original_question_hash": "04071c86"
    },
    {
      "question": "Which description best explains how copyright exceptions such as fair use or fair dealing balance creators' exclusive rights with the public interest?",
      "options": {
        "A": "By converting all uses that are convenient or useful to the public into automatic public-domain uses, replacing creators' exclusive rights for those uses.",
        "B": "By allowing any non-commercial use without permission, even when that use could harm the market for the original work.",
        "C": "By applying a contextual, multi-factor analysis — considering the purpose and character of the use, the nature of the work, the amount and substantiality taken, and the effect on the work's market — to decide when copying serves the public interest without unduly harming creators.",
        "D": "By expanding or extending protection for derivative works so creators retain stronger control while encouraging more variant copies under license."
      },
      "correct_answer": "C",
      "simplification_notes": "Question wording tightened for undergraduate level; legal citations and historical detail removed; fair use factors explicitly listed in option C; distractors reworded to remain plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Copyright",
      "x": 1.240319848060608,
      "y": 0.9062420725822449,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Copyright protects original expression, not ideas.",
        "Concept 2: The rights and limitations framework (reproduction, distribution, derivative works, moral rights) and how exceptions like fair use/fair dealing balance public interest.",
        "Concept 3: Territorial nature and lifecycle of rights (jurisdiction-specific, cross-border procedures, duration leading to public domain)."
      ],
      "original_question_hash": "53373d88"
    },
    {
      "question": "FOUPs and EFEMs in modern fabs are often purged with nitrogen, particularly when wafers use copper interconnects. What is the primary reason this nitrogen mini-environment increases device yield?",
      "options": {
        "A": "Because the nitrogen chemically dopes the wafer surface to form an oxidation‑resistant layer.",
        "B": "Because the nitrogen purge lowers the local partial pressures of oxygen and water vapor, reducing copper oxidation and the adsorption of contaminants that would cause defects.",
        "C": "Because nitrogen raises and uniformizes wafer temperature, enabling thermal 'self‑healing' of oxide defects.",
        "D": "Because the nitrogen purge provides a perfect mechanical barrier that completely isolates the wafers from cleanroom air and particles."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified to focus on copper interconnects and the role of nitrogen in FOUPs/EFEMs; distractor options made plausible but incorrect; technical cause (oxygen/moisture reduction causing less oxidation/contamination) preserved.",
      "content_preserved": true,
      "source_article": "Semiconductor device fabrication",
      "x": 1.619087815284729,
      "y": 1.0906673669815063,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Contamination control and yield—how mini-environments, nitrogen purges, and cleanroom practices reduce oxidation/contamination and improve wafer yield.",
        "Concept 2: Fab workflow and automation—how wafer flow (FOUPs, EFEM, automated handling) and die singulation create a scalable production pipeline.",
        "Concept 3: Interdependent fabrication steps—how sequential processes (oxidation, deposition, ion implantation, lithography, etching) interact to form integrated circuits and affect subsequent steps."
      ],
      "original_question_hash": "6ddee707"
    },
    {
      "question": "Why does inserting a transgene into a defined genomic locus usually give more predictable gene expression than inserting it randomly in the genome?",
      "options": {
        "A": "Because the host transcriptional machinery is equally accessible at all genomic sites, so expression does not depend on insertion location.",
        "B": "Because the chromosomal context at a defined locus — including local regulatory elements, chromatin state and controlled copy number — provides a stable regulatory environment, whereas random insertions can land in repressive or variable regions and/or have variable copy number, producing unpredictable expression.",
        "C": "Because random insertions always produce extra copies or concatemers of the transgene, which invariably cause higher expression than a single, targeted insertion.",
        "D": "Because targeting a single locus completely prevents any interaction with endogenous regulatory elements, eliminating position effects altogether."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified core idea using undergraduate-level terms (chromatin state, regulatory elements, copy number, position effects) and shortened the stem; preserved original options' intent and plausibility.",
      "content_preserved": true,
      "source_article": "Genetic engineering",
      "x": 2.0235068798065186,
      "y": 1.0605195760726929,
      "level": 2,
      "concepts_tested": [
        "Concept 1",
        "Concept 2",
        "Concept 3"
      ],
      "original_question_hash": "f893e7ee"
    },
    {
      "question": "Why is non-maleficence (\"first, do no harm\") best interpreted as a risk-minimizing decision rule that can justify doing nothing, rather than as an absolute ban on any action that might cause harm?",
      "options": {
        "A": "Because it requires always prioritizing potential benefits over harms, making the possibility of inaction irrelevant.",
        "B": "Because it requires weighing the expected harms and expected benefits of an intervention, and may justify doing nothing when expected harm exceeds likely benefit.",
        "C": "Because it holds that any action that could cause harm is impermissible, regardless of possible benefits.",
        "D": "Because it concerns only respect for patient autonomy and therefore does not direct assessments of harms and benefits."
      },
      "correct_answer": "B",
      "simplification_notes": "Question stem was condensed and language clarified to focus on non-maleficence as a risk-minimizing rule that permits inaction; distractor options were kept plausible and aligned with the original concepts (beneficence relation and misuse as an absolute rule or autonomy-only view). Historical detail from the article was omitted from the stem but the tested concept was preserved.",
      "content_preserved": true,
      "source_article": "Primum non nocere",
      "x": 1.1751039028167725,
      "y": 0.9640156626701355,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Non-maleficence as a decision-making principle that requires weighing potential harms against benefits, including doing nothing to avoid harm.",
        "Concept 2: The relationship between non-maleficence and beneficence, including debates about which principle takes priority in professional ethics.",
        "Concept 3: The historical origin and diffusion of the maxim (Primum non nocere), and how attribution and transmission can be contested or unclear."
      ],
      "original_question_hash": "5bff875c"
    },
    {
      "question": "Why model randomness as a family of random variables indexed by time, e.g. $\\{X(t)\\}_{t\\in T}$, instead of using a single random variable?",
      "options": {
        "A": "Because indexing by time defines joint distributions across epochs (e.g. joint laws of $(X(t_1),\\dots,X(t_n))$), so one can describe how the state evolves and study path-level properties such as continuity or jumps.",
        "B": "Because indexing by time forces the random variables to be identically distributed for all $t$, ensuring stationarity.",
        "C": "Because indexing by time implies that later times always give larger values of the state, so the process is monotone in time.",
        "D": "Because indexing by time makes the outcome deterministic once the time is specified, removing randomness."
      },
      "correct_answer": "A",
      "simplification_notes": "Question rewritten in concise academic language for undergraduates; introduced standard notation $\\{X(t)\\}_{t\\in T}$ and an explicit example of finite-dimensional joint laws; distractor options made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Stochastic process",
      "x": 1.6466151475906372,
      "y": 1.1506478786468506,
      "level": 2,
      "concepts_tested": [
        "Concept 1: A stochastic process as a family of random variables indexed by time (or another parameter), providing a dynamic model of randomness.",
        "Concept 2: Relationships among process classes (random walks, martingales, Markov, Lévy, Gaussian processes, random fields, renewal, branching) that capture different dependencies and behaviors within the modeling framework.",
        "Concept 3: Central canonical processes (Wiener/Brownian motion and Poisson process) as foundational models illustrating continuous evolution vs. discrete events and their broad applicability across disciplines."
      ],
      "original_question_hash": "a951f5c4"
    },
    {
      "question": "Why does choosing a coordinate frame a priori allow analytic geometry to express a geometric locus by a single algebraic equation?",
      "options": {
        "A": "Because fixing a coordinate frame gives a canonical map sending each point to coordinates $(x,y)$ (or $(x,y,z))$, so the condition “point belongs to the locus’’ can be written as an algebraic constraint on those coordinates.",
        "B": "Because it forces every curve or surface to be described by a polynomial of some fixed, bounded degree, so one polynomial equation always suffices.",
        "C": "Because it guarantees the identical algebraic equation will describe the same locus in any other coordinate frame without change.",
        "D": "Because it removes the need to translate geometry into algebra: coordinates already encode all spatial information intuitively, so no algebraic reformulation is required."
      },
      "correct_answer": "A",
      "simplification_notes": "Reworded for undergraduate readers, clarified ‘a priori coordinate frame’ as fixing a canonical map to coordinate tuples $(x,y)$ or $(x,y,z)$, and made all four options plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Analytic geometry",
      "x": 1.66764497756958,
      "y": 1.1968388557434082,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Representation of geometric objects via coordinates and algebraic equations, enabling numerical description and extraction of information from shapes.",
        "Concept 2: The principled link between algebra and geometry (using real-number algebra to study geometric continua), grounded in the Cantor–Dedekind axiom.",
        "Concept 3: The importance of a priori coordinate frameworks (as in analytic geometry) versus a posteriori imposition of coordinates on curves (as with Apollonius), highlighting a cause-and-effect distinction that enables geometry to be determined by equations."
      ],
      "original_question_hash": "130185a5"
    },
    {
      "question": "In $\\mathbb{R}^{3}$, the curl of a vector field $\\mathbf{F}$ is a pseudovector: under an orientation-reversing map (for example, a mirror reflection) curl $\\mathbf{F}$ changes sign while $\\mathbf{F}$ (a proper vector) does not. Which explanation best accounts for this pseudovector behavior?",
      "options": {
        "A": "Curl is computed from circulation using a cross product (effectively the cross product of two infinitesimal displacement vectors or an oriented area element). The cross product encodes handedness, so reversing orientation flips that handedness and changes the sign of any cross-product–based quantity such as curl, whereas a proper vector field $\\mathbf{F}$ does not pick up this extra sign.",
        "B": "Curl is actually a scalar quantity and scalars change sign under orientation reversal, so curl must flip sign like a gradient.",
        "C": "Curl depends only on gradients of scalar fields, so reversing orientation reverses gradient signs and therefore flips curl.",
        "D": "An orientation-reversing map preserves the local handedness of axes, so curl remains unchanged and does not flip sign."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording shortened and focused on handedness and cross product; used $\\mathbb{R}^{3}$ and $\\mathbf{F}$ in LaTeX; distractors kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Vector calculus",
      "x": 1.6827584505081177,
      "y": 1.186586618423462,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The central role of differentiating and integrating vector fields in connecting local field behavior to global quantities and applications.",
        "Concept 2: The distinction between vectors, scalar fields, and pseudovectors (e.g., curl as a pseudovector) and how orientation-reversing maps affect them.",
        "Concept 3: The limitation of the standard cross product for higher dimensions and the alternative of geometric algebra using the exterior product to achieve generalization."
      ],
      "original_question_hash": "cea57456"
    },
    {
      "question": "How does using absolute versus relative poverty as the primary measure change which policy levers are most effective for reducing poverty?",
      "options": {
        "A": "Absolute poverty sets a universal minimum for basic needs, so effective policies target raising basic consumption and access to services (e.g., cash transfers, subsidized essentials, public health, electrification). Relative poverty defines poverty by how far people fall below the society’s median standard, so effective policies emphasize redistribution, reducing inequality, inclusive growth and measures to combat social exclusion.",
        "B": "Because absolute poverty uses a dollar-based threshold, it encourages adopting the same international policy packages everywhere, making it easier to export standardized interventions.",
        "C": "Relative poverty removes basic needs from consideration, so policymakers focus mainly on symbolic or status-oriented reforms rather than food, shelter or health provision.",
        "D": "The choice between absolute and relative measures does not affect policy design; it only changes how poverty is reported in statistics."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording condensed and clarified; retained distinction between absolute (basic needs/floor) and relative (gap/inequality) measures and their policy implications; removed peripheral examples and technical history while keeping plausible distractors.",
      "content_preserved": true,
      "source_article": "Poverty",
      "x": 1.2065072059631348,
      "y": 0.8925538659095764,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Absolute vs. relative poverty as context-dependent measures and their implications for evaluating poverty and designing policy.",
        "Concept 2: Poverty as interconnected with other social and environmental issues (how poverty exacerbates or is exacerbated by gender, disability, climate change, deforestation, biodiversity loss, and conflict).",
        "Concept 3: Policy frameworks and governance for poverty alleviation (SDG 1, electrification, housing-first approaches, and linking poverty reduction to broader societal goals)."
      ],
      "original_question_hash": "112461f9"
    },
    {
      "question": "Why must technology management in complex systems include explicit defenses against hazards, and how do properties like nonlinearity and emergence justify that design choice?",
      "options": {
        "A": "Because nonlinearity lets small disturbances produce large, unpredictable cascades, and emergence creates new hazards from component interactions; therefore defenses (redundancy, monitoring, containment) are needed to reduce both the probability and impact of such cascades.",
        "B": "Because nonlinearity makes system responses predictable and proportional, so hazards are easily controlled and explicit defenses are unnecessary.",
        "C": "Because emergence implies all system risks can be anticipated and eliminated by perfect modeling, removing the need for defensive design.",
        "D": "Because adaptation mechanisms will automatically eliminate hazards over time, so explicit defensive measures are merely optional."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording condensed and clarified; preserved two-part rationale (need for defenses + justification from nonlinearity and emergence); examples of defenses (redundancy, monitoring, containment) added for concreteness; distractor options made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Technology policy",
      "x": 1.2546941041946411,
      "y": 0.9294720888137817,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Technology policy as active industrial policy to address market failures and align technology development with national goals.",
        "Concept 2: The distinction between technology policy and science policy, outlining different aims (development of technology vs development of science) and public measures for mobilising resources.",
        "Concept 3: Technology management in complex systems, including properties like nonlinearity, emergence, adaptation, feedback, and the need for defenses against hazards."
      ],
      "original_question_hash": "d86d8259"
    },
    {
      "question": "In a vector space model, documents and queries are vectors over the same vocabulary and similarity is measured by cosine of the angle between vectors. Why does using tf-idf weights improve the ability of cosine similarity to discriminate between documents compared with using raw term frequencies?",
      "options": {
        "A": "Because tf-idf multiplies local term frequency by inverse document frequency ($\\text{tf-idf}=\\text{tf}\\times\\text{idf}$), down-weighting very common terms (low idf) and up-weighting rarer, informative terms (high idf); as a result the dot product and cosine are dominated by informative overlaps, so similarity reflects meaningful matches more strongly.",
        "B": "Because tf-idf scales all terms to the same level so the angle between vectors depends only on whether terms appear or not, not on their informativeness.",
        "C": "Because tf-idf effectively converts cosine similarity into a distance metric that ignores vector direction, which reduces its ability to discriminate documents.",
        "D": "Because tf-idf causes longer documents to accumulate larger term weights (more term counts), so longer documents automatically dominate cosine similarity scores."
      },
      "correct_answer": "A",
      "simplification_notes": "Rewrote the question concisely for undergraduates, kept the vector-space and cosine-similarity context, explicitly noted tf-idf as $\\text{tf-idf}=\\text{tf}\\times\\text{idf}$, and made all options plausible while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Vector space model",
      "x": 1.6467543840408325,
      "y": 1.1712228059768677,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Representing documents and queries as vectors in a common term-space, with relevance inferred from geometric relationships (e.g., angle or distance between vectors).",
        "Concept 2: Term weighting (especially tf-idf) to encode term importance within vectors, influencing similarity and ranking.",
        "Concept 3: The dimensionality equals the vocabulary size under a bag-of-words representation, enabling vector operations to compare and rank documents."
      ],
      "original_question_hash": "15d7a85f"
    },
    {
      "question": "During the initial weeks of a progressive resistance training program, trainees often increase their maximal force before any measurable muscle hypertrophy. What primary mechanism best explains these early strength gains?",
      "options": {
        "A": "Rapid hypertrophy from accelerated protein synthesis that quickly enlarges muscle fibers and increases force.",
        "B": "Improved neural drive: better motor-unit recruitment, higher firing rates, and improved intermuscular coordination that raise force output without large size changes.",
        "C": "Fast structural remodeling of bones and tendons that immediately improves leverage and force transmission.",
        "D": "Quick upregulation of metabolic enzymes and energy pathways within days that directly increases the muscle's contractile force."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified ‘initial phase’ and restated the question concisely for undergraduates; emphasized the contrast between early force gains and later hypertrophy; kept original incorrect alternatives (hypertrophy, connective-tissue remodeling, metabolic changes) as plausible distractors.",
      "content_preserved": true,
      "source_article": "Strength training",
      "x": 1.7989567518234253,
      "y": 1.0995913743972778,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Progressive overload as the driver of strength adaptation (repeatedly increasing resistance over time leads to growth in muscle strength).",
        "Concept 2: Neurological adaptations as an early mechanism for strength gains (the brain's ability to drive muscular contractions contributes before large-scale hypertrophy occurs).",
        "Concept 3: The role of proper form versus cheating (how form affects targeted overload, safety, and the potential use of cheating to overcome plateaus)."
      ],
      "original_question_hash": "03709c98"
    },
    {
      "question": "For an alloy that can undergo both diffusion-controlled phase changes (e.g., pearlite, bainite, precipitation) and a diffusionless martensitic transformation, how does the cooling rate determine which mechanism dominates, and what is the typical effect on hardness and ductility?",
      "options": {
        "A": "Rapid cooling suppresses atomic diffusion, preventing diffusion-controlled transformations and forcing a diffusionless martensitic change; this normally increases hardness and reduces ductility.",
        "B": "Rapid cooling increases atomic diffusion, promoting fast diffusion-controlled transformations like pearlite or bainite, which increases ductility.",
        "C": "Slow cooling promotes diffusionless martensite formation because low cooling rates allow grains to rearrange into martensitic structures, increasing hardness.",
        "D": "Cooling rate has no effect; martensite formation is determined only by alloy composition and occurs independently of the cooling path."
      },
      "correct_answer": "A",
      "simplification_notes": "Compressed and clarified the original question: explicitly named diffusion-controlled products as examples, focused on the role of cooling rate and mechanical consequences (hardness, ductility), and made each option concise but plausible.",
      "content_preserved": true,
      "source_article": "Heat treating",
      "x": 1.7856645584106445,
      "y": 0.9997913241386414,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Heat treatment alters microstructure (grain structure and phase composition) by controlling diffusion rates and cooling rates, thereby changing mechanical properties such as hardness, strength, and ductility.",
        "Concept 2: There are two key mechanisms driving property changes: diffusion-controlled phase transformations and martensitic transformation, each depending on temperature paths.",
        "Concept 3: Precipitation strengthening involves nucleation and growth of second phases due to changes in solubility and allotropy/polymorphism, creating multiphase microstructures that affect material behavior."
      ],
      "original_question_hash": "6c4f3fd1"
    },
    {
      "question": "How do bibliometric indicators such as the impact factor and the h-index create incentives that affect researcher behavior and institutional rankings?",
      "options": {
        "A": "They perfectly capture all dimensions of research quality, so researchers have no incentive to change their behavior to improve metrics.",
        "B": "They serve as decision proxies used by funders, administrators, and policymakers; because hiring, funding, and rankings depend on these numbers, researchers and institutions alter publication and citation strategies to raise their metrics, which changes behavior and rankings.",
        "C": "They are too noisy and unreliable to influence decisions, so researchers maintain the same practices regardless of the metrics.",
        "D": "They only increase administrative reporting and bureaucracy and do not affect day-to-day research choices or publication strategies."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording was tightened for clarity and concision, emphasizing that metrics act as decision proxies and drive strategic behavior; extraneous historical detail was removed. Technical terms (impact factor, h-index) were retained.",
      "content_preserved": true,
      "source_article": "Scientometrics",
      "x": 1.1801786422729492,
      "y": 0.9931982159614563,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Quantitative measurement of scholarly impact (via citations and journal metrics) as a basis for policy and management decisions.",
        "Concept 2: The use of bibliometric indicators (e.g., impact factor, h-index) creates incentives that influence researcher behavior and institutional ranking.",
        "Concept 3: The interdisciplinary relationships and evolution of scientometrics (its connections to informetrics, science policy, sociology of science) and how data/computation advances enable analysis of scientific outputs."
      ],
      "original_question_hash": "ee80a17a"
    },
    {
      "question": "Why might simply increasing the volume of health care services fail to improve population health, given the four core functions of a health system (provision of services, resource generation, financing, stewardship)?",
      "options": {
        "A": "Provision of services alone determines population health, so expanding service volume will automatically improve outcomes.",
        "B": "All four functions interact; expanding service volume without proportional resource generation, financing, and effective stewardship can create bottlenecks, misallocation, waste, and greater inequities, so health outcomes depend on coherent coordination.",
        "C": "Financing is the sole driver of health outcomes; provision and stewardship are mainly cost centers and do not materially affect results.",
        "D": "Effective stewardship alone can reorganize the system so that the other functions (provision, resources, financing) become irrelevant to population health."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and focused; the four functions were named explicitly. Options were made more concise while keeping all plausible alternatives and preserving the original correct choice (B).",
      "content_preserved": true,
      "source_article": "Health system",
      "x": 1.2825207710266113,
      "y": 0.9021350145339966,
      "level": 2,
      "concepts_tested": [
        "The four vital functions of a health system: provision of care, resource generation, financing, and stewardship, and how their interaction drives system performance.",
        "The stated goals and evaluation dimensions (good health, responsiveness, fair financing; plus quality, efficiency, equity) and how trade-offs among them shape outcomes.",
        "Contextual influence and governance structures (centralized vs decentralized, cultural/history/economic factors) and the idea that performance standards vary by context, precluding universal criteria."
      ],
      "original_question_hash": "ea79fff9"
    },
    {
      "question": "Why does contour ploughing reduce soil erosion on sloped land?",
      "options": {
        "A": "It directs surface runoff to flow along the contour lines, slowing and spreading water in the furrows which increases residence time and infiltration, thereby reducing erosion.",
        "B": "It channels runoff straight downslope into narrow fast-flowing rills, accelerating soil removal and erosion.",
        "C": "It compacts and seals the soil surface, reducing infiltration and so increasing surface runoff and erosion.",
        "D": "It removes crop residues and leaves soil exposed to wind and water erosion."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified the physical mechanism (slows and spreads runoff, increases infiltration). Distractor options made plausible and concise.",
      "content_preserved": true,
      "source_article": "Soil conservation",
      "x": 1.7736884355545044,
      "y": 0.952818751335144,
      "level": 2,
      "concepts_tested": [
        "Erosion control mechanisms: how practices like contour ploughing, terracing, windbreaks, and perimeter runoff control reduce runoff, preserve soil, and maintain fertility.",
        "Soil carbon and climate link: how soil acts as a carbon sink and contributes to climate change mitigation through conservation practices.",
        "Governance and adoption of practices: how political/economic actions and policy (e.g., land valuation, agricultural policy) influence the implementation of soil conservation methods."
      ],
      "original_question_hash": "f0d5d13d"
    },
    {
      "question": "How do editorials and opinion columns in a periodical primarily affect how readers engage with and interpret the publication’s content?",
      "options": {
        "A": "They provide objective, neutral summaries of all articles so that readers arrive at uniform conclusions.",
        "B": "They articulate a guiding stance and frame interpretation, helping define the publication’s identity and encouraging ongoing reader dialogue.",
        "C": "They occupy space that could otherwise be used for more reporting or features, potentially reducing the range of topics covered.",
        "D": "They serve as a legal or formal designation that strictly separates opinion pieces from news reporting, preventing any overlap between sections."
      },
      "correct_answer": "B",
      "simplification_notes": "Question and options were shortened and clarified for undergraduate readers; retained the article’s point that periodicals include editorials and columns which frame interpretation and shape identity. Distractors remain plausible.",
      "content_preserved": true,
      "source_article": "Periodical literature",
      "x": 1.1261042356491089,
      "y": 0.9950487017631531,
      "level": 2,
      "concepts_tested": [
        "Serial publication and cadence: periodicals are published on a regular schedule (volumes and issues) to organize content and facilitate citation.",
        "Internal organizational structure: periodicals commonly include editorials, reviews, and opinion columns, reflecting their role in guiding reader engagement and presenting content.",
        "Distinction among related serials: periodicals differ from book series and encyclopedias/dictionaries, with the idea of a periodical series as a group sharing common characteristics."
      ],
      "original_question_hash": "501e360c"
    },
    {
      "question": "How does a security interest give a creditor priority in a debtor's bankruptcy, and what happens if the secured collateral does not cover the full debt?",
      "options": {
        "A": "It lets the creditor claim all of the debtor's assets in bankruptcy, eliminating the rights of unsecured creditors.",
        "B": "It attaches to specific collateral and gives the creditor priority to the sale proceeds from that collateral ahead of unsecured creditors; if those proceeds are insufficient, the remaining shortfall becomes an unsecured deficiency claim.",
        "C": "It automatically converts the creditor's claim into equity in the debtor's company, diluting existing shareholders.",
        "D": "It prevents the debtor from filing for bankruptcy, forcing enforcement of the security only outside insolvency."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduate readers; removed extended examples and jurisdictional detail while retaining the mechanism (attachment to collateral, priority in bankruptcy) and outcome when collateral is insufficient (deficiency becomes unsecured). Options kept plausible and distinct.",
      "content_preserved": true,
      "source_article": "Security interest",
      "x": 1.3222707509994507,
      "y": 0.8356161713600159,
      "level": 2,
      "concepts_tested": [
        "Mechanism: security interests provide recourse to collateral and establish creditor priority in bankruptcy",
        "Creation and scope: secured interests arise by agreement or operation of law; include third-party security and non-financial obligations; vary by country",
        "Economic rationale and debates: reasons for taking security, priority over unsecured creditors, and critiques (e.g., floating charges, debates among economists)"
      ],
      "original_question_hash": "7c00a1a9"
    },
    {
      "question": "Why does the concept of credit — a system of deferred payment — apply to goods and services as well as to money?",
      "options": {
        "A": "Because many non-financial items have intrinsic liquidity comparable to cash and can always be converted into money.",
        "B": "Because credit depends on legally enforceable promises to repay or perform, so it can be used for any exchange where deferred reciprocity is expected.",
        "C": "Because lenders require collateral for every non-monetary transaction, treating goods and services the same as cash loans.",
        "D": "Because credit for goods and services must use a fixed interest rate set by law for all non-monetary transactions."
      },
      "correct_answer": "B",
      "simplification_notes": "Question rephrased for clarity and concision; removed historical examples; emphasized that credit is deferred payment grounded in enforceable promises, and preserved the scope covering money, goods, and services.",
      "content_preserved": true,
      "source_article": "Credit",
      "x": 1.3512595891952515,
      "y": 0.8990389704704285,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Credit formalizes reciprocity via legally enforceable promises, enabling deferred payment across many participants.",
        "Concept 2: Credit covers financial loans as well as non-financial goods/services, i.e., any form of deferred payment.",
        "Concept 3: Revolving credit (as with credit cards) is a mechanism that allows ongoing borrowing and repayment over time with finance charges, expanding purchasing power."
      ],
      "original_question_hash": "521fa012"
    },
    {
      "question": "Why are fiat currencies widely accepted as a medium of exchange even though they have no intrinsic commodity value?",
      "options": {
        "A": "Because the state declares them legal tender and social conventions (including tax obligations and mutual expectations) make people confident others will accept them for payments and to discharge debts.",
        "B": "Because they are backed by gold reserves and their physical material has intrinsic value that guarantees their worth.",
        "C": "Because the physical banknote or digital record has inherent usefulness or value on its own, independent of whether others accept it.",
        "D": "Because the government guarantees each unit can be redeemed on demand for a fixed quantity of a commodity, as under a gold standard."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified; legal tender, social convention, and tax-backed demand were emphasized in option A; distractors were kept plausible and aligned with historical/alternative monetary systems.",
      "content_preserved": true,
      "source_article": "Money",
      "x": 0.8172651529312134,
      "y": 0.41736918687820435,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Functions of money (medium of exchange, unit of account, store of value, standard of deferred payment) and how these functions justify its use.",
        "Concept 2: Fiat money and legal tender as the basis of value, rooted in social convention and government declaration, rather than intrinsic value.",
        "Concept 3: Money supply composition and the banking mechanism (currency in circulation vs bank money; deposits enabling cashless payments; banks as creators of broad money)."
      ],
      "original_question_hash": "90721b58"
    },
    {
      "question": "In a fiat monetary system, why does most new money enter the economy through commercial-bank lending rather than direct central-bank issuance, and how do central-bank tools affect that process?",
      "options": {
        "A": "Because commercial banks only re-lend existing central-bank reserves, so loans simply move reserves around and do not expand deposits; therefore the central bank must print new money to fill any shortfall.",
        "B": "Because when a commercial bank makes a loan it creates a new deposit for the borrower, which expands the money supply; central-bank tools such as policy interest rates, reserve requirements, and short-term lending facilities influence banks' capacity and willingness to create those loans.",
        "C": "Because the central bank directly credits the borrower’s account with base money when a loan is approved, while commercial banks merely transfer those central-bank funds between customers.",
        "D": "Because the money stock is fixed by law or only changed by fiscal actions, so neither commercial-bank lending nor central-bank policy independently alters the total money supply."
      },
      "correct_answer": "B",
      "simplification_notes": "Rephrased the original multipart question into a single clear sentence for undergraduates; used 'commercial banks' consistently; preserved technical terms (policy interest rates, reserve requirements); kept the correct answer and made all distractors plausible misconceptions.",
      "content_preserved": true,
      "source_article": "Monetary system",
      "x": 1.274306058883667,
      "y": 0.8543382287025452,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Money creation mechanism in fiat systems—money is created primarily through bank lending, with central banks influencing the money supply via interest rates and reserve requirements rather than directly printing money.",
        "Concept 2: Distinction between fiat money and commodity/commodity-backed money—fiat money relies on government authority and legal tender status to enable discretionary monetary policy.",
        "Concept 3: Relationship between monetary system type and macroeconomic outcomes—the choice of monetary system affects inflation, exchange rates, and trade balances through the underlying mechanisms of money creation and policy tools."
      ],
      "original_question_hash": "2154281d"
    },
    {
      "question": "How do Rab proteins create docking specificity during intracellular vesicle trafficking, and what is the outcome if the Rab on a vesicle does not match the destination membrane's docking factors?",
      "options": {
        "A": "The Rab on the vesicle, in its GTP-bound state, binds specifically to tethering/docking factors on the target membrane; this selective Rab–tether interaction recruits tethering complexes and SNAREs, aligning the vesicle for fusion. If the vesicle's Rab does not match the target's tethering proteins, docking fails and fusion does not occur.",
        "B": "Docking specificity is determined only by coat proteins that formed the vesicle; Rab proteins only mediate long-range movement along cytoskeletal tracks, so a Rab mismatch has no effect on docking or fusion.",
        "C": "Any Rab present on a vesicle can permit fusion with any membrane as long as compatible SNAREs and lipids are present; specificity is governed solely by SNARE pairing or membrane lipid compatibility, not Rab–tether interactions.",
        "D": "Rab proteins act as ATPases that generate motor force for vesicle transport along microtubules or microfilaments; docking is a passive process that does not require a specific Rab identity."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was shortened and clarified for undergraduates; emphasized that Rab in GTP form binds tethering factors and recruits SNAREs; retained the contrast between successful docking/fusion and failure when Rab identity mismatches. Distractors were made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Membrane vesicle trafficking",
      "x": 2.218345880508423,
      "y": 1.1481791734695435,
      "level": 2,
      "concepts_tested": [
        "Budding: coat protein–driven formation of vesicles from the Golgi/ER and release into the cytoplasm.",
        "Intracellular transport: motor proteins (myosin, kinesin, dynein) moving vesicles along microtubules and microfilaments using ATP, establishing directionality (anterograde/retrograde).",
        "Docking and fusion specificity: Rab proteins and docking machinery ensuring vesicles fuse with the correct target membrane."
      ],
      "original_question_hash": "2cdfe731"
    },
    {
      "question": "Why does the endomembrane system behave as a single functional unit even though it consists of different organelles?",
      "options": {
        "A": "Because the organelles form one continuous membrane network so cargo can move freely between them without vesicle-mediated transport.",
        "B": "Because membranes and cargo are exchanged by vesicle budding/fusion and by direct membrane contact sites, creating an integrated transport and trafficking network.",
        "C": "Because every membrane in the system has the same lipid composition and protein content, so they all function identically.",
        "D": "Because chloroplasts (plastids) supply and coordinate the membranes via photosynthetic signaling, linking the compartments."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified for undergraduates; retained technical terms (vesicle budding/fusion, membrane contact sites); distractors made plausible but incorrect; kept original correct answer.",
      "content_preserved": true,
      "source_article": "Endomembrane system",
      "x": 2.1050164699554443,
      "y": 1.1535496711730957,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The endomembrane system is a single functional and developmental unit connected by direct membrane contacts or vesicle-mediated transport.",
        "Concept 2: The membranes of the endomembrane system, while structurally and functionally diverse, share a common lipid bilayer, illustrating unity amid specialization.",
        "Concept 3: The endomembrane system is distinct from mitochondria and plastids but may have evolved partly from actions of mitochondria, highlighting evolutionary relationships among cellular membranes."
      ],
      "original_question_hash": "ccb5e009"
    },
    {
      "question": "A shared platform in a software ecosystem defines common interfaces, data formats, and governance rules so modules from different participants can interoperate. Why does this arrangement typically speed up the co-evolution of the projects in the ecosystem compared to projects developed in isolation?",
      "options": {
        "A": "Because it forces every participant to implement identical features, producing uniform evolution across projects.",
        "B": "Because it centralizes decision-making so individual projects lose autonomy and cannot adapt to local needs.",
        "C": "Because it provides common standards, stable integration points, and feedback channels that reduce integration friction and align incentives across participants.",
        "D": "Because it removes the need for interface contracts entirely, since the platform automatically handles all interactions."
      },
      "correct_answer": "C",
      "simplification_notes": "Rewrote the question in clearer, more concise academic language while keeping the core scenario (shared platform enabling interoperability). Shortened option wording and kept all four choices plausible; preserved the original correct rationale as option C.",
      "content_preserved": true,
      "source_article": "Software ecosystem",
      "x": 1.4184439182281494,
      "y": 1.065547227859497,
      "level": 2,
      "concepts_tested": [
        "Shared platform enabling interaction, exchange of information/resources/artifacts, and relationships among ecosystem participants",
        "Co-evolution of software projects within a common organizational/social/technical environment",
        "Ecosystem-level software analysis as a methodological approach that analyzes multiple projects within the ecosystem context"
      ],
      "original_question_hash": "496ad80c"
    },
    {
      "question": "How does the presence of an audience transform a private leisure activity into \"entertainment,\" and how do passive versus active audience roles contribute to that transformation?",
      "options": {
        "A": "The audience provides a social frame that assigns meaning and legitimacy to the activity; passive audiences observe and offer approval or critique that situates the private act as a shared cultural experience, while active audiences participate and help co-create the event, altering its pace, emphasis, and outcomes.",
        "B": "An activity becomes entertainment only when it is commercialized; audience presence simply signals marketability or potential revenue rather than changing the activity's nature.",
        "C": "Entertainment arises only from formal ritual or ceremony, so audience presence by itself is irrelevant to whether a private activity is entertaining.",
        "D": "The creator's skill alone determines whether something is entertaining; the audience's role does not affect the transformation from private recreation to entertainment."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was tightened for clarity and academic tone; technical phrase \"social frame\" retained. Distractor options rephrased to be plausible alternatives without changing correct choice.",
      "content_preserved": true,
      "source_article": "Entertainment",
      "x": 1.3198261260986328,
      "y": 1.0252646207809448,
      "level": 2,
      "concepts_tested": [
        "The audience–entertainment relationship: how audience roles (passive vs. active) transform private recreation into entertainment.",
        "Media/technology and industry as drivers: how forms evolve with technology, distribution expands (royal courts to global industries), and cross-media remixes sustain themes.",
        "Multifunctional and context-dependent purpose: entertainment can amuse, educate, ritualize, critique, or provoke; its meaning and acceptability vary by cultural and ethical context."
      ],
      "original_question_hash": "430627ac"
    },
    {
      "question": "According to Coleridge's idea of \"poetic faith,\" how does adding \"human interest\" and a \"semblance of truth\" to an otherwise improbable story help an audience accept unreal events?",
      "options": {
        "A": "It lowers cognitive scrutiny by shifting the audience to emotional heuristics instead of analytical evaluation.",
        "B": "It anchors the unreal with familiar human stakes and credible-seeming details, so readers interpret unlikely events within a believable frame.",
        "C": "It makes the events objectively true because emotional engagement completely overrides skepticism.",
        "D": "It relies on the author's authority or reputation to force readers to accept implausible elements regardless of plausibility."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was tightened and focused on Coleridge's 'poetic faith'; complex phrasing was replaced with clearer, concise alternatives while preserving the original conceptual distinctions among options.",
      "content_preserved": true,
      "source_article": "Suspension of disbelief",
      "x": 0.9244331121444702,
      "y": 1.1561098098754883,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The suspension of disbelief as a deliberate, voluntary mechanism enabling enjoyment of unreal narratives.",
        "Concept 2: The role of \"human interest and a semblance of truth\" in making implausible elements credible to the audience.",
        "Concept 3: The historical/theoretical framework linking theatre, poetry, and philosophy (Coleridge's poetic faith; Horace’s Ars Poetica) as the basis for understanding suspension of disbelief."
      ],
      "original_question_hash": "c9e98d1a"
    },
    {
      "question": "Performance studies integrates theories and methods from the arts, anthropology, sociology, literary theory, and related fields. How does this interdisciplinary approach most fundamentally improve the study of performance?",
      "options": {
        "A": "It restricts inquiry to a single disciplinary vocabulary to ensure methodological consistency.",
        "B": "It allows a single dominant theory from one field to be applied universally across all performance contexts, simplifying interpretation.",
        "C": "It supplies diverse theoretical perspectives and methods to treat performance as process, context, and mode of transmission, enabling methodological triangulation and a richer, more contextualized understanding.",
        "D": "It elevates artistic technique and aesthetics above social and cultural context, making form the primary determinant of meaning."
      },
      "correct_answer": "C",
      "simplification_notes": "Question text shortened and clarified; kept emphasis on interdisciplinarity and how it supports analysing performance as process, context, and transmission (triangulation). Distractors rewritten to be plausible opposites. Core concepts and correct answer preserved.",
      "content_preserved": true,
      "source_article": "Performance studies",
      "x": 1.178885579109192,
      "y": 1.0352128744125366,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Performance as a broad process (not limited to art) that acts as a lens to study the world, a mode of transmission, and a means of intervening in social and cultural contexts.",
        "Concept 2: Interdisciplinarity as a core mechanism, combining theories and methods from multiple fields (arts, anthropology, sociology, literary theory, etc.) to investigate performance.",
        "Concept 3: Practice-based/research (PAR) as a methodological approach, integrating creative performance practice with traditional research methods to generate knowledge (as illustrated by PARIP)."
      ],
      "original_question_hash": "2d0bef72"
    },
    {
      "question": "How does combinatorial regulation by multiple transcription factors produce context-dependent control of a gene's expression?",
      "options": {
        "A": "A single transcription factor binding is usually sufficient to recruit the transcription machinery, so the presence of multiple TFs mainly provides redundancy.",
        "B": "The gene's regulatory region integrates inputs from several TFs; only a specific combination of active TFs can recruit coactivators and induce chromatin changes that permit transcription, producing tissue- or condition-specific expression.",
        "C": "Transcription requires every possible TF for that gene to be bound simultaneously, so expression occurs only when all TFs are present and is therefore independent of cellular context.",
        "D": "TFs mainly compete for overlapping binding sites, so whether a gene is expressed depends on which single TF outcompetes the others rather than on a cooperative combination."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was tightened and technical phrases clarified. Emphasized integration of multiple TF inputs at regulatory regions, recruitment of coactivators, and chromatin remodeling as the mechanism for context-specific expression. Distractors were kept plausible and aligned with mechanisms discussed in the article.",
      "content_preserved": true,
      "source_article": "Transcription factor",
      "x": 2.143700361251831,
      "y": 1.1758462190628052,
      "level": 2,
      "concepts_tested": [
        "Concept 1: DNA-binding domains define target recognition and TF classification; TFs regulate genes by binding specific DNA sequences adjacent to the genes they control.",
        "Concept 2: Activation vs. repression mechanisms; TFs influence transcription by promoting or blocking RNA polymerase recruitment, often via interactions with coactivators and chromatin-modifying proteins.",
        "Concept 3: Combinatorial regulation; multiple transcription factors bind to a gene’s regulatory regions, enabling context-dependent, coordinated control of gene expression."
      ],
      "original_question_hash": "d39fe04b"
    },
    {
      "question": "How do different tactics and communication channels (e.g., letters, petitions, boycotts, rallies, art, hacktivism, and social media) affect an activist movement's reach, effectiveness, and relationship with power structures beyond its stated goals?",
      "options": {
        "A": "They shape who can participate, how messages flow through media ecosystems, and how authorities and publics perceive and respond, thereby changing reach, tempo of mobilization, and possibilities for coalition‑building.",
        "B": "They determine the absolute truth or moral correctness of the cause; without choosing the single 'right' tactic the cause cannot succeed.",
        "C": "They are irrelevant; outcomes depend only on the amount of funding and material resources the movement has.",
        "D": "They guarantee success if the tactic is moved online; placing a tactic on social media ensures success regardless of context or audience."
      },
      "correct_answer": "A",
      "simplification_notes": "Rewrote the question in clearer academic language for undergraduates, listed concrete tactic examples from the article, and made each option plausible while preserving the original correct choice and core concepts (participation, media ecosystems, perceptions by power holders, reach, tempo, coalition-building).",
      "content_preserved": true,
      "source_article": "Activism",
      "x": 1.1865975856781006,
      "y": 0.8747603893280029,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Collective action that is purposeful, organized, and sustained over time differentiates activism from a mere protest and leads to the development of social movements.",
        "Concept 2: A range of tactics and channels (letters, petitions, boycotts, rallies, art, hacking, social media) affect the reach, effectiveness, and dynamics of activism.",
        "Concept 3: Activism interacts with power structures and media ecosystems (ideological differences, partisan media, platform migration, wealth influence, and the boundary with terrorism), shaping legitimacy and outcomes."
      ],
      "original_question_hash": "c5e42a02"
    },
    {
      "question": "Why does knowledge sharing reduce duplication of effort within an organization?",
      "options": {
        "A": "It formalizes exclusive ownership of knowledge so others cannot access or reuse it.",
        "B": "It creates visibility into who has relevant expertise and which solutions already exist, enabling reuse, alignment of work, and faster organizational learning.",
        "C": "It slows progress by forcing everyone to agree on a single approach, thereby discouraging experimentation.",
        "D": "It replaces formal documentation with informal conversations, which increases redundant work and fragmented records."
      },
      "correct_answer": "B",
      "simplification_notes": "Question rephrased concisely for undergraduates; removed background detail while keeping the causal mechanism that sharing creates visibility and enables reuse/alignment. Options kept plausible and aligned with the article's themes.",
      "content_preserved": true,
      "source_article": "Knowledge organization",
      "x": 1.267369270324707,
      "y": 1.0143707990646362,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The goal of knowledge organization to improve findability through description, indexing, and classification, enabling both targeted searches and browsing.",
        "Concept 2: The knowledge management process, especially knowledge creation, implementation, and in particular knowledge sharing as a mechanism for learning, trust-building, and reducing duplication.",
        "Concept 3: The shift from traditional, human-centered KO to computational approaches, and the distinction between knowledge-organizing processes (KOP) and knowledge organizing systems (KOS) (including taxonomy and ontology) as frameworks for organizing knowledge."
      ],
      "original_question_hash": "c60bb25a"
    },
    {
      "question": "Why do spectrum auctions generally lead to more efficient use of radio frequencies than administrative assignment or lotteries?",
      "options": {
        "A": "Auctions generate price signals that reflect scarcity and bidders' expected returns, allocating rights to parties who can deploy and monetize services most efficiently and thus creating incentives for efficient use.",
        "B": "Auctions guarantee incumbents keep exclusive rights, removing competition and thereby preventing innovation and efficient reallocation.",
        "C": "Auctions always drive prices so high that they deter market entry and reduce overall social welfare, making them less efficient than administrative methods.",
        "D": "Auctions allocate spectrum randomly like lotteries, so they provide no price discovery or incentives to use spectrum efficiently."
      },
      "correct_answer": "A",
      "simplification_notes": "Question condensed to an undergraduate-level prompt; clarified 'price signals' and 'efficient use' and kept technical terms. Options were rewritten to be concise and plausible critiques or misconceptions while preserving the original correct answer.",
      "content_preserved": true,
      "source_article": "Spectrum management",
      "x": 1.5106593370437622,
      "y": 1.05016028881073,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Spectrum management aims to maximize efficient use of the RF spectrum while mitigating interference and delivering social/economic benefits.",
        "Concept 2: Licensing mechanisms (exclusive licensing, auctions, lotteries, unlicensed access, privatization, authorized shared access) as techniques that shape incentives, innovation, and spectrum efficiency.",
        "Concept 3: Multi-level governance of spectrum (national sovereignty, regional coordination, global ITU role) and the conceptualization of spectrum as a national resource that is reusable, requiring coordinated regulation and policy across levels."
      ],
      "original_question_hash": "0a1c3b3f"
    },
    {
      "question": "Why are grounding and shielding regarded as general-purpose EMC techniques, and how do they act on emissions, coupling paths, and a device's susceptibility (immunity)?",
      "options": {
        "A": "Grounding minimizes impedance mismatches so only intended signals flow, and shielding converts stray RF energy to heat; this mainly reduces emissions but does not affect susceptibility or coupling.",
        "B": "Grounding gives a low-impedance return and stabilizes potentials, while shielding provides a conductive barrier that attenuates electric and magnetic fields; together they lower emissions, weaken coupling paths, and improve the device's immunity.",
        "C": "Grounding and shielding only influence external radiated emissions and have little effect on a device's susceptibility or on coupling between internal parts of a system.",
        "D": "Grounding removes all RF currents and shielding completely blocks all field penetration, so emissions, susceptibility, and coupling are entirely eliminated."
      },
      "correct_answer": "B",
      "simplification_notes": "Question wording shortened and clarified; 'susceptibility' labeled as '(immunity)' for clarity; options rewritten to be concise and technically plausible while preserving the original correct choice.",
      "content_preserved": true,
      "source_article": "Electromagnetic compatibility",
      "x": 1.6183245182037354,
      "y": 1.0498244762420654,
      "level": 2,
      "concepts_tested": [
        "The three EMC issues (emission, susceptibility, immunity) and the role of coupling as the mechanism by which interference reaches the victim.",
        "The cause–effect framework for interference mitigation: reduce emissions, block coupling paths, or harden victims.",
        "The role of grounding and shielding as general techniques that apply to emissions, susceptibility, and coupling."
      ],
      "original_question_hash": "776588bf"
    },
    {
      "question": "Why is it better to give bureaucrats more autonomy when the state has high capacity, but impose tighter central regulation when the state has low capacity?",
      "options": {
        "A": "Because a high-capacity state has competent, well-resourced bureaucrats who can implement policies efficiently and adapt to local conditions without micromanagement; in low-capacity states, stronger central oversight reduces misimplementation, corruption, and capture.",
        "B": "Because bureaucratic autonomy always produces better outcomes regardless of state capacity, while regulation mainly harms innovation.",
        "C": "Because state capacity only affects revenue collection, not the quality of policy implementation by bureaucrats.",
        "D": "Because regulation should always be minimal so that administrations remain flexible and responsive, irrespective of state capacity."
      },
      "correct_answer": "A",
      "simplification_notes": "Wording condensed and clarified 'state capability' as 'state capacity'; preserved the causal logic linking capacity, bureaucratic autonomy, policy implementation, and risks (misimplementation/capture). Removed extraneous examples of public goods and institutional mechanisms while keeping core reasoning.",
      "content_preserved": true,
      "source_article": "Good governance",
      "x": 1.2277026176452637,
      "y": 0.9199300408363342,
      "level": 2,
      "concepts_tested": [
        "Concept 1: State capability and bureaucratic autonomy as determinants of governance quality, with autonomy appropriate to capability level (more autonomy when capable; more regulation when less capable).",
        "Concept 2: Governance effectiveness as measured by deliverables/public goods (security, health, education, water, contract enforcement, property protection, environmental protection, voting rights, fair wages).",
        "Concept 3: Institutional accountability mechanisms (checks and balances, independent judiciary, rule of law) that protect citizens and enable responsive governance."
      ],
      "original_question_hash": "0e04b7f3"
    },
    {
      "question": "Why, for a point placed uniformly at random in a planar region $R$, is the probability that it falls in a subregion $S$ equal to $\\frac{\\text{area}(S)}{\\text{area}(R)}$? What basic mechanism explains this formula?",
      "options": {
        "A": "A uniform density per unit area gives every infinitesimal area element equal weight; integrating this constant density over $S$ yields the probability $\\frac{\\text{area}(S)}{\\text{area}(R)}$.",
        "B": "The probability is governed by the total perimeter length, not area, because random points are more likely to lie near boundaries.",
        "C": "Only the shape or curvature of $S$ matters (not its size), since geometric features of $S$ determine how sampling is biased.",
        "D": "Probability scales with the square of the subregion's linear size (reflecting dimensional scaling) rather than directly with the area fraction."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording was shortened and clarified for undergraduates; mathematical expression $\\frac{\\text{area}(S)}{\\text{area}(R)}$ preserved; distractor options made plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Geometric probability",
      "x": 1.6518876552581787,
      "y": 1.1809751987457275,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Probability in geometric problems is proportional to area (area fraction), forming a fundamental principle for geometric probability and applications like stereology.",
        "Concept 2: Invariance under transformation groups guides natural probability models in integral geometry, shaping how expected values of geometric objects are computed.",
        "Concept 3: The field encompasses two interrelated yet distinct emphases—integral geometry (focus on formulas and invariants for geometric objects) and stochastic geometry (focus on random geometric objects and processes, e.g., random lines, Poisson processes)."
      ],
      "original_question_hash": "5fb11024"
    },
    {
      "question": "Start with a homogeneous Poisson point process and build a Boolean model by placing at each Poisson point a random compact object. Define the coverage field by $I(x)=\\mathbf{1}\\{x\\ \\text{is inside at least one object}\\}$. Why does this construction produce spatial dependence (correlation) in $I(x)$ across nearby locations, even though the underlying Poisson points are independent?",
      "options": {
        "A": "Because the Poisson points remain independent, the coverage indicator $I(x)$ must also be independent across disjoint regions.",
        "B": "A single object can cover multiple nearby locations, and overlaps between objects make the events $I(x)=1$ at neighboring $x$ positively correlated; geometry of objects thus induces interaction in coverage.",
        "C": "The union of objects is equivalent to randomly thinning the Poisson points, which preserves independence of coverage between locations.",
        "D": "If the random shapes were fixed in size and arranged so they never overlap, the geometry would not create dependence in the coverage field."
      },
      "correct_answer": "B",
      "simplification_notes": "Clarified coverage field with the indicator $I(x)$, removed advanced technical terms (Palm conditioning, random closed sets) while keeping the key geometric mechanism (single objects and overlaps create spatial correlation), and made all options plausible.",
      "content_preserved": true,
      "source_article": "Stochastic geometry",
      "x": 1.6475613117218018,
      "y": 1.181861400604248,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Point processes as foundational building blocks, with the homogeneous Poisson process serving as the baseline and extensions to interacting or more expressive models (e.g., Boolean model, area-based tilting) to capture more realistic spatial patterns.",
        "Concept 2: Random objects and random closed sets as a framework for inference and analysis (Palm conditioning, test-set hitting probabilities, marked point processes) linking geometry with measure-theoretic ideas.",
        "Concept 3: Representing extended spatial objects (lines, flats) as points in a representation space to map to standard point-process theory and enable analysis (e.g., using a cylinder for directed lines in the plane)."
      ],
      "original_question_hash": "6e1adf7e"
    },
    {
      "question": "Why should a logic model explicitly state the theories, evidence, assumptions, and beliefs that justify the causal links between inputs, activities, outputs, and outcomes?",
      "options": {
        "A": "To keep the plan rigid and identical across different contexts so it is applied the same way everywhere.",
        "B": "To make the rationale for each causal link visible, exposing assumptions so they can be critiqued, tested, and adapted if needed.",
        "C": "To simplify planning by treating causal links as certain and removing the need to gather supporting evidence.",
        "D": "To ensure that outcomes will occur automatically if the plan is followed, regardless of how well it is implemented."
      },
      "correct_answer": "B",
      "simplification_notes": "Wording was tightened and academic phrasing applied; the core idea—that making underlying theory and evidence explicit enables critique, testing, and adaptation—was preserved. Distractors were kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Logic model",
      "x": 1.3942631483078003,
      "y": 1.0585649013519287,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Causal chain / theory of change — mapping how inputs and activities are expected to produce outputs and, ultimately, outcomes via \"if-then\" relationships.",
        "Concept 2: Underlying justification — the inclusion of theories, evidence, assumptions, and beliefs that support the modeled causal relationships.",
        "Concept 3: Form and purpose — using graphical or narrative representations to plan and evaluate programs, linking action to change in a structured, testable way."
      ],
      "original_question_hash": "8bfab933"
    },
    {
      "question": "Why is careful design of risk transfer and incentive alignment important in a public–private partnership (PPP) for achieving value for money, and what trade-offs can arise?",
      "options": {
        "A": "It lets the private partner receive larger fixed payments regardless of project outcomes, effectively shifting all risk away from the private sector and eliminating public exposure.",
        "B": "It encourages the private partner to reduce lifecycle costs and maintain quality by making them bear design, construction, and performance risks, but too much risk transfer or poorly aligned incentives can produce inflated bids, risk-averse behaviour, renegotiation, or lower service efficiency.",
        "C": "It guarantees that taxpayers bear all financial risk so the private partner faces no downside, making the contract risk-free for the private entity.",
        "D": "It does not matter for outcomes because payments are predetermined and independent of performance, so incentive design has no effect on value for money."
      },
      "correct_answer": "B",
      "simplification_notes": "Question and options were condensed into clearer academic language appropriate for undergraduates; technical terms (risk transfer, lifecycle costs, renegotiation) retained; trade-offs were clarified and distractors made plausible.",
      "content_preserved": true,
      "source_article": "Public–private partnership",
      "x": 1.3372942209243774,
      "y": 0.9034007787704468,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Risk transfer and incentive alignment in PPP contracts",
        "Concept 2: Financing structure and lifecycle costs (private funding up-front funded by taxes/users, and implications for public budgets)",
        "Concept 3: Accountability, transparency, and evaluation challenges in PPPs (secrecy of financial details, mixed evidence on value for money)"
      ],
      "original_question_hash": "548c384f"
    },
    {
      "question": "Fire safety typically uses multiple layers: ignition prevention (e.g., safe electrical design, proper storage), compartmentation (fire‑rated walls and doors), and active detection/suppression (smoke detectors, alarms, sprinklers). Why is this layered design more robust to real‑world variability than relying on any single measure?",
      "options": {
        "A": "It provides redundancy: if one layer fails to stop ignition or slow fire spread, the other layers limit damage and buy time for detection, suppression, and evacuation.",
        "B": "It guarantees zero probability of ignition once all layers are installed.",
        "C": "It ensures occupants will always evacuate immediately and correctly regardless of the fire's behavior.",
        "D": "It relies exclusively on automatic systems and eliminates the need for inspections, training, or human intervention."
      },
      "correct_answer": "A",
      "simplification_notes": "Question wording shortened and clarified; examples added for each layer; preserved emphasis on redundancy, codes/inspections, and human factors. Distractors kept plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Fire safety",
      "x": 1.6560944318771362,
      "y": 0.9257410168647766,
      "level": 2,
      "concepts_tested": [
        "Concept 1: Fire safety as a layered system that includes prevention of ignition and limitation of fire spread.",
        "Concept 2: The regulatory linkage between building/fire codes, occupancy rules, inspections, and the resulting safety features and compliance.",
        "Concept 3: The role of human factors—training, education, and drills—in ensuring that safety measures are effective in practice."
      ],
      "original_question_hash": "5775d093"
    },
    {
      "question": "Why do major ethical failures and public-safety disasters commonly lead to the creation of formal engineering codes of ethics and professional licensure?",
      "options": {
        "A": "Because public safety requires consistent, enforceable expectations beyond individual discretion; high-profile failures expose gaps, and codes plus licensure establish standardized obligations and a credentialing mechanism to ensure competence and accountability.",
        "B": "Because disasters create public pressure that prompts professional societies and firms to increase outreach, publicity, and rebranding, which may include producing new documents rather than substantive regulation.",
        "C": "Because such incidents show that technical knowledge alone is sufficient, so formal ethics codes and licensure are unnecessary.",
        "D": "Because licensure turns practice into standardized procedures or checklists, eliminating the need for professional judgment."
      },
      "correct_answer": "A",
      "simplification_notes": "Condensed and clarified the original question wording; removed historical examples and background detail while preserving the causal idea that failures reveal gaps and motivate codes and licensure; kept answer A unchanged and made distractors plausible.",
      "content_preserved": true,
      "source_article": "Engineering ethics",
      "x": 1.2556129693984985,
      "y": 0.9259410500526428,
      "level": 2,
      "concepts_tested": [
        "Engineers' moral obligations to society, clients, and the profession (principle of engineering ethics)",
        "How ethical failures and public safety incidents drive the creation of codes of ethics and licensure (cause-effect mechanism)",
        "The role of professional societies in shaping ethics and professional identity through formal codes (institutional relationship)"
      ],
      "original_question_hash": "0d2f3492"
    },
    {
      "question": "In active remote sensing (radar or lidar), why does measuring the time delay between emitting a pulse and receiving its echo allow determination of a target's position and motion?",
      "options": {
        "A": "The emitted pulse's energy increases with distance, producing a unique time signature that directly encodes position.",
        "B": "The frequency shift of the returned signal by itself encodes height, because frequency is inversely proportional to distance.",
        "C": "The signal travels at a known finite speed $c$, so the round‑trip time $t$ gives range $R=\\frac{c\\,t}{2}$; the Doppler shift of the echo gives radial velocity, and scanning the beam provides azimuth/elevation (direction).",
        "D": "Because the atmosphere's refractive index is constant, the strength of the backscattered signal maps directly to altitude."
      },
      "correct_answer": "C",
      "simplification_notes": "Reworded question for undergraduates, condensed background, added the range formula $R=\\frac{ct}{2}$, and made distractors plausible but incorrect.",
      "content_preserved": true,
      "source_article": "Atmospheric physics",
      "x": 1.8176469802856445,
      "y": 0.9766651391983032,
      "level": 2,
      "concepts_tested": [
        "Concept 1: The atmosphere is modeled using physics-based equations (fluid flow, radiation budget, energy transfer) and is coupled to boundary systems like the oceans.",
        "Concept 2: Remote sensing operates via passive and active methods; active sensing uses emitted signals and time delays to determine location, height, speed, and direction.",
        "Concept 3: Microphysical and statistical-mechanical processes (scattering, cloud physics, wave propagation) connect small-scale mechanisms to large-scale atmospheric dynamics and weather systems."
      ],
      "original_question_hash": "c04dbcf7"
    },
    {
      "question": "A team of psychologists conducts a 2-year study (N = 200) examining the relationship among chronic psychosocial stress, hippocampal volume (measured with MRI), and performance on a probabilistic decision-making task. At baseline they find a cross-sectional correlation: higher perceived stress ⇄ smaller hippocampal volume ⇄ poorer decision accuracy. Over 2 years, within-person increases in perceived stress predict greater hippocampal volume reduction and larger declines in decision accuracy. Additionally, the researchers randomly assign a stratified subsample (n = 60) to a validated stress-reduction program; compared with the randomized control subsample, the intervention group shows reduced perceived stress, attenuated hippocampal shrinkage, and smaller declines in decision accuracy. Which of the following inferences is most justified by these methods and results?",
      "options": {
        "A": "The converging longitudinal associations and the randomized stress-reduction manipulation provide reasonable empirical evidence that chronic psychosocial stress contributes causally to hippocampal volume loss, and that hippocampal change plausibly mediates part of the effect of stress on decision-making (i.e., an indirect effect $ab$), consistent with an emergent brain basis of cognitive behavior and a biopsychosocial model of intervention.",
        "B": "No causal claims are justified: because most data are observational, the only defensible conclusion is that stress, hippocampal volume, and decision accuracy are correlated; the randomized subsample is too small to alter this interpretation.",
        "C": "The results demonstrate that decision-making deficits cause hippocampal shrinkage and higher perceived stress, showing that mental states drive brain structure (brain changes are epiphenomena), so biological explanations are unnecessary.",
        "D": "These findings prove that social factors alone (e.g., socioeconomic status and culture) fully account for the observed changes, so biological measures add no explanatory value and interventions should ignore neural outcomes."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete longitudinal + randomized intervention scenario to test students' ability to distinguish correlational vs causal inference, recognize evidence for brain–behavior (emergent) relationships, and appreciate biopsychosocial integration; distractors present common incorrect interpretations.",
      "concepts_tested": [
        "mind–brain relationship and neural basis of behavior",
        "empirical inference of causal vs correlational relationships (longitudinal designs and randomized interventions)",
        "biopsychosocial model and translational application of integrated findings"
      ],
      "source_article": "Psychology",
      "x": 1.2051137685775757,
      "y": 0.9926488399505615,
      "level": 2,
      "original_question_hash": "00546683"
    },
    {
      "question": "Three island populations (A, B, C) of a beetle species were founded from the same mainland source ~100 years ago (common descent). At founding each island had allele frequencies for elytral colour allele G (green) = 0.6 and R (red) = 0.4. Effective population sizes are Ne(A)=10,000, Ne(B)=200, Ne(C)=2,000. Predators introduced to the islands preferentially eat red beetles (so R has lower fitness). After 50 generations observed allele frequencies are: A: R = 0.10, G = 0.90; B: R = 0.90, G = 0.10; C: R = 0.39, G = 0.61. A neutral DNA marker places A and B as sister populations, with C more divergent. Occasional migrants from the mainland introduce a distinct melanic allele M into population C at low frequency; mutation rates between G and R are negligible. Which explanation best accounts for these results, integrating natural selection, other population-genetic mechanisms, and common descent?",
      "options": {
        "A": "Natural selection against R reduced its frequency in the large populations (A and C), but random genetic drift in the small population B overwhelmed selection and by chance drove R to high frequency; rare gene flow from the mainland explains the presence of M in C; the neutral-marker phylogeny grouping A and B reflects their more recent common ancestry, so phenotypic differences among islands result from descent with modification plus population-genetic processes.",
        "B": "A high local mutation rate G→R in population B produced the large increase of R there, whereas selection was the dominant force reducing R in A and C; the neutral-marker similarity of A and B is coincidental and does not indicate common descent.",
        "C": "Ongoing gene flow from the mainland continuously supplies R to B so that R stays high despite predation; A and B cluster in the neutral tree because gene flow has homogenised their genomes, implying they are not true sister taxa by descent.",
        "D": "Balancing selection in B maintains a high R frequency there (R confers frequency-dependent advantage), while directional selection against R in A and C reduces R; the neutral-marker tree grouping A and B must reflect convergent evolution at the marker locus rather than common ancestry."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete island-beetle scenario with given Ne and allele frequencies to test how selection, drift, and gene flow interact, and to link neutral-marker phylogeny with descent vs convergence.",
      "concepts_tested": [
        "Natural selection and differential fitness",
        "Genetic drift, gene flow, mutation and their interaction with selection",
        "Descent with modification and phylogenetic inference (common ancestry)"
      ],
      "source_article": "Evolution",
      "x": 1.8442435264587402,
      "y": 1.1173399686813354,
      "level": 2,
      "original_question_hash": "4def744a"
    },
    {
      "question": "Researchers study five related beetle species (A–E). Numerical (morphological) analysis yields tree T1 with topology ((A,B),(C,(D,E))) and short branch separating A and B; biochemical (mitochondrial DNA) analysis yields a conflicting tree T2 with topology ((A,(B,C)),(D,E)) and a long branch on the lineage leading to B in T2. Experimental crosses show A×B produce fertile offspring while D×E produce sterile offspring. Geographic ranges: A and B occur on mainland X, C on island Y, D and E on mainland Z. Which of the following interpretations best reflects principles of systematics (topology vs branch length, multiple data approaches, and phylogenetic applications to traits and biogeography)?",
      "options": {
        "A": "T1’s topology indicates A and B are sister taxa (a statement about branching order independent of branch length); the long branch in the molecular tree indicates a greater amount of molecular change on the B lineage since its divergence; fertile A×B hybrids are consistent with reduced reproductive isolation (they may be the same evolutionary unit or recently diverged ones), whereas sterile D×E hybrids indicate stronger reproductive isolation (separate evolutionary units); the conflict between numerical and biochemical trees illustrates why systematists combine multiple data types (numerical, biochemical, experimental) to infer a robust phylogeny; once a robust phylogeny is obtained it can be used to study the evolution of traits and to make hypotheses about ancestral geographic ranges (biogeography).",
        "B": "Because T1 and T2 disagree, topology is meaningless and only branch lengths matter; the long branch to B in T2 proves B is the oldest (basal) species; experimental crosses are the primary evidence for topology so A and B must be identical taxa because they produce fertile hybrids; phylogenies cannot be used to study biogeography or trait evolution because tree-building methods give contradictory results.",
        "C": "Sterile hybrids between D and E indicate D and E are very closely related (recently diverged) because hybrid sterility is a hallmark of close kinship; biochemical systematics (mtDNA) is always superior to numerical systematics so T2 must be correct without further scrutiny; branch lengths only reflect sampling error and never reflect amount of evolution; phylogenies are solely nomenclatural tools and cannot be used to infer ancestral character states or geographic history.",
        "D": "A and B cannot be inferred as sister taxa from T1 because branch lengths, not topology, determine relationships; a long branch always means a lineage is ancestral rather than having accumulated more change; conflicting results mean experimental systematics (crosses) should be ignored in tree inference; phylogenies only provide classification and are not suitable for testing hypotheses about trait evolution or biogeography."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete five-species scenario combining numerical, biochemical, and experimental data; options contrast correct interpretations about topology vs branch length, the value of multiple data types, and phylogenetic uses for trait evolution and biogeography.",
      "concepts_tested": [
        "Topology versus branch length in phylogenetic trees",
        "Use of multiple systematics approaches (numerical/biochemical/experimental) to infer relationships",
        "Application of phylogenies to study trait evolution and biogeography"
      ],
      "source_article": "Systematics",
      "x": 1.841935634613037,
      "y": 1.106779932975769,
      "level": 2,
      "original_question_hash": "8f0f4bb2"
    },
    {
      "question": "Consider a mid-sized hospital evaluating the risks of launching a telehealth system. They identify three scenarios and represent risk as triplets $R=(s_i,p_i,x_i)$: $s_1$ = cyberattack with $p_1=0.02$ and $x_1= -\\$500{,}000$, $s_2$ = system downtime with $p_2=0.10$ and $x_2= -\\$200{,}000$, and $s_3$ = higher-than-expected patient adoption with $p_3=0.30$ and $x_3= +\\$150{,}000$. According to the ISO 31000 definition of risk as the \"effect of uncertainty on objectives\" (which admits both positive and negative effects) and the triplet model, which statement is most accurate?",
      "options": {
        "A": "Correctly form R = {(s_1,0.02,-$500,000),(s_2,0.10,-$200,000),(s_3,0.30,+$150,000)}; risk includes threats and opportunities; aggregate expected outcome $\\sum_i p_i x_i = 0.02(-500{,}000)+0.10(-200{,}000)+0.30(150{,}000)=+$15,000; ISO 31000 recommends identifying, analysing and evaluating these scenarios and treating threats (mitigate/transfer) while managing opportunities (exploit/monitor).",
        "B": "Exclude $s_3$ because risk denotes only harm; compute expected risk only from threats as $0.02(-500{,}000)+0.10(-200{,}000)= -$30,000; ISO guidance treats positive outcomes separately and they should never be recorded in the risk register.",
        "C": "Because Knightian uncertainty applies to technology projects, probabilities cannot be assigned and ISO 31000 therefore forbids quantification; the hospital should use purely qualitative labels (low/medium/high) and avoid numeric triplets or expected-value calculations.",
        "D": "ISO 31000 is relevant only to safety and environmental domains; financial consequences must be handled by finance tools using volatility $\\sigma$ and beta, so the hospital should discard the triplet model and treat all scenarios via portfolio-style variance measures."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete telehealth scenario expressed with triplets and numerical probabilities/consequences to test (1) risk as effect of uncertainty on objectives via scenarios/probabilities/consequences, (2) inclusion of positive outcomes as opportunities, and (3) the role of ISO 31000 in guiding identification/analysis/evaluation and appropriate treatments.",
      "concepts_tested": [
        "Risk as effect of uncertainty on objectives represented by triplets (scenario, probability, consequence)",
        "Risk includes both negative threats and positive opportunities",
        "Role of ISO 31000 and risk frameworks in guiding identification, analysis, evaluation and treatment across domains"
      ],
      "source_article": "Risk",
      "x": 1.5058410167694092,
      "y": 1.0598335266113281,
      "level": 2,
      "original_question_hash": "e155b973"
    },
    {
      "question": "A conservation agency manages two small, isolated reserves for a threatened mammal. Reserve A contains 40 individuals (10 males, 30 females); reserve B contains 30 individuals (15 males, 15 females). Current annual dispersal between reserves is negligible (<1%). Using the approximate sex‑structured effective population size formula $N_e\\approx\\dfrac{4N_mN_f}{N_m+N_f}$, compute $N_e$ for each isolated reserve and for the reservoirs if a habitat corridor restored natural dispersal so the populations functionally merged (i.e., $N_m$ and $N_f$ summed). Which management action best integrates ecological and evolutionary theory with policy/practice to reduce extinction risk by increasing $N_e$, reducing inbreeding depression, and sustaining evolutionary processes?",
      "options": {
        "A": "Establish a habitat corridor linking the two reserves. Isolated $N_e$ values are $N_e^A=\\dfrac{4(10)(30)}{10+30}=30$ and $N_e^B=\\dfrac{4(15)(15)}{15+15}=30$; combined $N_e=\\dfrac{4(25)(45)}{25+45}\\approx64$, so connectivity approximately doubles $N_e$, restores dispersal, and maintains evolutionary potential.",
        "B": "Implement scheduled translocations of 5–10 adults between reserves every 10 years to augment gene flow. This occasional intervention will mimic connectivity and maintain genetic diversity without altering landscape structure.",
        "C": "Start an ex‑situ captive breeding program in zoos to increase census size and periodically release individuals into each reserve; focus on maximizing the number of offspring produced per year under controlled conditions.",
        "D": "Increase legal protection within each reserve to eliminate human disturbance (no access) but do not alter the landscape connectivity; focus resources on enforcement and habitat quality inside the current reserve boundaries."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete fragmented population scenario, required computation of sex‑structured $N_e$ with given formula, and asked which management integrates ecology/evolutionary genetics with policy to reduce extinction risk; provided four plausible management alternatives.",
      "concepts_tested": [
        "Integration of ecological and evolutionary theory with conservation policy and practice",
        "Importance of effective population size, inbreeding depression, and sustaining evolutionary processes",
        "Use of population ecology metrics (dispersal, migration, demographics) to inform management of endangered species"
      ],
      "source_article": "Conservation biology",
      "x": 1.4529409408569336,
      "y": 0.9067904353141785,
      "level": 2,
      "original_question_hash": "2944fe85"
    },
    {
      "question": "A 45-year-old registered nurse is required by her licensing board to complete 30 hours of continuing education every two years to maintain her license. In addition, she attends voluntary digital-literacy workshops at the public library and participates in a local lifelong learning institute's seminars on philosophy and civic engagement. Based on the concepts in the article, which statement best synthesizes how these activities and institutions together relate to the principle and outcomes of lifelong learning?",
      "options": {
        "A": "Because the license requirement forces the nurse to take courses, her learning is no longer 'lifelong learning' as defined (ongoing, voluntary, self-motivated); only her voluntary library and institute attendance qualify as lifelong learning.",
        "B": "The mandated continuing education secures minimum professional competence and public safety, while the voluntary library workshops and lifelong learning institute broaden her personal development, social inclusion, and employability; together they exemplify how institutional mechanisms and self-motivated activity promote lifelong learning and community outcomes.",
        "C": "Lifelong learning primarily consists of obtaining additional formal credentials, so only the continuing-education hours that carry university credit meaningfully affect her employability; community-based offerings are peripheral.",
        "D": "Community institutions such as libraries and lifelong learning institutes replace professional regulation as the main mechanism for maintaining workplace competence, making licensing requirements redundant."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete professional scenario (nurse with mandated CE plus voluntary community learning) and created four plausible interpretations that test definitions, the role of institutions (licensure, libraries, institutes), and links to competence, employability, and social inclusion. Option B synthesizes article content.",
      "concepts_tested": [
        "Definition of lifelong learning (ongoing, voluntary, self-motivated) and its outcomes",
        "Institutional mechanisms supporting lifelong learning (professional licensure, libraries, lifelong learning institutes)",
        "Relationship between continuous learning, professional competence, employability, and community/social outcomes"
      ],
      "source_article": "Lifelong learning",
      "x": 1.2312307357788086,
      "y": 0.9793177843093872,
      "level": 2,
      "original_question_hash": "d0676398"
    },
    {
      "question": "You are hired to design a one-term course for a culturally diverse cohort of 16–18 year-olds in an urban school. The department's stated aims for the course are: (1) transmit core disciplinary knowledge, (2) develop students' critical and reflective reasoning, (3) build collaborative interpersonal skills, and (4) cultivate appropriate psychomotor competence for laboratory tasks and responsible ethical dispositions. The class includes students from several linguistic and socioeconomic backgrounds and school time and resources are constrained. Which pedagogical stance best coheres with these aims while respecting the article's claims about pedagogy as both practice and discourse, the role of tacit teacher knowledge, and the context-dependent nature of teaching?",
      "options": {
        "A": "Adopt a strict didactic model focused on efficient transmission: deliver weekly lectures covering the canonical content, test comprehension with standardized assessments, and reserve limited supervised lab time strictly for procedural practice. This minimizes variability and ensures measurable learning outcomes.",
        "B": "Treat pedagogy primarily as an art of apprenticeship: rely on the teacher's tacit skills to guide informal, conversation-based lessons with few written objectives, emphasizing exemplification and emulation. Adjustments are made ad hoc according to the teacher's judgement rather than explicit theory.",
        "C": "Implement an integrated, context-responsive pedagogy: use concise explicit instruction to establish core knowledge; schedule Socratic/dialogic seminars and formative assessments to develop critical reasoning and interpersonal skills; provide scaffolded lab sessions for psychomotor learning; and adapt materials and assessment modes to students' linguistic, cultural and resource constraints, while documenting practices as a basis for reflective professional discourse.",
        "D": "Follow a politically neutral, one-size-fits-all curriculum dictated by central authorities: adhere strictly to prescribed content and testing regimes to avoid pedagogical controversies, assuming equal treatment produces fairness across diverse learners."
      },
      "correct_answer": "C",
      "generation_notes": "Created a realistic curriculum-design scenario requiring synthesis: choose an approach that acknowledges pedagogy as both practice/discourse, values tacit teacher knowledge and reflective documentation, adapts to social/cultural context, and aligns instructional strategies (explicit instruction, Socratic/dialogic methods, scaffolded labs) with aims including cognitive, social, psychomotor, and affective outcomes.",
      "concepts_tested": [
        "Pedagogy as practice and discourse; tacit knowledge in teaching",
        "Context-dependent nature of pedagogy and bidirectional influence with social/political/cultural environments",
        "Alignment of pedagogical aims (knowledge transmission; intellectual, social, psychomotor, affective development) with instructional strategies such as Socratic/dialogic and scaffolded practice"
      ],
      "source_article": "Pedagogy",
      "x": 1.2653768062591553,
      "y": 0.994347870349884,
      "level": 2,
      "original_question_hash": "d011022b"
    },
    {
      "question": "A city (the principal) must decide which contractor receives a public bridge contract. Each contractor i has a privately known quality type θ_i∈Θ. The city’s planner wants to implement the social choice function f(θ) that awards the contract to the highest-quality firm and possibly sets transfers t(θ) to finance payments. Which of the following statements best describes how mechanism design and the revelation principle guide the planner’s design task?",
      "options": {
        "A": "By the revelation principle the planner can restrict attention to direct mechanisms y({\\hat{\\theta}}) that ask firms to report types and then choose allocation x({\\hat{\\theta}}) and transfers t({\\hat{\\theta}}); she then finds t(θ) so that truthful reporting {\\hat{\\theta}}(θ)=θ satisfies the incentive-compatibility (IC) and participation (IR) constraints, thereby implementing f(θ) in equilibrium.",
        "B": "Because contractors have private information, the planner must enumerate all possible strategic reporting functions {\\hat{\\theta}}(θ) and every possible extensive-form game; the revelation principle does not reduce this search, so the designer’s problem is to analyze every Bayesian Nash equilibrium individually.",
        "C": "The planner can implement f(θ) simply by committing to award the contract to the highest reported type and making no transfers; truthful reporting will then be a dominant strategy for all contractors because higher quality is strictly rewarded.",
        "D": "The revelation principle implies that if any mechanism implements f(θ) then there exists a mechanism that makes truthful reporting a dominant strategy for every type, so the planner need not check incentive-compatibility with respect to beliefs or Bayesian equilibria (only dominant-strategy IC matters)."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete public-procurement scenario with private types θ_i and social choice f(θ); options contrast correct application of revelation principle (direct mechanisms + IC/IR) with common misconceptions (need to search all games, no transfers suffices, or revelation implying dominant-strategy truth-telling).",
      "concepts_tested": [
        "Mechanism design as inverse problem (designer chooses mechanism to implement f(θ))",
        "Revelation principle and incentive compatibility (restrict to direct, truth-telling mechanisms subject to IC and IR)"
      ],
      "source_article": "Mechanism design",
      "x": 1.4250231981277466,
      "y": 1.0845093727111816,
      "level": 2,
      "original_question_hash": "9b875e8e"
    },
    {
      "question": "Researchers study a lizard species occupying two neighboring islands. Island A is arid and Island B is mesic. A quantitative trait related to drought tolerance shows strong between-island differentiation with $Q_{ST}=0.60$, while a neutral SNP panel shows weak neutral differentiation $F_{ST}=0.02$. Within Island A, an immune-locus maintains multiple alleles at intermediate frequencies and rare alleles tend to increase in frequency when uncommon. In mesocosm experiments, removal of a common lizard genotype from a multispecies assemblage led to collapse of local species diversity. Which interpretation best explains the empirical patterns and indicates the most appropriate conservation action?",
      "options": {
        "A": "The large $Q_{ST}$ relative to $F_{ST}$ indicates diversifying (local) selection on drought-tolerance alleles between islands; the immune-locus polymorphism is maintained by negative frequency-dependent selection; the mesocosm result shows within-species genetic diversity supports broader community diversity. Conservation should prioritize preserving local adaptive variation (e.g., source individuals from other arid populations or introduce specific adaptive alleles) rather than indiscriminate mixing of Island B individuals into Island A, which could swamp local adaptation.",
        "B": "Because neutral differentiation is low ($F_{ST}=0.02$), the observed trait divergence is best explained by neutral accumulation under the neutral theory; the immune-locus pattern is likely drift, and mixing individuals from Island B into Island A is the simplest way to increase overall genetic diversity and rescue the population.",
        "C": "Frequency-dependent selection acting differently on each island can explain both the high $Q_{ST}$ for drought tolerance and the immune-locus polymorphism; therefore local adaptation is unimportant and managers should translocate many individuals from Island B to Island A to homogenize allele frequencies and stabilize community diversity.",
        "D": "High mutation rates in Island A produced novel adaptive alleles that fixed locally, so the trait divergence reflects recent mutation-driven divergence rather than selection; conservation should focus only on increasing population size to reduce drift without regard to which source populations are used for translocation."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed an island scenario with numeric $Q_{ST}$ and $F_{ST}$ to test ability to compare adaptive vs neutral differentiation, included negative frequency-dependent selection maintaining polymorphism, and linked within-species diversity to community-level biodiversity and conservation decisions (genetic rescue vs. risk of swamping local adaptation).",
      "concepts_tested": [
        "Natural selection acting on genetic variation (local/adaptive selection vs neutral processes)",
        "Interdependence of within-species genetic diversity and broader biodiversity/community structure",
        "Population-genetics mechanisms: neutral theory, diversifying selection, frequency-dependent selection and their implications for conservation management"
      ],
      "source_article": "Genetic diversity",
      "x": 1.8492344617843628,
      "y": 1.1152844429016113,
      "level": 2,
      "original_question_hash": "56194610"
    },
    {
      "question": "The country of Granvia has experienced a decade-long rise in income inequality concentrated in former manufacturing regions. During the same period Granvia liberalized trade, reformed labor laws limiting collective bargaining, and saw increasing electoral volatility as new parties emerged. You are asked to design a research program to explain why (a) inequality rose and (b) policymakers adopted the observed mix of reforms. Which of the following research designs best exemplifies a political economy approach as distinguished from a narrow economics approach, and why?",
      "options": {
        "A": "Construct a calibrated general equilibrium model with a representative household and sectoral labor markets; estimate productivity shocks and show that exogenous trade-induced productivity changes alone generate the observed wage divergence; recommend standard market-based retraining and labor-market flexibility policies.",
        "B": "Estimate a macroeconometric vector autoregression (VAR) focused on inflation, output and unemployment to show that tightening monetary policy was the best short-run response; propose technocratic adjustments to monetary and fiscal rules without examining electoral incentives or legal institutions.",
        "C": "Combine quantitative cross-regional analysis of trade and wage trends with qualitative case studies of party competition and union legal changes; model policymakers as strategic actors (game theory/public choice) subject to institutional constraints (constitution, labor law, central-bank statutes) and historical legacies to explain both the economic shocks and the political choices that produced the reform package.",
        "D": "Attribute rising inequality primarily to skill-biased technological change; use microeconometric methods to evaluate retraining interventions and recommend scaling up human-capital programs while treating policy implementation as neutral and apolitical."
      },
      "correct_answer": "C",
      "generation_notes": "Created a country case contrasting narrow economic methods (A,B,D) with an interdisciplinary political-economy design (C) that emphasizes institutions, political actors, international shocks and mixed methods.",
      "concepts_tested": [
        "interdependence of politics and markets",
        "interdisciplinary political science+economics framework",
        "historical/terminological distinction between political economy and narrow economics"
      ],
      "source_article": "Political economy",
      "x": 1.201844334602356,
      "y": 0.9372008442878723,
      "level": 2,
      "original_question_hash": "7c043f07"
    },
    {
      "question": "A mid-size pharmaceutical company is entering Phase III clinical trials for a new drug. The risk register contains three quantified negative threats and one strategic opportunity: (A) clinical trial failure with probability 0.20 and impact $200M; (B) supply‑chain delay with probability 0.40 and impact $10M; (C) cybersecurity breach with probability 0.05 and impact $5M; and (O) a timing/market opportunity to accelerate regulatory submission that — if successful — yields an expected additional revenue of $30M. Using the standard risk‑magnitude formula $\\text{Risk} = p\\times\\text{impact}$ the baseline expected net downside (sum of the three threats) is $0.2\\times200+0.4\\times10+0.05\\times5=\\$44.25\\text{M}$. Management has a single mitigation budget of $15\\text{M}$ and the following mitigation options (costs and effects):\n- Pilot study to reduce trial failure probability from 0.20 to 0.12 (cost $8\\text{M}$).\n- Dual‑sourcing to reduce supply‑chain delay probability from 0.40 to 0.10 (cost $3\\text{M}$).\n- Cybersecurity program + insurance that lowers expected cybersecurity loss by 90% (cost $4\\text{M}$).\n- Accelerate regulatory submission to capture the opportunity (cost $12\\text{M}$, expected revenue +$30\\text{M}$).\n\nWhich allocation of the $15\\text{M}$ budget best follows risk management principles (identify → evaluate → prioritize → select appropriate response strategies: avoid/reduce/transfer/retain; and treat positive opportunities) to maximize expected net benefit while properly handling risk versus uncertainty across domains?\n",
      "options": {
        "A": "Fund the pilot study ($8M), dual‑sourcing ($3M), and cybersecurity program ($4M). This uses the full $15M to reduce all three quantified threats (apply 'reduce' and partial 'transfer' strategies).",
        "B": "Fund dual‑sourcing ($3M) and accelerate regulatory submission ($12M). This spends $15M to both reduce an operational threat and exploit the strategic opportunity (mix of 'reduce' for supply chain and 'exploit' for the opportunity).",
        "C": "Spend the entire $15M on the pilot study (8M) and buy an expensive insurance policy for trials and regulatory risk with the remaining $7M (attempting a pure 'transfer' approach to the largest threat).",
        "D": "Retain all quantified threats (do not spend the $15M), keeping the budget as contingency and accept the baseline expected losses (an explicit 'retain/accept' strategy)."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a budget‑allocation decision problem with numeric expected‑value calculations. Option B yields the largest net expected improvement by combining a high‑leverage opportunity (exploit) with a low‑cost reduction, illustrating prioritization, selection of response strategies, and distinction between risk (quantified threats) and uncertainty/opportunity.",
      "concepts_tested": [
        "risk management process flow and prioritization (identify → evaluate → prioritize → respond → monitor)",
        "risk response strategies (avoid, reduce, transfer, retain) and application to threats vs opportunities",
        "distinction between risk and uncertainty and adapting treatments across domains (financial/project/operational)"
      ],
      "source_article": "Risk management",
      "x": 1.4407463073730469,
      "y": 1.0094265937805176,
      "level": 2,
      "original_question_hash": "08e98cf1"
    },
    {
      "question": "An anthropological team studies an isolated community. Adults use a fixed set of whistle calls to warn about nearby predators or to indicate food location; these calls have a small finite inventory and are always tied to immediate contexts. A cohort of deaf children with limited exposure to the adult whistles spontaneously develop a manual signed system over a few years. Linguistic analysis shows the signed system has (i) combinatorial rules that allow novel utterances (productivity), (ii) grammatical constructions that refer to past and future events as well as hypothetical scenarios (displacement), and (iii) recursive embedding of clauses. The sign system is later transcribed into writing and encoded in braille without loss of grammatical properties. The children also display early development of theory of mind and frequent joint intentional activities in play. Which of the following conclusions is best supported by these observations?",
      "options": {
        "A": "The emergence of the signed system supports the modality-independence of language (it can be fully realized in manual and written/tactile media), demonstrates productivity and displacement as core properties of human language, and—given the role of shared intentionality and early theory of mind in its creation—provides strong empirical support for continuity-based, social-cognition accounts of language emergence (without excluding some biological predispositions).",
        "B": "Because the adults' whistle system did not develop into a full language, the data indicate that human language uniquely requires the vocal-auditory modality and cannot naturally arise in manual or tactile modalities.",
        "C": "The rapid appearance of complex syntax and displacement in the children indicates a discontinuity caused by a single genetic mutation that implanted a full Universal Grammar; social interaction and shared intentionality are peripheral to language emergence.",
        "D": "The fact that the children recombined a finite set of manual gestures into novel meanings implies non-human animal communication systems are equivalent to human language, so productivity and displacement are not distinctive of humans."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete field scenario contrasting an adult closed whistle system with a spontaneously created child sign language to probe modality-independence, productivity/displacement, and whether social cognition (theory of mind/shared intentionality) supports continuity-based origins versus innateness/discontinuity.",
      "concepts_tested": [
        "Productivity and displacement as defining properties of human language",
        "Modality-independence of language and encoding across modalities (spoken, signed, written, braille)",
        "Role of social cognition (theory of mind, shared intentionality) in language development and debates about continuity vs discontinuity / innateness"
      ],
      "source_article": "Language",
      "x": 1.24186110496521,
      "y": 1.0804492235183716,
      "level": 2,
      "original_question_hash": "ee5b088e"
    },
    {
      "question": "Two research teams independently evaluate the same six‑month community art programme intended to improve youth well‑being. Team Q uses a standardized well‑being scale with a large sample (n = 400) and reports a non‑significant mean change (p = 0.12). Team L conducts 12 semi‑structured, in‑depth interviews and finds recurring first‑person accounts of increased belonging, purpose, and motivation. Which of the following best explains (a) why the two teams reached apparently conflicting conclusions, (b) how underlying philosophical assumptions affect what each team treats as evidence, and (c) when a mixed‑methods design would be justified along with its main implication for interpreting results?",
      "options": {
        "A": "The two teams focused on different observable facets of the programme: Team Q operationalized well‑being as a numerical score (emphasizing reliability and group‑level generalizability), while Team L prioritized participants' subjective meanings (emphasizing depth and contextual validity). Their philosophical commitments differ: a positivistic stance privileges statistical significance as decisive evidence, whereas an interpretivist stance treats rich narratives as valid evidence of meaningful change. A mixed‑methods design is justified when a phenomenon has both measurable outcomes and complex subjective dimensions; it permits complementarity and triangulation but requires explicit integration of quantitative and qualitative findings to reconcile or explain apparent contradictions.",
        "B": "The discrepancy is most likely sampling error: Team L's small interview sample is unrepresentative and therefore irrelevant; only the quantitative result matters. Philosophical assumptions do not affect evidence—empirical facts speak for themselves. Mixed‑methods is unnecessary because a sufficiently powered quantitative study is always decisive for truth claims.",
        "C": "Team Q and Team L actually measured the same construct but used instruments with different reliability; therefore the conflict is purely a measurement‑error problem. Philosophical assumptions are secondary and only concern style of writing. Mixed‑methods is useful only to increase sample size or to validate a quantitative scale, not to address conceptual differences about what counts as evidence.",
        "D": "Both teams used valid methods and thus their conflicting conclusions indicate that the programme produced ambiguous real effects that cannot be resolved by methodology. Philosophical assumptions are irrelevant because methods should be used, not studied. Mixed‑methods designs are mainly pragmatic conveniences that complicate interpretation without improving validity."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete comparative scenario (quantitative vs qualitative study) to probe how measurement/operationalization, philosophical stances (positivist vs interpretivist), and rationale/implications of mixed‑methods explain divergent findings and guide integration.",
      "concepts_tested": [
        "Method choice influences conclusions via operationalization and measurement (construct validity, what is measured vs what is experienced)",
        "Philosophical assumptions determine what counts as evidence (positivist emphasis on statistical significance vs interpretivist emphasis on narrative depth)",
        "Rationale for mixed‑methods: complementarity, triangulation, when phenomena require both generalizability and contextual understanding and implications for integration"
      ],
      "source_article": "Methodology",
      "x": 1.3526579141616821,
      "y": 1.0467337369918823,
      "level": 2,
      "original_question_hash": "aea54eb5"
    },
    {
      "question": "You are advising a master's student who proposes to study how a severe pandemic-induced economic contraction in 2030 affects nuclear deterrence stability between two rival nuclear states (Rivalland and Contenda). The student must (1) incorporate cross-field variables from public health, political economy, and social unrest, (2) engage with classical deterrence theory (e.g., Schelling, Kissinger) to model crisis signaling and credibility, and (3) preserve theoretical coherence of security studies rather than diluting it by an overly broad scope. Which of the following research designs best satisfies these constraints?",
      "options": {
        "A": "A narrow realist game-theoretic project that models only nuclear crisis bargaining and second-strike capabilities using classical deterrence assumptions (à la Schelling), deliberately excluding epidemiological and macroeconomic variables to maintain strict theoretical coherence.",
        "B": "An interdisciplinary, multi-method study that integrates an epidemiological timeline (to time shocks to military readiness), macroeconomic indicators (to measure fiscal stress and regime vulnerability), and formal deterrence models to derive mechanisms linking public-health shocks to signaling and credibility; the study explicitly delimits its scope to interstate nuclear interactions to preserve theoretical coherence and tests hypotheses with case-comparative analysis and calibrated models.",
        "C": "A human-security oriented qualitative project that foregrounds civilian vulnerability, healthcare capacity, and humanitarian relief responses during the pandemic, treating nuclear deterrence as a marginal policy framing and prioritizing domestic public-health outcomes and norm-based prescriptions.",
        "D": "A programmatic expansion that constructs a global security index combining environmental, economic, health, and social indicators and argues for dropping specialized deterrence scholarship in favor of broad, aggregate measures of ‘security’ across all sectors to capture systemic threats holistically."
      },
      "correct_answer": "B",
      "generation_notes": "Created a concrete thesis-advising scenario requiring choice among research designs; correct option integrates interdisciplinary methods with classical deterrence theory while preserving scope/coherence. Distractors reflect plausible alternative positions (pure deterrence, human-security focus, indiscriminate expansion).",
      "concepts_tested": [
        "Interdisciplinary integration across public health, political economy, and military/strategic studies",
        "Nuclear deterrence theory and its application to crisis signaling and credibility",
        "Debate over field expansion vs. maintaining theoretical coherence in security studies"
      ],
      "source_article": "Security studies",
      "x": 1.1825592517852783,
      "y": 0.883911669254303,
      "level": 2,
      "original_question_hash": "033f2006"
    },
    {
      "question": "A vertical flat plate at temperature $T_{p}=500\\,\\mathrm{K}$ is suspended in a large sealed enclosure whose interior walls are held at $T_{w}=300\\,\\mathrm{K}$. Consider three experimental conditions: (1) the enclosure is evacuated (near vacuum), (2) the enclosure is filled with quiescent air at atmospheric pressure, and (3) the same air but a fan forces a uniform flow parallel to the plate surface. Which one of the following descriptions correctly identifies the dominant heat‑transfer mechanisms in each case, the expected dependence of heat flux on the temperature difference, and the effect of the fan on the heat transfer coefficient and total heat transferred during a transient cooling to $T_{w}$?",
      "options": {
        "A": "(Correct) (1) In vacuum radiation dominates and the surface heat flux follows $\\phi''=\\varepsilon\\sigma\\bigl(T_p^{4}-T_w^{4}\\bigr)$ (strongly non‑linear in $\\Delta T$). (2) In quiescent air natural convection plus diffusion (conduction in the boundary layer) dominate; the buoyancy‑driven flow depends on the Rayleigh number so the convective heat flux is generally non‑linear with $\\Delta T$ and Newton's law of cooling need not hold exactly. (3) With the fan forced convection dominates; the fan increases the convective heat transfer coefficient $h$ (often substantially) so the convective component can be approximated as $\\phi''\\approx h\\,\\Delta T$ (approximately linear for moderate $\\Delta T$). In every case net heat flows from the hot plate to the colder walls (second law) and the cumulative heat transferred during a transient cooling depends on the process (path), because $h$ (and hence the heat flow history) changes with the mechanism.",
        "B": "In vacuum conduction through the residual gas is the dominant mechanism so the heat flux is proportional to $\\Delta T$; with quiescent air radiation dominates and follows a $T^{4}$ law; using the fan reduces the heat transfer coefficient because it suppresses natural convection, so total heat removed during transient cooling is smaller with the fan.",
        "C": "Radiation is negligible in the evacuated case and heat transfer there is controlled by conduction through supports; quiescent air case is dominated by conduction through air (linear in $\\Delta T$) and is well described by Newton's law; forced flow merely redistributes heat but does not change the heat transfer coefficient $h$.",
        "D": "In vacuum the dominant mechanism is forced‑convection of photons which produces a heat flux proportional to $\\Delta T$; in quiescent air forced convection occurs spontaneously so Newton's law always applies; turning on a fan can make heat flow from the colder walls to the plate if the flow velocity is high enough."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete experiment (vertical heated plate in enclosure) and compared vacuum, natural (buoyancy) convection, and forced convection. Options combine mechanism identification, scaling laws ($\\sigma T^{4}$ vs $h\\Delta T$), Rayleigh dependence and path dependence of heat transfer; one option is fully consistent with the article.",
      "concepts_tested": [
        "Three primary heat transfer mechanisms and their driving forces (conduction/diffusion, natural vs forced convection, radiation)",
        "Heat transfer coefficient and the relation between heat flux and thermodynamic driving force; path dependence of heat transfer",
        "Directionality toward thermal equilibrium and buoyancy‑driven natural convection (Rayleigh‑number dependence); coexistence of mechanisms"
      ],
      "source_article": "Heat transfer",
      "x": 1.779079556465149,
      "y": 1.020654320716858,
      "level": 2,
      "original_question_hash": "726139b2"
    },
    {
      "question": "You are presented with a conformable, upright stratigraphic succession (oldest at bottom). Layer A: well-sorted, well-rounded quartz sand with large-scale, high-angle cross-bedding and frosted grain surfaces. Layer B: thinly laminated, dark, organic-rich mudstone/shale with high total organic carbon, no bioturbation, and fine laminations. Layer C: interbedded gypsum and halite with desiccation cracks and mud-flat surface textures. Applying the principle of uniformitarianism and using sedimentary rock types and stratigraphic relations, which of the following depositional-history interpretations is most consistent with the observations?",
      "options": {
        "A": "Layer A = aeolian (wind-blown) siliciclastic sand dunes (aeolian sandstone); Layer B = deposition under a stratified, oxygen-depleted water body (organic-rich lacustrine or restricted marine shale); Layer C = subsequent aridification and evaporation forming a playa/sabkha evaporite sequence. Interpretation: a transition from high-energy subaerial dunes to quiet anoxic basin conditions and then to shallow evaporitic conditions due to basin restriction and drying.",
        "B": "Layer A = fluvial channel point-bar sandstone; Layer B = deep-marine turbidite shale deposited by frequent gravity flows; Layer C = shallow-marine carbonate bank with minor evaporite lenses. Interpretation: a fluvial-to-deep-marine-to-carbonate transgression driven by sea-level rise.",
        "C": "Layer A = shoreface/upper-foreshore sandstone with storm-generated cross-beds; Layer B = tidal flat peat that later became coal; Layer C = glacial-marine diamictite with dropstones. Interpretation: a coastal system punctuated by glaciation.",
        "D": "Layer A = submarine sandwave deposited by strong tidal currents; Layer B = peat-rich swamp that later oxidized to brown shale; Layer C = biogenic carbonate reef built on top of an exposed basement. Interpretation: a tidally influenced shelf evolving into reefal carbonate deposition."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a vertical succession with diagnostic sedimentary features (aeolian cross-beds, anoxic laminated black shale, evaporites) to require application of uniformitarianism, rock-type genesis, and stratigraphic interpretation; created plausible but incorrect alternatives.",
      "concepts_tested": [
        "Uniformitarianism and using modern processes to interpret past environments",
        "How depositional processes control sedimentary rock types (siliciclastic, organic shale, evaporite) and their diagnostic features",
        "Stratigraphic reconstruction of paleoenvironmental change using sedimentary structures, lithology, and vertical facies relationships"
      ],
      "source_article": "Sedimentology",
      "x": 1.784373164176941,
      "y": 0.935629665851593,
      "level": 2,
      "original_question_hash": "77fac5c3"
    },
    {
      "question": "A firm requires an investment of $I=\\$100$ million. Available financing alternatives are (i) issue debt at yield $r_d=4\\%$, (ii) issue equity with required return $r_e=12\\%$. The corporate tax rate is $t=25\\%$. Households hold $\\$200$ million of liquid savings in bank deposits paying 1\\%, and banks can intermediate by purchasing corporate bonds or lending directly. Which one of the following best describes how the financial system channels saving into the firm while addressing risk and value optimisation?",
      "options": {
        "A": "Household savings are channelled to the firm either indirectly (via banks) or directly (via bond/equity markets); the firm chooses debt and equity weights $w_d,w_e$ to minimise WACC, given by $\\mathrm{WACC}=w_d r_d(1-t)+w_e r_e$ (e.g. 100% debt yields $r_d(1-t)=4\\%(1-0.25)=3\\%$ vs. 100% equity $=12\\%$). Lower pre-tax cost of debt reduces WACC but increases default/volatility risk, so managers balance value maximisation against increased financial risk and use risk management (hedging, diversification, insurance) to control volatility. Personal, corporate and public finance play distinct roles in supplying, allocating and regulating these flows.",
        "B": "Because household deposit rates (1\\%) are below corporate bond yields (4\\%), banks will not intermediate and all savings must flow directly to equity markets; therefore the firm must issue equity, which is the only feasible way to fund the $\\$100$ million investment.",
        "C": "To maximise firm value managers should always finance entirely with the instrument that has the lowest nominal yield — here 4\\% debt — so 100% debt is optimal; risk considerations are secondary and can be ignored because diversification in the market removes all firm-level volatility.",
        "D": "The central bank is the primary allocator of household savings to firms: it sets deposit and bond yields and directly transfers household deposits into corporate loans, so personal, corporate and public finance distinctions are not important for capital allocation or risk management."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete financing scenario with numeric yields and tax rate; tested understanding of capital allocation (intermediation and markets), WACC minimisation with the formula $\\mathrm{WACC}=w_d r_d(1-t)+w_e r_e$, and trade-offs between lower cost debt and increased volatility/risk management measures; distractors assert common misconceptions.",
      "concepts_tested": [
        "Capital allocation mechanisms: savers → intermediaries/markets → borrowers",
        "Risk management vs value optimisation: WACC minimisation, trade-off between cost of capital and financial risk",
        "Structure of financial system: roles of personal, corporate, and public finance and intermediation"
      ],
      "source_article": "Finance",
      "x": 1.351603627204895,
      "y": 0.944442629814148,
      "level": 2,
      "original_question_hash": "187765cc"
    },
    {
      "question": "Country X has a fixed public research budget of $200\\text{M}$. Current allocations are 60% basic research, 25% applied research, and 15% facilities/equipment. A national high-cost microscopy facility would cost $30\\text{M} if built as a single centralized installation, or $36\\text{M} if implemented as a distributed network of nodes (20% higher capital cost) — the distributed network is estimated to increase novel idea production by ~15% by enabling broader collaborations and shared equipment access. Private firms have pledged an additional $50\\text{M} of R&D funding only if the public applied-research share is raised to at least 30% of the public budget. An international partnership program that costs $10\\text{M} would substantially improve access to overseas expertise and accelerate technology translation. Which combination of policy choices is most likely to maximize translation into societal outcomes and the production of novel science and engineering ideas?",
      "options": {
        "A": "Increase applied research to 30% (unlocking the $50\\text{M} private contribution), fund the distributed networked facility at $36\\text{M} despite the 20% premium, and allocate $10\\text{M} to the international partnership program — funding the increases by modest reductions in basic research.",
        "B": "Keep current allocations (applied 25%), build the centralized $30\\text{M} facility, and do not fund the international partnership — rely on public basic research to generate breakthroughs without private or international engagement.",
        "C": "Raise applied research above 30% by cutting facilities funding and choose the centralized $30\\text{M} facility to minimize public capital outlay; do not invest in the international partnership — rely on private funding but concentrate expensive equipment centrally.",
        "D": "Shift more funds into basic research (increase to ~70%), leave applied at 25%, build the distributed networked facility, but do not pursue private pledges or international partnerships — prioritize breakthrough discovery over translation and multi-actor engagement."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a budget allocation scenario with explicit costs and conditional private funding to test how reallocations, equipment distribution (centralized vs networked), and international partnerships affect translation, novel idea production, and multi-actor engagement.",
      "concepts_tested": [
        "Resource allocation and funding influence research agendas and translation into societal outcomes",
        "Role of knowledge networks, collaborations, and equipment distribution in producing novel science and engineering ideas",
        "Multi-actor science policy ecosystem (government, private firms, international partnerships) and its effect on policy outcomes"
      ],
      "source_article": "Science policy",
      "x": 1.2735531330108643,
      "y": 0.9810090661048889,
      "level": 2,
      "original_question_hash": "4b449278"
    },
    {
      "question": "Consider these four concrete examples and the notion of symmetry defined as invariance under a transformation that leaves an object’s overall form unchanged. Identify the primary symmetry type for each example (choose the option that pairs all four correctly):\n\n1. A nautilus shell whose whorls repeat the same general shape at progressively smaller scales (self-similarity under rescaling by a factor $s$).\n2. A cylindrical spiral staircase that both rotates and rises along its central axis so that each step maps onto the next.\n3. A decorative border on a wall where a motif is reflected across a line and then shifted along that line to produce a repeating band.\n4. A human face that is approximately identical on the left and right when divided by the sagittal plane.\n",
      "options": {
        "A": "1 — Scale symmetry (self-similarity under scaling by $s$); 2 — Helical (screw) symmetry (simultaneous rotation and translation along an axis); 3 — Glide reflection symmetry (reflection followed by translation along the axis of reflection); 4 — Reflectional (mirror) symmetry across a plane.",
        "B": "1 — Rotational symmetry about a fixed point; 2 — Pure rotational symmetry about the central axis (no translation); 3 — Pure translational symmetry (simple repetition by translation only); 4 — Reflectional symmetry across a plane.",
        "C": "1 — Scale symmetry combined with rotoreflection; 2 — Helical symmetry; 3 — Rotoreflection symmetry (rotation followed by reflection); 4 — Translational symmetry along the midline.",
        "D": "1 — Translational symmetry along the shell direction; 2 — Reflectional symmetry across a vertical plane through the axis; 3 — Scale symmetry (self-similarity of the motif at different sizes); 4 — Rotational symmetry about the midline."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete four-item matching task that requires recognizing invariance under transformations (defining symmetry), classifying by transformation type (scale, helical, glide reflection, reflection), and connecting mathematical symmetry concepts to natural and architectural examples.",
      "concepts_tested": [
        "invariance under transformations",
        "classification of symmetry types (reflectional, rotational, translational, helical, scale, glide reflection, rotoreflection)"
      ],
      "source_article": "Symmetry",
      "x": 1.6630512475967407,
      "y": 1.1922389268875122,
      "level": 2,
      "original_question_hash": "bac69a31"
    },
    {
      "question": "A publicly listed firm with diffuse shareholders has experienced weak monitoring of management and recurring concerns about financial reporting. The board wants to implement changes that (i) reflect the Cadbury/OECD/Sarbanes–Oxley governance principles, (ii) strengthen the governance relationships among management, board, shareholders and other stakeholders, and (iii) use contract-based mechanisms from corporate finance to reduce principal–agent problems. Which of the following combined actions best achieves all three objectives?",
      "options": {
        "A": "Create an independent audit committee and disclose audited, timely financial information; adopt a formal code of conduct per OECD/Cadbury guidance; and replace a portion of the CEO’s fixed pay with long‑dated, performance‑vesting equity tied to multi‑year operating metrics (while prohibiting the external auditor from providing non‑audit consulting services).",
        "B": "Increase annual dividend payouts to satisfy shareholders and authorize a large share buyback to support the share price; keep the current CEO as both chair and CEO to preserve decision speed.",
        "C": "Consolidate financial reporting lines under the CFO, centralize decision‑making in the executive team, and eliminate separate audit and compensation committees to cut governance costs.",
        "D": "Hire a new external PR firm to improve stakeholder communications, appoint an internal compliance officer reporting to the CEO, and grant managers discretionary cash bonuses based on quarterly revenue targets."
      },
      "correct_answer": "A",
      "generation_notes": "Created a scenario requiring selection of governance measures that (1) map to principles (disclosure, independence, auditor rules), (2) emphasise relationships among board/management/shareholders/stakeholders, and (3) use contract-based pay (long‑dated equity) to mitigate agency problems — option A integrates all three.",
      "concepts_tested": [
        "Governance as relational system among board, management, shareholders and stakeholders",
        "Governance principles from Cadbury/OECD/SOX (disclosure, independence, audit controls)",
        "Firm as governance structure using contract-based mechanisms (executive compensation, equity incentives) to address principal–agent problems"
      ],
      "source_article": "Corporate governance",
      "x": 1.3299872875213623,
      "y": 0.8944252729415894,
      "level": 2,
      "original_question_hash": "920d33ec"
    },
    {
      "question": "A multinational firm headquartered in Country A must decide how to allocate production of two goods—smartphones and textiles—between Country A and Country B. A single worker in Country A can produce either 10 smartphones or 5 textiles per month; a single worker in Country B can produce either 6 smartphones or 4 textiles per month. Recent improvements in communication and coordination technology have reduced coordination costs by 40%, and trade barriers have been partially liberalized. The firm also faces varying political and regulatory risk in Country B and significant cultural differences in two target markets. Which of the following strategic decisions best applies the principle of comparative advantage, exploits globalization-enabled reductions in barriers to entry and coordination costs, and incorporates appropriate market analysis, risk assessment, and local adaptation?",
      "options": {
        "A": "Consolidate all production of both smartphones and textiles in Country A to retain full control and avoid Country B’s political and regulatory risks, then export to both markets without product or marketing adaptation.",
        "B": "Shift all smartphone production to Country B because of lower nominal wages, while keeping textiles production in Country A, and use a single global marketing campaign to minimize costs.",
        "C": "Specialize: concentrate smartphone production in Country A (where the opportunity cost of a smartphone is lower) and outsource textile production to Country B, using the new communication technology to coordinate remote suppliers, while conducting market analysis to adapt product features and marketing locally and performing political and operational risk assessments for the B-based suppliers.",
        "D": "Fragment production by splitting smartphone and textile manufacturing 50/50 across Countries A and B to diversify political and economic risk, and use identical product specifications and branding in all markets to maintain global consistency."
      },
      "correct_answer": "C",
      "generation_notes": "Created a numeric two-good two-country scenario to compute opportunity costs and identify comparative advantage; incorporated technological reduction in coordination costs and strategic elements (market analysis, risk assessment, local adaptation) to select the best international strategy.",
      "concepts_tested": [
        "Comparative advantage and opportunity cost",
        "Role of globalization and technology in reducing coordination and entry barriers",
        "Importance of market analysis, risk assessment, and local adaptation in international strategy"
      ],
      "source_article": "International business",
      "x": 1.3159611225128174,
      "y": 0.9567903280258179,
      "level": 2,
      "original_question_hash": "716d9b17"
    },
    {
      "question": "A country, Novia, privatizes in 1990 and secures private property rights. A representative firm in Novia produces output Y according to Y = A K^{1/2} L^{1/2} and accumulates capital by ΔK = sπ − δK, where s is the fraction of profits π reinvested and δ is the depreciation rate. In 2000 one firm develops a process innovation that raises its productivity parameter A by 20%; profits and employment (wage labor) rise immediately, and the firm reinvests a large share of profits. Over the next decade, entry from other firms tends to erode excess profits unless barriers, intellectual property or regulations intervene. The Novian government must choose a policy package to promote sustained capital accumulation, continued innovation, competitive markets and rising real wages while keeping private ownership and the profit motive central. Which policy package best embodies a mixed-market capitalist approach that balances market incentives with state action to achieve those goals?",
      "options": {
        "A": "Legally strengthen private property and contract enforcement; maintain time-limited, moderate IP rights; enforce antitrust/competition law to prevent monopolies; fund basic public R&D and education; provide progressive social safety nets — thereby preserving profit incentives while limiting rent-seeking and supporting wage growth.",
        "B": "Nationalize leading firms and centralize investment decisions in state-owned enterprises; allocate capital by administrative planning; restrict private ownership of means of production to a residual private sector — prioritizing state control over market signals.",
        "C": "Abolish corporate taxes and most business regulation; eliminate antitrust enforcement and labor protections to maximize after-tax profits and private reinvestment, trusting owners to supply capital and jobs without public intervention.",
        "D": "Provide large, permanent subsidies and high import tariffs to incumbent firms that developed the innovation; favor these firms with regulatory exemptions to secure domestic industry growth and postpone foreign competition."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete Novia scenario using a Cobb–Douglas production function and capital-accumulation equation to test how private property, profit motive, capital accumulation, wage labor, innovation, competition, and state policy interact; options map to mixed-market capitalism (A), state capitalism (B), laissez-faire oligopoly-prone policy (C), and protectionist cronyism (D).",
      "concepts_tested": [
        "Private property and profit motive as structural basis of capitalist production",
        "Mechanisms of capital accumulation, competitive entry, wage labor and innovation driving growth and business-cycle dynamics",
        "Relationship between markets and the state: forms of capitalism, regulation, mixed economies and policy trade-offs"
      ],
      "source_article": "Capitalism",
      "x": 1.2008112668991089,
      "y": 0.9387081861495972,
      "level": 2,
      "original_question_hash": "9b05ab4f"
    },
    {
      "question": "A mid-sized city's air-quality monitors show a steady rise in PM2.5 concentrations attributable to private vehicle traffic. The city council opens a policy process to reduce pollution while protecting low-income residents. Which of the following options best describes a policy cycle sequence, the appropriate actor-network, and a mix of policy instruments whose design and evaluation would most plausibly achieve the city's goals and allow iterative adjustment?",
      "options": {
        "A": "Agenda-setting triggered by monitoring indicators and media coverage; policy formulation convenes elected officials, civil servants, transport experts, community representatives and business stakeholders to design a mixed package: a congestion charge (tax) to internalize externalities, revenues hypothecated to increased public-transit spending (Make) and targeted subsidies for low-income EV conversions (Subsidize); legitimation achieved through council votes and public consultations (co-production); implementation led by transit and enforcement agencies with clear administrative authority; evaluation uses air-quality indicators, ridership, and distributional metrics with feedback to adjust tax levels and subsidy rates.",
        "B": "Agenda-setting is followed by a top-down formulation in which the mayor issues an immediate prohibition (Prohibit) on all diesel vehicles; legitimacy is assumed from executive authority, implementation enforced by police; experts and citizens are consulted only post-implementation; evaluation focuses solely on immediate emissions readings and therefore no further adjustments are planned.",
        "C": "Agenda-setting emphasizes public education; the city relies primarily on an information campaign (Inform) developed by NGOs and communication teams while avoiding fiscal or regulatory instruments; legitimacy is built through media endorsements and voluntary pledges by drivers; implementation is low-cost and decentralized; evaluation uses surveys of self-reported behaviour and non-binding commitments as success metrics.",
        "D": "Agenda-setting leads to formulation dominated by automotive industry lobbyists and economists who recommend subsidies and tax breaks for electric vehicle manufacturers (Subsidize and Tax exemptions) without increasing public transit spending; legitimation is obtained through legislative votes influenced by campaign contributions; implementation is delivered via procurement contracts to manufacturers; evaluation measures only EV sales, with no attention to air-quality or equity outcomes, and no mechanism to revise instruments."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete municipal air-pollution scenario to require application of the policy cycle stages, actor-network interactions (elected officials, civil servants, experts, citizens, lobbyists), and trade-offs among instruments (taxes, spending, subsidies, regulation, information), with emphasis on legitimation, implementation responsibilities, evaluation indicators and feedback.",
      "concepts_tested": [
        "Policy cycle stages (agenda setting, formulation, legitimation, implementation, evaluation, feedback)",
        "Actor-network roles and co-production (elected officials, civil servants, experts, interest groups, citizens)",
        "Policy instruments and design trade-offs (taxes, spending, subsidies, regulation, information) and their effects on objectives like pollution reduction and equity"
      ],
      "source_article": "Public policy",
      "x": 1.2379401922225952,
      "y": 0.930698037147522,
      "level": 2,
      "original_question_hash": "b7de6606"
    },
    {
      "question": "Country Azania had a total population of $10{,}000{,}000$ in 2000 with an urban population of $3{,}000{,}000$ (urbanization = $30\\%$). By 2020 the total population was $13{,}000{,}000$ and the urban population was $6{,}500{,}000$ (urbanization = $50\\%$). A ministry of planning must choose metrics to guide policies on water supply, housing and transport, and to report progress toward SDG 11 (“Sustainable cities and communities”). Which of the following policy- and measurement-oriented statements is most accurate for Azania given these data and the broader links between urbanization, industrialization and globalization?",
      "options": {
        "A": "Because Azania’s urbanization rose from $30\\%$ to $50\\%$, the appropriate single indicator to guide policy is the urbanization rate (proportion of population in urban areas); absolute urban population is redundant because SDG 11 targets urban shares rather than counts.",
        "B": "Urbanization (the share) increased from $30\\%$ to $50\\%$ while absolute urban population rose by $3.5$ million; therefore planners should monitor both the proportion and the absolute urban population because infrastructure demand, resource scarcity (water, land) and service delivery scale with absolute numbers, and SDG 11–oriented responses should combine urban resilience, demand‑efficiency and controls on growth drivers linked to industrialization and globalization.",
        "C": "Only absolute urban growth matters for environmental pressure, so the ministry should ignore the urbanization rate and focus exclusively on limiting the urban population increase to reduce problems such as food waste and eutrophication.",
        "D": "Because globalization and industrialization inevitably drive urbanization, the ministry’s best policy is to prioritize rapid economic expansion (attracting industry and capital) and defer sustainability interventions until after the urban population stabilizes; measurement should emphasize GDP growth rather than urban metrics."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a numeric scenario illustrating difference between urbanization (share) and urban growth (absolute change) and created options testing metric choice, policy implications, and links to SDG11, globalization and industrialization.",
      "concepts_tested": [
        "Distinction between urbanization (proportion) and urban growth (absolute numbers)",
        "How measurement choices affect policy for resource scarcity, infrastructure and SDG 11; role of industrialization/globalization as drivers"
      ],
      "source_article": "Urbanization",
      "x": 1.2120298147201538,
      "y": 0.8871142268180847,
      "level": 2,
      "original_question_hash": "f9f644fe"
    },
    {
      "question": "Country W experienced the following sequence between 1970 and 2010: GDP per capita rose from $1{,}000$ to $10{,}000$ and tertiary enrollment increased sharply by 1995; its economy shifted from manufacturing to services with the service sector exceeding $50\\%$ of employment by 2000 (a post‑industrial shift). Despite these changes, its democracy index remained low until a marked rise in \"self‑expression values\" among the population during the mid‑1990s, after which the democracy index increased in the 2000s. Which theoretical interpretation best accounts for Country W's trajectory?",
      "options": {
        "A": "Lipset's classical modernization claim that economic development (wealth and education) directly causes democratization, since Country W's rising GDP and education ultimately preceded democracy.",
        "B": "Reverse causality: democratization causes economic modernization, because Country W's democracy followed economic change, showing political reform is the necessary precursor to sustained modernization.",
        "C": "The Inglehart–Welzel revision: economic modernization leads to post‑industrial cultural change (self‑expression values), and these cultural shifts mediate the transition to democracy—explaining why democracy followed the rise in self‑expression values rather than immediately after GDP growth.",
        "D": "Modernization aids democratic survival but does not cause transitions; Country W's experience shows that high GDP merely stabilizes regimes once democracy exists, so its eventual democratization must be due to exogenous institutional reforms unrelated to cultural or economic change."
      },
      "correct_answer": "C",
      "generation_notes": "Designed a temporal case (economic modernization → post‑industrial cultural shift → democracy) to discriminate Lipset (direct effect), reverse causality, survival‑only claims, and Inglehart–Welzel's mediation by self‑expression values.",
      "concepts_tested": [
        "Economic development as a driver of democratization (Lipset)",
        "Alternative causal directions and conditional effects (reverse causality and survival vs transition)",
        "Inglehart–Welzel revision: post‑industrial self‑expression values mediating democratization"
      ],
      "source_article": "Modernization theory",
      "x": 1.2195931673049927,
      "y": 0.929759681224823,
      "level": 2,
      "original_question_hash": "784b1c44"
    },
    {
      "question": "A national government is debating whether to permit oil extraction in an ultra-diverse rainforest area. The extraction would yield an immediate public revenue of $1.0 billion today but would irreversibly destroy ecosystem services that ecologists estimate are worth $2.0 billion spread over the next 50 years. A conventional cost–benefit analysis using a discount rate of $r=5\\%$ computes the present value of the future ecosystem losses as $\\mathrm{PV}=\\frac{2.0\\ \\text{billion}}{(1+0.05)^{50}}\\approx 0.175\\ \\text{billion}$, so the net present value appears positive. Which of the following policy positions best exemplifies the analytical and normative stance of ecological economics (including concerns about strong sustainability, irreversibility, intergenerational equity, uncertainty, and justice/care values as found in feminist economics and ecosocialist thought)?",
      "options": {
        "A": "Reject the extraction permit: treat the rainforest's natural capital as non-substitutable (strong sustainability), apply the precautionary principle because of irreversibility and deep uncertainty, and use positional analysis to prioritize intergenerational equity and local community care values rather than relying solely on discounted monetary NPV.",
        "B": "Approve the permit because conventional cost–benefit analysis with $r=5\\%$ shows a positive net present value (immediate $1.0$B > PV of future losses $\\approx0.175$B), implying the economic gains justify irreversible trade-offs.",
        "C": "Approve the permit conditional on creating tradable ecosystem-service credits and biodiversity offsets that monetize the $2.0$B loss so markets internalize the cost and compensation payments fund conservation elsewhere, assuming market valuation will correct the externality.",
        "D": "Approve the permit but invest proceeds in technological solutions (water treatment, synthetic pollination, engineered carbon capture) on the assumption that man-made capital can substitute for lost ecosystem functions (weak sustainability and techno-optimism)."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete policy scenario with numerical discounting to force comparison between mainstream NPV decision rules and ecological-economics principles: strong sustainability, precautionary principle, irreversibility, intergenerational equity, uncertainty, and normative valuation (positional analysis, justice/care).",
      "concepts_tested": [
        "Economy as a subsystem of Earth's ecosystem; strong vs weak sustainability (non-substitutability of natural capital)",
        "Intergenerational equity, irreversibility, uncertainty, critiques of monetary cost–benefit analysis and discounting; positional analysis and normative valuation",
        "Cross-disciplinary normative frameworks: connections to feminist economics (care values) and ecosocialism (justice, distribution)"
      ],
      "source_article": "Ecological economics",
      "x": 1.222957730293274,
      "y": 0.9528908729553223,
      "level": 2,
      "original_question_hash": "f6353028"
    },
    {
      "question": "Country X liberalized trade and reduced transport costs. Over two decades a coastal metropolis attracted high‑tech firms, venture capital, and skilled migrants; it now exhibits higher wages, dense interfirm supplier–customer networks, frequent informal knowledge exchanges (e.g., co‑located R&D labs and employee mobility), and strong local institutions supporting startups. Inland regions, with similar factor endowments and lower nominal wages, remain agriculturally specialized and see little innovation. Which theoretical explanation best accounts for the persistence of this core–periphery inequality despite lower transport costs?",
      "options": {
        "A": "A neoclassical location explanation: factor price equalization and comparative advantage mean differences reflect initial endowments and transaction costs; persistence is due to immobile factors and impediments to capital flow predicted by standard location theory.",
        "B": "The New Economic Geography (NEG1) explanation: increasing returns to scale, transport costs and the balance of centripetal and centrifugal forces produce agglomeration in the metropolis and a core–periphery pattern, so spatial concentration persists through market‑based scale economies.",
        "C": "A combined explanation emphasizing economies of agglomeration plus social, cultural and institutional factors (NEG2): linkages, tacit knowledge spillovers, dense labour pools, trust‑based networks and supportive local institutions generate endogenous increases in productivity and path‑dependent lock‑in of the core, maintaining inequality even when transport costs fall.",
        "D": "A geographic determinist explanation: the coastal metropolis succeeds because of immutable physical advantages (port access, favourable climate) that permanently raise productivity relative to the inland periphery, which cannot be overcome by policy or social factors."
      },
      "correct_answer": "C",
      "generation_notes": "Constructed a case scenario of trade liberalization producing persistent spatial concentration; options contrast neoclassical, NEG1, NEG2/institutional, and geographic determinism to test agglomeration/linkages, core–periphery dynamics, and social/institutional roles.",
      "concepts_tested": [
        "Economies of agglomeration and linkages",
        "Core–periphery dynamics and globalization-driven regional inequality",
        "Role of social, cultural, and institutional factors (tacit knowledge, networks) in spatial economic processes"
      ],
      "source_article": "Economic geography",
      "x": 1.2581939697265625,
      "y": 0.9311760067939758,
      "level": 2,
      "original_question_hash": "5c5c3a22"
    },
    {
      "question": "A mid-size e-commerce firm must schedule weekend deliveries for 5 vans to serve 50 packages across a metropolitan area. Each package has a promised delivery time-window, each van has a capacity limit, and travel times are uncertain due to variable traffic and weather. The planner's stated objective is to minimize expected total delivery time while ensuring that the probability of meeting all promised time-windows is at least $0.9$ and vehicle capacity constraints are respected. The team has only one commodity server with a one-hour runtime budget for optimization each night. Which modeling and solution strategy best reflects operations-research practice given the system description, objective, probabilistic constraint, and computational limit?",
      "options": {
        "A": "Model the problem as a deterministic mixed-integer linear program (MILP) using expected travel times, minimize total route time with capacity and time-window constraints, and solve with an exact branch-and-bound MILP solver within the one-hour limit.",
        "B": "Formulate a Markov decision process (MDP) that treats each van position and outstanding package set as the state, then compute the optimal policy via dynamic programming to minimize expected total delivery time subject to the $0.9$ service-probability constraint.",
        "C": "Frame the problem as a chance-constrained stochastic vehicle routing problem (a stochastic integer program with the constraint $P(\text{all windows met})\\ge 0.9$), approximate the stochastic objective and constraints using sample average approximation (Monte Carlo), and solve the resulting sampled instances with heuristic/metaheuristic algorithms (e.g., tabu search or genetic algorithms) within the one-hour runtime.",
        "D": "Use a robust optimization formulation that replaces uncertain travel times with their worst-case bounds and solve a conservative MILP to guarantee time-window satisfaction in all scenarios, trusting the branch-and-cut solver to return a feasible robust routing within the runtime budget."
      },
      "correct_answer": "C",
      "generation_notes": "Created a concrete vehicle-routing scenario with stochastic travel times and probabilistic service constraint; tested modelling as stochastic integer program and pragmatic solution via SAA plus heuristics given computational limits. Distractors include deterministic MILP, MDP/dynamic programming, and conservative robust optimization.",
      "concepts_tested": [
        "Mathematical modeling of real systems (stochastic vehicle routing, chance constraints)",
        "Optimization techniques and approximation methods (stochastic programming, sample average approximation, heuristics)",
        "Method selection and problem framing considering system nature, goals, and computational constraints"
      ],
      "source_article": "Operations research",
      "x": 1.67019784450531,
      "y": 0.7453072667121887,
      "level": 2,
      "original_question_hash": "a21060c3"
    },
    {
      "question": "A 52-year-old patient ingests a large potassium supplement and develops a modest rise in plasma [K+]. Within minutes the body initiates homeostatic responses that restore plasma [K+] to the normal range. Which of the following descriptions best identifies (1) the receptor, (2) the control center, (3) the effector, and (4) the cellular-level mechanism and negative feedback that together explain this correction in accordance with physiological homeostasis?",
      "options": {
        "A": "High plasma [K+] directly depolarizes zona glomerulosa cells in the adrenal cortex (receptor); these adrenal cells act as the control center by secreting aldosterone; aldosterone diffuses into renal distal-tubule epithelial cells (effector tissue), binds intracellular mineralocorticoid (nuclear) receptors and up‑regulates transcription of ENaC and basolateral Na+/K+‑ATPase, increasing renal K+ secretion and restoring plasma [K+]; the fall in plasma [K+] reduces zona glomerulosa depolarization and thus aldosterone secretion (negative feedback).",
        "B": "The juxtaglomerular apparatus senses the elevated plasma [K+] (receptor) and secretes renin as the control signal; renin → angiotensin II stimulates adrenal aldosterone release which immediately inserts preformed ion channels into distal‑tubule membranes (effector action) to excrete K+; normalized [K+] then halts renin release (negative feedback).",
        "C": "Hypothalamic osmoreceptors detect the rise in plasma [K+] (receptor) and the hypothalamus/pituitary acts as control center to secrete ADH; ADH acts on the kidney collecting ducts (effector) to conserve water and thereby dilute plasma K+, and when [K+] returns to normal ADH secretion stops (negative feedback).",
        "D": "Pancreatic β‑cells sense the increased plasma [K+] (receptor) and secrete insulin as the control signal; insulin binds intracellular nuclear receptors in skeletal muscle (effector cells) to up‑regulate Na+/K+‑ATPase gene expression, driving K+ into muscle cells and restoring plasma [K+]; the lowered [K+] then stops insulin release (negative feedback)."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a clinical hyperkalemia vignette to require identification of receptor (zona glomerulosa), control center (adrenal cortex), effector (renal distal tubule), and cellular mechanism (aldosterone acting via nuclear receptor to change gene expression), contrasting with plausible but incorrect alternative pathways.",
      "concepts_tested": [
        "three-component regulatory loop (receptor, control center, effector)",
        "negative feedback and set-point maintenance",
        "multilevel integration (hormonal control via nuclear receptors and gene expression)"
      ],
      "source_article": "Homeostasis",
      "x": 1.8998578786849976,
      "y": 1.116573691368103,
      "level": 2,
      "original_question_hash": "9c5ffac6"
    },
    {
      "question": "A 10-year-old child from a town that recently began using a household detergent containing nanosized surfactants develops chronic atopic dermatitis and recurrent nasal infections. Skin biopsy shows disrupted tight junctions in the epidermis, decreased microbial diversity with Staphylococcus overgrowth, elevated epithelial cytokines IL-33 and TSLP, and increased deposition of complement component C3b on mucosal surfaces. Which sequence of events best explains how epithelial barrier dysfunction and innate immune mechanisms link to the observed allergic phenotype and recurrent infections?",
      "options": {
        "A": "Detergent-induced epithelial barrier damage increases permeability and permits microbial translocation and antigen penetration → dysbiosis with Staphylococcus overgrowth amplifies PAMP signalling to resident PRR-bearing cells → epithelial cells and resident innate cells release IL-33/TSLP and other cytokines while the complement cascade deposits C3b to opsonize microbes, recruiting neutrophils and macrophages → dendritic cells phagocytose antigens and present them to naïve T cells, promoting a type‑2 adaptive response (allergy) while innate clearance is overwhelmed, permitting recurrent infection.",
        "B": "Complement activation (C3b deposition) is the initiating event caused directly by detergent molecules; complement both opsonizes microbes and independently instructs naïve T cells to differentiate into type‑2 effectors, so adaptive allergy develops without need for antigen presentation by dendritic cells.",
        "C": "Adaptive immune activation occurs first because detergent constituents act as haptens that directly stimulate naïve T cells in lymph nodes; those T cells then cause epithelial barrier disruption, secondary dysbiosis, and recruitment of innate cells and complement as a downstream consequence.",
        "D": "Epithelial barrier damage reduces innate immune signalling, decreasing cytokine and complement production; the resultant lack of innate recruitment prevents antigen presentation and therefore the adaptive system cannot generate allergic responses, explaining recurrent infections but not the dermatitis."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a clinical vignette linking detergent-induced epithelial barrier disruption, dysbiosis, innate cytokine/complement responses, recruitment of phagocytes, and antigen presentation by dendritic cells that drive a type‑2 adaptive response—testing causal sequencing from the epithelial barrier hypothesis and innate-to-adaptive linkage.",
      "concepts_tested": [
        "Epithelial barrier function and the epithelial barrier hypothesis (dysfunction, dysbiosis, antigen translocation)",
        "Innate immune recruitment and clearance mechanisms (cytokines, complement, phagocyte recruitment)",
        "Antigen presentation by innate phagocytes (dendritic cells) as the bridge to adaptive immunity and type‑2 polarization"
      ],
      "source_article": "Innate immune system",
      "x": 2.0826656818389893,
      "y": 1.1580983400344849,
      "level": 2,
      "original_question_hash": "9a143167"
    },
    {
      "question": "You are investigating why an isogenic cancer cell line exhibits two stable phenotypic states (drug-sensitive vs drug-tolerant) despite identical genomes. Single-cell RNA-seq reveals two transcriptional clusters; single-cell proteomics and metabolomics show correlated differences in key signaling proteins and metabolite levels. You hypothesize that bistability is an emergent property of a signaling–metabolic network with positive feedback. Which of the following workflows best exemplifies a systems biology approach that tests this hypothesis, integrates multi-omics data with mathematical modeling, and follows the iterative model–experiment cycle?",
      "options": {
        "A": "Integrate the single-cell transcriptomic, proteomic, and metabolomic datasets; construct a mechanistic ODE model for the core network (states x(t), y(t) with feedback terms and kinetic parameters $k_i$); estimate parameters by fitting the model to the quantitative data; perform bifurcation analysis to predict perturbations that shift the system between stable states; carry out targeted perturbation experiments (e.g., kinase inhibitor, metabolite supplementation, siRNA) and single-cell assays to validate predictions; use the new quantitative measurements to re-fit and refine the model and repeat.",
        "B": "Use the multi-omics data to compute pairwise correlations and identify a single gene most strongly associated with tolerance; perform a gene knockout and measure whether the phenotype changes. If it does, conclude that gene is the cause of the tolerant state without building a mathematical model.",
        "C": "Train a black-box machine-learning classifier on the multi-omics profiles to predict cell state (sensitive vs tolerant). Use the trained classifier to classify new samples and rank features by importance; select top features as potential drivers but do not perform mechanistic modeling or targeted perturbations.",
        "D": "Construct a literature-derived Boolean network of the signaling pathway and simulate asynchronous state transitions to see if two attractors exist. Do not collect new quantitative data nor perform experiments to validate the predicted attractors; publish the Boolean model as the explanation for bistability."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete cancer cell-line scenario requiring integration of multi-omics, mechanistic ODE modeling with parameter estimation and bifurcation analysis, and an explicit iterative perturbation–measurement–refinement cycle; distractors violate one or more core systems biology principles.",
      "concepts_tested": [
        "Emergence and dynamic interactions (bistability arising from network feedback)",
        "Data integration with mathematical modeling (multi-omics + ODEs and parameter fitting)",
        "Iterative modeling–experiment cycle (prediction, perturbation, validation, model refinement)"
      ],
      "source_article": "Systems biology",
      "x": 1.5423415899276733,
      "y": 1.0864123106002808,
      "level": 2,
      "original_question_hash": "66e412b0"
    },
    {
      "question": "Three research vignettes are described below. Which assignment correctly identifies (i) whether vignette 1 is a synchronically or diachronically oriented study, (ii) whether vignette 2 exemplifies the theory–practice interface and interdisciplinarity described in the article, and (iii) whether vignette 3 is aimed at uncovering universal properties and building a general theoretical framework?\n\nVignette 1: A team compares corpora of Old English, Middle English, and Modern English to reconstruct historical sound and morphological changes.\n\nVignette 2: Psycholinguists, biolinguists, and language educators collaborate: psycholinguistic experiments on child sentence processing inform a new reading curriculum, while neurogenetic studies probe brain mechanisms underlying syntactic acquisition.\n\nVignette 3: Linguists conduct a cross-linguistic comparison of syntactic dependency relations in 40 typologically diverse languages to propose a minimal set of syntactic primitives.\n\nWhich option correctly labels the vignettes?",
      "options": {
        "A": "1 = Diachronic; 2 = Theory–practice interface (applied education) and interdisciplinarity (psycholinguistics + biolinguistics); 3 = Investigation of universals / development of a general theoretical framework.",
        "B": "1 = Synchronic; 2 = Diachronic historical study; 3 = Applied curriculum design aimed at literacy.",
        "C": "1 = Diachronic; 2 = Synchronic descriptive fieldwork; 3 = Narrow typological description with no claim about universals.",
        "D": "1 = Synchronic; 2 = Example of theory informing practice but not interdisciplinary; 3 = Language documentation rather than universal-theory building."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed three concrete research vignettes mapping to synchrony/diachrony, theory–practice + interdisciplinarity, and universals/general frameworks; provided four plausible labelings with one correct mapping.",
      "concepts_tested": [
        "Synchrony vs diachrony in linguistic research",
        "Theory–practice interface and interdisciplinarity (applied linguistics, psycholinguistics, biolinguistics)",
        "Search for universal properties and construction of general theoretical frameworks (generative/typological approaches)"
      ],
      "source_article": "Linguistics",
      "x": 1.249065637588501,
      "y": 1.0845246315002441,
      "level": 2,
      "original_question_hash": "cb99f8c0"
    },
    {
      "question": "Company Orion, headquartered outside Region R, holds a 55% share of the cloud‑storage market inside Region R. Orion (a) bundles its cloud storage with a popular productivity suite sold globaly (a tying practice), (b) has long‑term exclusive supply contracts with most resellers in Region R, and (c) has notified a proposed acquisition of local rival Nimbus. Which of the following accurately describes the competition‑law tools and enforcement mechanisms Region R and other affected jurisdictions are most likely to deploy to address Orion's conduct?",
      "options": {
        "A": "Region R's authority can treat the tying and exclusive contracts as abusive conduct by a dominant firm and may prohibit or require remedies (e.g., compulsory licensing of the bundled product or divestiture of the acquired business) as a condition for merger approval; enforcement can be public (competition authority investigation) and private (damages claims), and foreign authorities may assert jurisdiction over Orion's extraterritorial conduct under an effects doctrine and coordinate enforcement through international networks (while WTO does not itself act as a global competition enforcer).",
        "B": "Region R is limited to imposing criminal sanctions only for abuse by a dominant firm; private civil damages claims are unavailable, and cross‑border issues must be resolved exclusively by the WTO dispute settlement body which can directly order divestitures across jurisdictions.",
        "C": "Competition law in Region R applies only to cartels and explicit price‑fixing agreements; tying, exclusive supply contracts and mergers are outside competition law, so authorities cannot block the Orion–Nimbus merger or impose behavioral or structural remedies.",
        "D": "Region R may investigate Orion's conduct but can only apply remedies within its territorial borders (e.g., local fines); it cannot require divestiture of a global business unit nor compel licensing, and foreign authorities have no basis to act unless Orion is domiciled in their territory."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete cross‑border scenario combining cartel/abuse/merger elements; options contrast correct triad of tools, remedies (licensing/divestiture), public/private enforcement, and extraterritorial application via effects doctrine versus common misconceptions (WTO enforcement, cartels‑only, territorial limits).",
      "concepts_tested": [
        "Triad of competition‑law instruments: prohibiting anti‑competitive agreements, banning abusive dominance, supervising mergers",
        "Enforcement and remedies: public and private enforcement; remedies such as divestiture and compulsory licensing",
        "Cross‑border jurisdiction: effects doctrine, extraterritorial jurisdiction, and international enforcement coordination"
      ],
      "source_article": "Competition law",
      "x": 1.2906686067581177,
      "y": 0.8858802914619446,
      "level": 2,
      "original_question_hash": "766f9a56"
    },
    {
      "question": "Lavelle Soap, a mid-size personal-care firm, undertakes a 12-month integrated branding program that included a new logo and packaging, a clarified brand personality in advertising, coordinated in-store scent touchpoints, and active social-media engagement. After 12 months the firm reports: aided brand recognition up by $+25$ percentage points, unaided brand recall up $+15$ percentage points, repeat-purchase rate up $+18$ percentage points, the firm can charge an average price premium increasing from $\\$0.50$ to $\\$1.20$ on annual volume of $3{,}000{,}000$ units, and market capitalization rose from $\\$120\\text{M}$ to $\\$138\\text{M}$. Which interpretation best aligns with the concepts of branding as an integrated toolkit, brand equity as a measurable value derived from familiarity and loyalty, and branding as an ongoing strategic management process?",
      "options": {
        "A": "These results indicate the integrated toolkit (identity, personality, design, communication) improved consumer perception and recognition, which elevated loyalty (higher repeat purchases) and allowed a sustainable price premium; the computed incremental annual brand-driven revenue and the increase in market capitalization together demonstrate measurable brand equity translating into shareholder value — consistent with strategic brand management.",
        "B": "The improvements in recall and recognition show only short-term advertising effects; because market capitalization rose by only $\\$18\\text{M}$ the branding program failed to create meaningful brand equity or shareholder value — brand identity changes alone cannot explain pricing power or loyalty.",
        "C": "The ability to charge a higher price while maintaining volume proves product improvements, not branding; since the campaign changed packaging and scent but not product formulation, the observed sales and market-cap change are unrelated to brand management and therefore do not reflect brand equity.",
        "D": "Higher aided recognition and brand recall indicate successful communication, but increases in repeat purchases and market capitalization are likely caused by distribution expansion; therefore brand equity did not increase because only recognition rose while loyalty and price premium cannot be attributed to branding."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete corporate scenario with integrated branding actions and numerical outcomes; asked students to identify the interpretation that ties identity/personality/communication to measurable brand equity and shareholder value, distinguishing branding effects from alternative explanations.",
      "concepts_tested": [
        "Branding as an integrated toolkit (identity, personality, design, communication) shaping perception and differentiation",
        "Brand equity as measurable value arising from familiarity and loyalty and its link to shareholder value",
        "Branding as a strategic, ongoing management effort to create lasting impressions and financial returns"
      ],
      "source_article": "Brand",
      "x": 1.3389726877212524,
      "y": 0.9739302396774292,
      "level": 2,
      "original_question_hash": "1c1440fa"
    },
    {
      "question": "Riverton, a mid-sized city, has documented a 35% rise in emergency department visits for asthma over the past two years. Surveillance maps show geographic clustering of cases in low-income neighborhoods downwind of an industrial zone; concurrent indicators show elevated ambient PM2.5, high household crowding, increased school absenteeism, and rising reports of psychological stress among caregivers. Which of the following public health strategies best aligns with (a) recognition of physical, psychological and social determinants of population health, (b) the use of surveillance and health indicators to guide and evaluate interventions, and (c) an interdisciplinary, system-level response integrated with the healthcare system to reduce disparities?",
      "options": {
        "A": "Maintain targeted surveillance of ED visits, PM2.5, absenteeism and caregiver stress; form a multidisciplinary task force (environmental health, housing, primary care, schools, social services, and community representatives); implement interventions including industrial emission controls and indoor ventilation improvements, subsidized home repairs to reduce crowding, school-based asthma management and caregiver stress-reduction programs, and streamlined access to controllers via primary-care clinics; and monitor those indicators to evaluate impact.",
        "B": "Scale up clinical services by offering free inhalers and expanded hospital asthma clinics in the affected neighborhoods, with outreach encouraging symptomatic residents to seek care; do not alter environmental or housing conditions, but measure only clinic uptake as the program metric.",
        "C": "Launch a citywide media campaign advising individuals to avoid outdoor activity on poor-air days, to practice better hygiene, and to ‘manage stress’ on their own; continue existing surveillance but do not change interagency coordination or structural policies.",
        "D": "Immediately shut down the industrial facilities in the windward zone and levy heavy fines on owners to deter future emissions, without coordinating with health providers, housing authorities, schools, or community organizations; treat the action as the primary solution and discontinue routine surveillance when case counts begin to fall."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete urban outbreak scenario requiring selection of a multi-level, surveillance-driven, interdisciplinary public health response that integrates environmental, social, and healthcare actions; distractors portray incomplete or single-sector approaches.",
      "concepts_tested": [
        "Multi-level determinants of population health (physical, psychological, social)",
        "Use of surveillance and health indicators to guide and evaluate interventions",
        "Interdisciplinary, system-level public health collaboration and integration with healthcare to address disparities"
      ],
      "source_article": "Public health",
      "x": 1.2719862461090088,
      "y": 0.9077054262161255,
      "level": 2,
      "original_question_hash": "71dbdd27"
    },
    {
      "question": "The City of Riverton sets a goal to reduce new cases of type 2 diabetes by 20% over five years. The public health department proposes four intervention packages. Which package best exemplifies a WHO-style health promotion strategy that enables people to increase control over their health by addressing social determinants and embedding health across public policy (Health in All Policies), rather than merely providing individual-level health education?",
      "options": {
        "A": "Distribute culturally tailored pamphlets and run evening community workshops on healthy eating and physical activity; evaluate changes in health literacy and self-reported behavior after six months.",
        "B": "Implement a municipal sugar-sweetened beverage tax, revise zoning to attract full-service supermarkets into low-access neighborhoods, create protected bike lanes and subsidized public transit passes, and introduce paid parental leave and breastfeeding-friendly public-space regulations.",
        "C": "Fund free community clinics that offer regular blood-glucose screening, immediate pharmaceutical treatment for people diagnosed with hyperglycemia, and clinical follow-up to optimize medication adherence.",
        "D": "Launch a workplace program offering on-site fitness classes, smoking-cessation counseling, and financial incentives for employees who meet biometric targets, with employer-led promotional campaigns."
      },
      "correct_answer": "B",
      "generation_notes": "Designed a concrete municipal scenario comparing interventions: (A) health education, (C) clinical/preventive care, (D) workplace-focused program, and (B) cross-sectoral policy addressing social determinants and HiAP—B is correct.",
      "concepts_tested": [
        "Health promotion as enabling control over health and addressing social determinants",
        "Health in All Policies (HiAP) and cross-sectoral public policy mechanisms",
        "Distinction between health promotion (environmental/social interventions) and health education (individual-level learning)"
      ],
      "source_article": "Health promotion",
      "x": 1.2777637243270874,
      "y": 0.9478376507759094,
      "level": 2,
      "original_question_hash": "32d0b27d"
    },
    {
      "question": "A developmental biologist designs a comparative experiment to test mechanisms of regional specification, morphogenesis, and differences between animal and plant development. She implants a tiny bead that releases a diffusible inducer at a single site in (i) a gastrulating vertebrate embryo and (ii) the shoot apical meristem of a young Arabidopsis seedling. She also performs two separate perturbations: (P1) pharmacological inhibition of actin–myosin–dependent cell motility in the vertebrate embryo, and (P2) chemical stiffening of plant cell walls to block cell elongation and differential growth. Which set of experimental observations best matches the expectations described in the article (assume the inducer forms a decaying concentration gradient, e.g. $C(x)=C_0e^{-kx}$)?",
      "options": {
        "A": "Vertebrate embryo: the bead creates concentric zones of different developmental transcription factors (regional specification) because the inducer gradient causes distinct gene activation thresholds; after P1 those transcriptional zones still form but the cells fail to rearrange into correct germ-layer positions (morphogenetic movements are blocked), so tissue architecture is abnormal though cell-type markers are present. Plant meristem: the bead induces local changes in gene expression appropriate to organ identity (using plant-specific regulators), but because plant cells are immotile morphogenesis occurs by local differential growth; after P2 the induced pattern of gene expression may still appear but the expected organ outgrowth, bending or shape change is abolished because cell elongation/differential growth is prevented.",
        "B": "Vertebrate embryo: the bead fails to produce distinct transcription factor zones because regional specification requires cytoplasmic determinants only (not diffusible inducers); after P1 neither gene expression nor germ-layer rearrangement occurs. Plant meristem: the bead has no effect because plants lack morphogen gradients and always rely solely on positional information from meristem geometry, so P2 produces no change in organogenesis.",
        "C": "Vertebrate embryo: the bead produces a gradient but inhibition of cell motility (P1) prevents both the formation of transcription factor zones and any subsequent germ-layer formation, because transcriptional patterning depends on cell movements to expose cells to the inducer; Plant meristem: because plant cells cannot move, the bead must re-specify whole regions by causing long-range cell migration of meristematic cells, so P2 will alter which cells migrate and thus change organ position.",
        "D": "Vertebrate embryo: the bead creates transcription factor zones and, because growth rather than movement generates germ layers, P1 (blocking motility) has little effect on final tissue arrangement—morphogenesis is driven mainly by differential proliferation (allometry). Plant meristem: the bead induces identical signaling and gene families as in animals, and P2 only partially reduces organ size but not shape because plant morphogenesis relies primarily on cell rearrangements rather than cell elongation."
      },
      "correct_answer": "A",
      "generation_notes": "Created a comparative experimental scenario that requires applying: (1) regional specification via morphogen gradients and transcription factor thresholds, (2) morphogenesis via cell movements versus differential growth/allometry, and (3) differences between animal and plant mechanisms (immotile plant cells, different regulators). Option A matches article-based expectations.",
      "concepts_tested": [
        "Regional specification via cytoplasmic determinants, inductive signals and transcription factor combinations",
        "Morphogenesis and tissue growth driven by cell movements, differential growth (allometry), and extracellular constraints; comparative differences between animal and plant development"
      ],
      "source_article": "Developmental biology",
      "x": 2.025913953781128,
      "y": 1.1520973443984985,
      "level": 2,
      "original_question_hash": "143ecee8"
    },
    {
      "question": "In a set of cross‑phyletic experiments, researchers used Drosophila eyeless (pax-6) mutants that lack compound eyes. They made transgenic flies with (1) the mouse Pax6 coding sequence placed under the control of the native Drosophila eyeless enhancer, which restored normal eye development, and (2) the same mouse Pax6 coding sequence fused to a Drosophila leg‑specific enhancer, which produced ectopic eye structures on the fly legs. Molecular analysis of the pax-6 loci in diverse animals revealed highly conserved protein‑coding sequences but large, modular cis‑regulatory regions with many tissue‑specific enhancers. Which of the following interpretations is best supported by these results?",
      "options": {
        "A": "These results demonstrate deep homology of pax-6 protein function across phyla, indicate that changes in cis-regulatory control (when/where pax-6 is expressed) can produce novel morphologies, and explain why pax-6 coding sequence is conserved due to pleiotropy across multiple developmental contexts.",
        "B": "The experiments show that eyes in flies and mice are analogous rather than homologous, because the presence of ectopic eyes on legs indicates eyes can form anywhere regardless of ancestral relationships; therefore eye evolution is solely due to convergent acquisition of new structural genes.",
        "C": "The data imply that morphological differences between species are mainly due to divergence in structural protein sequences, since only the mouse Pax6 coding sequence (not regulatory DNA) was capable of restoring eye formation in the fly mutant.",
        "D": "Findings indicate that toolkit genes like pax-6 are evolutionarily labile in their coding regions and that pleiotropy is unimportant; novel forms arise primarily through repeated mutations in the pax-6 protein that create new biochemical functions in different tissues."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete experimental scenario (cross‑species coding swap and enhancer swap) that mirrors classic pax-6/evo-devo findings; options contrast correct inference (deep homology, regulatory evolution, pleiotropy-driven conservation) with plausible but incorrect interpretations.",
      "concepts_tested": [
        "Deep homology and conserved toolkit gene function (pax-6)",
        "Regulatory (cis‑regulatory) changes drive morphological evolution",
        "Pleiotropy and conservation of developmental toolkit genes"
      ],
      "source_article": "Evolutionary developmental biology",
      "x": 1.8542999029159546,
      "y": 1.1143890619277954,
      "level": 2,
      "original_question_hash": "23698b16"
    },
    {
      "question": "A 1D embryonic tissue strip of length $L=1000\\ \\mu$m is exposed to a morphogen with concentration profile $C(x)=C_0 e^{-x/\\lambda}$, where $C_0=1$ and $\\lambda=200\\ \\mu$m. Two concentration thresholds $T_2=0.5$ and $T_1=0.2$ implement a French-flag readout: \"blue\" for $C>T_2$, \"white\" for $T_1<C\\le T_2$, and \"red\" for $C\\le T_1$. Superimposed on this field, a reaction–diffusion system produces a stable activator pattern with wavelength $\\lambda_{RD}=150\\ \\mu$m. Simultaneously, tissue growth generates compressive strain $\\varepsilon=0.02$; mechanical buckling occurs when $\\varepsilon>\\varepsilon_c=0.015$ and, when it does, produces folds with characteristic spacing $\\lambda_{mech}=300\\ \\mu$m. Based on these parameters, which of the following best describes the resulting spatial organization of cell fates, chemical peaks, and mechanical folds along the strip?",
      "options": {
        "A": "There are three morphogen zones: blue from $x=0$ to $x\\approx138.6\\ \\mu$m, white from $x\\approx138.6$ to $x\\approx321.9\\ \\mu$m, and red from $x\\approx321.9$ to $1000\\ \\mu$m. The RD wavelength $\\lambda_{RD}=150\\ \\mu$m yields approximately 1 peak in blue, 1 peak in white, and 4 peaks in red (about 6 activator peaks total). Because $\\varepsilon>\\varepsilon_c$, mechanical folds form with spacing $\\lambda_{mech}=300\\ \\mu$m (about 3 folds). Since $\\lambda_{mech}=2\\lambda_{RD}$, each mechanical fold will tend to coincide with every second RD peak, reinforcing a larger-scale periodicity, particularly across the broad red zone.",
        "B": "There are two morphogen zones because the decay length is long: blue from $x=0$ to $x\\approx321.9\\ \\mu$m and red from $x\\approx321.9$ to $1000\\ \\mu$m, so white is negligible. The RD pattern (150\\ \\mu$m) produces about 7 peaks across the strip, and mechanical buckling does not occur because $\\varepsilon<\\varepsilon_c$, so no folds form; the RD pattern alone determines the finescale segmentation.",
        "C": "There are three morphogen zones as in A, but the RD wavelength ($150\\ \\mu$m) is much larger than the mechanical spacing, so RD peaks are sparser than folds. Approximately 10 RD peaks occur across the strip while mechanical buckling (since $\\varepsilon>\\varepsilon_c$) produces about 3 folds; folds interleave between RD peaks and therefore reduce RD amplitude.",
        "D": "There are three morphogen zones as in A. The RD system yields about 6 activator peaks, and mechanical buckling does occur producing $\\approx3$ folds. However, because $\\lambda_{mech}$ and $\\lambda_{RD}$ are incommensurate, folds align exactly with morphogen boundaries, so mechanical folds strictly coincide with the blue/white and white/red transitions and do not interact with RD peaks."
      },
      "correct_answer": "A",
      "generation_notes": "Calculated morphogen threshold positions from $x=-\\lambda\\ln T$, divided tissue lengths by $\\lambda_{RD}$ and $\\lambda_{mech}$ to estimate counts; assessed alignment by comparing wavelengths ($\\lambda_{mech}=2\\lambda_{RD}$) and buckling condition $\\varepsilon>\\varepsilon_c$.",
      "concepts_tested": [
        "Morphogen gradients and French flag positional information",
        "Reaction–diffusion (Turing) pattern wavelength and peak counts",
        "Mechanical/elastic instability (buckling) thresholds and interaction with biochemical patterns"
      ],
      "source_article": "Pattern formation",
      "x": 1.668015718460083,
      "y": 1.1129075288772583,
      "level": 2,
      "original_question_hash": "47c35b05"
    },
    {
      "question": "A mid-sized coastal city, Port Nueva, faces chronic marine pollution, rapidly growing informal settlements, and strained landfill capacity. The city council must adopt an environmental governance strategy for a 10-year redevelopment plan. Which of the following proposed strategies best exemplifies the core principles of environmental governance—i.e., multi-level engagement of state, market, and civil society across local, national and international scales; embedding environmental considerations into all levels of decision-making and economic/political life; and promoting a transition from open-loop to closed-loop (cradle-to-cradle) systems?",
      "options": {
        "A": "The city unilaterally adopts strict bans on single-use plastics and imposes heavy fines on polluters, funded entirely from municipal taxes. Enforcement is centralized within the city council; private firms may comply or be penalized, and NGOs are expected to protest if needed. The plan focuses on enforcement and punitive measures without restructuring waste streams or coordinating with higher-level governments.",
        "B": "The city forms a multi-tiered programme: a municipal–national–regional taskforce secures national funding and aligns with an international coastal pollution treaty; municipal zoning and procurement policies embed circular-economy criteria; public–private partnerships finance local material-recovery facilities; community cooperatives run neighbourhood repair and composting hubs; and performance indicators require waste-to-resource rates to rise from 20% to 70% within 10 years.",
        "C": "Port Nueva privatizes waste management through a long-term concession to a single multinational company and relies on market pricing (user fees) to reduce waste. The concession contract focuses on cost-minimization and landfill diversion targets set by the private operator, with limited public oversight and no formal role for civil society or higher-level governments.",
        "D": "The city designates large coastal areas as strictly protected ecological reserves and relocates industries inland under emergency authority. Economic activity is curtailed in rezoned districts, and environmental monitoring is funded by a special municipal levy. There is minimal consultation with businesses or local communities, and no explicit measures to redesign production or promote material circularity."
      },
      "correct_answer": "B",
      "generation_notes": "Created a concrete urban redevelopment scenario and four plausible governance strategies. Option B includes multi-level actors, embedding environment into policy/economy, and cradle-to-cradle closed-loop measures; other options omit one or more core principles.",
      "concepts_tested": [
        "Multi-level governance (state, market, civil society across scales)",
        "Embedding environment into decision-making and economic/political life",
        "Transition from open-loop to closed-loop (cradle-to-cradle) systems / circular economy"
      ],
      "source_article": "Environmental governance",
      "x": 1.3825302124023438,
      "y": 0.8695408701896667,
      "level": 2,
      "original_question_hash": "05096618"
    },
    {
      "question": "Aurania, a dualist coastal state, has signed (but not ratified) the Coastal Protection Treaty (CPT), which requires parties to prevent transboundary marine pollution and creates a regional supranational tribunal with competence to adjudicate compliance. An oil spill originating in Aurania contaminates the waters of neighbouring Borealia. Borealia invokes (1) the CPT before the regional tribunal, (2) customary international law obligations on transboundary harm before the International Court of Justice, and (3) diplomatic sanctions through the UN General Assembly; the UN Security Council considers binding economic sanctions. Which statement best describes the applicable law and likely enforceability against Aurania?",
      "options": {
        "A": "Because Aurania has only signed but not ratified the CPT and follows a dualist system, the CPT does not automatically create enforceable domestic rights or obligations in Aurania until implementing legislation is adopted; however, Aurania remains bound by customary international law on preventing transboundary harm (which requires state practice plus opinio juris) unless it can show it was a persistent objector. The regional tribunal can exercise jurisdiction only to the extent Aurania has consented to cede authority (supranational competence); enforcement will rely on non‑centralized mechanisms — diplomacy, reputational pressure, sanctions or Security Council measures — rather than an internal global coercive power, and any treaty provision conflicting with a jus cogens norm would be invalid.",
        "B": "By signing the CPT Aurania is immediately and directly bound domestically because under international law signature creates full domestic effect; the regional supranational tribunal’s decision will automatically displace Aurania’s municipal law regardless of whether Aurania enacted implementing legislation, and customary international law has no independent effect unless embodied in a treaty.",
        "C": "Aurania is not bound by the CPT or by similar customary rules because States must explicitly consent to every international obligation; Borealia’s only remedy is unilateral use of force or domestic litigation in Borealia — multilateral measures such as Security Council sanctions are impermissible without Aurania’s consent.",
        "D": "The CPT binds Aurania internationally upon signature so the regional tribunal can order immediate remedies against Aurania’s private companies and municipal authorities; customary international law is relevant only as guidance and cannot be enforced by the ICJ or regional tribunals unless a state has ratified a treaty adopting it."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a scenario combining treaty consent/dualism, customary law (state practice + opinio juris), supranational tribunal competence, and enforcement mechanisms (diplomacy, sanctions, UNSC) to test students' integrated understanding.",
      "concepts_tested": [
        "Consent-based authority and enforcement mechanisms in international law",
        "Sources and binding nature of international law (treaties, custom, general principles, jus cogens)",
        "Relationship between national and international law (dualism, treaty implementation, supranational competence)"
      ],
      "source_article": "International law",
      "x": 1.2309828996658325,
      "y": 0.8286166191101074,
      "level": 2,
      "original_question_hash": "e1c42486"
    },
    {
      "question": "Researchers propose the sediments of Lake Novus as the Global Boundary Stratotype Section and Point (GSSP) to define the base of a formally ratified Anthropocene. The varved core shows: a pronounced $^{239}$Pu/$^{240}$Pu peak centered on 1952, a near‑ubiquitous increase in microplastic fragments from 1950 onward, a post‑1950 doubling of fixed reactive nitrogen deposition, elevated black carbon concentrations after 1950, and a distinct $^{14}$C spike between 1954–1964. The Anthropocene Working Group (AWG) endorses the mid‑20th century (c.1950) as the boundary because these signals coincide with the Great Acceleration. When the proposal reaches the Subcommission on Quaternary Stratigraphy (SQS) and then the International Commission on Stratigraphy (ICS), it is rejected. Which of the following explanations best reconciles why the empirical strength of the mid‑20th century signals would make the Lake Novus case scientifically compelling but still lead to formal rejection by the stratigraphic authorities?",
      "options": {
        "A": "Although the lake's mid‑20th century markers are clear evidence of the Great Acceleration, formal adoption of an epoch requires a demonstrably robust, long‑term stratigraphic record and a globally synchronous, unambiguous boundary. The proposed GSSP is judged too shallow and too recent (prone to recency bias) and the onset of human impacts is diachronous, so the SQS/ICS concluded it cannot be constrained as a formal geologic epoch despite the strong signals.",
        "B": "The proposal was rejected because plutonium isotopes and microplastics are chemically unstable and will not be preserved in any sedimentary record beyond a few centuries, so they cannot serve as stratigraphic markers for geologic time.",
        "C": "The SQS/ICS rejected the Lake Novus GSSP because formal epochs must begin with pre‑industrial events such as the Industrial Revolution or Neolithic land clearance; mid‑20th century signals therefore cannot legally define a new epoch regardless of their stratigraphic clarity.",
        "D": "The committee concluded that the Lake Novus record lacked any geochemical evidence (e.g., $\\delta^{13}$C shifts, radionuclide peaks, black carbon) linking human activity to planetary‑scale change, so there was insufficient cause‑and‑effect evidence to support the Anthropocene concept."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a realistic GSSP scenario with mid‑20th century Great Acceleration markers and tested student understanding of why strong empirical signals can nonetheless fail formal epoch criteria (shallow/ recent strata, diachronous processes, recency bias), plus alternatives reflecting common misconceptions.",
      "concepts_tested": [
        "Anthropocene as framework linking human activity to planetary‑scale geophysical/biogeochemical change",
        "Great Acceleration as mid‑20th century mechanism/signature (radionuclides, plastics, nitrogen, black carbon)",
        "Scientific process and criteria for defining a geological epoch (AWG/SQS/ICS, GSSP, diachronous onset, recency bias, stratigraphic robustness)"
      ],
      "source_article": "Anthropocene",
      "x": 0.8849407434463501,
      "y": 0.20028506219387054,
      "level": 2,
      "original_question_hash": "bf9a4e02"
    },
    {
      "question": "An ecologist surveys 30 islands distributed across three latitudinal bands (tropical, temperate, polar). For each island she records species richness S and island area A and fits the species–area relationship $S=cA^{z}$. Her results are: (i) $z\\approx0.25$ in all three bands while the intercepts follow $c_{tropics}>c_{temperate}>c_{polar}$, (ii) across all species she finds a robust positive correlation between mean local abundance and geographic range size, and (iii) within each band, community composition varies idiosyncratically among islands and is strongly influenced by local habitat heterogeneity. Which interpretation best exemplifies macroecological reasoning about these results?",
      "options": {
        "A": "These findings reveal emergent statistical regularities: a near-constant exponent $z$ indicates a general species–area scaling, the higher $c$ in the tropics represents a latitudinal richness gradient, and the abundance–range correlation is a broad-scale pattern. Macroecology therefore emphasizes system-level explanations (e.g., large-scale variation in energy/productivity, speciation/extinction balance, dispersal constraints) for these regularities while treating local compositional idiosyncrasies as fine-scale variance.",
        "B": "Because local composition is highly idiosyncratic and driven by habitat heterogeneity, the primary conclusion is that only local-scale processes and species interactions matter; the large-scale patterns are artifacts and macroecology should be abandoned in favor of local community ecology.",
        "C": "The identical $z$ across latitudes proves that identical assembly rules operate at all scales and latitudes; therefore management and predictive models can apply the same area-based rules (e.g., reserve size thresholds) regardless of latitude or regional context.",
        "D": "The positive abundance–range correlation demonstrates a direct causal effect: increasing a species' local abundance will necessarily expand its geographic range, so species with conservation concern can be restored simply by boosting local population sizes."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed an island survey scenario using the power-law species–area relationship $S=cA^{z}$, a latitudinal intercept gradient, and an abundance–range correlation; options contrast macroecological, localist, overgeneralizing, and causal-misinterpretation readings to test top-down and emergent-pattern reasoning.",
      "concepts_tested": [
        "Emergent statistical order across spatial scales",
        "Top-down, system-level inference in macroecology",
        "Species–area relationship and abundance–range size relationship; latitudinal diversity gradient"
      ],
      "source_article": "Macroecology",
      "x": 1.6639890670776367,
      "y": 1.0356775522232056,
      "level": 2,
      "original_question_hash": "c28c5318"
    },
    {
      "question": "Consider a single homogeneous good market with N = 10 identical consumers and F = 5 identical price‑taking firms. Each consumer has income $I=20$ and utility that implies the Marshallian demand $x^*(P)=I/P$. Each firm's cost function is $C(q)=4q+q^2$, so its marginal cost is $MC(q)=4+2q$. Assuming consumers maximize utility subject to their budget constraint, firms maximize profit taking price $P$ as given, and all agents have full and relevant information, which of the following statements correctly describes the market equilibrium?",
      "options": {
        "A": "The market clears at a unique equilibrium price $P^*\\approx 11.17$ and quantity $Q^*\\approx 17.92$. Individual demand is $x^*=20/P$, individual firm supply is $q=(P-4)/2$, and $P^*$ solves $N\\cdot\\frac{20}{P}=F\\cdot\\frac{P-4}{2}$. At this $P^*$ price equals each firm's marginal cost and results from utility and profit maximization under complete information.",
        "B": "The market clears at $P^*\\approx 11.17$, but at this price firms earn zero economic profit because price equals average cost; consumers' demand does not respond to price because utility is ordinal, so aggregate demand is fixed.",
        "C": "No finite positive equilibrium exists because aggregate demand $Q_d=200/P$ (hyperbolic in $P$) cannot intersect the linear aggregate supply $Q_s=(5/2)(P-4)$ for any positive price; hence the supply–demand framework fails here.",
        "D": "Equilibrium occurs at the shutdown price $P=4$ since firms' marginal cost at zero output is $4$ and consumers buy until marginal utility equals $4$, giving $Q^*=50$, so the market clears at $P=4$."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete market with numerical consumer demand $x=20/P$ and firm supply from profit maximization using $MC=4+2q$, solved for $P^*$ from $N\\cdot20/P=F\\cdot(P-4)/2$, and provided distractors that misuse average cost, ignore price responsiveness, or misapply shutdown logic.",
      "concepts_tested": [
        "Utility maximization by individuals and profit maximization by firms as drivers of price and allocation",
        "Rational choice theory and full information assumptions guiding behavior",
        "Supply–demand intersection as market‑clearing mechanism"
      ],
      "source_article": "Neoclassical economics",
      "x": 1.268757700920105,
      "y": 0.9298917055130005,
      "level": 2,
      "original_question_hash": "002b59ae"
    },
    {
      "question": "A mid-sized post-industrial city that suffered recent corruption scandals commissions a new central courthouse. The client brief emphasizes (1) long-term resilience and low maintenance due to constrained budgets, (2) efficient circulation and security for multiple courtroom types, public waiting areas, and legal aid clinics, and (3) an architectural expression that restores public trust by signalling transparency, civic service, and respect for local history. Four competing schematic designs are proposed. Which one best synthesizes Vitruvius’ triad (firmitas, utilitas, venustas), the evolved meaning of “form follows function” (including psychological and cultural dimensions), and functions effectively as a cultural and social statement for this context?",
      "options": {
        "A": "A full-height glass-and-steel curtain-wall block with exposed structural bays, modular column grid, open-plan courtrooms separated by glass partitions, and rooftop photovoltaic canopies. Emphasizes visual openness and modern efficiency, minimal ornament, and clearly expressed structural frame.",
        "B": "A new stone-clad building in a classical idiom: a symmetrical façade with a grand portico, Corinthian columns, ceremonial stair, richly ornamented interiors, deep vaults, and axial approach aligned with the main civic axis. Security is managed by controlled access behind the monumental frontage.",
        "C": "An adaptive-reuse scheme converting a solid-brick 19th-century factory shell into the courthouse: retains load-bearing masonry and rhythm of fenestration for durability; inserts a new lightweight glazed atrium to bring daylight into public circulation, reconfigures interiors for efficient courtroom adjacencies and secure circulation, and places a public plaza that interprets industrial heritage motifs to signal civic continuity and transparency. Incorporates passive cooling and targeted insulation upgrades.",
        "D": "A monolithic, exposed-concrete structure with recessed slit windows, deep floor plates, cantilevered masses expressing ‘authority’, raw finishes, austere interiors with circulation corridors oriented for rigid security control; minimal glazing to reduce glare and maintenance."
      },
      "correct_answer": "C",
      "generation_notes": "Presented a realistic civic-design brief and four schematic proposals that each emphasize different historical and theoretical positions; selected the adaptive-reuse option as best integrating durability, utility, beauty, expanded form–function concerns (psychological trust, cultural resonance), and sustainability.",
      "concepts_tested": [
        "Vitruvian triad (firmitas, utilitas, venustas) application in design evaluation",
        "Evolution of 'form follows function' to include psychological and cultural factors",
        "Architecture as cultural symbol and social commentary in civic buildings"
      ],
      "source_article": "Architecture",
      "x": 0.586574912071228,
      "y": 0.979682207107544,
      "level": 2,
      "original_question_hash": "da511b9e"
    },
    {
      "question": "BlueSpring, a mid-sized beverage firm, plans to launch \"BlueSpring Pure\": a sustainable bottled-water offering targeted at urban millennials in three coastal regions. Their marketing research finds that (a) target consumers prioritize environmental credentials and long-term brand relationships, (b) 68% prefer subscription or online reordering for convenience, and (c) a 12% price premium would be acceptable. Additionally, new municipal regulations will require 30% recycled content in packaging within two years. Which one of the following integrated marketing plans best embodies (i) use of the marketing mix as a decision framework driven by environment, research and target-market characteristics, (ii) relationship-building and customer-value creation to capture value, and (iii) a stakeholder- and society-oriented marketing purpose?",
      "options": {
        "A": "Design the product with recyclable bottles meeting upcoming 30% recycled-content rules; price at a 12% premium with a subscription discount for repeat buyers; distribute via a direct-to-consumer subscription platform plus selective eco-focused retailers for convenience; promote through two-way social media engagement, community beach-clean partnership campaigns, and co-creation contests that invite customer input on new flavors; and publish sustainability impact metrics to stakeholders.",
        "B": "Use the cheapest plastic to maximize margins and deploy aggressive introductory coupons to drive trial; price below competitors to accelerate market share; distribute widely through all supermarket chains and vending channels; run mass-market TV and billboard ads emphasizing low price; and postpone any sustainability reporting until demand is secured.",
        "C": "Develop a premium product with ornate packaging (no emphasis on recycled content), set a high prestige price well above the 12% research threshold to signal quality, focus distribution exclusively through upscale boutique grocery stores, and run a one-way national advertising campaign (TV and print) highlighting luxury positioning without customer engagement or community partnerships.",
        "D": "Sell primarily via large institutional and B2B buyers (cafés, event venues) to achieve bulk volume; offer volume-based negotiated pricing and leasing options for dispensers; rely on salesforce-driven personal selling and trade promotions; and emphasize short-term revenue metrics to investors rather than communicating sustainability outcomes to consumers or communities."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete product launch scenario requiring students to evaluate integrated marketing-mix choices (product, price, place, promotion) informed by research and regulation, plus strategies for building customer relationships and addressing stakeholder/societal concerns; provided three distractors each violating one or more core concepts.",
      "concepts_tested": [
        "Marketing mix decision-making informed by environment, research, and target-market characteristics",
        "Relationship-building and customer-value creation (engagement, subscriptions, co-creation) and capturing value",
        "Stakeholder- and society-oriented marketing (sustainability, partnerships, reporting)"
      ],
      "source_article": "Marketing",
      "x": 1.3568601608276367,
      "y": 0.9903647303581238,
      "level": 2,
      "original_question_hash": "a117af02"
    },
    {
      "question": "Alpine Cycles is a corporation that operates two business units: Road Bikes (traditional, high-performance bicycles) and E-Bikes (electric-assisted commuting bikes). Corporate HQ must decide whether to expand into a new international e-bike market and whether certain functions (R&D, procurement) should be shared across units. The Road Bikes business unit must choose how to compete in its segment. Senior managers formulate a strategy: Road Bikes will pursue a premium differentiation strategy based on a unique value proposition—ultra-light carbon frames with bespoke fit—with the explicit trade-off that they will not compete on lowest price. HQ approves a corporate decision to increase shared R&D funding but to leave manufacturing decentralized at the unit level. Implementation actions include reallocating R&D budget, renegotiating supplier contracts for higher-grade carbon, retraining sales staff, setting KPIs (customer Net Promoter Score, unit cost, premium margin), and establishing monthly review meetings. After three quarters, NPS rises but unit costs exceed targets; managers then renegotiate longer-term supplier contracts and adjust promotional messaging while maintaining premium positioning. Which single statement best aligns with strategic management theory as described in the article?",
      "options": {
        "A": "This sequence exemplifies strategic management as a dynamic, resource-informed process with a clear feedback loop: formulation (premium differentiation and corporate choices about scope and shared R&D), implementation (resource reallocation, contracts, training, KPIs), monitoring (monthly reviews), and adaptation (supplier renegotiation and messaging). It also reflects Porter’s principles by establishing a unique market position, making the trade-off to avoid price competition, and creating internal fit among R&D, procurement, manufacturing, and marketing; moreover, the corporate-level choice about shared R&D and market scope is distinct from the business-level competitive choice, while operational management remains responsible for cost control within the strategy’s boundaries.",
        "B": "Because unit costs rose the strategy has failed and therefore this was not an example of strategic management; true strategy requires immediate cost reduction even at the expense of the chosen market position, so HQ should have centralized manufacturing to cut costs.",
        "C": "The correct interpretation is that corporate strategy and business strategy are the same in this case because HQ’s decision to fund shared R&D automatically dictates the Road Bikes unit’s competitive approach; additionally, pursuing differentiation while later renegotiating suppliers indicates the firm abandoned its original trade-offs.",
        "D": "This case shows that Alpine attempted to pursue both cost leadership and differentiation simultaneously (hybrid strategy), which is consistent with Porter’s recommendation to combine the two approaches to maximize market share; operational managers should therefore focus only on efficiency metrics and ignore brand positioning."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete two-unit corporate scenario requiring students to identify iterative strategy processes, apply Porter’s three principles (position, trade-offs, fit), and distinguish corporate vs business-level decisions and operational roles. Options include common misconceptions as distractors.",
      "concepts_tested": [
        "Strategic management process (formulation → implementation → monitoring → adaptation)",
        "Porter’s principles: unique position, trade-offs, and activity fit",
        "Distinction between corporate strategy (scope, resource allocation) and business strategy (how to compete), and role of operational management"
      ],
      "source_article": "Strategic management",
      "x": 1.3660175800323486,
      "y": 1.0047203302383423,
      "level": 2,
      "original_question_hash": "0a8b5fe8"
    },
    {
      "question": "A mid-sized coastal town (population 45,000) experiences increasingly frequent storm surges. The town council commissions a multi-year emergency management program. Which of the following program descriptions best demonstrates (1) the integrated four-phase framework (preparedness, response, mitigation, recovery), (2) a cyclical risk-management planning process (risk identification, evaluation/ranking, planning, training/testing), and (3) multi-actor, multi-level collaboration and joint training?",
      "options": {
        "A": "The town constructs a seawall (mitigation) and establishes an annual fund for post-storm repairs (recovery). The public works department writes technical specifications, executes construction, and manages the fund without formal involvement of other agencies or community groups.",
        "B": "The program begins with a hazard assessment that ranks flood risks by neighborhood, leads to engineered mitigation (elevating critical infrastructure and floodproofing homes), develops evacuation and sheltering plans, runs quarterly multi-agency drills with municipal, regional and NGO partners, and uses after-action reviews to revise building codes and public education—with shared roles across households, local government, regional emergency services and national agencies.",
        "C": "Local businesses are required to produce individual continuity plans and employee go-kits; the town distributes a public brochure on evacuation routes but does not conduct drills or coordinate with neighboring jurisdictions, relying instead on ad hoc mutual aid requests after events.",
        "D": "The town invests heavily in an emergency operations center and a rapid-response rescue team; during storms it coordinates incoming federal rescue assets, and after storms it solicits charitable donations to support recovery, but it does not perform pre-event risk ranking or regular preparedness training."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete coastal-flood scenario and created four plausible program descriptions; option B intentionally incorporates all three target concepts (four-phase framework, cyclical risk-management, multi-level collaboration) while other options omit one or more elements.",
      "concepts_tested": [
        "Four-phase emergency management framework (preparedness, response, mitigation, recovery)",
        "Cyclical risk management planning (risk identification, evaluation/ranking, planning, training/testing)",
        "Multi-actor and multi-level collaboration and joint training"
      ],
      "source_article": "Emergency management",
      "x": 1.4076255559921265,
      "y": 0.8026406764984131,
      "level": 2,
      "original_question_hash": "77caa274"
    },
    {
      "question": "A Religious Studies department hiring committee must evaluate four tenure-track research proposals studying healing rituals in different traditions. Proposal 1: an ethnographic study using participant observation to document the \"lived religion\" of Pentecostal healing services. Proposal 2: a confessional theological monograph that presumes the doctrinal truth of a particular church and aims to defend its sacramental theology. Proposal 3: a comparative historical analysis tracing cross-cultural antecedents of healing rites across Asia and Europe, using textual philology and archival sources. Proposal 4: an experimental psychology project testing cognitive mechanisms of conversion with controlled behavioral tasks. According to the methodological stance and intellectual character of Religious Studies described in the article, which hiring decision best aligns with the discipline’s norms?",
      "options": {
        "A": "Invite all four candidates, since Religious Studies is methodologically pluralistic and values empirical, historical, and cross-cultural approaches while permitting scholars who are believers so long as methodological assumptions are made explicit and the research can be treated independently of doctrinal commitments.",
        "B": "Reject Proposals 1 and 4 and hire only Proposals 2 and 3, because Religious Studies privileges theological and historical-philological methods over ethnographic and experimental work.",
        "C": "Hire only Proposal 2, because Religious Studies is best carried out by scholars who defend and explicate religious doctrines from inside the tradition (theologians).",
        "D": "Hire Proposals 1, 3, and 4 but exclude Proposal 2, because Religious Studies requires investigators to bracket and suspend confessional commitments and therefore should not employ overtly confessional theological projects."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a hiring-committee scenario with four concrete methodological proposals; correct option reflects methodological pluralism, empirical/historical/cross-cultural emphasis, and acceptance of believer-scholars provided methodological transparency (epoche-like neutrality).",
      "concepts_tested": [
        "Methodological stance and objectivity (epoche, independence from single viewpoint)",
        "Pluralism of approaches and methods (anthropology, sociology, psychology, history, theology as method)"
      ],
      "source_article": "Religious studies",
      "x": 1.1266013383865356,
      "y": 1.0386378765106201,
      "level": 2,
      "original_question_hash": "1c1df8a8"
    },
    {
      "question": "A chemical factory suffers a fire. Investigators find that at the time of ignition a short circuit occurred in a control panel, there was a large quantity of a flammable solvent stored nearby, and the automatic sprinkler system had been disabled for maintenance. Which of the following sets of statements about causality correctly applies to this scenario?\n\n1. The short circuit can be characterized as an INUS condition for the factory fire (an insufficient but non-redundant part of a condition that is itself unnecessary but sufficient).\n\n2. According to Aristotle's four causes, identifying the solvent's molecular composition exemplifies citing the efficient cause of the fire.\n\n3. David Hume would maintain that the proposition “the short circuit caused the fire” can be established purely by a priori reason without relying on repeated empirical observation and habit.\n\n4. Because causes lie in the past relative to their effects, any causal factors for the fire must have occurred at times earlier than or equal to the observed time of ignition.",
      "options": {
        "A": "All four statements (1, 2, 3, and 4) are correct.",
        "B": "Only statements 2 and 3 are correct.",
        "C": "Only statements 1 and 4 are correct.",
        "D": "Only statements 1, 2 and 4 are correct."
      },
      "correct_answer": "C",
      "generation_notes": "Constructed a concrete factory-fire scenario to test (a) INUS/contributory causes, (b) Aristotle's distinction (efficient vs material cause), (c) Hume's epistemology that causal knowledge derives from experience/habit, and (d) the temporal orientation of causes preceding effects.",
      "concepts_tested": [
        "INUS and contributory causes (multiple causes)",
        "Aristotle's four causes (efficient vs material)",
        "Humean epistemology of causality",
        "Temporal orientation of cause and effect"
      ],
      "source_article": "Causality",
      "x": 1.204470157623291,
      "y": 1.083905816078186,
      "level": 2,
      "original_question_hash": "9ccbe4f1"
    },
    {
      "question": "DeltaForge Inc. is a mid-sized manufacturer that for 30 years produced one standardized metal component per customer specifications. Its organization has a Weberian bureaucracy: promotions are merit-based, rules govern decision-making, authority is clearly bounded in a chain of command, and employees are highly specialized in narrowly defined tasks (strong formalization and division of labor). Recently, a large portion of its market shifted to made-to-order components with high variability in specifications, short lead times, and frequent design feedback from customers. According to contingency theory and the principle of requisite variety, which combination of organizational changes would best enable DeltaForge to adapt to the new environment while still leveraging the advantages of bureaucracy and specialization?",
      "options": {
        "A": "Decentralize routine decision authority to cross-functional teams (empowering shop-floor engineers and customer liaisons), create modular work cells combining related specialized skills, relax some formal rules for customization while keeping formal career tracks and written procedures for non-custom processes.",
        "B": "Retain the existing strict hierarchy and narrow specialization but add output-based piece-rate incentives and more detailed formal rules to force faster compliance with customer orders.",
        "C": "Double down on formalization by adding more written approval steps and stronger central control over all customer interactions to ensure consistency, and hire additional narrowly specialized workers to increase throughput of the standardized component line.",
        "D": "Abolish the chain of command entirely, eliminate all written rules and job boundaries so every employee handles every task interchangeably, and remove formal promotion tracks to maximize short-term flexibility."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete firm-level scenario requiring trade-offs among Weberian bureaucracy, division of labor/formalization, and contingency theory (requisite variety). Options present plausible mixes; A illustrates adaptive restructuring consistent with theory.",
      "concepts_tested": [
        "Weberian bureaucracy and its effects on authority and decision-making",
        "Division of labor and formalization: specialization benefits and costs",
        "Contingency theory and requisite variety: aligning structure with environmental variability"
      ],
      "source_article": "Organizational theory",
      "x": 1.2749298810958862,
      "y": 0.9792605042457581,
      "level": 2,
      "original_question_hash": "74723d6b"
    },
    {
      "question": "State Z lawfully enacts compulsory acquisition of two contiguous parcels to build a public hydroelectric dam. Parcel 1 is held collectively by an indigenous community (natural persons exercising communal ownership). Parcel 2 is a privately owned multinational corporation (a legal person) holding development rights. Relying on the Universal Declaration of Human Rights (Article 17), Protocol I to the European Convention on Human Rights (ECtHR case-law), the African Charter on Human and Peoples' Rights (Article 14) and the International Covenants (ICCPR/ICESCR), which of the following best describes the ordinary international/regional legal position on property protection in this mixed factual scenario?",
      "options": {
        "A": "All instruments guarantee an absolute, inviolable right to property for both natural and legal persons; expropriation for public projects is prohibited unless owners consent.",
        "B": "The UDHR proclaims a right to property but is not a binding covenant; Protocol I (ECtHR) explicitly protects peaceful enjoyment for natural and legal persons yet permits deprivation in the public interest subject to law and conditions; the African Charter permits encroachment for public need under law — together demonstrating property rights are protected but not absolute and differ in scope and enforcement across instruments.",
        "C": "The ICCPR and ICESCR contain the most robust property guarantees and forbid any state interference with property used for production, whereas regional treaties like Protocol I and the African Charter merely recommend compensation without permitting lawful expropriation.",
        "D": "International human rights law uniformly limits protection of property to personal, consumptive holdings of natural persons; corporations and communal or production-oriented property are excluded from these protections in all regional and universal instruments."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete expropriation vignette contrasting communal natural-person ownership and corporate legal-person ownership, then asked which summary best captures (1) non-absolute nature of property rights, (2) contested scope (natural vs legal persons, communal vs corporate), and (3) variation across UDHR, Protocol I (ECtHR) and African Charter.",
      "concepts_tested": [
        "Property rights are not absolute and may be restricted for public interest under law",
        "Contested scope of protection: natural persons vs legal persons, communal/consumptive vs productive property",
        "Variation among international and regional instruments (UDHR Article 17, ECtHR Protocol I, African Charter Article 14) in recognition and enforceability"
      ],
      "source_article": "Right to property",
      "x": 1.256926417350769,
      "y": 0.8357032537460327,
      "level": 2,
      "original_question_hash": "9a883198"
    },
    {
      "question": "You are presented with data from three related languages in a single family. For a set of basic vocabulary items the following regular consonant correspondences are observed: L1: p, L2: p, L3: b (e.g. L1 ped- ‘foot’, L2 pet- ‘foot’, L3 bet- ‘foot’). L1 has written attestations dated to $800$ CE, L2 to $1200$ CE, and L3 is poorly attested. A field linguist proposes (i) that the proto-form was *ped-, (ii) that the change L1/L2 p → L3 b is due to lenition/voicing processes similar to those observed synchronically in related languages, and (iii) that the protolanguage can be dated to roughly $3{,}000$–$5{,}000$ years ago. Which of the following assessments best reflects the principles and limits of historical linguistics?",
      "options": {
        "A": "This assessment is well supported: by the uniformitarian principle it is legitimate to invoke phonological processes observed today (lenition/voicing) as plausible causes; the comparative method (systematic correspondences) combined with internal reconstruction justifies reconstructing a proto-form such as *ped-; and the proposed date of $3{,}000$–$5{,}000$ years is a defensible, but inherently approximate, estimate — confidence declines substantially as one approaches a rough time-depth ceiling of about $10{,}000$ years.",
        "B": "The assessment is invalid because without direct written attestations for L3 one cannot reconstruct any proto-form; therefore *ped- is unjustifiable and the comparative method cannot be applied when one language lacks early texts.",
        "C": "The assessment is invalid because the correspondence p → b proves that the proto-form must have been voiced (*bed-), so invoking lenition is unnecessary; the uniformitarian principle is irrelevant to phonological reconstruction.",
        "D": "The assessment is unreliable: mass lexical comparison can precisely date the protolanguage and reliably push the time depth well beyond $10{,}000$ years, so the $3{,}000$–$5{,}000$ year estimate is certainly too conservative."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete comparative-data scenario to test uniformitarian principle, use of comparative method/internal reconstruction for proto-forms, and the approximate time-depth limit (~10,000 years) for dating proto-languages; provided plausible distractors reflecting common misunderstandings.",
      "concepts_tested": [
        "Uniformitarian principle in historical linguistics",
        "Comparative method and internal reconstruction for proto-language inference",
        "Time-depth limits and approximate dating of proto-languages (around $10{,}000$ years)"
      ],
      "source_article": "Historical linguistics",
      "x": 1.217950701713562,
      "y": 1.0879830121994019,
      "level": 2,
      "original_question_hash": "a30fcb0c"
    },
    {
      "question": "Two societies are studied by anthropologists. Society X is a kinship-oriented hunter-gatherer band of 200 people with informal sharing norms, negligible private property, low measured income variance, and near-universal intragroup redistribution when resources are acquired. Society Y is a 50-million–person industrial state with a large occupational division of labor, formal property rights, a Gini coefficient of $0.55$, and the top 10\\% owning $75\\%$ of wealth; elite children routinely attend exclusive schools and many high-status occupations require credentials and institutional memberships that are difficult for outsiders to obtain. Which theoretical explanation best accounts for (a) why social stratification is markedly greater in Society Y than X, and (b) why the upper strata in Society Y persist across generations?",
      "options": {
        "A": "A conflict/Marxist explanation: as societies grow in complexity they develop differentiated relations to the means of production and institutionalized property rights that concentrate valued resources; therefore greater inequality produces distinct social strata, and intergenerational reproduction of the upper strata follows from ownership of productive assets, control over institutional allocations, and barriers to mobility (e.g., exclusive education and legal entitlements).",
        "B": "An action-theory/dominance-hierarchy explanation: more complex societies require formal dominance hierarchies to maintain order and coordinate specialized tasks; stratification exists because elites naturally assume superior positions to regulate society, and their persistence is primarily due to cultural acceptance of hierarchical authority rather than material control of resources.",
        "C": "A structural-functionalist (Davis–Moore) explanation: stratification in Society Y arises because scarce, functionally important positions require high rewards to attract qualified people in a complex division of labor; the upper strata persist because their high compensation and prestige create incentives for families to prepare children for these roles, producing stable intergenerational placement.",
        "D": "A kinship-orientation explanation: stratification varies with social complexity because kinship-oriented norms disappear as societies grow; therefore the upper strata persist in Society Y mainly because kin networks deliberately hoard resources and exclude outsiders through lineage-based inheritance rather than through occupational or institutional mechanisms."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a comparative scenario (egalitarian hunter-gatherer vs. industrial state) and asked which theory best explains (1) complexity→greater stratification via concentrated resources and (2) intergenerational persistence via institutionalized ownership and barriers—correct answer aligns with Marxist/conflict perspective.",
      "concepts_tested": [
        "Degree of social inequality determines social stratum",
        "Greater social complexity leads to more pronounced stratification",
        "Theoretical mechanisms for maintenance/reproduction of stratification (conflict/Marxism vs action theory vs functionalism)"
      ],
      "source_article": "Social stratification",
      "x": 1.189574122428894,
      "y": 0.9429702758789062,
      "level": 2,
      "original_question_hash": "b1542c4f"
    },
    {
      "question": "A VC-backed SaaS startup (three cofounders acting as principals) must hire an external sales agent. Before hiring, the agent privately knows the true quality of her sales leads (so adverse selection may be present). After hiring, the agent chooses unobservable effort $e$ and can shirk or manipulate customer interactions (moral hazard). Market demand causes observable fluctuations $y$ and there are unobserved shocks $x$. The firm models wages with the linear form $w=a+b(e+x+gy)$, where $a$ is base salary, $b$ is incentive intensity, and $g$ weights observed exogenous effects. The agent is risk‑averse and monitoring is costly. Which of the following contract/governance packages best reduces expected agency costs given (i) there is both adverse selection and moral hazard, (ii) the agent is risk‑averse, and (iii) the three cofounders face a multiple‑principal coordination problem?",
      "options": {
        "A": "Offer a pure piece‑rate contract (set $a=0$, $b$ very large so wages closely track sales outcomes), no bond or probation, standard sales commission paid monthly; let all three cofounders jointly set ad hoc sales targets and approve bonuses (no single representative principal).",
        "B": "Offer a moderate base salary $a>0$ plus moderate incentive slope $b$ (so the agent shares risk), include relative performance evaluation (RPE) to set $g$ so pay is adjusted for observed market shocks $y$, require pre‑hire screening/probation and a refundable performance bond to deter adverse selection, include clear termination and deferred/profit‑sharing elements for long‑term alignment, and appoint a single designated manager (representative principal) to set objectives and reduce the multiple‑principal collective‑action problem.",
        "C": "Pay the agent a fixed retainer ($b\\approx0$, large $a$) and a long‑run profit‑share payable annually, rely on the three cofounders to monitor daily and rotate oversight responsibilities, no pre‑hire screening or bond.",
        "D": "Minimize base pay ($a$ small), rely on heavy ex post monitoring (frequent audits and direct observation) to eliminate shirking, set $b$ extremely small to avoid variance in wages, and require unanimous agreement among the three cofounders for major incentive changes (so each principal retains veto power)."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete startup hiring scenario embedding adverse selection (hidden pre‑contract information) and moral hazard (hidden effort), used the linear wage model $w=a+b(e+x+gy)$ to motivate choices for $a,b,g$, and included the multiple‑principal coordination constraint. Option B combines screening/bond (adverse selection), moderate $b$ and base salary (risk sharing for risk‑averse agent), RPE for exogenous shocks, deferred pay/termination (moral hazard), and a single representative principal to solve the multiple‑principal problem.",
      "concepts_tested": [
        "Information asymmetry and agency costs (misaligned incentives)",
        "Moral hazard vs adverse selection mechanisms",
        "Incentive alignment mechanisms and governance solutions including multi‑principal coordination"
      ],
      "source_article": "Principal–agent problem",
      "x": 1.3174810409545898,
      "y": 0.9704108238220215,
      "level": 2,
      "original_question_hash": "7afbae09"
    },
    {
      "question": "A mid-sized hospital is deploying an electronic health-record (EHR) system that will store sensitive patient data and support clinicians 24/7. Management demands strong protection but also insists clinicians must not be impeded in patient care. The hospital must comply with HIPAA and wishes to follow best-practice standards (e.g., ISO/IEC 27001). Which of the following strategies best aligns with the central objectives of information security (CIA triad), uses a structured risk-management approach, and reflects the relationship between information security and IT security plus applicable standards and laws?",
      "options": {
        "A": "Encrypt all EHR data at rest and in transit, implement two-factor authentication for every access, and forbid any remote access; defer formal risk assessment because encryption and strong authentication are sufficient to meet HIPAA and productivity needs.",
        "B": "Conduct a formal risk assessment to identify and value assets, threats, vulnerabilities, and impacts; select proportional administrative, technical, and physical controls that balance confidentiality, integrity, and availability with clinician productivity; document controls in an ISO/IEC 27001–aligned program, assign IT security specialists to implement and monitor controls, and ensure HIPAA-required policies, logging, incident response and business-continuity plans are in place.",
        "C": "Prioritize availability above confidentiality and integrity by focusing on redundant servers, failover clustering, and immediate restore procedures; implement minimal access control to avoid delaying clinicians, relying on post-incident audits to detect misuse.",
        "D": "Outsource the entire EHR to a third-party cloud vendor, rely exclusively on the vendor’s SLAs and certifications as proof of compliance, and eliminate internal security roles and documentation because contractual liability transfers the risk away from the hospital."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a clinical EHR deployment scenario requiring trade-offs among confidentiality, integrity, and availability; created distractors that each violate one or more core concepts (overreliance on single control, availability-only focus, blind outsourcing) and one correct option emphasizing structured risk management, proportional controls, IT security roles, and legal/standards compliance.",
      "concepts_tested": [
        "CIA triad and productivity trade-offs",
        "Structured risk management process and control selection",
        "Relation of information security to IT security and role of standards/laws (HIPAA, ISO/IEC 27001)"
      ],
      "source_article": "Information security",
      "x": 1.380502462387085,
      "y": 1.030281662940979,
      "level": 2,
      "original_question_hash": "bff2a736"
    },
    {
      "question": "A municipal water treatment plant uses Internet-connected PLCs and IoT sensors to control valves and monitors. A security researcher discovers a buffer‑overflow flaw in the PLC web interface, develops a proof‑of‑concept that toggles valves remotely, and files a CVE entry documenting the flaw. Weeks later, attackers use a botnet to run the same proof‑of‑concept and cause a multi‑hour service outage. Which statement best describes the vulnerability–threat–exploit relationship, the role of system interdependence, and the effect of CVE disclosure in this scenario?",
      "options": {
        "A": "The buffer‑overflow is a vulnerability; when a working proof‑of‑concept exploit existed it became an exploitable vulnerability; the attackers are threat actors who used that exploit to cause unauthorized disruption. The plant's dependence on networked IoT/PLC components increased its attack surface and made the outage more likely and severe. Publishing the issue to CVE enabled coordinated defensive actions (patches, IDS signatures, monitoring), although it can also inform attackers.",
        "B": "Because the researcher published the flaw in the CVE database, the vulnerability ceased to be exploitable — public disclosure automatically prevents exploitation by forcing vendors to remove the bug before attackers can act.",
        "C": "The existence of a proof‑of‑concept exploit is irrelevant to whether the issue is a vulnerability; system interdependence (IoT/PLCs) does not materially change risk because only the software bug itself determines if an attack succeeds.",
        "D": "An exploitable vulnerability requires physical access or insider knowledge; since the incident was carried out remotely via a botnet it cannot be classified as an exploit-driven attack on a vulnerability but rather as a generic denial‑of‑service unrelated to the documented software flaw."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete IoT/PLC incident to test: (1) the formal relation that a vulnerability becomes 'exploitable' when a working exploit exists and threats leverage exploits to cause harm; (2) how system interdependence/IoT expands attack surface and consequences for critical infrastructure; (3) role of CVE disclosure in coordinating defense (and potential attacker awareness). Distractors reflect common misconceptions.",
      "concepts_tested": [
        "Vulnerability–threat–exploit relationship",
        "System complexity and IoT interdependence increasing attack surface",
        "Vulnerability disclosure and CVE's role in defense and risk"
      ],
      "source_article": "Computer security",
      "x": 1.4014613628387451,
      "y": 1.0637760162353516,
      "level": 2,
      "original_question_hash": "c877bc75"
    },
    {
      "question": "A municipal wastewater system has three storage tanks (stocks), variable inflows from households and industry, and pumps (actuators) that move water between tanks. Operators observe sustained oscillations in tank levels because of time delays, nonlinear pump curves, and synchronized industrial discharges. The engineering team proposes four interventions. Which single intervention best exemplifies systems thinking by (1) treating the system as an integrated whole with attention to relationships and information flows, (2) using feedback/dynamical principles to improve stability (rather than only changing components), and (3) exploiting leverage points or black‑box modeling to produce a large systemic change with a small intervention?",
      "options": {
        "A": "Design PID controllers for each pump by linearizing the local nonlinear dynamics about the current operating point $x_0$ (so $\\\\dot{x} \\approx A(x-x_0)+B(u-u_0)$), then tuning gains to damp oscillations—i.e., a local feedback control solution.",
        "B": "Provide real‑time tank‑level and inflow data to industrial dischargers and introduce dynamic discharge pricing that incentivizes temporal smoothing of their outflows—creating an information feedback loop and incentive structure change to reduce peak inflows.",
        "C": "Replace each pump with higher‑capacity units so that peak inflows can be accommodated without changing controls or information flows—i.e., increase physical capacity of parts.",
        "D": "Reconfigure the plant to decouple the three tanks so each runs on an independent timer schedule, isolating subsystems to prevent interactions that produce oscillations."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete urban infrastructure scenario testing holistic systems perspective, feedback/dynamical control (including linearization), and leverage points/black‑box approaches; option B is the highest‑leverage, information‑feedback intervention.",
      "concepts_tested": [
        "Systems as integrated wholes and relationships (not just parts)",
        "Feedback control and dynamical systems principles, including linearization",
        "Leverage points, information flows, and black‑box/organizational interventions"
      ],
      "source_article": "Systems thinking",
      "x": 1.4393538236618042,
      "y": 1.0535874366760254,
      "level": 2,
      "original_question_hash": "2e159dc3"
    },
    {
      "question": "You are the lead engineer responsible for diagnosing frequent intermittent false alarms in a hospital patient-monitoring system. Collected evidence includes: (1) a correlation between alarm spikes and network packet loss above 2%, (2) sporadic sensor-driver exceptions in system logs, and (3) staff reports that false alarms tend to occur during shift changes. Your task is to identify the most probable underlying cause(s) and recommend mitigations that consider uncertainty and multiple interacting domains (networking, device drivers, human factors). Which diagnostic approach is most appropriate and why?",
      "options": {
        "A": "Construct a fault tree analysis that models the monitoring stack (sensor → driver → middleware → network → staff), compute minimal cut sets deterministically, and fix any component that appears in the smallest cut set; this yields the definitive root cause to remediate.",
        "B": "Build a Bayesian-network-based diagnostic model that integrates telemetry (packet-loss metrics), log-derived events (driver exceptions), and contextual variables (shift change), compute posterior probabilities $P(\\text{cause}|\\text{evidence})$ for candidate causes, and use the probabilistic rankings (with Occam's razor as a model-selection heuristic) to prioritize mitigations and experiments.",
        "C": "Apply a clinician-style differential diagnosis: enumerate plausible causes (network fault, driver bug, misconfiguration, training issue), then sequentially rule them out by targeted checks (e.g., reproduce under controlled load, review recent deployments, interview staff) until a single cause remains.",
        "D": "Use the Five Whys technique with on-shift staff to trace the chain of events (why did the alarm fire? why was packet dropped? why was the router congested? ...) until a single human-actionable cause (e.g., inadequate training) is identified and corrected."
      },
      "correct_answer": "B",
      "generation_notes": "Created a concrete cross-domain scenario requiring integration of heterogeneous evidence and uncertainty; options contrast probabilistic (Bayesian) vs deterministic (fault tree), rule-out (differential), and heuristic (Five Whys) methods to test understanding of method suitability and cross-domain diagnostic transfer.",
      "concepts_tested": [
        "Diagnosis as linking symptoms to causes (cause-and-effect identification)",
        "Comparative suitability of diagnostic methods (Bayesian networks, fault tree analysis, differential diagnosis, Five Whys)",
        "Cross-domain diagnostic reasoning integrating technical and human factors"
      ],
      "source_article": "Diagnosis",
      "x": 1.4755346775054932,
      "y": 1.079780101776123,
      "level": 2,
      "original_question_hash": "1e17f360"
    },
    {
      "question": "A national public health agency is considering a population screening program for Disease X after a Cochrane-style systematic review of five randomized controlled trials in predominantly 50–60-year-old participants reported an absolute reduction in 10-year mortality of 1% (NNT = 100). The trials largely excluded older adults and people with multiple chronic conditions. A 72-year-old patient with moderate dementia and limited life expectancy asks you, their clinician, whether they should have the screening. Which course of action best exemplifies evidence-based medicine — integrating clinician expertise, patient values, and the evidence hierarchy — while recognizing implications for policy and the right to health?",
      "options": {
        "A": "Explain the systematic review's high position in the evidence hierarchy and the reported NNT (NNT = 100), describe how the trials excluded people like them so benefits/harm may not apply, elicit the patient's values/goals and recommend against routine screening for this patient while supporting targeted, evidence-informed public policy that offers screening to eligible populations with safeguards to ensure equitable access and monitoring.",
        "B": "Recommend screening because the systematic review (the highest-quality evidence) showed a mortality benefit; argue that public health policy should mandate universal screening for all adults to maximize population benefit, regardless of trial exclusions or individual comorbidity.",
        "C": "Decline to use the systematic review and rely on your own clinical judgment and experience; since trials are imperfect, follow local expert opinion which advises against screening and lobby against any policy implementation.",
        "D": "Inform the patient that implementation of screening is solely a policy decision: if the government mandates the program you will enroll them, but otherwise you will not offer screening; do not discuss the trials, NNT, or the patient's preferences."
      },
      "correct_answer": "A",
      "generation_notes": "Created a clinical-policy scenario that requires applying the EBM integration principle, recognizing evidence hierarchy (systematic review vs. applicability), and linking individual decisions to population-level policy and equity/rights considerations; included NNT as a numeric measure.",
      "concepts_tested": [
        "Integration principle: clinician expertise + patient values + external evidence",
        "Evidence hierarchy: weight of systematic reviews and applicability limitations",
        "Practice-to-policy relationship: how EBM informs public health policy and equitable access"
      ],
      "source_article": "Evidence-based medicine",
      "x": 1.317619800567627,
      "y": 0.9821073412895203,
      "level": 2,
      "original_question_hash": "37f3479a"
    },
    {
      "question": "Country A has ratified the ICCPR but not the ICESCR. Recent independent reports document systematic torture by security forces and new domestic legislation that bans foreign human rights NGOs as \"incompatible with local traditions.\" Which of the following statements best describes the international human-rights framework and likely international responses, consistent with established doctrine and institutions?",
      "options": {
        "A": "Because the Universal Declaration of Human Rights (UDHR) is a United Nations instrument, its provisions are directly legally binding on Country A and the UN can unilaterally impose binding remedial legislation on the country to stop torture and re-open NGOs.",
        "B": "Ratification of the ICCPR makes civil and political rights legally binding on Country A at the international level; the UDHR itself is non‑binding but influential and part of customary law; treaty bodies (e.g., the Human Rights Committee) and NGOs can investigate and exert pressure; cultural‑relativist arguments do not legally justify violations of non‑derogable norms (e.g., torture), and enforcement relies on state compliance, treaty monitoring, possible Security Council action or ICC jurisdiction for core international crimes, not unilateral UN law‑making.",
        "C": "Country A can lawfully exempt itself from its international obligations by invoking cultural relativism whenever a right conflicts with local tradition, because human rights are purely culturally contingent and treaties are optional moral guidelines rather than juridical commitments.",
        "D": "International human rights mechanisms allow NGOs and UN treaty bodies to override domestic courts: they can immediately substitute domestic legal remedies with international prosecutions for any human‑rights violation, so Country A's courts are replaced by international tribunals once abuse is alleged."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete ratification and abuse scenario to probe students' understanding of universality/inalienability, binding vs non‑binding instruments (UDHR vs ICCPR/ICESCR), roles of UN treaty bodies, NGOs, and limits of cultural relativism and enforcement mechanisms.",
      "concepts_tested": [
        "Universality and inalienability of human rights",
        "International mechanisms and institutions for protection (UDHR, ICCPR, UN bodies, NGOs, ICC)",
        "Debates over cultural relativism, prioritization, and limits of enforcement"
      ],
      "source_article": "Human rights",
      "x": 1.1437218189239502,
      "y": 0.8368682861328125,
      "level": 2,
      "original_question_hash": "31ce3f2b"
    },
    {
      "question": "A med‑tech startup is introducing a new home telemonitoring device in a mid‑sized city (population 10,000). The social system exhibits high homophily within neighbourhood clusters (strong ties inside clusters, weak ties between clusters). The product has clear health advantages but is seen as somewhat complex and not yet fully compatible with existing home routines. The company can choose one of four diffusion strategies. Which strategy is most consistent with Rogers' five‑element diffusion framework and most likely to push adoption past the \"marketing chasm\" (the boundary at about $2.5\\%+13.5\\%=16\\%$ cumulative adopters between early adopters and the early majority) so that adoption becomes self‑sustaining?",
      "options": {
        "A": "Launch a large mass‑media advertising campaign citywide emphasizing low price and broad awareness; after six months offer a steep discount to all households to accelerate uptake.",
        "B": "Seed the device with a small number of isolated innovators in one neighbourhood, publish technical whitepapers to demonstrate superiority, and rely on organic spread from that neighbourhood.",
        "C": "Identify and recruit the local respected physician (an opinion leader) plus several well‑connected early adopters distributed across different clusters, provide free trial units and visible demonstrations (observability and trialability), and facilitate inter‑cluster meetings so these adopters can influence the early majority.",
        "D": "Target late majority and laggard segments by making the device extremely simple and standardized, and mandate its use through local regulation to force rapid citywide adoption."
      },
      "correct_answer": "C",
      "generation_notes": "Constructed a city case requiring application of the five‑element framework: innovation attributes, adopter categories and chasm, communication channels, time, and social system. Options contrast mass media, isolated seeding, opinion‑leader and bridge seeding (correct), and coercive mandate to test understanding of social capital, heterophily, trialability/observability, and critical mass.",
      "concepts_tested": [
        "Five‑element diffusion framework (innovation, adopters, channels, time, social system)",
        "Adopter categories and the marketing chasm (innovators, early adopters, early majority, critical mass)",
        "Role of social networks, opinion leaders, heterophily/bridges, and social capital in achieving self‑sustaining diffusion"
      ],
      "source_article": "Diffusion of innovations",
      "x": 1.2933298349380493,
      "y": 1.003447413444519,
      "level": 2,
      "original_question_hash": "3d04dd6c"
    },
    {
      "question": "A multinational firm has a high-volume production division serving a stable market and an R&D innovation lab operating in a rapidly changing market. The board must choose structures for each unit. Which assignment and rationale best reflect (a) how structure determines who participates in decision-making and thereby shapes organizational actions, (b) how structure affects outcomes such as efficiency, flexibility and innovation, and (c) Burns & Stalker's and Weberian prescriptions about mechanistic versus organic designs?",
      "options": {
        "A": "Assign a mechanistic (Weberian) bureaucratic structure to the production division — clear roles, hierarchical authority, standardized procedures and centralized decision rights — because this channels participation through formal authority and maximizes efficiency and control in a stable environment; assign an organic/post-bureaucratic (adhocratic) structure to the R&D lab — decentralized authority, mutual adjustment, and consensus-based decision-making — because broader participation directly shapes experiment-driven actions, increasing flexibility and innovation; together this alignment supports sustainable competitive advantage by matching structure to environmental complexity.",
        "B": "Assign an organic/post-bureaucratic structure to production and a mechanistic bureaucratic structure to R&D, arguing that broad participation in production will increase employee buy-in and reduce defects while centralized control in R&D will speed development by reducing coordination overhead; this configuration therefore maximizes both efficiency and innovation by reversing conventional role assignments.",
        "C": "Use the same mechanistic bureaucratic structure for both units so that decision-making remains concentrated at the top; centralized participation ensures uniform standards, which increases efficiency and prevents the R&D lab from producing risky or costly innovations that could destabilize the firm, thereby protecting long-term competitive advantage.",
        "D": "Use the same organic/post-bureaucratic structure for both units, decentralizing decision rights and emphasizing mutual adjustment across the firm; this maximizes flexibility and innovation everywhere, and because participation is widespread it automatically yields higher efficiency and sustainable advantage across stable and dynamic environments."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete production vs R&D scenario and applied Weberian characteristics and Burns & Stalker mechanistic/organic distinction to link who participates in decisions to performance outcomes (efficiency, flexibility, innovation) and sustainable competitive advantage.",
      "concepts_tested": [
        "Structure determines participation in decision-making and shapes organizational actions",
        "Organizational structure influences efficiency, flexibility, innovation, and sustainable competitive advantage",
        "Mechanistic (bureaucratic) versus organic (post-bureaucratic/adhocratic) alignment with task complexity and environment (Weberian characteristics; Burns & Stalker)"
      ],
      "source_article": "Organizational structure",
      "x": 1.3422750234603882,
      "y": 0.9994455575942993,
      "level": 2,
      "original_question_hash": "45d0df9a"
    },
    {
      "question": "Consider three leaders operating in distinct contexts: (1) Lieutenant Moreno, a naval officer who issues rapid, rank-based orders during a maritime rescue; (2) Dr. Lin, a respected community elder in an East Asian village who coordinates projects through ritualized consultation, benevolence, and appeals to filial duties; (3) Priya, CEO of a multinational start‑up who mobilizes a diverse team with an articulated vision, charisma, tailored coaching, and contingent incentives. Which analytical statement best synthesizes the Wikipedia account of leadership as a scholarly concept?",
      "options": {
        "A": "Leadership is a process of social influence — a reciprocal power‑relationship between leader and followers — that cannot be reduced to a single theory; effectiveness is context‑dependent and arises from combinations of traits, behaviors, situational contingencies, power, vision, values, charisma, intelligence, and culturally specific virtues (e.g., intelligence, trustworthiness, humaneness, courage, discipline).",
        "B": "Leadership is primarily an enduring individual trait: leaders like Priya succeed because stable dispositional attributes (e.g., high intelligence and extraversion) determine emergence and effectiveness across contexts, rendering situational and cultural factors secondary.",
        "C": "Leadership is essentially positional authority: Lieutenant Moreno exemplifies leadership because formal legitimate power and command produce follower compliance, whereas informal influence, vision, or culturally embedded virtues are peripheral to explaining outcomes.",
        "D": "Leadership is merely a socially constructed narrative or media artifact: apparent leadership (e.g., Dr. Lin’s rituals or public storytelling) explains followers’ perceptions, while actual social influence, behavioral mechanisms, or situational contingencies have little explanatory value."
      },
      "correct_answer": "A",
      "generation_notes": "Created a three‑case scenario to force integration of: leadership as social influence/power relation, multi‑theoretical/contextual explanations, and culturally/historically grounded virtues. Distractors reflect trait‑only, authority‑only, and pure constructivist positions.",
      "concepts_tested": [
        "Leadership as social influence and power‑relationships",
        "Leadership is multi‑theoretical and context‑dependent (traits, behavior, situational, power, vision, values, charisma, intelligence)",
        "Leadership is culturally and historically constructed with normative virtues (e.g., intelligence, trustworthiness, humaneness, courage, discipline)"
      ],
      "source_article": "Leadership",
      "x": 1.2715747356414795,
      "y": 0.9874862432479858,
      "level": 2,
      "original_question_hash": "f94024ed"
    },
    {
      "question": "You are given three distinct real-world recording scenarios. For each, identify the producer's most appropriate scope of responsibility and the principal technological factor enabling that scope.\n\nScenario A — A solo electronic pop single is created in a bedroom studio: the producer programs synths, sequences parts in a DAW, records and edits vocal takes, and mixes the final stereo track without additional musicians.\n\nScenario B — A commercial recording of a Mahler symphony: the conductor leads the orchestra, the recording sessions use many microphones and specialized room techniques, and the label expects a faithful rendition of the score.\n\nScenario C — A 1960s-style rock session: the band records a rhythm “bed” live to tape, then the producer overdubs lead guitars and horn parts on separate passes and assembles the final takes.\n\nWhich of the following answer choices best matches the producer role and enabling technology for A, B, and C respectively?",
      "options": {
        "A": "A: The producer functions as a creative auteur and often the sole artist, leveraging DAWs and virtual instruments to realize the entire sound; B: The producer serves chiefly as liaison between conductor and engineering team, coordinating sessions and delegating musical leadership to the conductor; C: The producer uses multitrack tape/overdubbing to construct the record from bed tracks and later overdubs, expanding editing and arrangement possibilities.",
        "B": "A: The producer acts primarily as an executive producer, managing budgets and delegating all musical choices to hired session musicians despite using a DAW; B: The producer composes and restructures the symphony to fit a pop-oriented single, using electronic sampling to replace orchestral parts; C: The producer avoids overdubbing on tape, insisting on single-take authenticity identical to 1920s phonograph practices.",
        "C": "A: The producer’s role is limited to operating the recording console while an external arranger supplies all creative choices; B: The producer replaces the conductor’s authority and directs tempo and phrasing to modernize the work, enabled by digital pitch/time editing; C: The producer cannot feasibly overdub on analog tape, so all parts must be captured simultaneously.",
        "D": "A: The producer is indistinguishable from the audio engineer and only handles microphone placement and gain staging in a DAW-based session; B: The producer functions as the album’s executive producer—solely handling contracts, budgets, and release strategy with no studio attendance; C: The producer relies on digital DAWs to emulate tape-era techniques, making analog multitrack irrelevant to the workflow."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a comparative scenario across pop, classical, and tape-era rock to test distinctions in producer scope (creative auteur vs liaison), and causality from recording technologies (DAWs, multitrack tape) to production practices; distractors deliberately invert roles or misattribute technological capabilities.",
      "concepts_tested": [
        "Producer as flexible integrative leader distinct from engineer/executive producer",
        "Impact of recording technologies (tape/multitrack, electronic instruments, DAWs) on production practices",
        "Genre-specific production roles and relationships (pop auteur vs classical liaison)"
      ],
      "source_article": "Record producer",
      "x": 0.4384283125400543,
      "y": 1.8137986660003662,
      "level": 2,
      "original_question_hash": "5a740396"
    },
    {
      "question": "Asha (25) completed a 7‑day experience-sampling diary and concurrent physiological monitoring. Her entries show the following pattern: (1) Day 1 — she encounters a snake, exhibits rapid heart rate and trembling, appraises the situation as dangerous, shows a fearful facial expression and withdraws (avoidance). (2) Day 2 — after strong caffeine intake she has elevated heart rate but initially labels it as 'restless' until a friend suggests she might be having panic, after which she reports feeling afraid. (3) Day 3 — low mood from a dispute persists into the next day with an emotional autocorrelation of approximately $r_{t,t+1}=0.7$ (high inertia). (4) Across the week she sometimes reports undifferentiated negative affect (\"I just feel bad\") and at other times distinct labels (anger, sadness, fear); a separate older adult sample shows lower variability and lower peak intensity than Asha. Which theoretical account best integrates (a) the multi‑component character of emotion (subjective feeling, physiology, expression, action tendencies, appraisal), (b) the debate over whether cognition and emotion are separate or integrated, and (c) the observed emotion dynamics (intensity, variability, inertia, differentiation and lifespan differences)?",
      "options": {
        "A": "James–Lange somatic account: emotions are the conscious perception of bodily changes, so Asha's physiological arousal should always precede and determine her subjective labels; differences with age reflect age‑related dampening of bodily reactivity.",
        "B": "Two‑factor (Schachter–Singer) account: a general physiological arousal plus a rapid cognitive label explains the diary (caffeine arousal becomes fear after social labeling), and inertia/variability are incidental consequences of recurring arousal episodes rather than central to emotion structure.",
        "C": "Scherer's Component Process Model combined with contemporary emotion‑dynamics research: appraisal processes coordinate multiple components (physiology, expression, action tendencies, subjective feeling); cognition is integrated via appraisal (not wholly separate), and temporal properties (intensity, variability, inertia, differentiation) explain day‑to‑day trajectories and lifespan differences in emotional experience.",
        "D": "Cognitivist judgment theory (Solomon): emotions are judgments or evaluative beliefs, so all components (physiology, expression, inertia) are downstream or epiphenomenal and variability and differentiation reflect shifts in evaluative judgments only."
      },
      "correct_answer": "C",
      "generation_notes": "Designed a concrete 7‑day diary vignette that reveals component mismatches, social labeling, high inertia ($r_{t,t+1}=0.7$), differentiation and lifespan differences; asked which theoretical synthesis best accounts for all three target concepts. Options contrast James–Lange, Two‑factor, Component Process + dynamics (correct), and pure cognitivist positions.",
      "concepts_tested": [
        "Multi‑component nature of emotion (subjective, physiological, expressive, action, appraisal)",
        "Relationship between cognition and emotion (separation vs integration; appraisal role)",
        "Emotion dynamics in daily life (intensity, variability, inertia, differentiation, lifespan differences)"
      ],
      "source_article": "Emotion",
      "x": 1.2825608253479004,
      "y": 1.037428379058838,
      "level": 2,
      "original_question_hash": "24c1fc5f"
    },
    {
      "question": "Dr. Marin is a PhD candidate working on a paper while holding a part-time tutoring job. She reports enjoying the research (intrinsic motivation), explicitly seeks a tenure-track job after graduation (conscious extrinsic goal), and discovers later that some of her drive also stems from wanting to impress a critical mentor (an unconscious motive). After receiving detailed, positive verbal feedback from her supervisor, her weekly research hours rise from $10$ to $20$ and she maintains that pace for $8$ weeks. In contrast, a one-time stipend increase of $\\$200 raises weekly hours from $10$ to $12$ but the increase fades within $2$ weeks. Which single explanation best integrates (a) how direction, intensity, and persistence shaped Dr. Marin's behavior, (b) the roles of intrinsic/extrinsic and conscious/unconscious motives, and (c) why a process-theory account is more explanatory for the observed short-term changes than a content-theory account?",
      "options": {
        "A": "Supervisor feedback increased perceived instrumentality and valence and raised Marin's expectancy that extra effort would yield valued outcomes (a process-theory explanation). That amplified the intensity (more hours) and persistence (sustained 8 weeks) toward the same research goal (direction). Her baseline intrinsic enjoyment and conscious extrinsic career goal provide stable content-level motives, while the unconscious need to impress the mentor modulates motivation but is less predictive of short-term change; content theories explain background needs but not the dynamic effort increases seen after feedback.",
        "B": "Maslow-style content theory best explains both effects: feedback and the stipend each satisfied higher-order esteem needs, so both increases should have sustained effects. Because intrinsic enjoyment is primary, any external reward simply reinforces an already-activated hierarchy, and unconscious motives are negligible.",
        "C": "Both behavioral changes are explained entirely by extrinsic reinforcement: the stipend and supervisor praise acted as rewards that directly increased effort via operant conditioning. Intrinsic motives and cognitive expectancies are irrelevant; persistence reflects only reinforcement strength proportional to reward magnitude.",
        "D": "Unconscious motives (the need to impress authority) are the dominant cause of all observed behavior. Conscious goals and process-level calculations (expectancy/instrumentality/valence) play little or no role; the temporary stipend effect occurred because it briefly primed unconscious achievement drives."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete PhD-research scenario with measured changes in weekly hours and durations to require application of direction/intensity/persistence, distinctions among intrinsic/extrinsic and conscious/unconscious motives, and comparison of content vs process theories (expectancy/instrumentality/valence). Option A integrates all three concepts; other options isolate or misapply theories.",
      "concepts_tested": [
        "Components of motivational states: direction, intensity, persistence",
        "Intrinsic vs extrinsic motivation and conscious vs unconscious motives; their influence on effort and persistence",
        "Content theories versus process theories (expectancy/instrumentality/valence) in explaining dynamic motivation changes"
      ],
      "source_article": "Motivation",
      "x": 1.3070313930511475,
      "y": 1.0286569595336914,
      "level": 2,
      "original_question_hash": "4556fd7e"
    },
    {
      "question": "Aurum Coffee sells a premium bag of beans for $5. Senior management wants to increase brand equity so they can justify a price increase to $5.50, cultivate stronger customer loyalty, and be more resilient to supply‑chain disruptions or product crises. Which of the following strategic programs best operationalizes the brand management principles described in the article — namely the coordinated management of tangible elements (look, price, packaging) and intangible elements (experiences, relationships) to raise perceived value, and the deliberate building of credibility through relationships with target markets and the supply chain?",
      "options": {
        "A": "Invest solely in a premium packaging redesign and new price point of $5.50 to signal higher quality; maintain current customer experience and supplier relationships unchanged.",
        "B": "Invest heavily in customer experience (barista training, loyalty app, in‑store events) and community engagement while keeping packaging and price unchanged at $5; do not alter supplier contracts or transparency.",
        "C": "Implement an integrated program: upgrade packaging and visual identity, improve customer experiences and loyalty programs, and institute supplier transparency and retailer training; communicate these changes clearly to customers and raise price to $5.50.",
        "D": "Respond to competitive pressure by cutting price to $4.50 and running short‑term promotions to increase volume, without investing in packaging, customer experience, or supplier relationships."
      },
      "correct_answer": "C",
      "generation_notes": "Created a concrete managerial scenario requiring synthesis of tangible/intangible brand elements, brand equity outcomes (price premium, loyalty), and relationship/supply‑chain credibility; four plausible strategic options with one integrated choice as correct.",
      "concepts_tested": [
        "Brand perception management (tangible vs intangible elements)",
        "Brand equity as mechanism (raising perceived value to justify price and loyalty)",
        "Relationship building and credibility with customers and supply chain for resilience"
      ],
      "source_article": "Brand management",
      "x": 1.3362760543823242,
      "y": 0.9693506360054016,
      "level": 2,
      "original_question_hash": "93ca5d47"
    },
    {
      "question": "An e-commerce company collects clickstream, purchase, returns, and customer-support logs. Its analytics team produces: (1) weekly KPI dashboards summarizing sales by category; (2) a post-holiday investigation that finds a pricing error caused a spike in returns; (3) a machine-learning demand forecast for each SKU for next quarter; and (4) an optimization routine that recommends reorder quantities and promotion allocations to maximize profit under capacity constraints. Which statement best characterizes (a) the mapping of these four activities to the standard analytics typology and (b) the relationship between analytics, data analysis, and data science?",
      "options": {
        "A": "Activities (1)–(4) map to descriptive, diagnostic, predictive, and prescriptive analytics respectively; analytics is an umbrella that integrates multiple data analysis processes and data-science techniques to explain why events happened and to inform future decisions.",
        "B": "Activities (1)–(4) map to diagnostic, descriptive, predictive, and cognitive analytics respectively; data analysis is broader than analytics and subsumes data science as one of its components.",
        "C": "Activities (1)–(4) map to predictive, prescriptive, diagnostic, and descriptive analytics respectively; analytics is strictly statistical computation and excludes optimization or automated decision systems (those belong to operations research).",
        "D": "All four activities are examples of prescriptive analytics because they can lead to actions; analytics and data analysis are synonymous terms with no hierarchical or functional distinction."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete e-commerce scenario mapping concrete tasks to the five analytics types and asked about hierarchical relationship between analytics, data analysis, and data science.",
      "concepts_tested": [
        "analytics typology: descriptive, diagnostic, predictive, prescriptive, cognitive",
        "relationship among analytics, data analysis, and data science",
        "mechanism of analytics: using data patterns to describe, interpret, predict, and inform decisions"
      ],
      "source_article": "Analytics",
      "x": 1.3747165203094482,
      "y": 1.0254343748092651,
      "level": 2,
      "original_question_hash": "3b16969e"
    },
    {
      "question": "In a mid-sized agricultural region, clinicians report increasing human cases of a bacterial infection genetically linked to cattle. The emergence coincides with recent deforestation and seasonal flooding that displaced wildlife into farmland. Local farmers routinely administer broad-spectrum antibiotics prophylactically to herds. As an interdisciplinary public-health adviser asked to design the initial response, which strategy best embodies the One Health approach while minimizing the risk of accelerating antimicrobial resistance and respecting local leadership?",
      "options": {
        "A": "Activate a coordinated One Health incident team (human public health, veterinary services, environmental agencies, and community representatives) to implement integrated surveillance across clinics, farms, and ecosystems; deploy targeted diagnostics and shared data platforms; suspend non-therapeutic prophylactic antibiotic use in livestock while instituting antimicrobial stewardship protocols for both human and veterinary treatment; carry out prioritized environmental remediation to reduce exposure pathways; and request technical support from global bodies (e.g., FAO/WHO/WOAH/UNEP) while ensuring country and community ownership of priorities.",
        "B": "Initiate immediate mass antibiotic prophylaxis for all exposed humans and all herds to suppress transmission quickly, coupled with rapid culling of any clinically affected animals, but defer environmental actions and international consultations to speed containment.",
        "C": "Enforce an immediate regional ban on all antibiotic use in animals and humans, mandate large-scale culling of livestock in affected zones, and request that international donors impose top-down control measures without engaging local leaders or stakeholders.",
        "D": "Focus the response solely on environmental interventions—reforesting affected areas, constructing flood barriers, and relocating wildlife corridors—while clinical and veterinary care continue business-as-usual with existing antibiotic practices and no integrated surveillance or data sharing."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a realistic outbreak scenario linking ecological change, zoonotic spillover, and routine prophylactic antibiotic use; options contrast a multi-sector One Health response (A) with common but problematic single-sector approaches (B–D) to test understanding of integrated governance, surveillance, and antimicrobial stewardship.",
      "concepts_tested": [
        "interdependence of human, animal, and environmental health",
        "integrated multi-sector collaboration and governance (One Health, Quadripartite involvement)",
        "shared physiology/pharmacology and implications for antimicrobial stewardship and resistance"
      ],
      "source_article": "One Health",
      "x": 1.2595486640930176,
      "y": 0.8702399134635925,
      "level": 2,
      "original_question_hash": "38d1cec0"
    },
    {
      "question": "Alex and Priya are in a committed romantic relationship. Priya has accepted a job 400 miles away; for the next three months they alternate weekend visits with weekday video calls. Alex becomes increasingly uncertain about whether the relationship will endure and feels torn between wanting closeness and wanting to respect Priya's autonomy. Which communicative approach best integrates verbal and nonverbal cues to reduce Alex's uncertainty, manage information responsibly, and negotiate the autonomy–connection tension described by relational dialectics?",
      "options": {
        "A": "Rely primarily on asynchronous texting with frequent affectionate emojis and occasional vague affirmations (\"miss you\", \"we'll see\"); avoid revealing fears to protect autonomy; when together, let nonverbal affection (touch, proximity) speak for commitment.",
        "B": "Adopt interactive uncertainty-reduction strategies: engage in explicit self-disclosure about fears and expectations during synchronous video calls, attend to and comment on nonverbal signals (tone, facial expression, posture) to clarify meaning, and co-create mutually acceptable boundary rules (communication frequency, visiting plans) that balance autonomy and connection.",
        "C": "Use passive strategies: observe Priya's online activity and visiting behavior to infer commitment, formulate contingency plans alone (e.g., consider ending the relationship if visits decline), and limit face-to-face negotiation to avoid appearing needy.",
        "D": "Emphasize nonverbal mirroring and symbolic gestures (surprise gifts, matching social media posts) while minimizing verbal discussion; assume that shared symbols and implicit norms will maintain closeness without explicit talk about expectations or uncertainty."
      },
      "correct_answer": "B",
      "generation_notes": "Created a relocation scenario requiring integration of verbal/nonverbal coordination, uncertainty-reduction strategies (passive/active/interactive), and negotiation of relational dialectics (autonomy vs connection); option B maps to interactive strategies, coordinated meaning-making, and explicit boundary management.",
      "concepts_tested": [
        "integration of verbal and nonverbal cues to achieve relational goals",
        "uncertainty reduction/information-management strategies (passive/active/interactive)",
        "relational dialectics (autonomy vs connection) and boundary negotiation"
      ],
      "source_article": "Interpersonal communication",
      "x": 1.2517296075820923,
      "y": 1.0049691200256348,
      "level": 2,
      "original_question_hash": "ebbe4307"
    },
    {
      "question": "A mid-sized city reports an above-average prevalence of language delays and emotional regulation problems among 2–5 year-olds. Contributory factors identified in case files include a high rate of maternal depressive symptoms, persistent poverty, and a documented family history of language impairment in about 20% of affected children. The city must design an early-intervention policy consistent with contemporary developmental science (i.e., the interaction of maturation and environment, the importance of stable caregiver relationships, and the use of developmental theories to guide practice). Which of the following policy packages best aligns with those principles and is therefore most likely to optimize children’s cognitive, language and socioemotional outcomes?",
      "options": {
        "A": "Fund a city-wide genetic screening program at birth to identify children at risk of language impairment and then enroll identified infants into standardized language tutoring beginning at age 3; focus resources on individualized remediation driven by test results.",
        "B": "Implement a multi-component program that (1) provides low-threshold maternal mental-health treatment and home-visiting to promote sensitive caregiving, (2) expands access to high-quality, language-rich early childcare staffed by trained providers who use scaffolding techniques, and (3) funds regular developmental screening with referrals—guided by ecological and attachment-informed frameworks to intervene across family and community contexts.",
        "C": "Mandate a strictly age-based curriculum in all preschools derived from stage models (e.g., Piaget), grouping children by chronological age and delivering identical, stage-targeted lessons without changes for family circumstances or caregiver support.",
        "D": "Allocate resources to build a single centralized early-learning center that offers intensive academic drills (phonics and rote vocabulary) for 9 months a year, assuming that intensive exposure alone will overcome both familial instability and biological risk factors."
      },
      "correct_answer": "B",
      "generation_notes": "Created a realistic municipal policy-choice scenario requiring integration of nature–nurture interaction, caregiver stability/attachment, and the application of developmental theories (Bronfenbrenner, Vygotsky, Piaget, attachment). Option B synthesizes interventions across levels and theory; other options isolate single causes or rely on incomplete theory application.",
      "concepts_tested": [
        "Interaction of genetic maturation and environmental factors in development",
        "Role of caregiver/parental stability and sensitive caregiving on socioemotional and language development",
        "Use of developmental theories/models (ecological systems, attachment, Vygotsky/Piaget) to design multi-level educational and policy interventions"
      ],
      "source_article": "Child development",
      "x": 1.330277442932129,
      "y": 1.025478482246399,
      "level": 2,
      "original_question_hash": "a2de00e2"
    },
    {
      "question": "A mid-sized coastal city must choose between building a reinforced concrete floodwall or restoring 150 hectares of coastal wetlands to reduce storm surge risk. As an environmental studies consultant team, you must design a comprehensive assessment framework that captures the interdisciplinary nature of the problem, the human–environment relationships, and the role of environmental education and ethics. Which of the following assessment frameworks best embodies the principles of environmental studies?",
      "options": {
        "A": "Conduct hydrological modeling and biodiversity surveys to quantify physical risk and ecosystem function; perform an economic valuation of ecosystem services and a cost–benefit analysis using discounting (e.g., $PV=\\sum_{t=1}^T \\frac{ES_t}{(1+r)^t}$) to compare long-term benefits; carry out stakeholder mapping, policy and land‑use analysis, and urban planning scenarios; assess distributive justice and ethical implications of each option; and design a public education and outreach program with monitoring and adaptive management.",
        "B": "Prioritize an engineering optimization study that minimizes construction and maintenance costs of the floodwall, using life‑cycle costing and technical risk assessment, and implement the option with the lowest upfront and present costs; public outreach is limited to informational notices about construction timelines.",
        "C": "Focus the assessment on legal and market instruments: verify regulatory compliance, design tradable permit schemes or insurance adjustments to internalize flood risk, and rely on market valuations (e.g., willingness to pay) without conducting detailed ecological surveys or long‑term educational programs.",
        "D": "Emphasize a community‑led education campaign to change behaviors about land use and flood preparedness, prioritize ethical narratives about nature, and use qualitative community workshops to guide decisions, while deferring rigorous hydrological, ecological, and economic analyses to future phases."
      },
      "correct_answer": "A",
      "generation_notes": "Created a coastal floodwall vs. wetland restoration scenario requiring integration of physical science, economics (with present value formula), social policy/planning, and education/ethics; distractors omit one or more core disciplinary components.",
      "concepts_tested": [
        "Interdisciplinary integration of physical sciences, economics, social sciences, and humanities",
        "Human–environment relationships in built and natural systems and policy implications",
        "Role of environmental education and environmental ethics in decision making"
      ],
      "source_article": "Environmental studies",
      "x": 1.4114199876785278,
      "y": 0.9131346940994263,
      "level": 2,
      "original_question_hash": "9e7b5de7"
    },
    {
      "question": "Dr. Lee proposes hypothesis H: \"Administering compound X to cultured cells increases enzyme Z activity by 50% relative to untreated controls.\" She frames a specific prediction: mean enzyme activity will rise from a baseline of 100 units to 150 units. In an initial experiment (n = 6 per group) she observes a treated-group mean of 120 units versus 100 units for controls, with a two‑sided test returning $p=0.04$. Which of the following next steps best exemplifies adherence to the core principles of the scientific method (falsifiability and testable predictions, iterative/non‑linear inquiry, and empirical observation combined with rigorous skepticism)?",
      "options": {
        "A": "Recognize the result provides some support but is not definitive; design follow‑up studies that aim to potentially falsify H (e.g., larger sample size to increase power, blinded assays and vehicle controls, dose–response and different assay conditions), pre‑register protocols, publish methods and raw data so others can attempt replication, and revise the hypothesis as new data accumulate.",
        "B": "Treat the $p=0.04$ result as conclusive proof that H is true, publish immediately claiming compound X raises enzyme Z by 50%, and focus next on proposing molecular mechanisms without further experiments or independent replication.",
        "C": "Conclude H is falsified because the observed mean (120) is less than the predicted 150, stop investigating X entirely, and recommend redirecting funds to unrelated compounds.",
        "D": "Abandon hypothesis‑driven work; assemble a large panel of compounds and run thousands of post‑hoc tests on enzyme Z without pre‑specified predictions, then highlight any compound with $p<0.05$ as a discovery without adjustments for multiple comparisons or controls for bias."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete experimental vignette with numerical prediction and statistical outcome; options contrast proper scientific practice (falsifiable predictions, iterative testing, blinding, pre‑registration, data sharing, replication) against common methodological errors (premature confirmation, false falsification from a single inconclusive test, and data dredging).",
      "concepts_tested": [
        "Falsifiability and testable predictions",
        "Iterative and non‑linear nature of scientific inquiry (prediction → test → revision)",
        "Empirical observation combined with skepticism to mitigate cognitive biases (blinding, controls, replication, pre‑registration)"
      ],
      "source_article": "Scientific method",
      "x": 1.3085145950317383,
      "y": 1.052701473236084,
      "level": 2,
      "original_question_hash": "7da007cf"
    },
    {
      "question": "The Republic of Arcadia's constitution recognizes 10 provinces as the highest-level subnational units; each province is regularly subdivided into cantons. The coastal city of Vela lies within the boundaries of one province but is granted broad municipal legislative powers. The island of Norra lies outside Arcadia's metropolitan territory, has its own local government, and its foreign affairs are handled directly by Arcadia's central government. The northern territory of Karg is designated an \"autonomous region\" by statute, with competence to legislate on education and certain taxes. Which of the following statements best characterizes the hierarchy, nomenclature variability, and relational categories of these units consistent with principles of administrative divisions?",
      "options": {
        "A": "Provinces are Arcadia's first-level administrative divisions and cantons are second-level; Vela is a subnational administrative unit (a municipality) with devolved powers but remains part of a province; Norra is a dependent territory (not an integral first- or second-level unit) because its external affairs are controlled centrally; Karg is an autonomous region (a form of asymmetric autonomy), not necessarily a federated or constituent state.",
        "B": "Because Vela has broad legislative powers it ceases to be an administrative subdivision and should be classified as a first-level division equal to provinces; Norra is a first-level administrative division because it has its own local government; Karg is necessarily a federated state since it can legislate on taxes.",
        "C": "All territorial units described (provinces, cantons, Vela, Norra, Karg) must be counted as first-level administrative divisions in any comparative scheme because international standards require a single principal tier of subdivision per country; dependent territories such as Norra are always considered equivalent to provinces for statistical purposes.",
        "D": "Provinces and cantons are informal geographic regions with no administrative authority; Vela's municipal powers make it a dependent territory; Norra is an autonomous region since Arcadia handles only its external affairs; Karg is a lower-level municipality because taxation power is limited to local fees."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete multi-tiered scenario (Arcadia) to test hierarchy (first/second level), nomenclature variability (province/canton/municipality), and relational categories (dependent territory vs autonomous region vs federated state).",
      "concepts_tested": [
        "hierarchical structure of administrative divisions (first-level vs second-level)",
        "variation in nomenclature and levels across countries",
        "difference between dependent territories, federated states, and autonomous regions"
      ],
      "source_article": "Administrative division",
      "x": 0.6390641927719116,
      "y": -0.02017737179994583,
      "level": 2,
      "original_question_hash": "9ffab06f"
    },
    {
      "question": "State X is offered a federal grant conditional on increasing 8th-grade math proficiency by 10 percentage points on a national assessment within three years. State law gives districts broad autonomy over school organization and staffing. District A proposes converting 6 of its 12 low-performing public schools into privately managed charter schools, reducing class sizes in those schools from 30 to 20, introducing ability-based tracking in grades 6–8, and funding these changes by reallocating money away from district-wide remedial programs and outreach to vulnerable students. As an education policy analyst asked to evaluate the proposal, which assessment best integrates (1) the multi-level governance dynamics that will shape implementation, (2) clear policy objectives, instruments and a measurement strategy, and (3) the primary trade-offs and value conflicts the proposal raises?",
      "options": {
        "A": "Conclude the proposal is optimal because district autonomy will allow rapid implementation: the objective is solely to meet the federal test target, the instruments (charter contracting, class-size reduction, tracking) should increase average scores quickly, and measurement can rely primarily on the mandated national assessment; equity impacts are secondary and can be ignored if the federal target is achieved.",
        "B": "Recommend a mixed-methods evaluation acknowledging that federal conditional funding shapes state incentives, but local autonomy determines district implementation. State and district responsibilities should be mapped explicitly; policy objectives must include both attainment (math scores) and equity/access. Instruments (private contracting, class-size reductions, tracking, reallocation of remedial funds) should be modelled for their causal effects using quasi-experimental designs and cost-effectiveness analysis; measurement must combine disaggregated standardized-test results, enrollment and mobility statistics, resource-allocation metrics, teacher retention data, and qualitative case studies. Highlight trade-offs: potential short-term score gains versus risks of increased segregation, reduced support for vulnerable students, and accountability gaps with private managers.",
        "C": "Advise rejecting charter conversions because only the state can set educational goals; recommend the state unilaterally redistributes federal funds to the district to hire more teachers everywhere rather than allowing district decisions. Measurement should focus on inputs (per-pupil spending) rather than outcomes, since inputs determine quality.",
        "D": "Advise proceeding but replacing ability-based tracking with a universal accelerated math curriculum; measure success using district internal benchmarks and teacher evaluations. Assume redistribution away from remedial programs is acceptable because smaller classes in charters will compensate; do not prioritize disaggregated outcome metrics because aggregate district averages will reflect overall improvement."
      },
      "correct_answer": "B",
      "generation_notes": "Created a concrete district-level scenario linking federal conditional grants, state/district autonomy, and a local proposal (charters, class-size reduction, tracking). Options vary in attention to governance mapping, choice of instruments and measurement methods, and recognition of equity trade-offs; correct option integrates multi-level governance, mixed-methods measurement, and trade-off analysis.",
      "concepts_tested": [
        "Multi-level governance interactions (federal, state, district)",
        "Policy objectives, instruments, and mixed-methods measurement strategies",
        "Trade-offs and value conflicts (attainment vs equity, privatization, tracking, resource allocation)"
      ],
      "source_article": "Education policy",
      "x": 1.2004565000534058,
      "y": 0.8712193369865417,
      "level": 2,
      "original_question_hash": "de3794c3"
    },
    {
      "question": "You are designing a study to explain why real wages in the fictional Midlands textile region fell by \\(15\\%\\) between 1820 and 1870 even though measured output per worker rose. Which research design and interpretive stance best exemplifies the interdisciplinary integration, institutional focus, and plural theoretical framing characteristic of contemporary economic history?",
      "options": {
        "A": "Combine a cliometric time-series regression (e.g. \\(w_t=\\alpha+\\beta y_t+\\gamma X_t+\\varepsilon_t\\) where \\(w_t\\) = real wage, \\(y_t\\) = output per worker, and \\(X_t\\) includes measures of labour laws, property rights changes and currency reforms) with systematic archival work (factory records, court cases, union minutes, letters). Use that mixed evidence to evaluate competing explanations: technological substitution and changing relative factor prices (neoclassical/cliometric), employer power and proletarianization (Marxian), and institutional shifts in contract enforcement and credit (institutional/New economic history), highlighting path dependence in institutional change.",
        "B": "Rely primarily on a cliometric econometric analysis of newly digitized wage and output series, using instrumental variables to identify causal effects of mechanization. Conclude technical change caused wage decline because the coefficient on \\(y_t\\) is negative and statistically significant; treat archival anecdotes as secondary illustrations but not central to identification.",
        "C": "Produce a narrative history based on rich archival sources (letters, court records, local newspapers) documenting factory discipline, declining bargaining power and cultural attitudes toward labour. Emphasize qualitative causal inference and institutional description, but avoid formal econometric testing of wage-output relationships to prevent anachronistic modelling.",
        "D": "Adopt a strictly Marxian analysis that interprets the wage decline as evidence of intensified exploitation due to capital accumulation and class struggle. Use qualitative class analysis and selective quantitative aggregates (total profits, labour headcount) to demonstrate the capitalist mode of production explains the entire episode without reference to alternate theories or formal statistical testing."
      },
      "correct_answer": "A",
      "generation_notes": "Created an applied research-design multiple choice question contrasting mixed-method, institution-focused, plural-theory approaches (correct) with mono-method or mono-theory alternatives to test integration, institutional dynamics, and theoretical pluralism.",
      "concepts_tested": [
        "Interdisciplinary methodological integration (quantitative econometrics + qualitative archives)",
        "Institutions and dynamic economic structure (property rights, laws, contracts, path dependence)",
        "Diverse theoretical frameworks shaping analysis (cliometrics, Marxian, institutional interpretations)"
      ],
      "source_article": "Economic history",
      "x": 1.169308066368103,
      "y": 0.9472801089286804,
      "level": 2,
      "original_question_hash": "62abe859"
    },
    {
      "question": "Consider steady flow of a dilute gas through a straight microchannel of height $H=0.5\\ \\text{mm}$. At operating conditions the molecular mean free path is $\\lambda=0.15\\ \\text{mm}$ and the mean flow speed is $U=10\\ \\text{m/s}$. Which of the following statements is most accurate regarding the applicability of the continuum assumption, the Reynolds transport theorem, and the Navier–Stokes equations for modelling this flow?",
      "options": {
        "A": "The Knudsen number is $Kn=\\lambda/H=0.3$, which places the flow in the transition/slip regime. The microscopic conservation laws (mass, momentum, energy) remain fundamental, but the continuum assumption and the classical Reynolds transport theorem (as used to derive differential Navier–Stokes with no‑slip constitutive relations) are no longer strictly valid; kinetic descriptions (Boltzmann equation or DSMC) or modified continuum models with appropriate slip/correction terms are required — standard no‑slip Navier–Stokes CFD is not strictly applicable.",
        "B": "The Knudsen number is $Kn=\\lambda/H=0.03$, so the continuum assumption holds; incompressible Navier–Stokes with classical no‑slip boundary conditions applies (since $M\\approx 0.03$) and one can expect closed‑form solutions for this simple microchannel geometry.",
        "C": "Because $Kn=0.3$ the continuum laws fail and therefore conservation of mass, momentum and energy no longer apply; neither Navier–Stokes nor kinetic theory can be used for prediction in this regime.",
        "D": "Although $Kn=0.3$, the low Mach number and moderate Reynolds number mean the Navier–Stokes equations remain valid in their usual continuum form if one simply enforces a no‑slip condition; the Reynolds transport theorem continues to hold without modification."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a microchannel scenario with given mean free path to compute Knudsen number and test understanding of continuum assumption breakdown, the continued primacy of conservation laws (but need for kinetic models), and the limited applicability of classical Navier–Stokes/RTT with no‑slip.",
      "concepts_tested": [
        "Conservation laws as foundational (mass, momentum, energy) and their microscopic validity",
        "Continuum assumption and Knudsen number criterion for its validity",
        "Applicability and limitations of Navier–Stokes equations and need for kinetic/CFD approaches when continuum breaks down"
      ],
      "source_article": "Fluid dynamics",
      "x": 1.752428650856018,
      "y": 1.0737817287445068,
      "level": 2,
      "original_question_hash": "f4a642ae"
    },
    {
      "question": "Consider steady transport in a Newtonian fluid where momentum, heat, and a dilute species are being transferred in a boundary layer. Which of the following best separates the roles of conservation/continuity equations and constitutive relations, and correctly describes why heat, mass and momentum problems can be mapped onto one another mathematically?",
      "options": {
        "A": "Conservation/continuity provide local balance equations (e.g. $\\frac{\\partial \\phi}{\\partial t}+\\nabla\\cdot J_{\\phi}=0$) that enforce conservation of mass, momentum, or energy; constitutive relations close those balances by expressing fluxes in terms of driving forces (e.g. Fick: $J_{A}=-D_{AB}\\nabla C_{A}$, Fourier: $q=-k\\nabla T$, Newtonian viscous stress in simplified form: $\\tau=-\\mu\\nabla v$). The unifying framework is the linear diffusive form linking a flux to a gradient (with diffusivities $\\nu=\\mu/\\rho$, $\\alpha$, $D$), which allows analogies via nondimensional groups (Pr, Sc, Re) to map heat↔mass↔momentum problems.",
        "B": "The Navier–Stokes equations are purely constitutive flux–gradient laws analogous to Fourier and Fick (so conservation laws are separate and only apply to scalars); thus one can treat Navier–Stokes like $q=-k\\nabla T$ and dispense with separate momentum conservation in many problems.",
        "C": "Continuity equations alone determine the fluxes $J$ uniquely (for example $\\frac{\\partial \\phi}{\\partial t}+\\nabla\\cdot J_{\\phi}=0$ gives $J$ directly), so constitutive relations such as Fourier's or Fick's laws are redundant; the analogy comes solely from identical conservation equations.",
        "D": "The mathematical analogy between heat, mass, and momentum transport always holds without limitation, so one may always substitute $k\\leftrightarrow D\\leftrightarrow\\mu/\\rho$ and predict any flux from any other even in turbulent, multicomponent, or reactive flows."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete boundary-layer scenario and presented choices that distinguish continuity (conservation) from constitutive flux–gradient laws, and highlight the diffusive analogy and its limits; distractors assert common misconceptions (Navier–Stokes as only constitutive, conservation suffices to get flux, analogy always valid).",
      "concepts_tested": [
        "Conservation laws and continuity equations",
        "Constitutive relations (Fourier, Fick, Newtonian viscosity)",
        "Unifying mathematical framework and analogy among mass, momentum, and heat transport"
      ],
      "source_article": "Transport phenomena",
      "x": 1.7898527383804321,
      "y": 1.063521385192871,
      "level": 2,
      "original_question_hash": "7e347186"
    },
    {
      "question": "A mid-sized regional hospital has just completed a merger and, over six months, has experienced a 28% increase in nurse turnover, rising patient complaints about continuity of care, and declining staff morale. Hospital leadership hires an OD consultant to design a change initiative. According to organization development principles (including the action-research cycle, the core levers of climate/culture/strategy, and a stakeholder-driven interdisciplinary consultant–client relationship), which of the following plans best reflects an appropriate OD approach to reduce turnover and restore performance?",
      "options": {
        "A": "Begin with a joint, stakeholder-led diagnosis (senior leaders, nurse representatives, HR, physicians, and union reps), collect mixed-methods data (surveys on organizational climate, focus groups probing cultural norms, workflow observations), feed results back in open sessions, co-design pilot interventions (team-building, local decision-making authority changes, leadership coaching), implement pilots with clear outcome metrics (turnover rate, patient continuity scores, climate indices), and iterate through short action-research cycles using behavioral-science and systems-thinking tools.",
        "B": "Have the external consultant prescribe a standardized, hospital-wide workflow and performance checklist, mandate a 2-week compliance training for all nursing staff, link merit pay to adherence scores, and evaluate success after 12 months solely by comparing turnover and checklist compliance rates.",
        "C": "Install a sophisticated scheduling and shift-optimization software, tighten attendance policies, and roll out online competency modules for nurses; assume improved operational efficiency will reduce turnover and measure only vacancy and overtime costs as indicators of success.",
        "D": "Run multiple off-site laboratory (T-group) trainings using mixed-organization stranger groups to enhance general interpersonal skills, without performing an on-site organizational diagnosis or involving hospital stakeholders in intervention design; assess outcomes via participant self-reports six months later."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a merger scenario requiring application of action-research cycles, selection of levers (climate, culture, strategy), stakeholder collaboration, and interdisciplinary methods; provided plausible alternative plans that violate OD principles.",
      "concepts_tested": [
        "action research/planned change cycle (planning, action, measurement)",
        "organizational levers: climate, culture, strategy and their role in change",
        "stakeholder-driven collaborative consultant–client process and interdisciplinary foundations of OD"
      ],
      "source_article": "Organization development",
      "x": 1.3194867372512817,
      "y": 1.0051674842834473,
      "level": 2,
      "original_question_hash": "6a94963d"
    },
    {
      "question": "Two manufacturing plants produce the same widget. Their unit cost follows an experience curve C(N)=C_0 N^{-\\alpha}, where N is cumulative units produced and \\alpha is the plant's learning rate. Plant X has \\alpha=0.08 and management wants to raise it (steepen the learning curve so costs fall faster with experience). Which set of interventions is most likely to increase \\alpha, based on organizational learning theory?",
      "options": {
        "A": "Introduce a formal apprenticeship and on‑the‑job rotation program to build individual proficiency and tacit knowledge transfer; invest in improved tooling and automation; and codify shop‑floor routines along with a knowledge repository to retain and diffuse improvements.",
        "B": "Reduce the workforce and compress shift schedules so remaining workers produce more units per hour; eliminate documentation to force informal tacit learning; and stop interplant visits to concentrate learning locally.",
        "C": "Outsource a competitor benchmarking study and buy external best‑practice reports (exogenous knowledge); postpone internal training; and leave existing routines unchanged to avoid disruption.",
        "D": "Replace all hands‑on training with lengthy written manuals and online modules to convert tacit knowledge into documents; halt production for a month to implement the manuals; then resume with the same organizational structure."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a quantitative experience‑curve scenario $C(N)=C_0N^{-\\alpha}$ and asked which interventions increase learning rate. Options contrast improvements to individual proficiency, technology, routines and retention (A) vs plausible but counterproductive or incomplete alternatives (B–D).",
      "concepts_tested": [
        "Learning curves and determinants of learning rates (individual proficiency, technology, structure)",
        "Knowledge creation, transfer, and retention (tacit vs explicit, repositories, training)",
        "Levels of learning (individual, group/organizational, inter‑organizational transfer)"
      ],
      "source_article": "Organizational learning",
      "x": 1.3036391735076904,
      "y": 0.989583432674408,
      "level": 2,
      "original_question_hash": "cea63b18"
    },
    {
      "question": "A developmental psychologist wants to explain why two 12-year-olds attending the same public middle school show divergent trajectories in academic engagement over two years. Child A lives in a low-income inner-city household where a large extended family provides daily caregiving; Child B lives in an affluent gated community with parents who work long hours and rely on paid tutors. The researcher is deciding among study designs and interpretations. Which of the following approaches or conclusions best embodies Bronfenbrenner's ecological systems perspective (including the bioecological revision) regarding (1) contextual configuration, (2) bidirectional person–environment processes, and (3) ecological validity?",
      "options": {
        "A": "Conclude that socioeconomic status (SES) is the primary explanatory variable — treat SES as a stable \"social address\" and compare the children's trajectories using standardized demographic categories; recommend a cross-sectional survey of SES indicators and academic scores to attribute differences to SES.",
        "B": "Design a laboratory experiment in which both children complete standardized attention and memory tasks with unfamiliar adults to assess universal cognitive processes; generalize these lab-based performance differences to explain their real-world academic engagement.",
        "C": "Conduct a longitudinal, naturalistic study that samples multiple microsystems and their interconnections (home caregiving, school, peer groups), measures exosystem influences (parents' work schedules), tracks sociohistorical events (chronosystem), and models bidirectional effects (how each child's temperament alters parenting and peer selection while those contexts reshape the child).",
        "D": "Implement a single large-scale macrosystem intervention (e.g., a citywide educational policy change) and infer that any subsequent convergence or divergence in the two children's academic engagement will demonstrate the primary causal role of cultural-level factors, without requiring proximal longitudinal observation."
      },
      "correct_answer": "C",
      "generation_notes": "Created a vignette requiring selection of the study design/interpretation that aligns with Bronfenbrenner: emphasis on nested systems, bidirectional processes, and ecological validity; distractors reflect common mistakes (social-address attribution, lab-only inference, macrosystem-only intervention).",
      "concepts_tested": [
        "Contextual systems and their configurations (microsystem, mesosystem, exosystem, macrosystem, chronosystem)",
        "Bidirectional person–environment processes (active agency of the developing person)",
        "Ecological validity and preference for naturalistic/quasi-experimental designs"
      ],
      "source_article": "Ecological systems theory",
      "x": 1.2540335655212402,
      "y": 1.0069547891616821,
      "level": 2,
      "original_question_hash": "58f67d83"
    },
    {
      "question": "Country X currently relies on cheque clearing and an overnight ACH that nets transactions and settles the following business day. Policy makers plan to (1) implement an RTGS for large-value interbank transfers, (2) retain ACH for low-value bulk payments, (3) roll out an interoperable instant retail payment rail and expand debit/credit card acceptance, and (4) migrate messaging to an international standard to facilitate cross-border connectivity. Which of the following best describes the most likely combination of effects on settlement risk, liquidity management, interoperability, and macroeconomic outcomes?",
      "options": {
        "A": "Settlement risk for large-value payments will rise because RTGS processes individually; ACH will have no settlement risk after the reform; interoperable instant payments and card expansion will increase cash use; standardization will fragment international links; macroeconomically, transaction costs will increase and resource allocation will worsen.",
        "B": "RTGS will reduce settlement risk for large-value transactions by providing real-time, final settlement; ACH will continue to entail settlement (netting) risk due to delayed net settlement; interoperable instant and card systems will accelerate the shift from cash to cash-substitutes and raise payment volumes (including cross-border activity); standardization will improve interoperability and allow networks to scale internationally; macroeconomic effects will include lower transaction costs, improved resource allocation and financial development, but higher intraday liquidity demands and a need to strengthen operational resilience to avoid systemic disruption.",
        "C": "RTGS will eliminate all counterparty and systemic risk in the payment system; ACH will become instantaneous and therefore identical to RTGS; card and instant payments will reduce demand deposits uniformly and therefore reduce banks' need for liquidity; adopting international messaging standards will automatically eliminate foreign-exchange frictions; overall the economy will see no change in allocation efficiency but will face higher regulatory costs.",
        "D": "Implementing RTGS will slow down large-value payments because gross processing is slower than netting; ACH will become the preferred rail for urgent, high-value activity; expanding cards and instant payments will have negligible effect on cash usage or cross-border trade; standardization primarily increases operational costs with no interoperability benefits; the macroeconomic consequence will be marginal changes to transaction costs and risk-sharing."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a scenario-based multiple-choice question contrasting RTGS and ACH settlement characteristics, the effects of replacing cash with cash-substitutes and interoperable standards, and the resulting macroeconomic implications (efficiency, liquidity needs, systemic/operational risk). Distractors contain plausible but incorrect combinations.",
      "concepts_tested": [
        "Settlement mechanisms: RTGS vs ACH (real-time gross vs net settlement and settlement risk)",
        "Shift from cash to cash-substitutes and role of standardization in interoperability and global payment networks",
        "Macroeconomic impacts of payment-system efficiency and stability (transaction costs, resource allocation, liquidity and systemic risk)"
      ],
      "source_article": "Payment system",
      "x": 1.3572007417678833,
      "y": 0.9163758754730225,
      "level": 2,
      "original_question_hash": "2d6c020a"
    },
    {
      "question": "The city of Alderbrook commissions a policy task force to reduce chronic homelessness compounded by opioid addiction. Four proposals are on the table. Which single proposal best typifies 'social policy' as characterized in the article — i.e., an interdisciplinary, applied study of societies' responses to social need that may be seen as a holistic approach within or alongside public policy and that uses legislation, guidelines and activities to shape distribution and access to goods and resources when addressing a complex (wicked) social problem?",
      "options": {
        "A": "A cross-sector task force composed of social policy researchers, public health professionals, economists, legal scholars and urban planners recommends (1) a municipal ordinance guaranteeing access to supported housing prioritized by need, (2) expanded eligibility for subsidized healthcare and integrated addiction services, (3) a jobs-and-skills program linked to housing provision, and (4) new laws removing barriers that criminalize homelessness; the plan combines legislative reform, universal and targeted services, pilot evaluations and coordinated administrative practices.",
        "B": "A fiscal-policy team led by economists proposes a strictly means-tested cash-transfer program targeted to those judged most 'work-ready', accompanied by time-limited emergency shelter vouchers and a performance metric system focused on short-term cost–benefit ratios and rapid re-employment rates.",
        "C": "The city contracts private social enterprises to run market-based solutions: housing vouchers redeemable on the private market, incentives for landlords, and deregulation to encourage private clinics to offer addiction treatment without new public service provision or legislation.",
        "D": "Public health officials treat the problem primarily as a clinical issue and propose opening specialized addiction treatment clinics and expanding outpatient mental-health services, without changes to housing policy, labor programs, or legal frameworks that affect access to resources."
      },
      "correct_answer": "A",
      "generation_notes": "Created a municipal scenario with four distinct policy designs; option A embeds interdisciplinary, legislative and distributive mechanisms and frames the problem holistically, matching the article's definition of social policy.",
      "concepts_tested": [
        "Social policy as interdisciplinary, applied responses to social need",
        "Relationship between social policy and public policy (holistic vs narrow/subset)",
        "Mechanisms by which social policy shapes distribution and access through legislation, guidelines and services to address wicked problems"
      ],
      "source_article": "Social policy",
      "x": 1.2129929065704346,
      "y": 0.8918383717536926,
      "level": 2,
      "original_question_hash": "34c554cf"
    },
    {
      "question": "Novaland is a civil-law country with specialized administrative courts. Its National Environmental Agency (NEA) is statutorily empowered to (1) promulgate binding environmental regulations, (2) resolve permit disputes in internal adjudicative panels, and (3) impose administrative fines for non-compliance. Novaland is also a signatory to a regional treaty (the Regional Treaty on Environmental Standards, RTES) that sets minimum environmental rules and establishes a regional coordinating administrative body which issues interpretive guidelines but lacks direct sanctioning power. The NEA adopts a regulation that is stricter than the RTES minima and, after its internal panel finds a factory liable, imposes a substantial fine. The factory appeals to Novaland’s specialised administrative court arguing that (a) the NEA exceeded its statutory authority, (b) the fine is disproportionate, and (c) the NEA’s rule conflicts with RTES norms. Which outcome best reflects the interaction of an agency’s tripartite functions, the role of specialized administrative courts in a civil-law system, and the influence of supranational legal orders?",
      "options": {
        "A": "The specialised administrative court will review both the NEA’s internal adjudication and the substantive legality of the regulation; it can set aside the fine as ultra vires if the agency exceeded delegated authority or violated principles such as proportionality, and it will interpret domestic law in light of RTES guidance—potentially requiring the NEA to reconcile its rule with the treaty and prompting administrative coordination with the regional body.",
        "B": "Because the NEA performed adjudication and enforcement functions, its internal panel’s decision is final and immune from judicial review in Novaland; domestic courts cannot overturn agency fines, and RTES guidance is only persuasive for the legislature, not binding on agencies or courts.",
        "C": "The specialised administrative court will confine review to procedural fairness of the NEA’s internal hearing (e.g., notice and impartiality) and will not evaluate the substance of the regulation or the proportionality of the fine; supranational RTES norms cannot be applied by national administrative courts unless expressly transposed by statute.",
        "D": "The court will dismiss the appeal because an agency exercising the tripartite powers may promulgate stricter standards than RTES and enforce them without interference; any conflict with RTES must be resolved by the regional administrative body, which may directly fine the factory, bypassing national judicial review."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a comparative hypothetical showing an agency exercising rulemaking, adjudication, and enforcement; tested judicial review by specialized administrative courts in civil-law systems and supranational influence on domestic administrative law and agency behavior.",
      "concepts_tested": [
        "Tripartite functions of administrative agencies (rulemaking, adjudication, enforcement)",
        "Role of specialized administrative courts and judicial review in civil-law systems",
        "Influence of supranational legal orders on domestic administrative law and administrative coordination"
      ],
      "source_article": "Administrative law",
      "x": 1.2173277139663696,
      "y": 0.7963391542434692,
      "level": 2,
      "original_question_hash": "75dfb2d6"
    },
    {
      "question": "A cognitive neuroscience research group studies a patient who, after a stroke, has a lesion confined to the left posterior superior temporal gyrus and now shows severe deficits in understanding spoken language. In parallel, fMRI experiments in healthy participants show consistent activation in that same region during sentence comprehension tasks. Finally, a computational model of the language network simulates the lesion by removing a node representing that region and reproduces the comprehension deficits observed in the patient. Which conclusion is best supported by these three lines of evidence?",
      "options": {
        "A": "The left posterior superior temporal gyrus is a necessary component of a neural circuit for language comprehension; the converging lesion, neuroimaging, and computational-model evidence exemplifies how cognitive neuroscience integrates causal, correlational, and theoretical approaches to explain cognitive function.",
        "B": "Because fMRI only provides correlational data, these results cannot support any causal inference about the left posterior superior temporal gyrus and language comprehension; only the lesion study is relevant and it is insufficient to implicate that specific region.",
        "C": "The findings show that language comprehension is entirely localized to the left posterior superior temporal gyrus, so other brain areas cannot contribute meaningfully to this cognitive function.",
        "D": "The patient’s deficit must reflect nonspecific effects of brain injury (e.g., diaschisis or general cognitive decline), so the convergent imaging and modeling data are likely coincidental and do not indicate that the region participates in the language network."
      },
      "correct_answer": "A",
      "generation_notes": "Created a scenario combining a focal lesion (lesion study), fMRI activation (correlational evidence), and a computational model reproducing deficits to test inference about causal role and interdisciplinarity in cognitive neuroscience.",
      "concepts_tested": [
        "Neural circuits implement cognition and specific regions contribute to functions",
        "Lesion studies provide causal evidence linking brain areas to cognitive processes",
        "Interdisciplinary integration of cognitive theory, neurobiology, and computational models"
      ],
      "source_article": "Cognitive neuroscience",
      "x": 1.768761157989502,
      "y": 1.1373149156570435,
      "level": 2,
      "original_question_hash": "f7c305a2"
    },
    {
      "question": "A coastal municipality is evaluating two flood-protection strategies for a 100 ha shoreline: (1) restore and conserve an existing mangrove wetland (restoration cost $1.5 million, annual flow of measurable benefits: avoided flood damage $120,000; recreational/tourism receipts $30,000; fishery provisioning value $50,000; carbon sequestration valuation $20,000), or (2) build a concrete seawall (construction cost $6 million, annual maintenance $50,000). Assume a 20-year appraisal period and a discount rate of 4%. The wetland’s fish nursery function is argued to be a habitat process that enables fish harvests rather than the harvested fish itself. Using the annuity present-value formula $PV = B\\times\\frac{1-(1+r)^{-n}}{r}$ and the ecosystem services classification from the Millennium Ecosystem Assessment, which of the following statements is correct?",
      "options": {
        "A": "The wetland provides all four MA categories: provisioning (fishery), regulating (flood protection, carbon sequestration), supporting (fish nursery/habitat, nutrient cycling) and cultural (recreation). Supporting services (habitat) underpin the provisioning and regulating flows. Calculating PV: annual measurable benefits $B=120{,}000+30{,}000+50{,}000+20{,}000=220{,}000$, so $PV_{wetland}\\approx220{,}000\\times\\frac{1-(1.04)^{-20}}{0.04}\\approx$3.0 million; net benefit after restoration cost ~$1.5$ million. PV of seawall ~$6{,}000{,}000+50{,}000\\times\\frac{1-(1.04)^{-20}}{0.04}\\approx$6.68 million. Therefore restoration is more cost-effective on these assumptions.",
        "B": "The wetland’s fish nursery is a provisioning service and thus should be counted twice when valuing both habitat and harvest; because of double-counting the wetland’s PV exceeds $6.68$ million, so the seawall is the cheaper option. Supporting services do not underpin other services, they are economically separable.",
        "C": "Although the wetland supplies multiple services, it only supplies supporting and cultural services; flood protection and carbon sequestration are functions of engineered infrastructure, not ecosystems. Using the PV formula with the listed annual flows yields a wetland PV less than the seawall cost, so the municipality should build the seawall.",
        "D": "Supporting services (habitat, nutrient cycling) are the biophysical basis that enable provisioning and regulating services; ecosystems commonly provide different bundles of services and not every ecosystem offers all four categories. Given the numbers, the wetland’s annuity PV (~$3.0M) minus restoration cost ($1.5M) yields a positive net value, so the wetland is the better economic choice compared with the seawall (PV ~$6.68M). However, this avoided-cost calculation should be disregarded because non-market cultural values cannot be represented in monetary terms and therefore economic valuation is invalid for policy decisions."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete coastal-restoration vs seawall scenario. Tested identification of MA categories, the role of supporting services as underpinning others, context-dependence of service bundles, and basic avoided-cost present-value comparison using the annuity formula.",
      "concepts_tested": [
        "MA four-category framework (provisioning, regulating, supporting, cultural)",
        "Economic valuation of ecosystem services (avoided cost, present value)"
      ],
      "source_article": "Ecosystem service",
      "x": 1.571314811706543,
      "y": 0.918035626411438,
      "level": 2,
      "original_question_hash": "742f7ef7"
    },
    {
      "question": "Consider a 1,000 km^2 watershed receiving 800 mm yr^{-1} of precipitation (annual input = $0.8\\times10^9\\ \\mathrm{m^3}$). Under current conditions 50% of precipitation is lost to evapotranspiration, 30% discharges to the ocean, and 20% recharges the aquifer (groundwater recharge = $1.6\\times10^8\\ \\mathrm{m^3\\ yr^{-1}}$). Current groundwater abstraction (pumping) for municipal and irrigation use is $1.0\\times10^8\\ \\mathrm{m^3\\ yr^{-1}}$. Which of the following sets of human modifications to the watershed would most likely produce sustained groundwater depletion (overdraft) in the long term?",
      "options": {
        "A": "20% of the watershed is urbanized (impervious surfaces reduce recharge by 25% of its original value to $1.2\\times10^8$ m^3 yr^{-1}$), a new reservoir captures seasonal floods and is managed to induce artificial recharge of $0.4\\times10^8$ m^3 yr^{-1}, and industrial demand grows so pumping rises to $1.4\\times10^8$ m^3 yr^{-1}. (Net recharge = $1.2+0.4=1.6\\times10^8$ m^3 yr^{-1}.)",
        "B": "20% of the watershed is urbanized (impervious surfaces reduce recharge by 25% of its original value to $1.2\\times10^8$ m^3 yr^{-1}$), no artificial recharge or reservoir leakage is provided, and agricultural expansion raises pumping to $1.8\\times10^8$ m^3 yr^{-1}. (Net recharge = $1.2\\times10^8$ m^3 yr^{-1}.)",
        "C": "A desalination plant supplies an additional $0.2\\times10^8$ m^3 yr^{-1} of municipal water, allowing groundwater pumping to be reduced by $0.3\\times10^8$ m^3 yr^{-1} (new pumping = $0.7\\times10^8$ m^3 yr^{-1}), while modest urban development reduces recharge by 10% (new recharge = $1.44\\times10^8$ m^3 yr^{-1}).",
        "D": "A canal imports $0.6\\times10^8$ m^3 yr^{-1}$ of surface water for irrigation; this does not change the natural recharge fraction, but enables agricultural expansion so that pumping rises to $1.6\\times10^8$ m^3 yr^{-1} while recharge remains $1.6\\times10^8$ m^3 yr^{-1}."
      },
      "correct_answer": "B",
      "generation_notes": "Created a quantitative watershed scenario (precipitation, partitioning) and four development interventions; calculated net recharge versus pumping to identify which scenario causes pumping to exceed recharge (overdraft).",
      "concepts_tested": [
        "Hydrological balance and watershed partitioning of precipitation (ET, discharge, groundwater recharge)",
        "Effects of human modifications (urbanization, reservoirs, imports, desalination) on runoff, recharge and flow timing",
        "Groundwater sustainability vs. overdrafting (comparing recharge to abstraction)"
      ],
      "source_article": "Water resources",
      "x": 1.7096201181411743,
      "y": 0.9145485758781433,
      "level": 2,
      "original_question_hash": "5562da91"
    },
    {
      "question": "A small town discovers an industrial contaminant in its public groundwater at a measured concentration of 100 µg/L. A representative adult drinks 2 L/day of this water for 5 years before a remediation plan is implemented. Using body weight 70 kg and an averaging lifetime of 70 years, compute the lifetime average daily dose (LADD) from ingestion and select the best interpretation consistent with exposure-assessment principles. Use the formula $\\mathrm{LADD}=\\dfrac{(C)(IR)(ED)}{(BW)(AT)}$ where $C$ is in mg/L, $IR$ in L/day, $ED$ and $AT$ in the same time units. Which of the following is correct?",
      "options": {
        "A": "Correct LADD ≈ $2.04\\times10^{-4}\\ \\mathrm{mg\\ kg^{-1}\\ day^{-1}}$. This LADD summarizes magnitude and duration of ingestion exposure; the fastest short-term reduction in risk is eliminating ingestion (e.g., providing alternate water), while long-term risk reduction requires source remediation. A complete exposure assessment must also consider absorbed amount, chemical form, uptake rate and bioavailability, and can apply to humans, ecosystems and abiotic receptors.",
        "B": "Correct LADD ≈ $2.04\\times10^{-1}\\ \\mathrm{mg\\ kg^{-1}\\ day^{-1}}$. Because the contaminant concentration in the aquifer is known, only source control (shutting down the facility) is required to protect health; receptor-focused measures like bottled water are unnecessary and ineffective.",
        "C": "Correct LADD ≈ $2.04\\times10^{-7}\\ \\mathrm{mg\\ kg^{-1}\\ day^{-1}}$. Exposure assessment need only report concentration and time; bioavailability and route of entry are secondary and can be ignored when setting remediation targets.",
        "D": "Correct LADD ≈ $2.04\\times10^{-4}\\ \\mathrm{mg\\ kg^{-1}\\ day^{-1}}$. Because the LADD is nonzero, the only acceptable remediation target is zero environmental concentration; intermediate cleanup goals are irrelevant to risk managers."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a numeric scenario using the LADD equation to test calculation and conceptual interpretation: magnitude/duration, short-term receptor controls vs long-term source remediation, and need to consider absorbed amount/form/bioavailability and applicability across receptors.",
      "concepts_tested": [
        "Risk = f(exposure, hazard) and LADD quantifies magnitude, frequency and duration",
        "Role of exposure data in risk management and remediation (short-term receptor measures vs long-term source control)",
        "Scope of exposure across humans, ecosystems, abiotic systems and importance of absorbed amount, form, rate, bioavailability"
      ],
      "source_article": "Exposure assessment",
      "x": 1.795500636100769,
      "y": 0.999341607093811,
      "level": 2,
      "original_question_hash": "76335813"
    },
    {
      "question": "Dr. Lee plans to test whether episodic increases in weekly work stress predict later rises in blood pressure across adulthood. She is choosing among three designs: (1) a prospective longitudinal panel that recruits $1{,}000$ workers aged 30 and measures weekly stress and blood pressure for 10 years; (2) a retrospective longitudinal study that assembles medical and employment records from the past 10 years for a cohort born in 1970; (3) a cross-sectional survey that samples different age groups once and measures stress and blood pressure. Which of the following statements most accurately characterizes the comparative strengths and limitations of these designs for studying within-person change, temporal ordering, cohort effects, and bias?",
      "options": {
        "A": "Design (1) is best for observing within-person change and for establishing temporal ordering (stress preceding blood pressure changes), and it reduces confounding by cohort effects. Design (2) can also provide temporal ordering if records are time-stamped but is more susceptible to measurement inconsistency, missing data, and selection biases. Design (3) cannot distinguish within-person change from cohort differences and cannot reliably establish temporal order, so it is weakest for causal interpretation.",
        "B": "Design (2) is superior because using existing records eliminates attrition and practice effects, and retrospective data are equally good as prospective panels for causal inference because time-stamped events always reveal cause and effect.",
        "C": "Design (3) can separate short-term from long-term phenomena by comparing stress and blood pressure across age groups at one time; therefore it avoids cohort effects better than designs that follow the same individuals over time.",
        "D": "All three designs are essentially equivalent for causal interpretation because temporal ordering is irrelevant once large samples are obtained; prospective versus retrospective choice only affects cost and speed, not bias or inference about within-person change."
      },
      "correct_answer": "A",
      "generation_notes": "Created a scenario comparing prospective panel, retrospective records, and cross-sectional designs; the correct option highlights within-person change, temporal ordering, cohort effects, and biases of retrospective data per the article.",
      "concepts_tested": [
        "Longitudinal tracking of same individuals reduces cohort effects and permits within-person change",
        "Temporal ordering in longitudinal data aids distinguishing short-term vs long-term phenomena and supports causal interpretation (with caveats)",
        "Differences between prospective and retrospective longitudinal designs, including data sources and associated biases"
      ],
      "source_article": "Longitudinal study",
      "x": 1.3010622262954712,
      "y": 1.015932321548462,
      "level": 2,
      "original_question_hash": "5016633a"
    },
    {
      "question": "A regional healthcare network plans to deploy a new electronic health record (EHR) system across its hospitals and outpatient clinics. The CIO must produce an information management plan that governs the lifecycle of patient data, clarifies stakeholder responsibilities, and ensures the initiative supports organisational strategy rather than being treated as a purely operational IT task. Which of the following plans best exemplifies the principles of information management described in the article?",
      "options": {
        "A": "Define acquisition standards (data provenance, standard input interfaces and validation rules); assign custodianship roles (records managers for retention, IT for secure storage, clinicians for clinical accuracy) and role-based distribution controls; produce a retention and archival policy plus indexed long-term storage and formal deletion procedures; specify stakeholder rights to originate/change/distribute/delete under documented policies; and align the EHR with business processes, KPIs and strategic goals.",
        "B": "Purchase high-capacity storage and centralize all patient records in one repository; allow each department to control access informally; schedule nightly backups but leave retention and deletion decisions to individual clinicians to reduce bureaucracy; treat the project as an IT infrastructure upgrade only.",
        "C": "Issue detailed operational standard operating procedures (SOPs) for clerical staff to scan and file incoming documents and for IT to run weekly backups; do not define retention schedules, metadata standards or stakeholder governance, on the assumption that day-to-day procedures will suffice to keep information usable.",
        "D": "Outsource the entire EHR to a cloud vendor under a standard service contract that specifies uptime and basic security SLAs; rely on the vendor's generic retention and deletion defaults without establishing internal policies, stakeholder rights, or integration with internal business strategy."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete EHR deployment scenario to test IM lifecycle (acquisition, custodianship/distribution, disposal), stakeholder governance (roles/rights/policies), and the need to align IM with systems/processes/strategy rather than treating it as only operational/technical.",
      "concepts_tested": [
        "Information management lifecycle (acquisition, custodianship/distribution, disposal/archival/deletion)",
        "Stakeholder roles and governance (quality, accessibility, rights to originate/change/distribute/delete, policy controls)",
        "Relationship of IM to data, systems, technology, processes and organisational strategy (distinction from purely operational view)"
      ],
      "source_article": "Information management",
      "x": 1.3909472227096558,
      "y": 1.018588662147522,
      "level": 2,
      "original_question_hash": "32d5cde1"
    },
    {
      "question": "A mid-size hospital deploys a new Patient Information System (PIS) that integrates electronic health records, appointment scheduling, lab interfaces, clinician messaging, and clinical decision-support alerts. Nurses and physicians alter admission and documentation routines; the hospital creates new role descriptions and escalation procedures; and the IT team maintains the servers, EHR software, and network. Which of the following best characterizes this PIS according to the information-systems concepts described above?",
      "options": {
        "A": "The PIS is an information system in the sociotechnical sense: it combines task, people, structure (roles/procedures), and technology; it functions as a work system that captures, transmits, stores, retrieves, manipulates and displays patient data (forming a social memory of clinical interactions); and it has a definite boundary and users/processors/inputs/outputs so it supports operations (admissions, orders), management (resource allocation, dashboards), and decision-making (alerts and diagnostic support) rather than being merely an ICT artifact or a single business process.",
        "B": "The PIS is primarily an ICT artifact: because the value resides in the servers, EHR software and network, it should be viewed as a computer system whose success depends mainly on technical performance; people and procedures are secondary and can be treated as external to the system.",
        "C": "The PIS is best described as a business process or workflow automation tool: its principal role is to automate clinical and administrative workflows, so it is essentially the hospital’s process model rather than an information system with distinct boundaries and social memory functions.",
        "D": "The PIS is essentially a data system for storage and analytics: its main purpose is to store patient records and enable reporting and analytics, and it should be analyzed only as a data repository that feeds other activity systems, not as an integrated work system that affects roles and decision-making."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete hospital scenario requiring students to identify sociotechnical components, work-system functions (capture/transmit/store/retrieve/manipulate/display), social memory, and the distinction between IS and pure ICT or process automation.",
      "concepts_tested": [
        "IS as sociotechnical system (task, people, structure, technology)",
        "IS as a work system that captures/transmits/stores/retrieves/manipulates/displays information and serves as social memory",
        "Purpose and boundary of IS in organizations: supports operations, management and decision-making and differs from pure ICT or mere business processes"
      ],
      "source_article": "Information system",
      "x": 1.4085830450057983,
      "y": 1.0466266870498657,
      "level": 2,
      "original_question_hash": "3f9691ad"
    },
    {
      "question": "Meridian HealthTech, a mid-sized healthcare provider, must consolidate patient records across 12 independent business units while shifting to a telemedicine-first strategy and complying with new interoperability regulations. The CEO wants an approach that (1) proactively guides the organization through these disruptive changes, (2) uses a set of cohesive models describing business, information, process, and technology domains, and (3) coordinates technical solutions across units so they align with enterprise strategy. Which of the following actions best exemplifies the enterprise architecture practice described in the article?",
      "options": {
        "A": "A solutions architect for the largest business unit designs and deploys a new electronic medical record (EMR) integration that optimizes that unit's workflows and data model, leaving other units to adopt similar changes later as they see fit.",
        "B": "An enterprise architect, sponsored by the CIO, produces a layered architectural description comprising cohesive views (business capabilities, process maps, information/data models, application and infrastructure landscapes), defines target and transition states, establishes governance and standards, and coordinates solutions architects to implement interoperable EMR components consistent with the target architecture.",
        "C": "The CIO commissions a rapid replacement of legacy systems across all units with a single vendor's platform and tasks project managers to prioritize deployment speed and budget adherence, without creating cross-domain models or transition roadmaps.",
        "D": "Each project manager for the 12 units independently creates detailed project plans (timelines, resource allocations, and risk registers) focused on delivering their unit's telemedicine features on schedule, with integration left as a post-deployment activity."
      },
      "correct_answer": "B",
      "generation_notes": "Created a concrete healthcare consolidation scenario requiring proactive, holistic EA activity and contrasted enterprise architect (strategic, cross-domain models and governance) with solutions architects, CIO directives, and project managers; one option matches EA principles from the article.",
      "concepts_tested": [
        "Holistic proactive change management aligning business strategy with information systems",
        "Use of cohesive models/views across business, information, process, and technology domains",
        "Roles and coordination between enterprise architects and solutions architects"
      ],
      "source_article": "Enterprise architecture",
      "x": 1.4194689989089966,
      "y": 1.0650540590286255,
      "level": 2,
      "original_question_hash": "b1f697d3"
    },
    {
      "question": "In Country X a multi-year drought collapses smallholder agriculture, prompting 200,000 rural laborers to migrate to cities (a rapid population shift). The central government — relatively stable and capable — responds with land-reform subsidies, directed credit, and factory investment, while urban labor organizations form, strike, and win legislation that expands workers' rights. Which interpretation best (a) classifies the drought and the government interventions according to the dual-sources model of social change, (b) identifies the primary structural aspects, processes and direction of change, and (c) invokes the theoretical framework that most directly explains the sequence of events?",
      "options": {
        "A": "The drought is a unique (specific, exogenous) factor and the government's reforms and investments are systematic (institutional and resource-based) factors; the structural aspect is a population shift (rural→urban), the processes/mechanisms are migration, resource reallocation, and collective action, and the direction is a transformation from agrarian to industrial labour relations — best explained by a Marxist/materialist account emphasizing class struggle and changes in socio-economic structure.",
        "B": "The drought should be seen as a systematic factor demonstrating long-term environmental trends, while the government's reforms are unique policy interventions; the mechanism is a Hegelian dialectic in which the agrarian Thesis is countered by the migrants' Antithesis, producing a Synthesis in policy — hence Hegelian dialectics best explain the change.",
        "C": "Both the drought and the government's reforms are unique, idiosyncratic events; the principal mechanism is a Kuhnian paradigm shift in which social actors abandon an old economic paradigm for a new urban-industrial one once a better model is generally accepted — therefore Kuhn's model of paradigm replacement best explains the sequence.",
        "D": "The drought is the unique driver and the government's reforms are systematic, but the change is best characterized as gradual, continuous Heraclitan flux (constant small adjustments), implying that slow, imperceptible processes rather than conflict or structural breaks primarily produced the observed industrialization."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete country scenario combining an exogenous shock (drought) and institutional responses; asked students to map elements to the dual-sources model, to identify structural/process directions, and to choose the most fitting theoretical account (Marxist). Distractors misclassify factors or invoke alternative frameworks (Hegelian, Kuhnian, Heraclitan).",
      "concepts_tested": [
        "Dual-sources model of social change (unique vs systematic factors)",
        "Mechanisms, processes and directions of social change",
        "Comparative theoretical frameworks (Marxist vs Hegelian vs Kuhnian vs Heraclitan)"
      ],
      "source_article": "Social change",
      "x": 1.2163282632827759,
      "y": 0.982162356376648,
      "level": 2,
      "original_question_hash": "5d583aa0"
    },
    {
      "question": "In the coastal village of Lantau a novel net-weaving technique called the \"zig-net\" appears among three fishers. Within a decade the technique is adopted by 60% of fishers after a mix of apprenticeships, imitation on the beach, and high-quality instructional videos amplified by an AI platform. During the same period, ocean warming alters fish migration routes and several early adopters carry a heritable allele associated with finer finger motor control. Which interpretation best reflects the contemporary scientific account of cultural evolution as described in the article?",
      "options": {
        "A": "The spread of the \"zig-net\" is best explained by social transmission (teaching and imitation) plus processes of selective retention acting on cultural information; its propagation is shaped by social (learning networks), environmental (changed fish distribution), and biological (variation in motor dexterity) factors, and can be modeled using complementary frameworks (e.g., dual inheritance theory, memetics, and cultural-selection approaches) that are being integrated into a unified discipline.",
        "B": "The rise of the \"zig-net\" is predominantly a product of biological natural selection: fishers with the dexterity allele outcompeted others and genetically transmitted the superior foraging phenotype to their descendants, so cultural transmission plays only a negligible role.",
        "C": "The phenomenon is best understood purely as memetics: the \"zig-net\" is a selfish meme that spreads regardless of environmental constraints or social learning mechanisms, analogous to a viral contagion that requires no interaction with biological or ecological factors.",
        "D": "The adoption of the \"zig-net\" demonstrates unilinear cultural progress—Lantau is simply moving along a single, inevitable evolutionary trajectory toward more complex fishing technologies shared by all societies, independent of local social or environmental contingencies."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete village scenario where a new cultural trait spreads via teaching/imitation and is influenced by environmental change and genetic variation; options contrast single-cause explanations (biological-only, memetics-only, unilinear) with the integrative cultural evolution account.",
      "concepts_tested": [
        "social transmission and selective retention of cultural information",
        "multi-factor causation of cultural change (social, environmental, biological)",
        "plurality and integration of theoretical approaches to cultural evolution (memetics, dual inheritance, cultural selection)"
      ],
      "source_article": "Cultural evolution",
      "x": 1.2436926364898682,
      "y": 0.9927749633789062,
      "level": 2,
      "original_question_hash": "40f99080"
    },
    {
      "question": "A protein switches between a compact “closed” state and an extended “open” state with an extension difference Δx = 6 nm. In an optical-tweezers experiment at T = 298 K and pulling force f = 5 pN, the measured fraction of molecules in the open state is p_open = 0.30. SAXS indicates an increase in radius of gyration upon opening and circular dichroism shows a modest change in secondary structure. Which of the following is the best estimate of the zero-force free-energy difference ΔG_0 = G_open − G_closed (expressed in pN·nm and in kJ·mol^{-1}), and which set of methodological/theoretical statements is correct? Use the Boltzmann relation p_open = \\frac{e^{-β(ΔG_0 − fΔx)}}{1+e^{-β(ΔG_0 − fΔx)}}, with k_B T ≈ 4.114 pN·nm (298 K).",
      "options": {
        "A": "ΔG_0 ≈ 33.5 pN·nm ≈ 8.14 k_B T ≈ 20.2 kJ·mol^{-1}. Correct methodological/theoretical statements: x-ray crystallography (or high-resolution NMR) can provide Å-scale atomic structure; SAXS reports ensemble-averaged size/shape changes (not Å resolution); the population–free-energy relation follows from Boltzmann/statistical mechanics.",
        "B": "ΔG_0 ≈ 3.5 pN·nm ≈ 0.85 k_B T ≈ 2.1 kJ·mol^{-1}. Correct methodological/theoretical statements: SAXS gives Å-scale atomic resolution so it can replace crystallography; optical tweezers only measure kinetics not populations; the population relation is a kinetic (rate-law) result, not Boltzmann.",
        "C": "ΔG_0 ≈ −26.5 pN·nm (open lower in energy) ≈ −6.44 k_B T ≈ −16.0 kJ·mol^{-1}. Correct methodological/theoretical statements: AFM imaging routinely gives Ångström atomic coordinates comparable to crystallography; circular dichroism yields atomic-resolution structures; chemical kinetics (rate constants) are required to compute ΔG_0 from populations.",
        "D": "ΔG_0 ≈ 60.0 pN·nm ≈ 14.6 k_B T ≈ 36.2 kJ·mol^{-1}. Correct methodological/theoretical statements: neutron spin-echo gives Å-resolution static structures; SAXS measures single-molecule extension directly; the population–free-energy link is derived from thermodynamics/statistical mechanics (but the numeric ΔG_0 above is the experimentally implied value)."
      },
      "correct_answer": "A",
      "generation_notes": "Presented a concrete single-molecule force-population scenario; solved for ΔG_0 from Boltzmann relation and force work (ΔG_eff = ΔG_0 − fΔx), converted units to pN·nm, k_B T, and kJ·mol^{-1}; options mix correct/incorrect numerical computations and technique/theory attributions to test all three target concepts.",
      "concepts_tested": [
        "Application of physical principles (Boltzmann/statistical mechanics) to relate populations and free energies in biomolecular conformational equilibria",
        "Use and interpretation of biophysical techniques (SAXS, x-ray crystallography/NMR, optical tweezers) for structure, ensemble averages, and single-molecule force measurements",
        "Integration of thermodynamics and mechanical work (force × extension) to infer zero-force energetics of conformational transitions"
      ],
      "source_article": "Biophysics",
      "x": 1.830572485923767,
      "y": 1.075610876083374,
      "level": 2,
      "original_question_hash": "be0b92fb"
    },
    {
      "question": "Consider the Unicode string S = \"ab̲c𐐀\" which contains the letters a, b, a combining low line (U+0332), c, and the supplementary character DESERET CAPITAL LETTER LONG I (U+10400). Which of the following statements is fully correct about how S is represented and why modern encodings like UTF-8/UTF-16 were adopted?",
      "options": {
        "A": "S consists of five Unicode code points (U+0061, U+0062, U+0332, U+0063, U+10400). In UTF-32 it occupies five 32-bit code units, in UTF-16 it occupies six 16-bit code units (U+10400 encoded as a surrogate pair), and in UTF-8 it occupies nine octets (U+0332 uses two bytes and U+10400 uses four bytes). UTF-8’s variable-length design preserves ASCII compatibility and gives storage efficiency for common Latin text while still covering the full Unicode code space. Unicode and its encodings were developed to replace incompatible vendor code pages by mapping characters to universal numeric code points, enabling reliable storage, transmission and transformation across systems.",
        "B": "S has five Unicode code points and therefore uses five code units in every Unicode encoding; UTF-8 uses five octets and UTF-16 uses five 16-bit units. Variable-length encodings were chosen simply to allow any code point to be represented, not for backward compatibility with ASCII or storage efficiency.",
        "C": "S contains four characters because the combining low line merges with b, so there are four code points; UTF-16 therefore uses four 16-bit units and UTF-8 uses four octets. Fixed-length encodings such as UTF-32 are superior because they remove the complexity of variable-length parsing and were the historical solution adopted widely instead of UTF-8/UTF-16.",
        "D": "S contains five code points but can always be represented on-disk using an 8-bit fixed-length scheme by inserting escape sequences for higher code points; this strategy is simpler and was the reason the industry standardized on single-byte code pages rather than Unicode encodings like UTF-8/UTF-16."
      },
      "correct_answer": "A",
      "generation_notes": "Used the concrete example from the article (\"ab̲c𐐀\") to require students to count code points and code units/octets across UTF-8/UTF-16/UTF-32 and to state the design tradeoffs and historical rationale for Unicode/UTF-8.",
      "concepts_tested": [
        "Code points and code space as numeric representations enabling storage/transmission",
        "Fixed-length versus variable-length encodings (UTF-8, UTF-16, UTF-32) and associated tradeoffs",
        "Historical progression to Unicode and why standardization replaced vendor-specific code pages"
      ],
      "source_article": "Character encoding",
      "x": 1.3678065538406372,
      "y": 1.1446208953857422,
      "level": 2,
      "original_question_hash": "6658227f"
    },
    {
      "question": "An international consortium composed of manufacturers, national standards bodies, consumer groups and several governments proposes a voluntary technical specification for an electric vehicle charging connector to replace five incompatible national connectors. The consortium argues this will increase interoperability, reduce export costs, and make repair and maintenance more repeatable. Which statement best characterizes this initiative in terms of the concept of standardization as described in the article?",
      "options": {
        "A": "This is standardization: a consensus-driven, multi-stakeholder process producing voluntary technical specifications that enhance compatibility, interoperability, safety and quality, reduce coordination costs and trade barriers, and normalize previously custom processes.",
        "B": "This is primarily a regulatory imposition because only governments can produce standards that eliminate incompatible national variants and make compliance legally mandatory across markets.",
        "C": "This is merely the emergence of a de facto proprietary standard: market domination by manufacturers will dictate the connector, so multi-stakeholder consensus and formal standards organizations are irrelevant to interoperability or trade barriers.",
        "D": "This proposal guarantees product fitness and safety because any connector labelled with the consortium’s standard number is automatically fit for all intended uses and requires no further validation or verification by users or regulators."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete multinational EV charging-connector scenario to probe consensus-driven, voluntary nature of standards, their aims (compatibility, reduced coordination costs, normalization of custom processes) and to contrast with common misconceptions (government-only, de facto proprietary, guaranteed fitness).",
      "concepts_tested": [
        "Standardization as consensus-driven multi-stakeholder process",
        "Voluntary technical specifications reduce coordination costs and trade barriers",
        "Normalization of custom processes and historical link to interchangeability/industrial needs"
      ],
      "source_article": "Standardization",
      "x": 1.5083823204040527,
      "y": 0.9951016306877136,
      "level": 2,
      "original_question_hash": "5b65108d"
    },
    {
      "question": "The City of Elmton adopts a \"Downtown Active Transit Policy\" that (1) converts one car lane to a protected bike lane, (2) offers a $1{,}000 tax credit for e-bike purchases, and (3) mandates restricted delivery hours for commercial vehicles enforced by fines. After one year the city reports bike commuting up $20\\%$, average vehicle delays up $12\\%$, and downtown retail sales down $6\\%$. City officials call the initiative a policy (not a statute) and claim it is evidence-based because they relied on a successful foreign city's program. Which of the following statements best accords with the concepts of policy design, the policy–law distinction, and evidence-based evaluation discussed in the article?",
      "options": {
        "A": "Most accurate: Elmton's program is a mixed set of policy instruments; unlike statutes, policies primarily guide choices while regulatory fines function more like law because they compel behavior. The city's evidence-based claim is weak: it lacks comparative evidence showing the chosen package outperforms at least one alternative and does not provide a sound account linking its preferences to the cited evidence. The observed $-6\\%$ retail change is a plausible unintended effect from intervening in a complex adaptive system and indicates the need for formal evaluation and potentially restarting the policy cycle.",
        "B": "Correct: Because the program includes enforceable fines it should be classified as a statute rather than a policy, so the three conditions for evidence-based policy do not apply; the $20\\%$ increase in cycling proves the intervention's success regardless of retail effects.",
        "C": "Correct: The measurable $20\\%$ increase in bike commuting shows the initiative was objectively tested and therefore successful; any retail decline is incidental and does not warrant further evaluation because operational outcomes have been achieved.",
        "D": "Correct: Relying on a foreign city's results is sufficient to call the initiative evidence-based if outcomes elsewhere were positive, and the presence of fines makes the instrument regulatory rather than a policy issue so implementation effects on local businesses are outside the policy's evaluative remit."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete city policy scenario combining incentives, infrastructure, and regulatory enforcement; options probe policy vs law distinction, the three evidence-based conditions, unintended consequences in complex systems, and the role of evaluation in the policy cycle.",
      "concepts_tested": [
        "Policy as deliberate mechanism to guide decisions toward intended outcomes",
        "Distinction between policy, law, and rules; objective vs subjective decision-making",
        "Dynamic implementation and evaluation; evidence-based policy criteria and unintended consequences"
      ],
      "source_article": "Policy",
      "x": 1.2665820121765137,
      "y": 0.9433085322380066,
      "level": 2,
      "original_question_hash": "4c56879f"
    },
    {
      "question": "City A in the fictitious Union of Arcadia has adopted an ambitious air-quality plan. The national ministry sets a headline PM2.5 reduction target, a regional government designs road-pricing rules adapted to local geography, the city council negotiates a public–private partnership with two transit firms to electrify buses, an EU-style supranational fund co-finances infrastructure, and an environmental NGO coalition lobbies at regional and supranational fora while an independent certification body monitors emissions. Which description best characterizes this arrangement as an example of multi-level governance (MLG) rather than merely multi-level government?",
      "options": {
        "A": "Policy outcomes result from ongoing negotiation among supranational, national, regional and local public bodies plus private firms and NGOs; authority and resources are dispersed both vertically and horizontally across state and non-state actors, and analysis focuses on networks, procedures and interactions rather than a simple transfer of legal competences — this is MLG.",
        "B": "The central government legally delegates responsibility for transport emissions to regional governments but retains final approval authority; implementation follows a hierarchical chain from national law to regional enactments — this is a clear case of multi-level government (devolution) rather than MLG.",
        "C": "A supranational institution issues a uniform regulation that all member states must implement identically; while local actors execute the policy, they have no role in design or financing — this centralized supranational decision-making exemplifies MLG.",
        "D": "Nation-states negotiate an intergovernmental treaty that creates a new regional agency to standardize emissions monitoring across member states; the agency replaces national competencies and directly controls cities — this concentration of authority is the typical outcome MLG predicts."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete EU-style air-quality scenario that includes vertical (supranational→national→regional→local) and horizontal (city↔private firms, NGO networks) interactions; options contrast MLG (networks, dispersed authority, non-state actors) with delegation, centralization, and intergovernmental concentration to test distinction between governance framework and mere transfers of power.",
      "concepts_tested": [
        "Vertical and horizontal dispersion of authority across levels and actors",
        "Interaction between state and non-state actors in policymaking and implementation",
        "Distinction between multi-level governance as an analytical framework (networks, processes) and multi-level government as formal transfer/delegation of competences"
      ],
      "source_article": "Multi-level governance",
      "x": 1.2367987632751465,
      "y": 0.8824832439422607,
      "level": 2,
      "original_question_hash": "04dbf078"
    },
    {
      "question": "A pharmaceutical manufacturer in Country A fills vials of an active ingredient and records the mass of each filled vial with a floor scale. The company has a routine that a local accredited calibration laboratory calibrates the floor scale annually, and the calibration certificate states the result as $y=100.000\\ \\text{g}\\ \\pm\\ U$ with $U=0.05\\ \\text{g}$ (coverage factor $k=2$). The accreditation body in Country A is a signatory to ILAC MRA, and the calibration laboratory’s standards are traceable to the national metrology institute (NMI). The NMI realises the kilogram via a Kibble balance and participates in the CIPM Mutual Recognition Arrangement (MRA). An importer in Country B demands assurance that the vial masses are comparable to measurements made in Country B and acceptable for trade. Which of the following statements is correct?",
      "options": {
        "A": "Because the scale is calibrated by an ILAC-accredited laboratory whose standards are traceable to an NMI that realises the kilogram and participates in the CIPM MRA, the reported mass and uncertainty are internationally comparable and can be accepted for trade subject to the importing authority's regulatory checks.",
        "B": "Traceability to the SI requires the manufacturer itself to perform a primary realisation of the kilogram (e.g., operate a Kibble balance); calibrations by external laboratories cannot establish true SI traceability.",
        "C": "Once the company’s scale has been calibrated by any laboratory, no further link to national or international metrology infrastructure is required for international recognition of the measurement.",
        "D": "Because the SI base units were redefined in 2019, all previous calibrations are invalid and the company must re‑calibrate immediately since the numerical value of the kilogram changed."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a realistic industrial traceability scenario linking device calibration to an NMI that realises the kilogram via Kibble balance and participates in CIPM MRA; options probe understanding of traceability chain, roles of definition/realisation/traceability, and implications of SI redefinition.",
      "concepts_tested": [
        "Traceability linking practical measurements to reference standards",
        "Interrelationship of definition, realisation, and traceability in reliable measurement",
        "Role of national/international metrology infrastructure (NMI, BIPM, CIPM MRA, ILAC) in enabling international recognition and fair trade"
      ],
      "source_article": "Metrology",
      "x": 1.5919595956802368,
      "y": 1.0202696323394775,
      "level": 2,
      "original_question_hash": "c45c7ff5"
    },
    {
      "question": "A coastal region designates $40\\%$ of its nearshore waters as a no-take marine protected area (MPA). National tourism firms obtain exclusive access to high-value dive sites; small-scale fishers are excluded, and their average incomes fall by $60\\%$. Two years later fishers form organized protests and some resort to clandestine fishing in adjacent zones, accelerating reef degradation and reducing local fish stocks. Which interpretation best aligns with a political ecology analysis of this sequence?",
      "options": {
        "A": "A political-ecological interpretation: pre-existing political and economic inequalities produced an uneven distribution of conservation costs and benefits (fishers bore the costs; tourism firms gained benefits); the environmental interventions changed the political/economic status quo and produced feedbacks (clandestine fishing, degradation, and mobilization) that further altered future environmental outcomes; explaining this requires integrating ecological dynamics with political economy rather than treating the problem as purely technical. (Represented analytically as $E_{t+1}=f(E_t,P_t)$ where $E_t$ is ecological state and $P_t$ is the distribution of power/resources.)",
        "B": "A primarily ecological interpretation: the decline in fish stocks and reef health is best explained by natural limits and overexploitation; the MPA was a scientifically justified measure and subsequent illegal fishing reflects individual noncompliance rather than power inequalities or political economy dynamics.",
        "C": "A managerial failure interpretation: the problem is a poor implementation of conservation policy—if authorities had better enforcement and compensation schemes the conflict and degradation would not have occurred; structural analyses of power are unnecessary for corrective action.",
        "D": "A market-failure interpretation: the core problem is missing property rights and unpriced externalities; introducing tradable quotas or privatizing access would align incentives and resolve depletion without invoking politicization or social-movement explanations."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete MPA scenario illustrating unequal cost/benefit distribution, subsequent feedbacks between environmental change and political/economic relations, and the need to politicize ecological issues—then contrasted political-ecological reading with ecological, managerial, and market-focused alternatives.",
      "concepts_tested": [
        "Uneven distribution of environmental costs and benefits due to political/social/economic differences",
        "Feedback loop where environmental changes alter political/economic status quo and shape future environmental outcomes (E_{t+1}=f(E_t,P_t))",
        "Politicization of environmental issues and integration of ecological analysis with political economy to explain degradation, conflict, and social movements"
      ],
      "source_article": "Political ecology",
      "x": 1.296575665473938,
      "y": 0.9161190986633301,
      "level": 2,
      "original_question_hash": "d49d11a9"
    },
    {
      "question": "Country X has a long-run real GDP trend of $2\\%$ per year. Over the last 12 years the economy experienced recurrent deviations from that trend: expansions of about 3–6 years followed by recessions lasting several months, with simultaneous declines in real GDP, real income, employment and industrial production. These downturns coincided with abrupt oil-price spikes and falls in consumer confidence; meanwhile wealth concentration rose and some commentators argued that underconsumption (insufficient aggregate demand among lower-income households) amplified the downturns. Which of the following best integrates (i) how these deviations should be characterized relative to the long-run trend and identified empirically, (ii) the causal role of the oil-price and confidence episodes, and (iii) the relevance of the Sismondi/Owen underconsumption view?",
      "options": {
        "A": "These patterns are textbook business cycles—medium-term fluctuations (typically 2–10 years) around a long-run growth trend, best identified using multiple coincident indicators (real GDP, income, employment, industrial production) rather than a simple two-quarter rule. The oil-price spikes and confidence collapses act as largely unpredictable exogenous shocks that can trigger or deepen contractions. The rise in inequality and underconsumption fits the Sismondi/Owen tradition, which attributes cycles partly to distributional shortfalls in demand and therefore supports a possible role for corrective government policy.",
        "B": "Because GDP has a steady $2\\%$ trend, the observed deviations are simply long-term structural trends (not medium-term cycles) and should be modeled deterministically; oil-price and sentiment moves are endogenous outcomes of the trend rather than exogenous shocks, and Sismondi/Owen believed cycles stem from technology cycles rather than distributional problems, so inequality is irrelevant to policy.",
        "C": "These episodes are best labelled recessions whenever real GDP falls for two consecutive quarters; oil-price spikes and confidence drops are predictable business-cycle phases rather than random shocks, and Sismondi/Owen argued that only inventory mismanagement (not inequality or underconsumption) explains repeated downturns, so government intervention is unnecessary.",
        "D": "The deviations are minor random noise around a robust long-run trend and do not constitute business cycles; oil-price movements and confidence are secondary, and following Friedman-style views the cause is purely monetary mismanagement—distributional factors like inequality and underconsumption have no causal role and government intervention would only worsen outcomes."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete country vignette to test (1) medium-term cycle identification using multiple indicators, (2) role of unpredictable external shocks like oil-price/sentiment, and (3) Sismondi/Owen underconsumption and redistributional policy implications. Options mix plausible but incorrect alternatives.",
      "concepts_tested": [
        "Business cycles as medium-term fluctuations identified by multiple indicators",
        "External shocks (oil price, consumer sentiment) act as unpredictable drivers of expansions/contractions",
        "Historical underconsumption theory linking inequality to cycles and policy implications"
      ],
      "source_article": "Business cycle",
      "x": 1.299744963645935,
      "y": 0.9189769625663757,
      "level": 2,
      "original_question_hash": "767b2b7d"
    },
    {
      "question": "Nation X increases industrial fertilizer application by 150%, converts an additional 10% of its native forest to cropland, and its national average CO2-equivalent emissions push atmospheric CO2 concentration in the region to 420 ppm. A 2009 assessment of global Earth-system control variables flagged that climate change, biosphere integrity and the nitrogen cycle were already likely transgressed; a 2015 update refined the control variables and confirmed further transgressions. According to the Planetary Boundaries framework, which of the following policy interpretations best aligns with the framework’s scientific and normative claims?",
      "options": {
        "A": "Treat each planetary boundary as a legally enforceable numerical cap that must be met immediately by every nation, because the framework provides precise immutable thresholds applicable at national scale.",
        "B": "Recognize the Holocene range as a normative ‘‘safe operating space’’ and, given the framework’s evidence-based updates, prioritize reducing pressures (e.g. limiting excess N fixation, restoring land cover, and cutting emissions) because crossing boundaries risks non-linear, potentially abrupt Earth‑system responses.",
        "C": "Focus only on technological adaptation (e.g. geoengineering and synthetic fertilizers) because planetary boundaries are merely descriptive historical limits and can be overcome by engineering a new, preferable Earth-system state.",
        "D": "Assume that because the 2009 and 2015 papers revised which boundaries were crossed, the uncertainties mean no precautionary action is warranted until exact tipping points are known with high confidence."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete national scenario linking fertilizer, land conversion and CO2 to test understanding of thresholds/non-linear responses, Holocene as normative safe zone, and iterative, evidence-driven nature of boundary assessments.",
      "concepts_tested": [
        "Thresholds and non-linear responses",
        "Holocene as a normative safe zone",
        "Evidence-driven framework and subsequent updates"
      ],
      "source_article": "Planetary boundaries",
      "x": 1.3430968523025513,
      "y": 0.8782898187637329,
      "level": 2,
      "original_question_hash": "4ef9aaca"
    },
    {
      "question": "A developer plans a 12‑storey office retrofit and aims to achieve third‑party certification (e.g., LEED Gold or DGNB Silver) while minimizing total environmental impact across the building's life cycle (planning → design → construction → operation → maintenance → eventual demolition). Which of the following project strategies best demonstrates life‑cycle thinking and resource efficiency, requires integrated collaboration of contractor, architect, engineer and client from the concept stage, and aligns with the verification needs of certification systems?",
      "options": {
        "A": "Convene integrated design charrettes with contractor, architect, engineer and client at project inception; run comparative life‑cycle assessments (embodied energy, GWP) to choose a retained structural frame and low‑embodied‑energy, locally sourced materials; design passive solar/thermal mass and a high‑performance envelope; specify on‑site rainwater capture, dual‑plumbing and monitoring instrumentation; perform third‑party commissioning and O&M training and compile documented evidence of measured performance for LEED/DGNB submission.",
        "B": "Finalize an architecturally striking all‑new façade early to attract tenants, then add a large photovoltaic array and high‑efficiency HVAC to meet energy targets; engage the contractor only after detailed drawings are complete; select materials based on lowest initial cost and rely on an energy model to seek enough points for LEED.",
        "C": "Prioritize aesthetic impact by selecting an all‑glass curtain wall and advanced mechanical systems to control comfort; defer material and waste decisions to construction; plan to pursue a handful of innovation credits in BREEAM after construction to offset the expected higher operational loads.",
        "D": "Focus the project team primarily on aggressive water‑efficiency retrofits (greywater recycling and ultra‑low‑flow fixtures) chosen by the client; import a suite of recycled finish materials from overseas without LCA; keep contractor involvement minimal and plan to claim local green‑building points for water savings only."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a practical retrofit scenario with four actionable project strategies. Option A consolidates LCA, material choices, passive design, early integrated stakeholder collaboration, commissioning, monitoring and documentation to reflect life‑cycle resource efficiency and certification requirements; other options are plausible but lack one or more core concepts.",
      "concepts_tested": [
        "Life‑cycle thinking and resource efficiency (planning through demolition, LCA, embodied energy)",
        "Integrated stakeholder collaboration across project stages (contractor, architect, engineer, client)",
        "Certification and assessment mechanisms (LEED, DGNB, BREEAM documentation, commissioning and measured performance)"
      ],
      "source_article": "Green building",
      "x": 1.4982731342315674,
      "y": 0.8969116806983948,
      "level": 2,
      "original_question_hash": "2718b962"
    },
    {
      "question": "A mid-sized city commissions research after a decades-old petrochemical plant is linked to elevated cancer rates in a predominantly low-income, immigrant neighborhood. Three research teams submit reports. Which report best exemplifies the methodological and political commitments of critical theory — i.e., its critique of objectivity as power-knowledge, its insistence on praxis (theory linked to collective action), and its attention to intersecting forms of oppression and historical/contextual analysis including capitalist structures?",
      "options": {
        "A": "An interdisciplinary, participatory study that documents how corporate-funded science and municipal zoning decisions shaped official 'objective' assessments; situates contamination within the city’s history of racialized urban redevelopment and neoliberal outsourcing; centers residents’ embodied testimonies alongside epidemiological data; and concludes with a strategy co-designed with community groups for policy change, litigation support, and labor organizing.",
        "B": "A technical environmental impact assessment that uses blinded sampling, standardized risk models, and statistically controlled cohorts to produce an ‘objective’ concentration map; it recommends industry-standard remediation without addressing corporate influence, historical zoning, or community-led action, claiming neutrality of scientific methods.",
        "C": "A poststructural discourse analysis that deconstructs scientific reports, municipal press releases, and media narratives to show how language constructs danger and risk; it emphasizes epistemic contingency and the impossibility of pure objectivity but refrains from proposing concrete collective strategies or engaging directly with affected residents.",
        "D": "A Marxist economic study that explains the contamination solely as a consequence of capitalist accumulation and class exploitation; it foregrounds capital–labor relations and calls for a generalized working-class uprising, but largely ignores racialized housing policy, immigrant status, gendered care burdens, and local historical land-use decisions."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete urban environmental scenario and four plausible research-report responses. Option A integrates critique of objectivity (power-knowledge), praxis (community action), and intersectional historical analysis of capitalist structures — matching critical theory. Other options isolate technical neutrality, pure discourse critique without praxis, or narrow economic determinism to serve as distractors.",
      "concepts_tested": [
        "Power-knowledge and critique of objectivity",
        "Praxis (theory linked to collective action) and social transformation",
        "Intersectionality and historical/contextual analysis of capitalist structures"
      ],
      "source_article": "Critical theory",
      "x": 1.1475824117660522,
      "y": 0.9571592211723328,
      "level": 2,
      "original_question_hash": "8e395e66"
    },
    {
      "question": "In the rural village of Aranya, infants initially engage in undirected physical play, older children begin organizing a rule-governed game, adolescents initiate coordinated calls to recruit teammates and resolve disputes during play, and two particular players repeatedly coordinate tactics over several matches. Later, the village opens a formal school where adults adopt the titles “Teacher” and “Principal,” and kin terms are standardized. Which interpretive account best integrates Sztompka’s layered progression of behaviour, the role of symbols from symbolic interactionism, and Weberian concerns about how individual meaningful action aggregates into macro social forms (e.g., movement along a Gemeinschaft–Gesellschaft continuum)?",
      "options": {
        "A": "This vignette exemplifies Sztompka’s sequence: animal-like movements → purposive actions → social actions → social contacts; the adoption of titles and kin terms shows that symbols are constitutive of new social roles (not merely decorative), and these meaningful individual actions and symbol-mediated roles aggregate into structural change—e.g., institutionalization of schooling shifts some relations toward Gesellschaft characteristics—consistent with Weber’s emphasis on social action producing macro-level patterns.",
        "B": "The sequence is best read as macro structures (the school and its bureaucracy) determining micro interactions: titles and kin terms preexist and simply impose meaning on behaviour, so individual actions are epiphenomenal and cannot generate large-scale social change—this reflects a structuralist correction to Sztompka and symbolic interactionism.",
        "C": "The correct interpretation reverses Sztompka’s order: social contacts precede social actions and purposive actions, and symbols merely label already-stable relationships; therefore the school’s titles only codify relations that were fixed by earlier collective consciousness rather than creating new interactional meaning.",
        "D": "The vignette should be analyzed purely in psychological terms: instinctual play transitions to learned habits and then to interpersonal coordination, but symbols are irrelevant to the sociological structure; macro change is the accumulation of habits, not the product of meaningful social action or symbolic naming."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete village vignette showing the Sztompka progression, introduction of symbolic titles, and institutionalization to test understanding of symbolic interactionism and Weberian micro–macro linkage; options include plausible but incorrect theoretical readings.",
      "concepts_tested": [
        "Sztompka's layered progression from behaviour to social contacts",
        "Role of symbols in constituting social relationships (symbolic interactionism)",
        "Micro–macro link: Weberian social action and Gemeinschaft–Gesellschaft institutional shifts"
      ],
      "source_article": "Social relation",
      "x": 1.2332689762115479,
      "y": 1.0049042701721191,
      "level": 2,
      "original_question_hash": "7f447ab1"
    },
    {
      "question": "A scholar prepares a critical edition of a 12th-century Occitan troubadour poem known from three divergent manuscripts. She collates variants, proposes emendations to recover the author's likely original wording, analyzes archaisms and Romance etymologies to date layers of transmission, and situates the poem in contemporaneous patronage records to explain lexical choices. A sociolinguist, by contrast, investigates present-day Occitan phonology across villages without consulting medieval texts. Which statement best characterizes the philological method and its historical role as exemplified by the editor's work?",
      "options": {
        "A": "Philology is fundamentally diachronic: it integrates textual criticism, literary criticism, historical inquiry and comparative linguistic evidence (including etymology) to establish a text's authenticity and original form and to determine meaning, contrasting with synchronic studies focused on present structures.",
        "B": "Philology is essentially a synchronic enterprise equivalent to structural linguistics, prioritizing analysis of language systems at a single time (e.g., present-day phonology) rather than reconstructing past forms or establishing manuscript authority.",
        "C": "Philology is limited to lexicography and etymology—compiling dictionaries and tracing word origins—without engaging in literary interpretation, manuscript collation, or the reconstruction of original texts.",
        "D": "Philology rejects all attempts to reconstruct an author's original text and instead always insists on a diplomatic edition that preserves each manuscript's idiosyncrasies, because any editorial emendation destroys the reliability of transmitted documents."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete manuscript-collation scenario contrasting a philological (diachronic, interdisciplinary, textual-critical) project with a synchronic sociolinguistic study to test understanding of philology's methods, goals, and historical scope.",
      "concepts_tested": [
        "Philology as diachronic, interdisciplinary study integrating textual criticism, literary criticism, history, and linguistics",
        "Role of philology in establishing authenticity and original form of texts via textual criticism and etymology",
        "Historical development and contrast between philology (diachronic) and synchronic linguistic analysis"
      ],
      "source_article": "Philology",
      "x": 1.2091858386993408,
      "y": 1.075187087059021,
      "level": 2,
      "original_question_hash": "972d8630"
    },
    {
      "question": "An audio engineer must create a low-bitrate version of a multitrack mix intended for a portable device whose loudspeaker cannot reproduce frequencies below $200\\ \\text{Hz}$. The mix contains: (1) a bass line with a fundamental at $60\\ \\text{Hz}$ and harmonics at $120,\\ 180,\\ 240,\\ 300,\\ 360\\ \\text{Hz}$; (2) a vocal band concentrated at $1$–$3\\ \\text{kHz}$ with long-duration energy at $75\\ \\text{dB SPL}$; and (3) intermittent drum transients peaking at $4\\ \\text{kHz}$ and $90\\ \\text{dB SPL}$. Which justification best describes psychoacoustic principles the engineer should apply to remove sub-$200\\ \\text{Hz}$ energy while preserving the listener's perception of bass and minimizing audible artifacts?",
      "options": {
        "A": "Exploit the missing-fundamental percept: preserve the harmonic structure above $200\\ \\text{Hz}$ so the brain perceives the $60\\ \\text{Hz}$ pitch even if the fundamental is removed. Allocate fewer bits to sub-$200\\ \\text{Hz}$ energy because the speaker cannot reproduce it and the ear is relatively insensitive there (equal-loudness weighting), while using simultaneous masking by the louder vocal and temporal masking from the drum transients to reduce precision in overlapping bands and time windows without producing audible artifacts.",
        "B": "Because the ear–brain pathway requires the physical fundamental to perceive pitch, the encoder must preserve the $60\\ \\text{Hz}$ component exactly; therefore the correct strategy is to downsample and lowpass elsewhere but keep all low-frequency samples to avoid pitch loss, even if the playback speaker cannot reproduce them.",
        "C": "Rely on nonlinear distortion and intermodulation in the reproduction chain to synthesize the missing $60\\ \\text{Hz}$ fundamental from higher harmonics; aggressively remove harmonics above $200\\ \\text{Hz}$ because they are redundant, and use loudness compression to prevent masking effects from the vocals.",
        "D": "Because temporal masking from drum transients is dominant, drop the entire $0$–$200\\ \\text{Hz}$ band only during drum hits and fully restore it elsewhere; this minimizes bitrate by exploiting forward masking alone and ignores the harmonic content above $200\\ \\text{Hz}$."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete codec/speaker scenario combining missing-fundamental, equal-loudness sensitivity, simultaneous and temporal masking to test understanding of ear nonlinearities and practical perceptual coding strategies (MP3-style). Incorrect options present plausible but flawed applications of psychoacoustic principles.",
      "concepts_tested": [
        "Ear–brain transformation and nonlinear perception (missing fundamental, loudness sensitivity)",
        "Perceptual attributes and spectral/temporal relationships (tonalness/harmonics, equal-loudness curve)",
        "Perceptual masking and its exploitation in perceptual audio coding (simultaneous and temporal masking)"
      ],
      "source_article": "Psychoacoustics",
      "x": 1.6197589635849,
      "y": 1.0814573764801025,
      "level": 2,
      "original_question_hash": "21922555"
    },
    {
      "question": "Bayport is a coastal city of 500,000 facing more frequent cyclones and coastal flooding due to climate change. The municipal government has a one-time budget of US$20 million to reduce future disaster risk over the next decade. Four proposed packages are under consideration:\n\nA) US$14M to build a 3 km concrete seawall (hard resilience) and US$6M to expand centrally managed emergency response teams; no formal community consultation or gender-specific measures.\n\nB) US$8M to revise urban development and zoning to enforce resilient building standards, US$4M for mangrove restoration and other nature-based defenses, US$4M for community-led early-warning systems and volunteer training, and US$4M to implement gender-responsive evacuation planning and local participatory governance mechanisms.\n\nC) US$20M to fund national carbon-emission reduction projects located outside Bayport (climate mitigation) with no direct local adaptation or DRR measures.\n\nD) US$20M to subsidize private flood insurance premiums for households and businesses while DRR strategy and implementation remain centrally designed without local input or gender considerations.\n\nWhich package most plausibly achieves the greatest long-term reduction in Bayport's disaster risk, and why?",
      "options": {
        "A": "Prioritizes hard resilience (seawall) and professional response capacity, reducing immediate hazard impacts through structural protection and faster emergency actions; best because stronger structures and response lower losses.",
        "B": "Combines regulatory integration of DRR into development, nature-based defenses, community-led early warning and training, and gender-sensitive, participatory governance — thereby reducing vulnerability, increasing both hard and soft resilience, and aligning DRR with climate adaptation and development planning.",
        "C": "Focuses on national climate mitigation which is important long-term for global hazard trajectories but does little to reduce Bayport's present vulnerability or exposure; therefore poorest choice for near- and medium-term risk reduction.",
        "D": "Expands financial risk transfer (insurance) to help economic recovery but does not reduce underlying vulnerability or exposure; centralised planning without community or gender input risks misaligned measures and lower effectiveness."
      },
      "correct_answer": "B",
      "generation_notes": "Created a concrete municipal budget scenario comparing structural, ecosystem-based, governance and financial approaches; option B reflects DRR principles: lowering vulnerability, increasing resilience (hard + soft), cross-sector integration with climate adaptation, local participation, and gender sensitivity.",
      "concepts_tested": [
        "DRR reduces disaster risk by lowering vulnerability and increasing resilience (hard and soft)",
        "Integration of DRR with climate change adaptation and development planning improves effectiveness",
        "Importance of inclusive governance, active local participation, and gender-sensitive approaches for effective DRR"
      ],
      "source_article": "Disaster risk reduction",
      "x": 1.3901530504226685,
      "y": 0.7943434715270996,
      "level": 2,
      "original_question_hash": "881c038b"
    },
    {
      "question": "A product-development team includes members from Brazil (high-context, indirect communication), Japan (high-context, restrained emotional display), and the United States (low-context, direct feedback). Over three weeks the team experiences repeated miscommunication: Brazilian members interpret terse written summaries from the U.S. as rude; Japanese members remain silent in video calls and are later criticized for not contributing; U.S. members complain that asynchronous emails lack clarity. Which team-level intervention best demonstrates (a) mutual adaptation that fosters bicultural/multicultural collaboration rather than assimilation, (b) an explicit acknowledgment that culture shapes how messages are encoded, transmitted, and interpreted, and (c) cultivation of cultural sensitivity, empathic understanding, and cultural agility?",
      "options": {
        "A": "Mandate a single ‘company’ communication style modeled on U.S. low-context norms (direct, explicit emails and open brainstorming in live meetings) and require all team members to adopt it to avoid ambiguity.",
        "B": "Provide language-focused training (English proficiency and business vocabulary) for non-native speakers and install automated translation tools to standardize message wording across time zones.",
        "C": "Facilitate a negotiated team communication protocol: agree on mixed media (synchronous video for brainstorming with explicit turn-taking, asynchronous written summaries with supportive explanatory phrasing), rotate meeting facilitators from different cultures, and run brief intercultural workshops emphasizing perspective-taking, specific decoding rules, and flexible adjustment strategies.",
        "D": "Assign a single bilingual liaison to rewrite all messages into a neutral template and handle all external communication so the multicultural team can continue working without changing internal habits."
      },
      "correct_answer": "C",
      "generation_notes": "Constructed a realistic multinational team scenario to require choosing the option that embodies mutual adaptation (not forcing assimilation), recognizes culture's role in encoding/transmission/interpretation, and emphasizes cultural sensitivity, empathy, and agility through negotiated protocols and training.",
      "concepts_tested": [
        "Mutual adaptation vs assimilation (biculturalism/multiculturalism)",
        "Culture shaping encoding, transmission, and interpretation of messages",
        "Cultural sensitivity, empathic understanding, and cultural agility as competencies"
      ],
      "source_article": "Intercultural communication",
      "x": 1.2318248748779297,
      "y": 1.0160622596740723,
      "level": 2,
      "original_question_hash": "7076f511"
    },
    {
      "question": "Professor L. is investigating a local claim that a deity revealed instructions that led to the end of a severe drought. Her research plan includes: semi-structured interviews with eyewitnesses about their experiences; archival research into past claims of divine intervention in the region; participant-observation of the community's rituals; and formal philosophical analysis of the coherence of the community's account. She must also choose whether to treat the reported revelation as an authoritative theological datum or as empirical testimony to be critically evaluated. Which option best characterizes how this plan illustrates core features of theological inquiry as described in the article?",
      "options": {
        "A": "This plan exemplifies theological methodological pluralism—combining experiential (interviews), historical, ethnographic (participant-observation), and philosophical analysis—while recognizing that revelation functions as an epistemic claim that may be accepted as authoritative within a tradition or treated as evidence to be assessed; the inquiry can legitimately aim to understand the community's belief, compare it with other traditions, defend or critique the claim, or recommend reforms in practice.",
        "B": "The plan treats theology strictly as historical criticism: revelation is irrelevant and should be discarded; only archival and textual methods matter, and the sole aim is to disprove the community's belief by exposing historical inconsistencies.",
        "C": "The plan treats revelation as an unquestionable metaphysical fact, so philosophical analysis is unnecessary; the only legitimate theological aim here is apologetics—defending the belief against external criticism rather than comparing or reforming it.",
        "D": "The plan reflects the approach of secular religious studies rather than theology: it brackets revelation entirely, adopts a neutral outsider stance, and restricts aims to descriptive reporting without any normative evaluation, comparison, or engagement with apologetic or reforming goals."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete case of a theologian studying a reported revelation to test recognition of multiple methods (experiential, ethnographic, historical, philosophical), the epistemic status of revelation, and theology's possible aims (understand, compare, critique, reform, defend).",
      "concepts_tested": [
        "Methodological pluralism in theology (experiential, philosophical, ethnographic, historical)",
        "Revelation as an epistemic claim shaping theological inquiry",
        "Relational and evaluative aims of theology (understanding, comparison, critique, reform, defense)"
      ],
      "source_article": "Theology",
      "x": 1.0703181028366089,
      "y": 1.0613868236541748,
      "level": 2,
      "original_question_hash": "80b4a136"
    },
    {
      "question": "Two historians publish competing accounts of a wave of 1870s rural unrest in Country X. Historian R bases his book on state archives, land surveys, and diplomatic dispatches, uses close archival criticism and a narrative political-history style, and is publicly identified with pro-state positions. Historian S relies on oral testimonies, parish registers, folk songs, and aggregated census data, applies social‑history methods and Marxist analysis, and emphasizes peasant grievances and structural economic causes. Which explanation best accounts for why R and S reach different conclusions and also illustrates major developments in historiography since the 19th century?",
      "options": {
        "A": "Because R and S choose different sources, research techniques, and interpretive frameworks: R’s archival, elite‑focused, professionalized method—shaped by 19th‑century archival norms and his state loyalties—leads him to stress political agency and order, while S’s use of oral evidence, quantification, and social/Marxist theory—reflecting 20th‑century interdisciplinary and social‑history shifts—brings out structural class dynamics; historians’ loyalties affect evidence selection and emphasis, so differing methodologies and frameworks explain historiographical disputes.",
        "B": "R’s account is more reliable because state archives are intrinsically superior to oral or parish sources; modern interdisciplinary methods mainly introduce noise, and the professional archival approach developed in the 19th century therefore produces the objectively true narrative.",
        "C": "The divergence is solely ideological: if one could remove R’s pro‑state bias and S’s Marxist commitments, both would report the same facts; methodological choices and source types do not materially affect historical interpretation.",
        "D": "Historiography has progressed to the point that contemporary historians converge on one objective narrative if they combine sources; R and S differ only because they failed to pool all available evidence into a collaborative synthesis."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete research scenario contrasting archival/Rankean political history with oral/quantitative social‑history to test how source choice, method, theory, and historian loyalties produce divergent interpretations and reflect professionalization and interdisciplinary shifts in historiography.",
      "concepts_tested": [
        "relationship between sources, methodologies, and theoretical frameworks",
        "influence of historians’ loyalties/biases on interpretation and debates about objectivity",
        "evolution of historiography (professionalization and interdisciplinary growth) and effects on research topics and approaches"
      ],
      "source_article": "Historiography",
      "x": -0.02360515482723713,
      "y": 0.47027653455734253,
      "level": 2,
      "original_question_hash": "6ac322de"
    },
    {
      "question": "Country X and Country Y both experienced a catastrophic port explosion in 1946. In Country X, school curricula, annual commemorations, monument inscriptions, and popular films consistently describe the event as an act of deliberate sabotage by an external enemy and emphasize national heroism. In Country Y, university historians publish archival research showing the proximate cause was an accidental fire compounded by lax safety regulations; public discourse in Y highlights structural failures and promotes multi-perspective debates. A population survey finds that most citizens of X recall the sabotage narrative while most citizens of Y recall the accident-and-regulation account. Which explanation best accounts for the divergence in recollections and correctly distinguishes collective memory from historical scholarship?",
      "options": {
        "A": "The divergence reflects collective memory in Country X: a socially constructed, value-laden pool of memories transmitted by education, rituals, monuments and media that serves group identity and favors a single group perspective, whereas historians in Country Y pursue a multi-perspective, evidence-based historical account that may revise or contradict the group's received narrative.",
        "B": "The divergence is best explained by differences in individual episodic memory ability between the populations: citizens of X have poorer memory encoding/retention and therefore adopt a simpler sabotage story, while citizens of Y have better individual memory and thus recall the more nuanced accident account.",
        "C": "The divergence shows that historians in Country Y have conspired to suppress heroic elements of the port explosion; consequently, the true collective memory is identical in both countries but only X's elites publicize it, whereas Y's elites hide it from the populace.",
        "D": "The divergence results from collaborative inhibition in group recall: conversations in Country X disrupted individuals’ idiosyncratic retrieval schemes causing them to converge on the sabotage story, while Country Y avoided group recalling and therefore retained the original accident-focused memories."
      },
      "correct_answer": "A",
      "generation_notes": "Created a comparative country scenario to probe understanding that collective memory is socially constructed, transmitted via institutions/rituals, serves group identity and contrasts with multi-perspective, evidence-driven history; distractors invoke individual memory deficits, conspiracy/suppression, and collaborative inhibition as plausible but incorrect explanations.",
      "concepts_tested": [
        "Collective memory as socially constructed and identity-linked",
        "Difference between collective memory and history (aims, perspectives, biases)",
        "Mechanisms of construction and transmission of collective memory (education, rituals, media)"
      ],
      "source_article": "Collective memory",
      "x": 1.3126336336135864,
      "y": 1.018530011177063,
      "level": 2,
      "original_question_hash": "fc6fafe1"
    },
    {
      "question": "PanVision is a transnational conglomerate that owns studios, a dominant streaming platform, major advertising networks, and a record label. It systematically develops franchise films from pre-tested narrative templates, recycles classical musical excerpts in commercials, uses surveillance-driven ads to create demand for branded 'self-expression' goods, and times releases to maximize subscriber churn. Critics compare PanVision’s cultural output to both 1930s state propaganda and to monopolistic Hollywood studio practices. Which of the following claims best captures Adorno and Horkheimer’s critique of such a culture industry?",
      "options": {
        "A": "PanVision exemplifies a genuinely democratic mass culture: by producing widely appealing goods and enabling consumer choice through streaming, it empowers audiences and pluralizes cultural expression.",
        "B": "PanVision is a neutral market mechanism where supply and demand determine tastes; its standardized products simply reflect consumer preferences and increase overall welfare through competition and innovation.",
        "C": "PanVision mirrors a factory-like culture industry that standardizes and commodifies cultural goods, manufactures false psychological needs that can only be satisfied by capitalist commodities (crowding out true needs like creativity and freedom), and thereby reinforces obedience to economic and political power across different regimes.",
        "D": "PanVision’s practices are an inevitable technological evolution that intensifies individual imagination by giving more people access to diverse content and faster cultural exchange, ultimately undermining old elite controls over culture."
      },
      "correct_answer": "C",
      "generation_notes": "Constructed a concrete conglomerate scenario to test three core Adorno/Horkheimer ideas: standardization/commodification producing passive, homogenized publics; false psychological needs versus true needs; and the linkage between mass culture and power across regimes. Distractors reflect plausible opposing interpretations (democratic, neutral market, technological progress).",
      "concepts_tested": [
        "Standardization and commodification of cultural goods",
        "False psychological needs versus true needs (creativity, freedom)",
        "Culture's relationship to economic and political power across regimes"
      ],
      "source_article": "Culture industry",
      "x": 0.9636409878730774,
      "y": 1.0131202936172485,
      "level": 2,
      "original_question_hash": "f3ed5dec"
    },
    {
      "question": "In the hypothetical village of Ngaru, elders called \"memory-holders\" perform genealogies, laws, and ritual hymns as metrically patterned songs each harvest season; younger villagers learn by reciting these with formulaic refrains and attending public performances. Scholars studying Ngaru note (i) some traditions are later written down by missionaries, (ii) a few narratives are personal eyewitness accounts recorded on tape, and (iii) certain ritual songs are reserved for initiated priests. Which single statement most accurately synthesizes (a) the mechanism by which Ngaru preserves and transmits knowledge, (b) how that system relates to writing and to oral history, and (c) the social-cultural scope of the traditions described?",
      "options": {
        "A": "Ngaru’s practices exemplify oral tradition: knowledge is preserved via performance by specialized ‘‘walking libraries’’ using mnemonic devices (meter, repetition, formulaic phrases) and community validation; it can operate alongside or be transcribed into writing, is distinct from recorded oral history (first‑person testimony), and serves multiple social functions (religion, law, genealogy, literature) with both public (exoteric) and restricted (esoteric) forms.",
        "B": "Ngaru’s performances are merely informal entertainment with low mnemonic reliability; because some traditions were later written, oral transmission is redundant and inferior to writing, and oral history (recorded eyewitness tapes) is the only valid source for reconstructing past events.",
        "C": "The village traditions are fixed, unchanging texts preserved verbatim by rote memory alone; their exclusivity to initiated priests shows oral tradition cannot convey civic law or communal history and therefore cannot coexist with writing except as exact copies.",
        "D": "Oral transmission in Ngaru proves that oral tradition is identical to oral history and only functions in pre‑literate societies; once writing is introduced, oral forms always disappear and have no role in law, religion or broader cultural memory."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete village scenario to probe transmission mechanisms (mnemonics, performers), relationship to writing vs oral history, and sociocultural functions (law, religion, exoteric/esoteric). Provided one comprehensive correct synthesis and three distractors reflecting common misconceptions.",
      "concepts_tested": [
        "mechanisms of oral transmission and mnemonic devices",
        "relationship between oral tradition, writing, and oral history; coexistence and distinction",
        "social and cultural functions of oral tradition (law, religion, literature; exoteric vs esoteric)"
      ],
      "source_article": "Oral tradition",
      "x": 1.191564679145813,
      "y": 1.010817527770996,
      "level": 2,
      "original_question_hash": "e77f3456"
    },
    {
      "question": "A commercial salmon farm intends to increase mean harvest body weight by ~30% over several generations while minimizing loss of genetic diversity and avoiding inbreeding depression. The farm can implement one of four breeding programs. Which program best exploits artificial selection as a directed mechanism while balancing the genetic consequences of inbreeding, linebreeding, and outcrossing?",
      "options": {
        "A": "Apply strict inbreeding: each generation select the top 5% of individuals by weight and mate full- or half-siblings among them to 'breed the best to the best' until the target is reached.",
        "B": "Use continual outcrossing: each generation select the heaviest individuals but cross them with unrelated wild broodstock to maximize heterozygosity and keep gene flow from wild populations constant.",
        "C": "Adopt a linebreeding program with intense within-line selection (selecting superior families and individuals) complemented by planned, infrequent outcrosses (e.g., every 3–5 generations) to reintroduce variation and purge deleterious alleles.",
        "D": "Cease controlled mating and rely on natural selection in production ponds to increase average weight over time through survival and reproductive success of larger individuals."
      },
      "correct_answer": "C",
      "generation_notes": "Designed a practical aquaculture scenario testing (1) artificial selection as intentional breeder choice, (2) contrast with nondirected natural selection, and (3) genetic consequences of inbreeding, linebreeding, and outcrossing; option C balances rapid response with maintenance of genetic diversity.",
      "concepts_tested": [
        "Artificial selection as a directed evolutionary mechanism",
        "Difference between artificial (directed) and natural (non-directed) selection",
        "Effects of inbreeding, linebreeding, and outcrossing on genetic variation and trait propagation"
      ],
      "source_article": "Selective breeding",
      "x": 1.8485937118530273,
      "y": 1.11403226852417,
      "level": 2,
      "original_question_hash": "caf09347"
    },
    {
      "question": "Consider the agrarian polity of Valoria in the 19th century: communal land tenure, localized artisan workshops and little commodity exchange. Over thirty years, entrepreneurs introduce steam-powered looms and factory organization; productivity per worker rises, commodity markets expand, many artisans become wage laborers, and a class of capital-accumulating merchants emerges. Political institutions change as courts and police increasingly defend private property and contract enforcement. According to the historical materialist framework of Marx and Engels, which causal sequence best explains Valoria's social transformation?",
      "options": {
        "A": "Technological innovations expand Valoria’s forces of production → the relations of production reconfigure (emergence of bourgeoisie and proletariat) → the economic base becomes capitalist (new mode of production) → the superstructure (state, law, ideology) adjusts to secure the new property relations → intensified class conflict shapes subsequent historical change.",
        "B": "A new ideology of individual liberty and property first motivated local inventors and merchants to create factories; these ideas then produced the technological and economic changes that converted Valoria into a capitalist society.",
        "C": "State reforms and deliberate legal changes to property and contract law preceded and caused the rise of factories and capitalist relations; the state engineered the economic transformation which then triggered technological adoption.",
        "D": "Moral decline and the selfish behaviour of particular leaders produced class divisions; technology played at most an incidental role and social change is best explained by contingency and individual agency rather than structural economic factors."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete 19th-century scenario (Valoria) and presented competing causal chains to test recognition of historical materialism: forces of production → relations of production → mode of production/base → superstructure → class struggle. Distractors reflect idealist, state-first, and contingency explanations.",
      "concepts_tested": [
        "Mode of production as primary driver of historical change",
        "Technological development (forces of production) reshaping relations of production and society",
        "Class formation and class struggle as engines of historical transformation"
      ],
      "source_article": "Historical materialism",
      "x": 0.9478868842124939,
      "y": 0.9669820666313171,
      "level": 2,
      "original_question_hash": "7c21894e"
    },
    {
      "question": "In the river-valley polity of Kalinda, a new irrigation technique doubles cereal yields. However, land and water rights are legally vested in a temple theocracy that extracts surplus via mandatory corvée labour, stores grain in communal granaries, and legitimizes its rule through ritual. Merchants circulate excess produce regionally, but peasants cannot independently own land or freely sell labour. After several decades, tensions between increasingly productive agriculture and the theocracy’s labour exactions lead to periodic uprisings. Which interpretation best applies Marx’s concept of a mode of production to Kalinda?",
      "options": {
        "A": "Kalinda’s trajectory demonstrates technological determinism: the new irrigation (productive force) alone will inevitably remodel social, political and spiritual life regardless of existing property relations or class arrangements.",
        "B": "Kalinda’s mode of production is constituted by the interaction of the new productive forces (irrigation, improved yields) and the existing social relations of production (temple property, corvée, class rule); this configuration structures legal-ritual legitimation, modes of distribution (granaries), circulation (merchant networks) and consumption, reproduces its own conditions, and generates internal contradictions that produce uprisings and potential systemic change.",
        "C": "The essential feature of Kalinda’s system is its means of production (irrigation and tools); social and ideological forms (theocracy, rituals, laws) are merely epiphenomena and do not belong to the mode of production proper.",
        "D": "Kalinda’s social transformations are best explained by external shocks (trade disruptions, invasions) and demographic shifts; internal contradictions between productive capabilities and social arrangements play only a negligible role in socioeconomic change."
      },
      "correct_answer": "B",
      "generation_notes": "Presented a concrete river-valley scenario combining technological change and entrenched property relations; options contrast Marxist base–superstructure, technological determinism, purely materialist (means-only) views, and externalist explanations to test comprehension of all three target concepts.",
      "concepts_tested": [
        "Definition of mode of production as productive forces plus social/technical relations of production",
        "Mode of production determining social, political and economic forms (distribution, circulation, consumption)",
        "Organic totality and emergence of social change from internal contradictions between forces and relations"
      ],
      "source_article": "Mode of production",
      "x": 1.0938280820846558,
      "y": 0.9687406420707703,
      "level": 2,
      "original_question_hash": "1752ce35"
    },
    {
      "question": "On island Arida a flowering plant species P has historically been pollinated by a native bird species B with a long beak. After a recent wasp invasion the bird population declined sharply. Prior to the invasion the population of P carried an allele p1 at frequency $0.05$ that produces slightly shallower corollas; after 40 plant generations p1 is at frequency $0.70$. The plant population has an effective size $N_e=10{,}000$, so random genetic drift is expected to be weak. P also possesses sticky trichomes that fossil and historical records show predate the wasp invasion; these trichomes were previously interpreted as anti-herbivore defenses but are now observed to carry pollen between visiting generalist bees. Some naturalists claim the plants ‘‘evolved trichomes in order to recruit bees’’ after the bird decline. Which of the following explanations best integrates the roles of natural selection, co‑adaptation/co‑evolution, exaptation, and the teleology debate for these observations?",
      "options": {
        "A": "The rapid increase of p1 from $0.05$ to $0.70$ in a large $N_e$ population is consistent with positive natural selection acting on corolla depth because p1 increases reproductive success under the new, bird‑poor environment; the sticky trichomes are best interpreted as an exaptation (a pre‑existing trait co‑opted for pollen transfer) rather than a feature that evolved ‘in order to’ recruit bees; over time reciprocal selection could lead to co‑adaptation between P and bee pollinators, but this is a contingent, historical process rather than purposeful design.",
        "B": "The presence of trichomes that now carry pollen indicates that plants purposefully evolved that trait after the bird decline to recruit bees; the change in p1 frequency reflects an intrinsic drive in the plants to become better adapted, so teleological language (traits evolving in order to achieve a goal) is the most accurate description.",
        "C": "Because the island experienced demographic upheaval when birds declined, the p1 frequency change is most parsimoniously explained by genetic drift and founder effects; the trichomes remain defensive and coincidentally carry pollen — co‑evolution with bees is unlikely because bees are generalists and exaptation is not necessary to explain the observations.",
        "D": "The trichomes must have evolved de novo as a pollination adaptation after the invasion, so they are not an exaptation; the rise of p1 is probably due to migration of plants from nearby areas where p1 was common rather than selection; co‑adaptation plays no role and teleological descriptions are acceptable shorthand for adaptive change."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed an island scenario with quantitative allele‑frequency change and large effective population size to test recognition of natural selection versus drift, included pre‑existing trichomes to probe exaptation vs de novo adaptation, and asked about co‑evolution and teleology to assess conceptual synthesis.",
      "concepts_tested": [
        "Natural selection as cause of allele frequency change relating environment and fitness",
        "Co‑evolution / co‑adaptation between interacting species (plants and pollinators) and mimicry implications",
        "Exaptation (co‑option of traits) and rejection of teleological explanations of adaptation"
      ],
      "source_article": "Adaptation",
      "x": 1.8279459476470947,
      "y": 1.1179730892181396,
      "level": 2,
      "original_question_hash": "ab28e155"
    },
    {
      "question": "A chronic environmental mutagen preferentially induces single-strand DNA breaks in hepatocytes. Initially, constitutive DNA repair pathways compensate, but after months a clonal population of hepatocytes acquires a loss-of-function mutation in a gene encoding a key DNA repair enzyme. Which sequence of events best explains how this molecular lesion can propagate through the central dogma to produce a systemic disturbance of metabolic homeostasis?",
      "options": {
        "A": "Loss-of-function mutation in the repair gene (DNA) → defective or unstable mRNA transcribed from that locus → reduced or nonfunctional repair protein produced (central dogma) → impaired DNA repair and accumulation of DNA lesions in hepatocyte genomes → secondary mutations in genes encoding metabolic enzymes (e.g., gluconeogenic enzymes) → altered enzyme proteins and reduced hepatic gluconeogenesis → systemic hypoglycemia with compensatory increase in glucagon and sympathetic drive.",
        "B": "Loss-of-function mutation in the repair gene (DNA) → transcriptional upregulation of that gene to compensate → increased mRNA and overproduction of repair protein → hyperactive repair reduces all DNA damage to zero → improved metabolic homeostasis and lower circulating glucose variability.",
        "C": "Loss-of-function mutation in the repair gene (DNA) → immediate change in circulating hormone concentrations because DNA directly binds and neutralizes hormones → systemic metabolic disturbance without changes in mRNA or protein levels (bypassing the central dogma).",
        "D": "Loss-of-function mutation in the repair gene (DNA) → accumulation of mutations primarily in mature erythrocytes leading to impaired oxygen transport → tissue hypoxia triggers hepatic failure and systemic metabolic imbalance, because mature erythrocytes contain abundant nuclear DNA vulnerable to the mutagen."
      },
      "correct_answer": "A",
      "generation_notes": "Created a clinical-style hepatocyte scenario linking DNA damage/repair failure to downstream mutations, used central dogma to connect DNA→RNA→protein changes, and described system-level homeostatic compensation (glucagon, sympathetic response). Distractors violate central dogma or biological facts (compensatory overexpression, DNA directly altering hormones, erythrocytes lacking nuclei).",
      "concepts_tested": [
        "Homeostasis and systemic regulation (compensatory hormonal responses)",
        "Central dogma (DNA→RNA→protein) and protein-mediated cell function",
        "DNA damage, repair capacity, and accumulation of mutations leading to altered protein function"
      ],
      "source_article": "Human body",
      "x": 1.9787484407424927,
      "y": 1.1489979028701782,
      "level": 2,
      "original_question_hash": "b5f3a94f"
    },
    {
      "question": "A 45-year-old woman with chronic low back pain (6 months' duration) has failed a trial of NSAIDs and two months of physiotherapy. She asks your opinion about an intra‑paravertebral “herbal” injection offered by a local traditional practitioner who claims it will “cure” her pain; there are no published randomized trials or safety data for this injection. You recall the article’s points that (1) modern medicine synthesizes art (clinical judgment, skills, and patient-centred communication) and science (biomedical and molecular‑level knowledge), (2) evidence‑based medicine guides interventions though about 49% of interventions may lack sufficient evidence, and (3) traditional remedies may be used where scientific evidence is lacking but can cross into quackery when safety/efficacy are unsupported. Which single course of action best reflects an ethical, evidence‑informed, and professionally appropriate response?",
      "options": {
        "A": "Respect her autonomy and say you have no objection, referring her to the traditional practitioner without comment since patients may choose outside therapies.",
        "B": "Recommend immediate surgical consultation because conservative care failed and surgery is definitive; discourage any unproven traditional treatments as unsafe quackery.",
        "C": "Explain the current evidence (including that many interventions lack robust trials), recommend a further trial of evidence‑based non‑surgical options (e.g., structured exercise, CBT, optimized analgesia), document the informed discussion, and if she still wishes to pursue the herbal injection, discuss the unknown risks and arrange monitoring rather than administer it yourself.",
        "D": "Refuse to discuss the traditional treatment and admonish the patient for considering unscientific options, insisting she continue only therapies you approve of."
      },
      "correct_answer": "C",
      "generation_notes": "Created a clinical vignette requiring synthesis of art (communication, shared decision‑making) and science (evidence‑based options, uncertainty), highlighted the 49% limited-evidence fact, and contrasted ethical choices regarding traditional medicine versus quackery.",
      "concepts_tested": [
        "Medicine as synthesis of clinical art and biomedical science",
        "Principles and limitations of evidence‑based medicine and shared decision‑making",
        "Relationship between traditional/folk remedies, safety/efficacy concerns, and the boundary with quackery"
      ],
      "source_article": "Medicine",
      "x": 1.880645990371704,
      "y": 1.119452953338623,
      "level": 2,
      "original_question_hash": "c6112f31"
    },
    {
      "question": "The Ministry of Health in the country of Veridia has passed a national law mandating an HPV vaccination program to reduce cervical cancer. Before nationwide roll-out, randomized controlled trials (RCTs) in three pilot districts compared two delivery models: clinic-based delivery (baseline uptake $40\\%$) and community outreach (uptake $75\\%$). The Ministry must translate the law into sustainable services that achieve health outcomes at scale. Which of the following strategies best exemplifies the correct policy process, uses the RCT evidence appropriately, and creates the multi-level governance environment needed for sustainable scale-up?",
      "options": {
        "A": "Adopt the community outreach model indicated by the RCTs, issue national operational policies (standard guidelines, procurement rules, financing allocations), set measurable targets and monitoring systems, and coordinate responsibilities across national, regional, and municipal health authorities with dedicated recurrent funding and adaptive evaluation.",
        "B": "Rely on the national law alone to compel providers to offer the vaccine, instruct private clinics to implement it without new operational guidelines, and leave financing to out-of-pocket payments to preserve provider autonomy.",
        "C": "Accept the RCT result but implement the outreach model solely through a centrally run task-force that bypasses regional health agencies and local clinics, deploying temporary project grants with no long-term financing plan or local capacity building.",
        "D": "Delay implementation to resolve ideological debates about the right to health, then privatize delivery through competing insurance contracts assuming market competition will improve uptake and efficiency without issuing national operational rules."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete national vaccination roll-out scenario with RCT evidence and asked which option demonstrates correct translation from law to operational policy, evidence-based selection, and multi-level governance for sustainable scale-up.",
      "concepts_tested": [
        "Policy process and translation from law to programs and services",
        "Evidence-based policymaking using randomized trials",
        "Multi-level governance and supportive policy environment for scalable health interventions"
      ],
      "source_article": "Health policy",
      "x": 1.2816033363342285,
      "y": 0.9383191466331482,
      "level": 2,
      "original_question_hash": "bd78a085"
    },
    {
      "question": "A transregional movement called \"RiverGuardians\" forms to oppose construction of a hydroelectric dam. Early membership includes local fishers, university students, an environmental NGO, and performance artists. Over six months the group negotiates a shared platform (stop construction; enforce environmental remediation), decides on tactics (legal injunctions, city blockades, social media campaigns), develops intercity coordination networks, adopts shared symbols and protest songs, and forges an alliance with an upstream Indigenous community. Simultaneously, they face injunctions, police repression, limited funding, and a narrow electoral window. Which interpretation best applies Melucci’s three-part model of collective identity and its analytical usefulness for understanding RiverGuardians’ trajectory?",
      "options": {
        "A": "RiverGuardians simply instantiate a pre-existing, fixed ideological identity; their public actions are straightforward expressions of an already-given set of beliefs, so internal leadership, communication practices, and external alliances are analytically secondary.",
        "B": "RiverGuardians’ identity was negotiated through (1) a cognitive definition of goals, means, and environmental constraints; (2) activation of relationships and networks across sites; and (3) emotional investments manifest in shared symbols and rituals. This collective identity both oriented the group’s tactics within the opportunities/constraints (injunctions, elections, alliances) and is analytically useful for explaining internal dynamics (leadership, communication) and external relations (allies, opponents).",
        "C": "The development of RiverGuardians is best explained by individual-level social identity processes: members’ self-categorization into a group explains convergence on tactics, while group-level negotiation and external political constraints play little causal role.",
        "D": "RiverGuardians’ activities are primarily resource-mobilization outcomes: collective identity functions only as an instrumental coordination device to secure funds and logistics, while emotional investments and meaning-making are epiphenomenal and irrelevant to explaining the movement’s choices."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete social-movement scenario that requires applying Melucci’s three-part negotiated process (cognitive definition, active relationships, emotional investments) and the model’s analytic value for internal dynamics and external relations in a field of opportunities/constraints.",
      "concepts_tested": [
        "Melucci’s three-part process of collective identity (cognitive definition, active relationship, emotional investments)",
        "Collective identity as an analytical tool for social movements including internal dynamics and external relations",
        "Orientation of action within a field of opportunities and constraints and reciprocal shaping between identity and environment"
      ],
      "source_article": "Collective identity",
      "x": 1.237585425376892,
      "y": 0.9763872027397156,
      "level": 2,
      "original_question_hash": "dc82d61b"
    },
    {
      "question": "A field team samples a sedimentary sequence that includes a volcanic ash layer containing zircon. U–Pb dating of the zircon yields an age of $250\\pm2$ Ma. The same stratigraphic interval contains an index fossil assemblage whose documented range is 252–247 Ma, and the recovered paleomagnetic polarity pattern matches a segment of the apparent polar wander path dated to 251–249 Ma. Which interpretation best applies to these data?",
      "options": {
        "A": "The U–Pb zircon age provides an absolute numerical age of $250\\pm2$ Ma; the fossil assemblage (252–247 Ma) and paleomagnetic match (251–249 Ma) are relative constraints. The intersection of all three intervals is $[249,251]$ Ma, so combining the methods refines the sample age to about 250 Ma with improved precision compared to any single indicator.",
        "B": "Because radiometric dates are generally unreliable in sedimentary contexts, the fossil and paleomagnetic constraints (252–247 Ma and 251–249 Ma) should be preferred, giving a best age range of $[249,252]$ Ma; therefore the U–Pb age should be discarded.",
        "C": "The fossil assemblage alone gives an absolute numerical age (252–247 Ma); radiometric and paleomagnetic data are only relative indicators and cannot improve the numeric age estimate, so the best age is 252–247 Ma.",
        "D": "Each method yields similar uncertainty independently, so combining them does not change precision; the best estimate remains the radiometric value $250\\pm2$ Ma with no improvement from the relative data."
      },
      "correct_answer": "A",
      "generation_notes": "Created a field scenario with a U–Pb absolute age and two relative constraints; students must compute interval intersections and recognise that combining absolute and relative methods refines age estimates.",
      "concepts_tested": [
        "Absolute geochronology via radioactive decay and known half-lives (U–Pb yields numerical age)",
        "Relative geochronology using fossils and paleomagnetism places rocks within time intervals rather than exact ages",
        "Combining multiple geochronological and biostratigraphic indicators reduces uncertainty and strengthens age determinations"
      ],
      "source_article": "Geochronology",
      "x": 1.8259949684143066,
      "y": 0.9506072998046875,
      "level": 2,
      "original_question_hash": "32a07ee5"
    },
    {
      "question": "Consider an isolated rigid container holding an inviscid, incompressible conducting fluid that carries a passive dye whose density is y(r,t). The dye is advected by the fluid with velocity u(r,t) and there is no diffusion or sources. The Lagrangian describing the fluid–dye system is invariant under continuous time translations and rotations. Assess the following statements:\n\n1) The dye satisfies the local continuity equation $y_{t}+\\nabla\\cdot(y\\mathbf{u})=0$, and therefore for any fixed control volume V the rate of change of total dye in V equals minus the flux of dye across the boundary $\\partial V$. \n\n2) By Noether's theorem, the time-translation and rotational invariance of the Lagrangian guarantee conserved quantities that can be identified with the total energy and total angular momentum of the isolated system; these are exact conservation laws for this model. \n\n3) The rest mass of fluid elements is exactly conserved even if the fluid velocities become relativistic, so conservation of rest mass is an exact law for this isolated system at all speeds.\n\nWhich one of the following is correct?",
      "options": {
        "A": "All three statements (1, 2 and 3) are correct.",
        "B": "Only statements 1 and 2 are correct; statement 3 is false.",
        "C": "Only statement 1 is correct; statements 2 and 3 are false.",
        "D": "Only statements 2 and 3 are correct; statement 1 is false."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete fluid-advection scenario to test application of the continuity equation (local conservation and flux), the Noether correspondence between differentiable symmetries and exact conserved quantities (energy, angular momentum), and the distinction between exact mass-energy conservation and the approximate nature of rest-mass conservation at relativistic speeds.",
      "concepts_tested": [
        "Continuity equation and local conservation (flux through boundary)",
        "Noether's theorem: symmetry ↔ conservation (time translations → energy, rotations → angular momentum)",
        "Exact versus approximate conservation laws (mass-energy exact, rest mass approximate at nonrelativistic limit)"
      ],
      "source_article": "Conservation law",
      "x": 1.7242058515548706,
      "y": 1.1275818347930908,
      "level": 2,
      "original_question_hash": "976137e2"
    },
    {
      "question": "A new intravenous analgesic X is given as a 100 mg bolus. Plasma concentrations decline monoexponentially with a half-life $t_{1/2}=2\\,$h. In vitro, X is a full agonist at a G protein–coupled receptor with equilibrium dissociation constant $K_d=0.1\\,$nM and an experimentally measured receptor residence time (dissociation half-life from the receptor) of 16 h. The in vitro EC$_{50}$ for immediate second‑messenger generation is 0.5 nM. Clinically, robust analgesia lasts ~12 h after a single bolus even though measured plasma concentrations fall below the plasma EC$_{50}$ by 3 h. Which explanation best reconciles the pharmacokinetic and pharmacodynamic observations?",
      "options": {
        "A": "X is extensively metabolized to one or more active metabolites with elimination half-lives near 12 h; these metabolites sustain receptor activation after parent plasma levels fall, so the observed duration reflects metabolism (PK) rather than receptor kinetics.",
        "B": "X has a very large volume of distribution and slowly redistributes from adipose/tissue reservoirs back into plasma; tissue reservoirs maintain effective free concentrations at the receptor site for ~12 h despite falling measured plasma levels.",
        "C": "Because X has a very slow dissociation rate from its GPCR (residence time ≈ 16 h), receptor‑bound drug persists and continues to elicit amplified downstream signaling; thus pharmacodynamic duration exceeds plasma half-life even though ADME lowers free plasma concentration.",
        "D": "Renal excretion of X is inhibited in vivo, causing accumulation of the unchanged drug in plasma and prolonging effect; the prolonged analgesia therefore reflects reduced clearance (Cl) rather than receptor or signaling properties."
      },
      "correct_answer": "C",
      "generation_notes": "Designed a concrete IV bolus scenario with numeric PK (half-life) and PD (Kd, residence time, EC50) to require integration of ADME, receptor binding kinetics and signal transduction; options include plausible alternative PK explanations (active metabolites, tissue redistribution, reduced clearance) to test understanding of PD–PK interplay.",
      "concepts_tested": [
        "Interplay between pharmacokinetics (ADME) and pharmacodynamics in determining drug action",
        "Receptor-mediated drug action and the role of receptor binding kinetics and signal transduction in prolonging effect",
        "ADME processes (distribution, metabolism, excretion) as alternative explanations for duration of effect"
      ],
      "source_article": "Pharmacology",
      "x": 1.4693645238876343,
      "y": 1.0200660228729248,
      "level": 2,
      "original_question_hash": "b5c2debb"
    },
    {
      "question": "You are the lead clinician at a tertiary hospital participating in a first-in-human gene‑editing trial (CRISPR-based) for a debilitating inherited disorder. The trial has limited enrollment slots. The local community places strong emphasis on family decision‑making: several potential participants are adults with mild cognitive impairment and no advance directives; their adult children insist the parent be enrolled to honor family obligations. Some competent patients refuse participation citing religious objections. You must design a selection and consent process that conforms to principle‑based medical ethics and relevant historical research safeguards (e.g., Nuremberg Code, Declaration of Helsinki, Hippocratic ideals) while remaining sensitive to local cultural norms and the triadic relationships (clinician—patient, clinician—relatives, clinician—colleagues). Which course of action best satisfies these ethical requirements?",
      "options": {
        "A": "Accept the adult children's consent and enroll their cognitively impaired parents without further assessment because family consent reflects local cultural norms and will maximize community support for the research.",
        "B": "Allow the principal investigator (you) to select participants whom you judge most likely to benefit, relying on clinical judgement (beneficence) and omit formal independent review to expedite enrollment.",
        "C": "Establish transparent, clinically based eligibility criteria; obtain valid informed consent from competent individuals per the Nuremberg/Declaration of Helsinki standards; for those lacking capacity use ethically appropriate surrogate decision‑making guided by substituted judgment (not mere family convenience); allocate scarce slots by fair mechanisms (e.g., prioritization by medical prospect of benefit combined with random selection among equally eligible candidates); and convene a culturally diverse hospital ethics committee (including community representatives) to review borderline cases and ensure respect for local values while upholding core research safeguards.",
        "D": "Postpone the trial until unanimous community approval is obtained via a community referendum, since community endorsement supersedes individual consent in this cultural context."
      },
      "correct_answer": "C",
      "generation_notes": "Created a concrete clinical trial scenario that triggers conflicts among autonomy, beneficence, non‑maleficence, and justice; incorporated historical documents (Nuremberg, Helsinki, Hippocratic) and triadic relationships; options contrast paternalism, cultural deference, fair procedural justice, and communal decision‑making to test correct balancing.",
      "concepts_tested": [
        "Principle‑based ethics and resolving conflicts/hierarchy among autonomy, beneficence, non‑maleficence, justice",
        "Role of historical codes and documents (Nuremberg Code, Declaration of Helsinki, Hippocratic Oath) in shaping informed consent and research safeguards",
        "Importance of relationships and cultural context (clinician–patient, clinician–relatives, ethics committee composition) in applying medical ethics"
      ],
      "source_article": "Medical ethics",
      "x": 1.1911789178848267,
      "y": 0.9922716021537781,
      "level": 2,
      "original_question_hash": "369a862a"
    },
    {
      "question": "A regional watershed contains multiple farming catchments, two small towns, riparian wetlands, and a downstream reservoir that each summer develops harmful algal blooms (HABs). Scientists estimate that to restore aquatic health the reservoir’s peak algal biomass must be reduced by \\(50\\%\\) during summer months. Which management strategy best exemplifies systems-theoretic reasoning — explicitly addressing emergence, causal boundaries and context, feedback and adaptation, and equifinality — to achieve that target?",
      "options": {
        "A": "Implement an integrated, adaptive watershed-management program that (1) coordinates upstream agricultural best-management practices to reduce nutrient runoff, (2) restores riparian wetlands to enhance denitrification and slow flows, and (3) establishes a stakeholder-driven monitoring-and-feedback system that iteratively evaluates outcomes and adjusts interventions (allowing multiple pathways to reach the 50% reduction).",
        "B": "Allocate funding solely to upgrade the downstream water-treatment plant to remove more nitrogen and phosphorus at the reservoir outflow, while leaving upstream land use and hydrology unchanged.",
        "C": "Adopt municipal zoning within each town to restrict new impermeable surfaces and subsidize urban tree-planting, without coordinating actions across the watershed or with upstream agricultural stakeholders.",
        "D": "Apply seasonal algaecide treatments to the reservoir whenever blooms are detected and run a public-education campaign about fertilizer use, but do not alter landscape hydrology or set up continuous monitoring and adaptive management."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a watershed scenario where emergent algal blooms arise from interacting components. Options contrast a systemic, adaptive, boundary-aware approach (correct) with part-focused, reactive, or jurisdictionally fragmented strategies.",
      "concepts_tested": [
        "emergence and holism",
        "boundaries, context, interdependence",
        "dynamics, feedback, and adaptation",
        "equifinality"
      ],
      "source_article": "Systems theory",
      "x": 1.480358600616455,
      "y": 1.085618019104004,
      "level": 2,
      "original_question_hash": "0edd8b47"
    },
    {
      "question": "You are designing access control for a university facility that contains a BSL-3 wet lab (high biological risk), a locked server room with research data, and shared office space. Users include Principal Investigators (PIs), graduate students, short-term contract technicians, overnight cleaning staff, and remote collaborators. The design must: (1) adhere to the principle of least privilege (each user gets only the access needed and no more); (2) separate policy specification from enforcement so policies can be revised without replacing enforcement hardware; (3) enforce the physical \"who, where, and when\" constraints (who may enter, which door/area, and permitted times) including strict control for the BSL-3 suite; and (4) mitigate common failure modes such as tailgating, credential cloning, door levering, and sequential credential attacks. Which single design choice below best satisfies all requirements?",
      "options": {
        "A": "Use a centrally managed policy decision architecture (PDP) that stores high-level policies expressed as RBAC roles augmented by ABAC attributes (time, clearance level, contractor expiry). Enforce policies via distributed policy enforcement points (PEPs) at doors: IP-enabled intelligent readers with mutual-authenticated smartcards and a second factor (PIN or biometric) for access to server room and BSL-3. Protect BSL-3 with a mantrap and anti-tailgate turnstile, door-prop/forced-door sensors, high-holding-force strikes, encrypted credential transmission, randomized credential serials, short-lived temporary credentials for contractors, and full audit logging on the central server.",
        "B": "Use discretionary access control (DAC): data and door owners assign access lists per resource. Rely on mechanical keyed locks for interior lab doors and legacy proximity cards for server room. Keep per-door ACLs on local control panels; re-key or update each door when an employee leaves. Use guards to prevent tailgating and mechanical keys as fail-over if cards fail.",
        "C": "Adopt a capability-based model: give users unforgeable tokens that grant access to specific rooms; readers validate token possession and unlock. Rely on single-factor capability tokens (long-lived) and mechanical turnstiles at the main entrance, without centralized policy storage. Use the same token for physical and logical access to simplify management.",
        "D": "Adopt Mandatory Access Control (MAC) with fixed clearance labels on users and classification labels on areas; implement with basic readers that forward card IDs to a local control panel. Use proximity cards only (no second factor), and rely on periodic manual reviews of label assignments to enforce least privilege. Use mechanical free-egress and alarmed doors to address forced entry."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a realistic campus lab scenario requiring least privilege, separation of policy (PDP/PEP) from enforcement, physical who/where/when constraints, and common failure mitigations. Provided one hybrid RBAC+ABAC design with central policy and distributed enforcement (correct) and three plausible but flawed alternatives.",
      "concepts_tested": [
        "Principle of least privilege and temporary/role-scoped access",
        "Separation of access policy (decision point) from enforcement mechanisms (PDP/PEP); relation of authorization, policy, and access control models (RBAC, ABAC, capability, DAC, MAC)",
        "Physical access control 'who, where, when' and mechanisms (mantrap, turnstile, intelligent/IP readers, door sensors) plus typical failure modes (tailgating, credential cloning, leveraging, sequential attacks)"
      ],
      "source_article": "Access control",
      "x": 1.4598482847213745,
      "y": 1.053263545036316,
      "level": 2,
      "original_question_hash": "cf3a5b8e"
    },
    {
      "question": "A coastal community has a ceremonial oral tale about a sea-guardian told at seasonal rites. The teller uses gestures, a recurring sequence of three trials, stock phrases, and a viewpoint in which an elder narrator addresses listeners; over generations the tale accumulates local variants, is later recorded in a missionary pamphlet with a different narrator voice, and in the 21st century appears as a short film and an online interactive game. Which interpretation best synthesizes how this case illustrates the role and structure of storytelling across cultures and media?",
      "options": {
        "A": "The tale exemplifies storytelling as simultaneously entertainment, moral instruction, cultural preservation and pedagogical practice; its meaning is enabled by core narrative elements—plot (the three trials), characters (sea-guardian, hero, common ally) and narrative point of view (elder addressing listeners)—and its persistence and variation across oral, print and digital media demonstrate memetic spread, motif recycling, and the formation of story cycles and adapted authoritative versions without erasing communal ownership.",
        "B": "Because the story was later printed and filmed, its primary function shifted from cultural preservation to commercial entertainment; the original oral features like gestures and stock phrases are irrelevant to meaning once the tale exists in stable print or digital forms.",
        "C": "The community's tale shows that storytelling functions mainly as a repository of factual historical events; narrative elements such as point of view and motifs are incidental, and transmission across media simply preserves objective facts rather than shaping values or identity.",
        "D": "The case proves that only written records and films can reliably conserve a culture's stories; oral variants and motifs are harmful distortions that undermine the original story's moral lessons and should be corrected by centralized authorship before digital adaptation."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete scenario (coastal sea-guardian tale) showing oral ritual performance, motifs (rule of three, stock phrases), POV, and adaptations to print/digital; options contrast accurate synthesis with common misconceptions about function, structure and transmission.",
      "concepts_tested": [
        "Storytelling as social-cultural mechanism for entertainment, education, cultural preservation, and moral instruction",
        "Core narrative elements: plot, characters, narrative point of view enabling meaning-making",
        "Transmission and evolution of stories across media and cultures (oral → written → digital; motifs, memetic spread, formation of cycles)"
      ],
      "source_article": "Storytelling",
      "x": 1.186743140220642,
      "y": 1.0597509145736694,
      "level": 2,
      "original_question_hash": "603b6f21"
    },
    {
      "question": "A small engineering firm is contracted to design a 20 m pedestrian bridge. Finite element simulations of the initial steel-member design predict a maximum von Mises stress of $240\\ \\text{MPa}$ in a critical girder. The chosen steel has a yield strength of $250\\ \\text{MPa}$, so the computed factor of safety (FOS) is $\\mathrm{FOS}=\\frac{250}{240}\\approx1.04$. The client insists on minimizing cost and meeting a tight schedule. Engineering convention for public pedestrian structures in this jurisdiction calls for a target FOS of at least $2.0$ to allow for material variability, fatigue and unforeseen loads. Two retrofit options are available: (1) increase girder cross-sectional area by 10% (material cost +8%, delay +2 months), which the simulation shows would raise FOS to $\\approx1.8$; (2) switch to a higher-strength steel (cost +20%, no schedule delay), which the simulation shows would raise FOS to $\\approx2.1$. As the lead engineer, which course of action best follows the engineering design process, professional ethics, and appropriate lifecycle/economic considerations?",
      "options": {
        "A": "Choose option (2): adopt the higher-strength steel to meet the FOS target, re-run simulations and prototyping tests, document the decision and costs, inform the client about lifecycle maintenance benefits and schedule implications, and include nondestructive testing and a maintenance plan in the project scope.",
        "B": "Approve the original design (no changes) because the predicted stress is below yield ($240<250\\ \\text{MPa}$), sign off to meet cost and schedule, and rely on the client to accept any future risk.",
        "C": "Select option (1) (10% area increase) to reduce cost and delay, but add warning signage limiting maximum pedestrian load and include a liability waiver for the client to avoid further modifications.",
        "D": "Propose a cheaper, lower-strength steel that reduces initial cost and perform only visual inspections after installation; defer any rigorous testing or maintenance planning to the warranty period to avoid schedule impacts."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete bridge-design scenario with numerical FOS calculation to require trade-off evaluation among simulations, safety, ethics, economics, testing, and lifecycle planning; option A aligns with professional standards and the engineering design process.",
      "concepts_tested": [
        "Engineering design process and use of simulations/testing",
        "Professional ethics and prioritizing public safety and welfare",
        "Interdisciplinary lifecycle considerations, economics, and documentation"
      ],
      "source_article": "Engineering",
      "x": 1.4462064504623413,
      "y": 1.0346815586090088,
      "level": 2,
      "original_question_hash": "08cb31ed"
    },
    {
      "question": "A team is developing an unmanned aircraft collision-avoidance module. The initial requirements are informal and ambiguous (e.g., \"avoid collisions with moving obstacles\"). The project must (1) eliminate ambiguity in requirements, (2) produce an implementation that is demonstrably correct, and (3) obtain regulatory certification that permits replacing traditional testing with a formal verification artifact. Which development strategy best addresses these goals, taking into account that the space of candidate programs grows combinatorially (roughly like $b^n$ for branching factor $b$ and program size $n$)?",
      "options": {
        "A": "Write a complete formal specification of the collision-avoidance behaviour to resolve ambiguities; use deductive synthesis driven by that full specification for safety‑critical components while constraining the search (e.g., invariants, SMT/SAT constraints) to mitigate the $b^n$ search explosion; then apply a highly trusted (possibly certified) sign-off verification tool to prove the implementation satisfies the formal spec so it can substitute for traditional verification.",
        "B": "Collect a large set of input–output examples and use inductive (example-driven) synthesis to infer the controller; perform model checking on the inferred implementation to detect counterexamples; rely on the produced test traces and model checking reports as evidence for certification.",
        "C": "Keep the requirements informal but document them carefully; let engineers implement the controller and then use an automated theorem prover to produce a human-readable proof of correctness of the implementation; submit the human-directed proof to regulators for sign-off.",
        "D": "Adopt a lightweight semi‑formal notation (e.g., UML/Alloy) to sketch behaviours, use inductive synthesis for most modules to speed development, and depend on human inspection plus standard testing rather than formal verification to satisfy certification."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete avionics scenario; options map different uses of formal specification, synthesis (deductive vs inductive), search-space concerns, and verification vs sign-off verification; A aligns with article principles.",
      "concepts_tested": [
        "Formal specification as central artifact for ambiguity resolution and guidance of synthesis/verification",
        "Deductive vs inductive program synthesis and the challenge of large search spaces",
        "Formal verification vs sign-off verification (trusted/certified tools replacing traditional methods)"
      ],
      "source_article": "Formal methods",
      "x": 1.4646774530410767,
      "y": 1.1242125034332275,
      "level": 2,
      "original_question_hash": "eb4b12e6"
    },
    {
      "question": "A song released in 2018 from the island nation of Sanora, titled \"Lago\", exhibits the following attributes: it is 3:10 long, uses a clear verse–chorus–bridge structure with a highly repetitive, easily singable melodic hook, it was uploaded simultaneously to global streaming platforms and promoted by a major label's curated playlists, and it blends a Western pop chord progression with a traditional Sanoran drum pattern and a local string instrument. Within months the track charted in several non–Sanoran countries. Which set of explanations best accounts for both the track's memorability and its rapid international dissemination?",
      "options": {
        "A": "The song's verse–chorus structure and singable hook increase memorability; global digital distribution (streaming and playlist promotion) enabled rapid cross-border spread; and hybridization of Western pop elements with local instrumentation made it novel yet familiar to diverse audiences.",
        "B": "Memorability is best explained by the short duration alone (3:10), while rapid spread resulted only from the major label's traditional album-release strategy and radio play; the local instrumentation had negligible effect on international appeal.",
        "C": "Rapid dissemination was driven primarily by the use of multitrack studio production and high-fidelity recording; memorability stemmed from complex, through-composed sections that showcased technical musicianship rather than repetition or a singable melody.",
        "D": "The track's success stems mainly from grassroots oral transmission within Sanora and subsequent live-tour exposure; structural repetition and streaming playlists played a minor role, while hybridization limited its appeal to international listeners."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete release scenario combining distribution technology, song form, and cross-cultural fusion; four plausible explanations provided, with the correct option integrating all three target concepts.",
      "concepts_tested": [
        "Industry-driven distribution and digital dissemination (streaming, playlists)",
        "Structural characteristics of popular songs (verse–chorus form, repetition, singable melodies) facilitating memorability",
        "Globalization and hybridity (blending Western pop with local traditions) creating cross-cultural appeal"
      ],
      "source_article": "Popular music",
      "x": 0.36671459674835205,
      "y": 1.6708972454071045,
      "level": 2,
      "original_question_hash": "a6f370e5"
    },
    {
      "question": "State Veridia (population 30 million) seeks to secure a disputed border province without provoking large-scale international sanctions or long-term occupation. Veridia possesses 120,000 deployable troops, a robust cyber/intelligence capability, and strong diplomatic ties to regional states. Its political aim is limited: install a friendly local administration and preserve international legitimacy. Which of the following courses of action best exemplifies a military strategy that aligns military means with those political ends, distinguishes itself from pure tactics or grand strategy, and uses resources to degrade the opponent's ability and will to resist (including deception and asymmetric measures)?",
      "options": {
        "A": "Order a concentrated armored offensive using 90% of available forces to destroy the opponent's field armies in a decisive set-piece battle; follow with occupation of the province until full capitulation is achieved.",
        "B": "Allocate forces so that $70\\%$ conduct visible diversionary maneuvers while $30\\%$ perform special-operations raids, cyber operations, and support a maritime blockade; simultaneously mount targeted information campaigns and diplomatic initiatives aimed at convincing the province's leadership to accept a friendly administration.",
        "C": "Forego military employment and rely exclusively on sanctions, trade incentives, and a diplomatic coalition to compel the opponent to cede the province, keeping the armed forces in reserve.",
        "D": "Instruct local commanders to exploit any battlefield opportunities they encounter, permitting decentralized tactical initiative without an overarching plan linking operations to the political objective."
      },
      "correct_answer": "B",
      "generation_notes": "Designed a realistic scenario requiring selection of a campaign-level approach that links military means to limited political ends, contrasts strategy vs tactics and grand strategy, and highlights resource allocation, deception, and measures to reduce will to fight; included percentage allocations using LaTeX.",
      "concepts_tested": [
        "Alignment of military means with political aims",
        "Hierarchy of planning: tactics vs strategy vs grand strategy",
        "Use of resources and methods (deception, asymmetric actions, information operations) to reduce opponent's will and achieve strategic outcomes"
      ],
      "source_article": "Military strategy",
      "x": 1.2494722604751587,
      "y": 0.8431316018104553,
      "level": 2,
      "original_question_hash": "a7f3f430"
    },
    {
      "question": "A hospital committee must choose between three treatment protocols X, Y, and Z. They evaluate each protocol on three normalized criteria (higher = better): Efficacy $E$, Cost $C$, and Safety (inverse of risk) $S$. The explicit scores are: X: $(E,C,S)=(8,6,7)$; Y: $(7,8,6)$; Z: $(9,4,5)$. Expert A uses weights $(w_E,w_C,w_S)=(0.5,0.3,0.2)$ and thus computes the weighted sums $X_A=0.5\\cdot8+0.3\\cdot6+0.2\\cdot7=7.2$, $Y_A=7.1$, $Z_A=6.7$, ranking X>Y>Z. Expert B, drawing on long clinical experience (tacit knowledge about patient adherence and downstream costs), adjusts weights to $(0.2,0.6,0.2)$ and computes $X_B=6.6$, $Y_B=7.4$, $Z_B=5.2$, ranking Y>X>Z. Which of the following statements best captures the three ideas below: (1) decision-making as a cognitive process shaped by values/preferences and capable of being rational or irrational; (2) MCDA evaluates alternatives against criteria but can produce divergent rankings depending on method/weights (a decision paradox); and (3) tacit and explicit knowledge are often combined to fill gaps in complex decisions?",
      "options": {
        "A": "This scenario illustrates that decision-making is a cognitive process in which the experts' differing weightings reflect their values and beliefs (so rationality is perspective-dependent); the MCDA weighted-sum produced different top choices (X vs Y), demonstrating method/weight sensitivity (a decision-making paradox); and Expert B's experiential judgement is tacit knowledge supplementing the explicit scores to justify different prioritization of criteria.",
        "B": "Because the explicit numeric scores exist, there is an objectively correct treatment (the one with highest raw efficacy, Z with $E=9$); the fact that experts disagree shows one of them is irrational; tacit knowledge should be excluded from MCDA because it biases weights and undermines objectivity.",
        "C": "The divergence in rankings proves MCDA is invalid for medical decisions: any correct decision procedure would always yield the same ordering regardless of weights; therefore experts must ignore personal values and only use the highest single-criterion score (efficacy) when making choices.",
        "D": "Expert disagreement here indicates calculation error rather than conceptual issues: if cost and safety are measured on different scales, the paradox disappears; tacit knowledge is irrelevant because it cannot be quantified, so only explicit scores should determine the decision."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete MCDA example with two different weight vectors (one influenced by tacit knowledge) that yield different rankings; options test understanding of decision-making as value-laden, MCDA sensitivity/paradox, and role of tacit vs explicit knowledge.",
      "concepts_tested": [
        "Decision-making as a cognitive process influenced by values/preferences and capable of being rational or irrational",
        "MCDA evaluation of finite alternatives against criteria and method/weight sensitivity (decision paradox)",
        "Use of tacit and explicit knowledge together to fill gaps in complex decision-making"
      ],
      "source_article": "Decision-making",
      "x": 1.3210556507110596,
      "y": 1.0322155952453613,
      "level": 2,
      "original_question_hash": "94bba14a"
    },
    {
      "question": "A central bank is considering raising its policy rate from $3\\%$ to $5\\%$ to curb an observed inflation rate of $8\\%$. An empirical paper compares two models: Model A assumes representative, forward‑looking agents with perfect foresight and predicts unemployment rises by $0.5$ percentage point; Model B assumes agents with present bias and loss aversion and predicts unemployment rises by $2$ percentage points and consumption falls by $6\\%$. A minister says, “We should raise rates because persistent inflation disproportionately harms low‑income households.” Which single option correctly classifies (i) the type of analysis that compares aggregate inflation, unemployment and consumption changes, (ii) the minister’s statement as positive or normative economics, and (iii) Model A’s behavioural assumption as rational or behavioural economics?",
      "options": {
        "A": "(i) Macroeconomic analysis; (ii) Normative economics; (iii) Rational economics",
        "B": "(i) Microeconomic analysis; (ii) Positive economics; (iii) Behavioural economics",
        "C": "(i) Macroeconomic analysis; (ii) Positive economics; (iii) Behavioural economics",
        "D": "(i) Microeconomic analysis; (ii) Normative economics; (iii) Rational economics"
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete policy scenario (interest‑rate change, inflation, model predictions) and asked students to classify macro vs micro, positive vs normative, and rational vs behavioural assumptions in one choice.",
      "concepts_tested": [
        "Microeconomics vs. Macroeconomics",
        "Positive vs. Normative Economics",
        "Rational vs. Behavioral Economics"
      ],
      "source_article": "Economics",
      "x": 1.2170510292053223,
      "y": 0.929368257522583,
      "level": 2,
      "original_question_hash": "d29d57e4"
    },
    {
      "question": "Firm A (35% national market share) proposes to acquire direct competitor Firm B (20% share). Management wants to: (i) avoid an antitrust challenge under the Clayton Act, (ii) minimize assumption of B's uncertain environmental liabilities, and (iii) maximize the probability of a successful integration given limited previous M&A experience. Which combination of deal structure, regulatory step and integration approach best aligns with these objectives?",
      "options": {
        "A": "Structure the deal as an asset purchase that acquires only B's core operating assets while leaving identified environmental liabilities with the seller; commit to divesting overlapping product lines or regional operations so post-transaction market share no longer creates a dominant local position; file a Hart–Scott–Rodino (HSR) pre‑merger notification and wait for clearance; negotiate as a friendly transaction, retain key B managers with targeted retention incentives, and adopt a documented integration playbook (learning from serial acquirers).",
        "B": "Execute a stock/equity purchase so A inherits the company intact (avoiding asset transfer taxes), close quickly via a hostile tender offer to pre-empt rivals, delay any antitrust filing until after closing to gain leverage, then replace B's management immediately and pursue rapid cost-cutting.",
        "C": "Complete a statutory merger (A survives) without divestitures and argue post hoc that claimed efficiencies will cure any competitive concerns; avoid HSR filing because claimed efficiency defenses can be presented to regulators after closing; centralize all operations and reassign all B executives to junior roles to ensure cultural alignment.",
        "D": "Use an asset purchase but take all of B's assets and liabilities to simplify payroll and supplier contracts; do not divest overlapping lines; proceed without an HSR filing on the basis that the combined national share (55%) is acceptable if managed well; attempt integration by immediately folding B into A's departments with minimal retention incentives."
      },
      "correct_answer": "A",
      "generation_notes": "Created a scenario with market shares to test choice between asset vs equity/merger structures, role of Clayton Act/HSR, and integration tactics (retention, friendly negotiations, use of serial-acquirer playbook).",
      "concepts_tested": [
        "Merger vs acquisition structures (asset purchase vs equity purchase/statutory merger)",
        "Antitrust/regulatory process (Clayton Act substantive test; Hart–Scott–Rodino pre‑merger notification)",
        "M&A success factors (friendly vs hostile, retention of key managers, integration playbooks/serial acquirer advantage, divestitures to reduce antitrust risk)"
      ],
      "source_article": "Mergers and acquisitions",
      "x": 1.3293116092681885,
      "y": 0.9321695566177368,
      "level": 2,
      "original_question_hash": "34801044"
    },
    {
      "question": "Consider the following three procedures that operate on inputs of size $n$. Which option correctly classifies each procedure as (i) a deterministic algorithm, (ii) a randomized algorithm (specifying Las Vegas vs Monte Carlo if applicable), or (iii) a heuristic/non-algorithm, and also states whether it satisfies the formal requirement of being a finite algorithm (i.e., performs well-defined state transitions and terminates producing a single output)?\n\nProcedure P1: A program that executes a fixed sequence of deterministic state transitions, uses at most $O(1)$ extra memory, and is proven to halt after at most $100n^{2}$ steps, returning a single output value.\n\nProcedure P2: A search routine that uses fair coin flips during execution. It always returns a correct solution when it halts, its running time is a random variable with expected value $O(n^{2})$, and it halts with probability 1 (the tail of the runtime distribution decays rapidly).\n\nProcedure P3: An online recommender service that continuously ingests user interactions and updates recommendations in real time; it never reaches a designated final state, and there is no single well-defined \"correct\" recommendation for a given historical input.\n",
      "options": {
        "A": "P1 is a deterministic finite algorithm (halts within $100n^{2}$ steps and computes a function); P2 is a randomized (Las Vegas) algorithm (randomized state transitions, always correct when it halts, expected time $O(n^{2})$, halts w.p.1) and thus qualifies as an algorithm; P3 is a heuristic/program that is not a formal terminating algorithm (no single final output and does not satisfy the finite termination requirement).",
        "B": "P1 is not an algorithm because it uses more than constant time; P2 is a Monte Carlo randomized algorithm (may return incorrect answers with small probability) and thus is a heuristic rather than an algorithm; P3 is a deterministic algorithm because it is implemented as continuously running code and therefore counts as an algorithm.",
        "C": "P1 is a deterministic algorithm that may or may not be considered an algorithm depending on implementation language; P2 is not an algorithm because randomness prevents well-defined state transitions; P3 is a formal algorithm because recommender systems are commonly called \"algorithms\" in practice.",
        "D": "P1 is a heuristic (because it simply searches a list); P2 is a deterministic algorithm because coin flips can be replaced by a fixed pseudorandom sequence; P3 is a randomized algorithm since user behavior acts like random input and therefore it qualifies as a finite algorithm."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete three-procedure scenario to test (1) finiteness/termination and well-defined state transitions, (2) deterministic vs randomized classification including Las Vegas properties, and (3) distinction between heuristics/programs and formal algorithms.",
      "concepts_tested": [
        "Algorithms as finite terminating procedures operating on well-defined states",
        "Deterministic versus randomized algorithms (Las Vegas vs Monte Carlo) and effects of randomness on transitions",
        "Distinction between heuristic continuous systems and formal terminating algorithms; programs implementing heuristics vs algorithms"
      ],
      "source_article": "Algorithm",
      "x": 1.563927173614502,
      "y": 1.1645848751068115,
      "level": 2,
      "original_question_hash": "efd7b81d"
    },
    {
      "question": "A university virology laboratory discovers at time $t_0$ that a mislabelled vial released an aerosolized pathogen into a shared workspace. The event (a) threatens staff and the surrounding community, (b) occurred without warning, and (c) requires decisions within 24 hours $(t_1 - t_0 < 24\\ \\text{h})$. Which of the following response sequences best exemplifies crisis management — understood as a transformative, process-oriented response — rather than risk management, and correctly follows the crisis-management system flow (prevention/assessment/handling/termination) with clear roles, metrics, communication, planning, and post-crisis learning?",
      "options": {
        "A": "Immediately activate the crisis-management plan at $t_0$ (designate crisis team and single spokesperson); perform rapid on-site assessment and containment (evacuate, decontaminate), issue transparent public and regulator communications with status metrics; run simultaneous medical triage and continuity actions to keep essential functions running; after containment, conduct after-action review, revise contingency plans and operational protocols, and institutionalize changes to prevent recurrence.",
        "B": "Halt all lab operations and commission a probabilistic risk assessment over several weeks to redesign containment systems; resume work only after new engineering controls are implemented and staff retrained — communicating outcomes to stakeholders after the redesign is complete.",
        "C": "Delay public statements for 72 hours while collecting comprehensive internal reports; limit communications to internal memos, then issue a brief corporate-style denial; take no immediate containment steps, awaiting full legal advice before acting.",
        "D": "Focus immediate efforts on legal defence and attributing blame to a supplier; continue normal lab operations with minimal disclosure; later issue a short press release and offer financial compensation to affected parties without changing internal procedures."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a realistic lab-contamination scenario to force application of the three crisis elements (threat, surprise, short decision time) and to distinguish crisis management (immediate, communicative, adaptive, learning-oriented) from risk management (preventive, slower); options B–D provide plausible but incorrect contrasts.",
      "concepts_tested": [
        "Crisis as a transformative process: threat, surprise, short decision time",
        "Difference between risk management and crisis management; timelines and mechanisms",
        "Crisis-management system flow: prevention, assessment, handling, termination; roles, communication, metrics, planning, and post-crisis learning"
      ],
      "source_article": "Crisis management",
      "x": 1.3572092056274414,
      "y": 0.9953991174697876,
      "level": 2,
      "original_question_hash": "ef6dd7e2"
    },
    {
      "question": "A regional airline operates 1,000,000 flights per year in a climate with frequent icing and occasional wind shear. Its safety data show that 10% of accidents are primarily due to inflight icing, and 35% are loss-of-control-in-flight (LOC) events in which pilot performance is a major factor. Regulators require the operator to maintain a Safety Management System (SMS), perform periodic maintenance oversight, and ensure crew training standards. Management is evaluating four proposals. Which proposal best exemplifies aviation risk management (reducing risk through research/training/design/infrastructure), leverages regulation and oversight to reduce operational risk, and correctly distinguishes safety measures from security measures? Under the best proposal, if de-icing infrastructure reduces icing-related accidents by 80% and enhanced pilot CRM/instrument training reduces LOC accidents by 20%, what is the combined percentage reduction in the airline’s accident causes from these two interventions alone (to two significant figures)?",
      "options": {
        "A": "Invest in improved de-icing infrastructure and aircraft ice-detection sensors, fund targeted CRM and instrument/anti-icing system training for crews, and strengthen compliance with SMS and maintenance audits (regulatory oversight). Estimated combined reduction: $0.1\\times0.8+0.35\\times0.2=0.15$ (15%).",
        "B": "Prioritise hardened cockpit doors, expand passenger screening, and arm select flight crew — actions focused on preventing intentional harm — while cutting the de-icing and pilot training budget. Estimated combined reduction: $0.15$ (15%).",
        "C": "Reduce costs by outsourcing maintenance to the lowest bidder and accepting non-certified spare parts to keep aircraft flying more hours; rely on autopilot to compensate for degraded manual flying skills. Estimated combined reduction: $0.15$ (15%).",
        "D": "Install extra terminal CCTV and increase public-facing security announcements to reassure passengers; defer investments in de-icing infrastructure and crew training pending further study. Estimated combined reduction: $0.15$ (15%)."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete airline scenario requiring selection of the option that integrates risk-management measures (training, design, infrastructure) and regulatory compliance; included a small inline calculation $0.1\\times0.8+0.35\\times0.2$ to test quantification of risk reduction from weather and human-factor interventions and the distinction between safety (A) and security (B/D).",
      "concepts_tested": [
        "Risk management through training, design, research, and infrastructure",
        "Role of regulation and oversight (SMS, maintenance audits) in reducing operational risk",
        "Relationship between environmental factors (icing, wind shear) and safety outcomes and distinction between safety and security interventions"
      ],
      "source_article": "Aviation safety",
      "x": 1.6321051120758057,
      "y": 0.5944627523422241,
      "level": 2,
      "original_question_hash": "b67381de"
    },
    {
      "question": "A 7-member emergency resuscitation team in a large hospital has shown a rise in procedural errors, duplicated tasks, and patient delays. Staff surveys report unclear role assignments, irregular access to critical supplies, and frequent avoidance of speaking up during procedures. Observational analysis finds limited real-time information sharing and few opportunities for collective reflection. Based on teamwork research linking cohesion to performance, the centrality of communication, and the importance of interdependence, boundaries, and role clarity, which single intervention is most likely to produce the largest combined improvement in team performance, process quality, and team satisfaction?",
      "options": {
        "A": "Introduce a standardized pre-resuscitation briefing and post-event debrief protocol that assigns and documents roles before each case, uses a concise checklist to confirm supplies and responsibilities, and mandates a 5-minute structured debrief after each event to surface problems and reinforce mutual support.",
        "B": "Rotate the team leader role among members every week to distribute authority and broaden leadership experience, without changing existing communication protocols or role documents.",
        "C": "Hire two additional clinicians so individual workload decreases, assuming that lower individual strain will automatically reduce errors and interpersonal friction.",
        "D": "Implement individual performance bonuses tied to personal task completion times to incentivize faster individual contributions during resuscitations."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete hospital resuscitation scenario where poor outcomes stem from low cohesion, weak communication, unclear roles, and resource problems; option A addresses all three core concepts (cohesion, communication, interdependence/role clarity) while other options plausibly but insufficiently address them.",
      "concepts_tested": [
        "cohesion-performance relationship",
        "role of communication in teams",
        "interdependence, boundaries, and role clarity"
      ],
      "source_article": "Teamwork",
      "x": 1.2884658575057983,
      "y": 1.000427007675171,
      "level": 2,
      "original_question_hash": "5500aa7d"
    },
    {
      "question": "In an experiment participants view an ambiguous figure (it can be seen either as a vase or as two faces). On different trials the experimenters (1) spray a faint human-body odor near the participant's nose or a neutral odor, and (2) either tell participants the study is about ‘‘social perception’’ (creating an expectation) or give no context. Participants report seeing the face interpretation significantly more often on trials with the human odor and when they were told the study concerned social perception. Which theoretical account best explains these results?",
      "options": {
        "A": "Perception here results from the transformation of a distal stimulus into proximal neural signals that are processed by partly modular, modality-specific systems; cross‑modal olfactory input and top‑down expectations (attention, prior beliefs) bias the interpretation of the same ambiguous proximal stimulus, consistent with an active‑inference (hypothesis‑testing) view in which sensory evidence and priors combine to yield the final percept.",
        "B": "The odor physically alters the retinal image of the ambiguous figure, producing a different proximal stimulus that the visual system passively reads out; expectations are irrelevant because perception is determined only by altered bottom‑up data.",
        "C": "The effects are solely due to semantic priming from Bruner‑style social categorization: participants ignore sensory signals and use only learned social categories to invent a percept, so sensory transduction and modular processing do not contribute.",
        "D": "Closed‑loop perception explains the pattern because participants moved their heads and eyes differently when smelling the odor, and these motor adjustments alone reorganized sensory input until a face percept emerged; top‑down expectations played no role."
      },
      "correct_answer": "A",
      "generation_notes": "Designed an ambiguous‑figure cross‑modal experiment to require students to identify distal→proximal transformation, modular/cross‑modal interaction, and top‑down active inference as the best explanatory account.",
      "concepts_tested": [
        "Distal vs proximal stimulus and transduction",
        "Top‑down influences (expectation, attention) and active inference/hypothesis testing",
        "Modular brain organization and cross‑modal interactions"
      ],
      "source_article": "Perception",
      "x": 1.6496888399124146,
      "y": 1.105865478515625,
      "level": 2,
      "original_question_hash": "9768a7a7"
    },
    {
      "question": "In an experiment, mice are trained to report whether a drifting grating moves left or right. Researchers simultaneously record multi-electrode spiking activity from primary visual cortex (V1), posterior parietal cortex (PPC), and primary motor cortex (M1) while also acquiring fMRI to assess whole-brain functional connectivity. The team then induces synaptic potentiation selectively in V1 using an optogenetic protocol. After potentiation the mice make faster correct decisions; electrophysiology shows increased spike-time correlations between V1 and PPC, fMRI shows enhanced functional connectivity between visual and parietal regions, and a trained recurrent neural network (RNN) model reproduces the faster decisions when the V1→PPC synaptic weights are increased. Which interpretation best exemplifies systems neuroscience reasoning about these results?",
      "options": {
        "A": "The faster decisions reflect an emergent change in network dynamics: a molecular/cellular perturbation in V1 (synaptic potentiation) altered interactions among distributed circuits (V1↔PPC↔M1), producing a system-level change in perception-to-action that is supported by converging electrophysiology, fMRI, and computational-model evidence.",
        "B": "The behavioral improvement must be explained by a single neuron's increased firing in V1; the correlated activity and fMRI connectivity are secondary epiphenomena and unnecessary for explaining the decision-speed change.",
        "C": "Because the RNN reproduces the behavior, there exists a one-to-one mapping between RNN units and biological neurons, so computational models alone can replace invasive recordings and imaging to infer mechanisms.",
        "D": "Potentiation in V1 can only improve low-level sensory encoding and cannot causally influence decision-making circuits; therefore the observed PPC and behavioral changes are likely unrelated and caused by an unobserved region."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete multi-method experimental scenario (optogenetic, electrophysiology, fMRI, RNN) to probe emergent network behavior, cross-level causal inference, and multimodal modeling; distractors reflect common mistaken interpretations (single-neuron causation, overinterpreting model equivalence, and modular isolation).",
      "concepts_tested": [
        "Emergent behavior from interacting neural circuits forming larger brain networks",
        "Cross-level causal relationships linking molecular/cellular manipulations to system-level function and cognition",
        "Use of multimodal network-focused measurements and computational modeling to infer mechanisms"
      ],
      "source_article": "Systems neuroscience",
      "x": 1.776384711265564,
      "y": 1.1412584781646729,
      "level": 2,
      "original_question_hash": "e6b82d4e"
    },
    {
      "question": "A ten-person university research lab will work together for 12 months on an interdependent experimental program. The principal investigator (PI) wants to design organizational practices that build strong, durable group cohesion — i.e., cohesion that is multidimensional (social relations, task relations, perceived unity, emotions), dynamic (changes over time), and that supports both participation/persistence and good task performance while minimizing conformity-driven errors. Which of the following bundles of practices is most consistent with the empirical antecedents and mechanisms of cohesion described in the article?",
      "options": {
        "A": "Selective admission to the lab (high entry difficulty), frequent informal social events that pair members with their personal friends, and little role interdependence so individual contributions are easily substituted; performance evaluated and rewarded at the individual level to recognize friendships.",
        "B": "Open membership (low entry difficulty), largely independent tasks where individual outputs are aggregated, occasional top-down directives from the PI, and extrinsic, individual financial bonuses tied to publication count.",
        "C": "Moderate selection for commitment, clear shared mission and interdependent task design, regular team rituals and group reflection meetings (to foster a shared identity and collective emotions), team-based rewards for meeting joint milestones, explicit norms that encourage open dissent and problem-focused discussion.",
        "D": "Very selective admission followed by a demanding initiation ritual to raise perceived exclusivity, tight social-control norms discouraging dissent, and centralized decision making by senior members so the group projects a single unified front."
      },
      "correct_answer": "C",
      "generation_notes": "Created a realistic lab-team scenario and constructed four plausible practice bundles; option C deliberately incorporates antecedents (coordination, shared emotions, sense of belonging), multidimensional cohesion components, dynamic processes (reflection), instrumental/task commitment (interdependent tasks, team rewards), and safeguards against conformity.",
      "concepts_tested": [
        "Multidimensionality and dynamic nature of cohesion (social relations, task relations, perceived unity, emotions; change over time)",
        "Antecedents and mechanisms of cohesion (attraction vs social attraction, coordination, sense of belonging, shared emotions)",
        "Instrumental basis and emotional dimension of cohesion and their effects on participation, persistence, and performance"
      ],
      "source_article": "Group cohesiveness",
      "x": 1.293738603591919,
      "y": 1.001175045967102,
      "level": 2,
      "original_question_hash": "99c99b06"
    },
    {
      "question": "City X receives a steady influx of immigrants over several decades. Newcomers interact daily with local institutions: schools adopt bilingual curricula, markets begin selling hybrid food items, and some legal practices are adjusted to accommodate new religious holidays. At the same time, many immigrants retain core family rituals and maintain transnational ties via social media. The municipal government explicitly promotes multicultural policies and funds intercultural programs. According to academic definitions and models of acculturation, which interpretation best characterizes what is occurring in City X and the likely array of outcomes?",
      "options": {
        "A": "A unilinear, predominantly unidirectional assimilation process: immigrants quickly abandon their original culture and fully absorb the dominant culture, producing a homogenized monoculture and minimal institutional change in the host society.",
        "B": "A bidirectional, mechanism-driven acculturation process: sustained direct contact and exposure generate changes at both group and individual levels (institutional adaptations and second-culture socialization), producing possible integration/biculturalism and broader cultural evolution favored by supportive sociopolitical policies.",
        "C": "A phenomenon limited to individual enculturation: only immigrants undergo second-culture learning while host culture and institutions remain unchanged; outcomes are restricted to individual behavioral adjustments without societal-level effects.",
        "D": "An accelerated acculturation driven primarily by physical coercion: rapid cultural replacement occurs because migrants are compelled to adopt host norms by force, leading exclusively to marginalization of the immigrant group."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete urban vignette showing institutional and individual changes; options contrast unidirectional assimilation, bidirectional acculturation with multilevel effects (correct), individual-only change, and coercive rapid change to test understanding of mechanisms, levels, and outcomes.",
      "concepts_tested": [
        "Bidirectional acculturation mechanisms and sustained exposure",
        "Interplay between group-level institutional change and individual-level socialization (second-culture learning)",
        "Range of acculturation outcomes influenced by sociopolitical context (integration, cultural evolution, marginalization)"
      ],
      "source_article": "Acculturation",
      "x": 1.2423869371414185,
      "y": 0.9938169717788696,
      "level": 2,
      "original_question_hash": "574a99e5"
    },
    {
      "question": "An undergraduate lab prepares the perovskite oxide BaTiO3 by two routes. Route 1 (ceramic solid-state): commercially available powders BaCO3 and TiO2 (median particle size 5 \\(\\mu\\)m) are ground with a mortar and pestle, pressed into pellets and heated at 1250 \\(^\\circ\\)C for 24 h. The product shows a polycrystalline XRD pattern with residual TiO2 peaks, broad diffraction features, mean grain size ~2 \\(\\mu\\)m by SEM, low bulk density, and a dielectric constant \\(\\varepsilon_r\\) well below the literature single-crystal value. Route 2 (sol–gel): molecular precursors are mixed in solution, gelled, and calcined at 700 \\(^\\circ\\)C for 4 h. The product shows a single-phase XRD pattern with sharp Bragg peaks, mean crystallite size ~100 nm by TEM, higher bulk density, and \\(\\varepsilon_r\\) approaching the single-crystal value though with a larger dielectric loss (tan\\(\\delta\\)). Which one of the following interpretations best accounts for these observations in terms of (i) diffusion-driven phase formation during the ceramic method, (ii) the connection between structural order (crystalline vs. imperfect order) and the XRD/dielectric data, and (iii) how the synthetic route influences defect formation and purity?",
      "options": {
        "A": "The high temperature of the ceramic route guarantees complete reaction by volatilizing impurities; broad XRD peaks and residual TiO2 are caused by excessive grain growth that smears Bragg reflections. Structural order is not important for dielectric constant; the low \\(\\varepsilon_r\\) in Route 1 is due to grain coarsening alone. The sol–gel product shows sharp peaks because the lower calcination temperature prevents defect formation entirely.",
        "B": "Route 1 is limited by solid-state diffusion: coarse particles give small contact area and long diffusion distances so grain-boundary reactions are slow (diffusion length scales as \\(L\\sim\\sqrt{Dt}\\)), leading to incomplete conversion and residual TiO2, porosity and many grain-boundary defects; these reduce bulk density and lower \\(\\varepsilon_r\\). Broad XRD features reflect either small coherent domains at grain boundaries and microstrain or overlapping contributions from multiple phases. Route 2’s molecular mixing minimizes diffusion distances, enabling formation of a homogeneous, crystalline single phase at lower temperature with fewer extended defects (higher density and \\(\\varepsilon_r\\)), although low-temperature organics/point defects introduced during sol–gel processing can increase dielectric loss. This explanation accounts for the XRD, microscopy and dielectric observations.",
        "C": "Both routes produce identical defect populations; the residual TiO2 in Route 1 is an artefact of XRD sample preparation. Broad XRD peaks in Route 1 indicate a higher degree of crystallinity than the sharp peaks in Route 2. The lower dielectric constant in Route 1 arises from a different domain orientation rather than defects or porosity, and sol–gel necessarily yields more defects because of rapid nucleation.",
        "D": "The ceramic method always yields single crystals if heated long enough, so residual TiO2 indicates contamination from milling media; broad XRD peaks are diagnostic of an amorphous sample. Structural order only affects optical properties, not dielectric properties. The sol–gel route gives lower purity because solution chemistry introduces ions that substitute into the lattice and lower \\(\\varepsilon_r\\)."
      },
      "correct_answer": "B",
      "generation_notes": "Created a comparative, multi-observation scenario (ceramic vs sol–gel) requiring integration of diffusion-limited solid-state reactions (using $L\\sim\\sqrt{Dt}$), XRD/crystallinity interpretation, and how synthesis route controls defects and purity; four plausible distractors included.",
      "concepts_tested": [
        "Diffusion-driven phase formation and role of contact area/grain boundaries",
        "Relationship between structural order (crystalline vs disorder) and XRD/dielectric properties",
        "Influence of synthesis method on defect populations, purity, and resulting material properties"
      ],
      "source_article": "Solid-state chemistry",
      "x": 1.8419173955917358,
      "y": 1.0284647941589355,
      "level": 2,
      "original_question_hash": "f5f23225"
    },
    {
      "question": "A hospital deploys a redesigned infusion pump for nurses. The interface uses a color-coded visual display, adjustable auditory alarms, haptic vibration on the handle, and a conversational voice-user interface (VUI) that asks confirmation questions about dose and rate. Laboratory trials show faster task completion but post-deployment surveys report mixed nurse opinions and occasional mode errors. Drawing on HCI principles — that feedback channels (visual, auditory, tactile) shape user engagement, that interaction should be treated as a dialogue analogous to human-to-human exchange, and that end-user computing satisfaction is the central evaluation outcome — which of the following design and evaluation changes is most consistent with these principles to improve overall user satisfaction?",
      "options": {
        "A": "Adapt feedback modality to context (e.g., use subtle haptic for hands-on checks, glanceable visual cues for routine info, salient auditory alarms for critical deviations), redesign the VUI to mirror nurses’ mental model (allow clarifications and explicit confirmations rather than one-way announcements), and run iterative usability tests that measure satisfaction, error rates and workload—not just completion time.",
        "B": "Increase the number and loudness of distinct auditory tones so alarms are impossible to miss, simplify the VUI into short one-way directives to reduce dialogue length, and evaluate success only by task completion time and technical error logs.",
        "C": "Remove haptic and auditory channels to reduce sensory overload, rely on a single detailed visual display and an AI that automatically corrects suspected input errors without asking users, and perform expert heuristic review rather than user testing.",
        "D": "Keep all multimodal feedback but lock the interface to a rigid standard layout and disable the VUI to avoid ambiguity; validate deployment success solely via automated system telemetry to confirm correct parameter settings."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete infusion-pump scenario requiring integration of multimodal feedback, dialogue design, and satisfaction-driven evaluation; correct option prescribes context-appropriate feedback, dialogue aligned to mental models, and iterative user-centered measurement.",
      "concepts_tested": [
        "Multimodal feedback channels shaping user engagement (visual, auditory, tactile)",
        "Designing interaction as an open-ended dialogue analogous to human-human communication",
        "End-user computing satisfaction as the central outcome linking design and evaluation"
      ],
      "source_article": "Human–computer interaction",
      "x": 1.3832616806030273,
      "y": 1.0582321882247925,
      "level": 2,
      "original_question_hash": "eccada22"
    },
    {
      "question": "Consider a coastal region with a large Port City (population 1,000,000) that hosts an established high‑tech cluster of roughly $N=200$ firms on a low‑lying coastal plain vulnerable to coastal erosion, and a smaller Hinterland Town (population 100,000) 50 km inland surrounded by agricultural land. A recently completed highway and fiber backbone reduce transport and communication costs by 40% (so $t\\to 0.6t$). Suppose the net agglomeration advantage for a firm can be approximated by $B=\\alpha\\ln N-\\tau t$ with $\\alpha,\\tau>0$, and that investment flows respond positively to expected profits. According to theories of economies of agglomeration, core–periphery dynamics, and environment–economy–urban form interactions, which of the following outcomes is most likely over the next 20 years?",
      "options": {
        "A": "The reduced $t$ eliminates the agglomeration advantage so firms disperse: most high‑tech firms relocate to Hinterland Town and agricultural land is converted to suburban offices, equalizing income and employment between the two places.",
        "B": "Port City loses its cluster because coastal erosion makes the site unviable; capital and skilled labor entirely shift to Hinterland Town, which becomes the new core while Port City collapses into a marginal fishing/port economy.",
        "C": "High‑tech firms largely remain concentrated in Port City because $\\alpha\\ln N$ still outweighs the $\\tau t$ reduction; the highway/fiber enables some manufacturing and logistics to locate in the Hinterland (lower land rents, good access to the port), leading to reinforced core–periphery divergence (increasing investment and high‑skill jobs in the core, lower‑paid routine jobs in the periphery). Environmental constraints (coastal erosion and floodplain) constrain low‑density coastal expansion and incentivize denser inland development and climate‑adaptation investments in Port City; conversely, sprawling development in the Hinterland would raise transport energy use and reduce knowledge spillovers.",
        "D": "Environmental vulnerability dominates: both private investors and public planners prohibit new development in the region, freezing spatial change; existing firms neither cluster more nor disperse, and economic patterns remain unchanged despite lower transport costs."
      },
      "correct_answer": "C",
      "generation_notes": "Constructed a concrete coastal/core‑periphery scenario with a simple agglomeration benefit formula $B=\\alpha\\ln N-\\tau t$. Options contrast persistent agglomeration and cumulative causation (correct) against overstatements of dispersion, collapse, or policy paralysis.",
      "concepts_tested": [
        "Economies of agglomeration and linkages (knowledge spillovers, labor pooling, input sharing)",
        "Core–periphery dynamics and cumulative causation (investment flows, spatial inequality)",
        "Environment–economy–urban form interactions (coastal hazards, urban density vs sprawl, land‑use change)"
      ],
      "source_article": "Economic geography",
      "x": 1.278157114982605,
      "y": 0.9230713844299316,
      "level": 2,
      "original_question_hash": "f256e590"
    },
    {
      "question": "A public‑health researcher observes that regions with higher uptake of a new vaccine have lower incidence of Disease D (covariation). The vaccine campaign began in 2018 and the decline in incidence occurs after 2019 (time‑order). During the same period a large sanitation program rolled out preferentially in high‑uptake regions, and wealthier districts tended to both vaccinate more and have lower baseline incidence. A linear regression of incidence on vaccination rate yields a coefficient $\beta<0$ with $p=0.03$ (testing $H_0:\beta=0$). The researcher is deciding how strongly they can claim the vaccine caused the decline. Which of the following choices best describes the additional steps or evidence required to support a causal claim and avoid conflating association with causation?",
      "options": {
        "A": "Acknowledge that covariation and time‑order are necessary but not sufficient; strengthen causal identification by eliminating plausible alternative causes via (ideally) randomized assignment or a credible quasi‑experimental design (e.g., a valid instrumental variable, difference‑in‑differences exploiting exogenous rollout, or regression discontinuity), explicitly state and test $H_0:\\beta=0$ while interpreting $p$‑values as probabilistic evidence rather than proof, perform sensitivity analyses for unobserved confounding, consider a Bayesian analysis with transparent priors, and preregister the analysis to reduce risk of p‑hacking.",
        "B": "Because the vaccination rate covaries with incidence, the vaccine rollout precedes the decline, and the regression rejects $H_0:\\beta=0$ at $p=0.03$, the researcher can reasonably conclude the vaccine caused the decline without further design changes or robustness checks.",
        "C": "Simply adding a large set of observable covariates (demographics, wealth, sanitation indicators) to the linear regression guarantees causal identification; if the vaccination coefficient remains statistically significant the causal effect is established, and multicollinearity is only a minor technical issue.",
        "D": "Use the sanitation program as an instrumental variable for vaccination uptake because it predicts vaccination and therefore supplies exogenous variation; this single step is sufficient to identify the causal effect of vaccination on incidence."
      },
      "correct_answer": "A",
      "generation_notes": "Created a realistic observational vaccine rollout scenario to test students' understanding of causal identification criteria (covariation, time order, elimination of alternatives), counterfactual/design strategies, and statistical framework implications (null hypothesis, p‑values vs Bayesian priors, sensitivity analysis, preregistration).",
      "concepts_tested": [
        "Criteria for causal identification (covariation, time order, elimination of plausible alternatives)",
        "Distinction between causal inference and association (manipulation/counterfactual reasoning and need for experimental or quasi‑experimental designs)",
        "Statistical frameworks and limitations (null hypothesis testing, interpretation of p‑values, Bayesian priors, sensitivity analysis, and p‑hacking risks)"
      ],
      "source_article": "Causal inference",
      "x": 1.576578974723816,
      "y": 1.117832899093628,
      "level": 2,
      "original_question_hash": "851ec29e"
    },
    {
      "question": "An international firm's ethics committee drafts a policy: \"Employees may never accept personal gifts from suppliers.\" Four philosophers respond:\n\n- Philosopher P: \"The rule must apply to all employees everywhere; any exception undermines morality—moral rules are absolute.\"\n- Philosopher Q: \"A universal ethic should apply to all similarly situated individuals, but it can permit exceptions when consequences or competing values justify them (for example, small culturally-significant tokens).\"\n- Philosopher R: \"Morality is grounded in God's commands; the code is morally binding only insofar as divine law endorses it.\"\n- Philosopher S: \"Moral prescriptions are universal imperatives that do not state truth-apt propositions but prescribe action; they claim universal application by prescribing how anyone in the situation should act.\"\n\nWhich of these responses are consistent with moral universalism as described in the literature, including its internal variability (absolutist vs non-absolutist/value-pluralist forms) and its range of grounding theories (cognitivist and non-cognitivist)?",
      "options": {
        "A": "Only Q and S are consistent (non-absolutist universalism and universal prescriptivism).",
        "B": "All four (P, Q, R, and S) are consistent manifestations of moral universalism.",
        "C": "Only P and R are consistent (absolutist objectivism and divine command theory).",
        "D": "Only R and S are consistent (divine command theorists and non-cognitivist universal prescriptivists)."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete multinational workplace scenario and mapped four respondent positions to types of universalism (absolutist, non-absolutist/value-pluralist, divine command, universal prescriptivism) to test recognition that moral universalism admits multiple internal forms and grounding theories.",
      "concepts_tested": [
        "Universality of moral principles",
        "Variability within universalism",
        "Grounding theories and rational defensibility"
      ],
      "source_article": "Moral universalism",
      "x": 1.1817729473114014,
      "y": 1.0301311016082764,
      "level": 2,
      "original_question_hash": "66afa8a8"
    },
    {
      "question": "A socially owned economy has two enterprises producing goods X and Y. Production uses two inputs, steel and labor, with per-unit requirements: X uses 2 units of steel and 3 labor-hours; Y uses 1 unit of steel and 4 labor-hours. Available supplies are 100 units of steel and 200 labor-hours. The social planner's objective is aggregate use-value U = 5q_X + 7q_Y, where q_X and q_Y are physical output quantities. The planner can either (i) centrally solve the constrained maximization in physical units, (ii) adopt indicative (decentralized) planning by setting shadow prices for steel and labor and letting enterprises choose outputs, or (iii) use financial planning (monetary accounting and profit targets). Which of the following statements is correct about allocations and the implications of these planning choices for information, computation and evaluation metrics?",
      "options": {
        "A": "If the central planner accurately knows the input inventories and solves the linear program in physical units (maximize U subject to 2q_X+1q_Y≤100 and 3q_X+4q_Y≤200), the optimal solution is q_X=0, q_Y=50 and thus central physical planning attains the social optimum; physical planning avoids monetary valuation distortions but requires comprehensive aggregated information and iterative computation; indicative planning can replicate this allocation only if enterprises respond to the planner's shadow prices, but it is more vulnerable to local, time‑lag information problems.",
        "B": "Financial planning that converts all inputs and outputs into money and uses profit-like criteria will necessarily produce the same allocation q_X=0, q_Y=50 because monetary prices automatically reflect resource scarcities and map directly onto social use-value, so there is no substantive difference between physical and financial planning in outcomes.",
        "C": "A fully decentralized, bottom-up planning process in which enterprises negotiate outputs solely on the basis of local inventories will always outperform a centrally solved linear program because it requires less central information and eliminates the computational burden of solving constrained optimizations, so physical central planning is never preferable.",
        "D": "Modern computational power eliminates the practical differences among central physical planning, indicative planning and financial planning: a central authority can compute optimal physical targets and simultaneously convert them into monetary budgets, making the choice of physical versus financial units purely terminological with no effect on measurement or coordination challenges."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete two-good LP with numerical constraints and objective to test understanding of constrained optimization in physical units, contrasted central vs decentralized indicative planning (information flow, shadow prices), and the difference between physical and financial planning metrics.",
      "concepts_tested": [
        "Planning as constrained optimization in physical units replacing factor markets",
        "Centralization vs decentralization: information flow, shadow prices, and planner effectiveness",
        "Distinction between physical planning and financial planning: units of analysis, measurement and coordination"
      ],
      "source_article": "Economic planning",
      "x": 1.3081058263778687,
      "y": 0.9563500881195068,
      "level": 2,
      "original_question_hash": "e92de05d"
    },
    {
      "question": "States A and B have a long-standing dispute over a maritime boundary and fisheries. Their bilateral treaty contains a clause requiring unresolved disputes to be sent to binding arbitration; it also notes that parties may pursue consensual processes before arbitration. Recently, B's commercial fishing fleets have entered waters A claims as its exclusive economic zone and are depleting stocks. A seeks immediate cessation of the incursions and compensation, while B argues scarcity justifies cooperative quota-setting and capacity support. Which dispute‑resolution strategy best reconciles A's need for enforceable, coercive relief with B's interest in a durable, negotiated management regime, and why?",
      "options": {
        "A": "File a case at the International Court of Justice (ICJ) because ICJ judgments are binding and will promptly coerce B to stop fishing while creating a permanent legal boundary resolution.",
        "B": "Invoke the treaty's binding arbitration clause to obtain a prompt, enforceable award (including provisional relief) to halt incursions and claim damages, while concurrently offering mediated talks to design long‑term quota and capacity‑building arrangements; rely on state enforcement mechanisms to implement the award if necessary.",
        "C": "Decline adjudication and pursue only mediation and conciliation, since consensual resolution best preserves relations and will generate the cooperative quota system B seeks without coercion or enforcement complications.",
        "D": "Bring suit in A's domestic courts to enjoin the foreign vessels, because domestic courts have coercive power and can immediately impose sanctions and invoke domestic enforcement to stop B's fleets."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete interstate maritime dispute requiring both immediate coercive relief and long-term cooperation; contrasted adjudicative (binding arbitration) versus consensual (mediation) approaches and noted role of state enforcement and treaty clauses.",
      "concepts_tested": [
        "Adjudicative vs. consensual dispute resolution",
        "Role of enforcement power in choosing dispute resolution",
        "Impact of dispute settlement clauses and international mechanisms on state incentives"
      ],
      "source_article": "Dispute resolution",
      "x": 1.2372947931289673,
      "y": 0.853391170501709,
      "level": 2,
      "original_question_hash": "29d82be4"
    },
    {
      "question": "A coastal nation faces accelerating sea-level rise. Consider four policy arrangements for adapting coastal zones. Which arrangement best exemplifies the principles of multi-level governance (polycentric decision-making, vertical and horizontal linkages including non-state actors, and cross-level interactions between domestic and international authority) rather than a hierarchical multilevel government model?",
      "options": {
        "A": "The national government drafts a single, binding Coastal Adaptation Act that prescribes uniform building codes and allocates funds to provincial agencies to implement the law; provinces and municipalities must follow the Act, and external actors like NGOs or international bodies are excluded from decision-making and implementation.",
        "B": "A supranational basin commission negotiates regional adaptation standards with national ministries; national governments coordinate financing frameworks; regional authorities adapt zoning rules; municipalities pilot green infrastructure projects; a transnational network of cities shares best practices horizontally; NGOs and private banks co-finance local projects and monitor outcomes, and civil-society platforms mediate between these levels.",
        "C": "An international treaty obliges states to reduce coastal vulnerability; national parliaments transpose treaty requirements into national law, leaving subnational governments and non-state actors no formal responsibilities or roles in planning or implementation.",
        "D": "A regional intergovernmental forum holds annual meetings where central governments exchange technical reports and agree on voluntary guidelines; implementation is left entirely to individual national ministries with no cooperation among municipalities, regions, or private and civil-society actors."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete coastal adaptation scenario with four distinct governance arrangements; option B contains polycentric, vertical and horizontal linkages, inclusion of non-state actors, and supranational–domestic interaction, matching multi-level governance concepts.",
      "concepts_tested": [
        "Dispersion of decision-making across vertical and horizontal levels and inclusion of non-governmental actors (polycentric governance)",
        "Cross-level interactions linking domestic and international authority (global–local interdependence)",
        "Distinction between governance networks and hierarchical multilevel government structures"
      ],
      "source_article": "Multi-level governance",
      "x": 1.230754017829895,
      "y": 0.8696460723876953,
      "level": 2,
      "original_question_hash": "da4b9fa3"
    },
    {
      "question": "Consider the following life-course vignette. Maya is raised in a family that explicitly teaches collectivist norms and rituals (primary socialization), so she internalizes obligations to kin via repeated instruction and modeled behavior. In adolescence she spends increasing time with peers who contest some parental norms while she negotiates identity (Erikson's adolescence stage); as an adult she enlists in the military and undergoes intensive resocialization into a hierarchical institutional culture. Genetic testing indicates Maya carries an allele previously associated with higher impulsivity. Which of the following statements best integrates sociological theories of socialization (internalization via learning/teaching and stage-based development) with empirical gene–environment interaction?",
      "options": {
        "A": "Maya's early internalization of collectivist norms makes her values essentially fixed for life; later peer influences and institutional resocialization will at most create superficial role changes, and her genotype has negligible effect on long-term behavior.",
        "B": "Maya's ultimate behavior and moral reasoning will be determined solely by her genotype for impulsivity; socialization agents (family, peers, military) merely reveal an underlying biological destiny without altering moral-stage development.",
        "C": "Maya's values and actions will be produced by ongoing socialization across life stages—primary socialization, peer-group identity work, and resocialization—where learning and teaching by agents and institutions internalize norms, but the expression of behavioral tendencies (e.g., impulsivity) will vary because her genotype interacts with these social contexts to amplify, attenuate, or redirect outcomes.",
        "D": "Because resocialization in a total institution like the military enforces strict norms, Maya will inevitably progress to a higher moral-reasoning stage (post-conventional) that overrides both her childhood internalization and any genetic predispositions."
      },
      "correct_answer": "C",
      "generation_notes": "Constructed a concrete life-course vignette (primary socialization, adolescent peer influence, adult resocialization) and asked which option synthesizes internalization mechanisms, stage-based lifelong development, and gene–environment interaction; created plausible distractors emphasizing deterministic gene- or social-only explanations.",
      "concepts_tested": [
        "Internalization mechanism (learning and teaching across agents/institutions)",
        "Lifelong developmental role of socialization (stages, resocialization, moral and psychosocial development)",
        "Gene–environment interaction (social environment modulates genetic predispositions)"
      ],
      "source_article": "Socialization",
      "x": 1.2697864770889282,
      "y": 1.0111191272735596,
      "level": 2,
      "original_question_hash": "16f7a371"
    },
    {
      "question": "Consider two firms competing in the same industry. Firm M manufactures mechanical pumps on automated assembly lines using steel and other raw materials. Firm K develops embedded control algorithms for pumps, provides analytics-as-a-service, holds several patents on its algorithms, and hires data scientists and systems engineers who collaborate across disciplines and international networks. Which option best explains why Firm K exemplifies a knowledge-based economy, consistent with the ideas that knowledge is a primary production input (human capital and intangible assets), that a knowledge economy emphasizes a highly skilled adaptable workforce with both technical and relational skills, and that economic value is shifting from physical inputs to information and intellectual property?",
      "options": {
        "A": "Firm K's economic value derives principally from human capital and intangible assets (patents, proprietary algorithms, customer data) rather than raw materials; it organizes production around knowledge-intensive activities (R&D, software and services), requires employees with specialized technical and relational skills to adapt and collaborate across disciplines and networks, and thus exemplifies the Information Age shift toward information and IP as central production inputs.",
        "B": "Firm K exemplifies the knowledge economy because it replaces all need for skilled labour by automating decision making; its competitive advantage comes mainly from distributing identical software at scale rather than from employees' specialised expertise or from protecting intellectual property.",
        "C": "Firm K should be classified with industrial manufacturing firms because its services are tied to a physical product (pumps), so its principal inputs remain steel and assembly capacity; patents and analytics are marginal additions that do not change the firm's economic character.",
        "D": "Firm K's advantage is indistinguishable from Firm M's since information is equivalent to knowledge; therefore both firms rely on the same mix of capital and unskilled labour and neither particularly depends on intangible assets or specialised human capital."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete firm-level comparison (manufacturing vs knowledge-intensive software/analytics with patents) to test three core concepts: knowledge as primary input/intangible assets, emphasis on skilled adaptable workforce with relational and technical skills, and the shift from physical inputs to information/IP within the Information Age.",
      "concepts_tested": [
        "Knowledge as a primary production input and source of economic value (human capital and IP)",
        "Importance of a highly skilled, adaptable workforce with technical and relational skills in a knowledge economy",
        "Shift from physical/capital intensity to intangible assets, information and intellectual property as drivers of growth"
      ],
      "source_article": "Knowledge economy",
      "x": 1.2929633855819702,
      "y": 0.9780811071395874,
      "level": 2,
      "original_question_hash": "125c54a5"
    },
    {
      "question": "An oncology clinic sequences and profiles tumors from three newly diagnosed breast cancer patients. Patient 1 has HER2 overexpression by IHC (3+) and HER2 amplification by FISH. Patient 2 is ER+/PR+ and HER2– but genotyping shows a CYP2D6 *4/*4 poor‑metabolizer genotype. Patient 3 is triple‑negative (ER–/PR–/HER2–) but tumor sequencing reports a high tumor mutational burden (TMB). Which clinician strategy best illustrates: (i) the use of molecular profiling and biomarkers to stratify patients and guide therapy (including pharmacogenomics); (ii) the practical distinction between precision medicine (targeting molecularly defined subgroups) and personalized medicine (using an individual’s molecular profile to inform care); and (iii) diagnostic testing and analytics translating molecular characteristics into optimal therapy selection?",
      "options": {
        "A": "Prescribe trastuzumab for Patient 1 based on the HER2 companion diagnostic; avoid tamoxifen for Patient 2 (choose a non‑tamoxifen endocrine option or dose adjustment) because the CYP2D6 poor‑metabolizer genotype impairs tamoxifen activation; and consider immune checkpoint inhibitor therapy for Patient 3 given the high TMB — decisions documented with the tumor sequencing and pharmacogenetic reports.",
        "B": "Treat all three patients with the same standard anthracycline‑based chemotherapy regimen because it is the established first‑line approach for breast cancer; reserve molecular tests for research only.",
        "C": "Commission three bespoke drugs synthesized uniquely for each patient’s entire genome so that each receives a patient‑specific molecular therapy never used in others; do not rely on existing companion diagnostics or pharmacogenomic guidelines.",
        "D": "Base treatment decisions only on imaging phenotypes (mammography and MRI enhancement patterns) and population risk models (e.g., average recurrence scores), ignoring tumor sequencing and genotyping; use the same hormone therapy for all ER+ patients without adjusting for pharmacogenomic variants."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a clinical vignette with concrete biomarker results (HER2, CYP2D6, TMB) and four plausible management strategies to test: biomarker‑driven stratification and pharmacogenomics (correct), one‑size‑fits‑all care, literal per‑patient bespoke drug (misinterpretation of 'personalized'), and imaging/population‑based decisions (insufficient).",
      "concepts_tested": [
        "Molecular profiling and biomarkers for patient stratification (HER2, CYP2D6, TMB)",
        "Distinction between precision medicine (subgroup targeting) and personalized medicine (individual molecular guidance)",
        "Role of diagnostic testing and analytics in translating molecular data into therapy selection (companion diagnostics, pharmacogenomics, sequencing)"
      ],
      "source_article": "Personalized medicine",
      "x": 1.3496547937393188,
      "y": 0.969965398311615,
      "level": 2,
      "original_question_hash": "d66c46ec"
    },
    {
      "question": "A computational physics group simulates a time-dependent Schrödinger equation for a chain of $N=30$ interacting spin-1/2 particles. They discretize time and space, implement an explicit algorithm that performs a finite sequence of arithmetic operations on floating-point numbers, and monitor both truncation and round-off errors. They report the outputs as “simulation results” and compare them with laboratory measurements of the same system. Which of the following best captures the correct statements about the computational and epistemic status of this work and the scalability of the underlying problem?",
      "options": {
        "A": "Because the algorithm executes a finite number of operations and monitors errors, the numerical outputs are exact representations of the continuous quantum model; moreover, the computational cost for quantum many-body wavefunctions scales polynomially with $N$, so exact solutions are feasible for large $N$.",
        "B": "Numerical simulations approximate continuous mathematical models via a finite sequence of operations and therefore require explicit error estimation (truncation and round-off); quantum many-body problems typically have Hilbert-space dimensions growing like $2^N$, so exact solutions become infeasible as $N$ increases; computational physics thus acts as a bridge supplementing both theory and experiment rather than wholly replacing laboratory measurement.",
        "C": "Simulations performed on computers are equivalent to physical experiments because the computer ‘measures’ the system; classical $N$-body problems always scale exponentially with $N$; careful error estimation is unnecessary if the algorithm is implemented in double precision.",
        "D": "Computational physics is purely a subfield of theoretical physics with no experimental role; numerical approximations always converge rapidly to analytic solutions regardless of problem nonlinearity or chaos; computational complexity does not depend on the number of degrees of freedom in the model."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete quantum many-body simulation scenario to probe (1) numerical approximation and error estimation, (2) the debate/status of computation as bridge between theory and experiment, and (3) computational scaling (exponential Hilbert space growth $2^N$). Option B states the three correct principles.",
      "concepts_tested": [
        "numerical approximation and error estimation",
        "epistemic status of computational physics as bridge between theory and experiment",
        "computational complexity and scalability (exponential growth for quantum many-body)"
      ],
      "source_article": "Computational physics",
      "x": 1.7154420614242554,
      "y": 1.1025489568710327,
      "level": 2,
      "original_question_hash": "d3785264"
    },
    {
      "question": "The City of Springfield enacts the Emergency Public Safety Act authorizing immediate seizure of privately owned homes in designated zones and summary detention of occupants for up to 30 days without a pre-deprivation hearing; all property may be transferred to the city. Carlos Álvarez files a federal constitutional challenge claiming a due process violation. Which statement best describes the constitutional analysis a U.S. court is likely to apply and how that analysis reflects the historical divergence between English and American conceptions of due process?",
      "options": {
        "A": "The court will evaluate both procedural and substantive aspects of the claim: procedural due process requires meaningful notice and a hearing when government deprives someone of life, liberty, or property, and substantive due process allows courts to invalidate laws that unreasonably infringe fundamental liberty interests even if procedures exist. That willingness of judges to define \"fundamental fairness\" is controversial but flows from a U.S. tradition (rooted historically in Magna Carta) in which courts can strike down legislative acts; this contrasts with the English doctrine of parliamentary supremacy that historically limited judicial review of primary legislation.",
        "B": "The court's analysis will be purely procedural: so long as the city later provides a post-seizure hearing and compensation, the statute survives. Substantive review is not appropriate because due process governs only the timing and form of procedures, not the content of legislative policy; Magna Carta was concerned only with landowners and does not justify substantive judicial invalidation, and English and American approaches are essentially the same.",
        "C": "The Due Process Clause only incorporates protections against federal government abuse, so Álvarez cannot bring a federal due process claim against a city; judicial invalidation of legislation is precluded except by constitutional amendment, and Magna Carta is merely a historical curio with no continuing influence on modern constitutional adjudication.",
        "D": "In emergencies the legislature may suspend due process so the Act is constitutionally immune; substantive due process forbids judges from substituting their view for legislative judgments about public safety, and due process as a doctrine was developed post‑Revolution in the United States without meaningful roots in Magna Carta or the English rule of law."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete municipal emergency statute scenario to test students' understanding of procedural vs substantive due process, judicial review authority in U.S. courts, and the historical origin/divergence from Magna Carta and English parliamentary supremacy.",
      "concepts_tested": [
        "Due process as a constraint on government power and guarantor of fair procedure",
        "Distinction and controversy between procedural due process and substantive due process (judicially defined fundamental fairness)",
        "Historical origin in Magna Carta and divergence between English parliamentary supremacy and American judicial review"
      ],
      "source_article": "Due process",
      "x": 1.241949200630188,
      "y": 0.7843385934829712,
      "level": 2,
      "original_question_hash": "f3b60629"
    },
    {
      "question": "A nation with 100 legislative seats has three parties receiving national vote shares of $45\\%$, $35\\%$, and $20\\%$. Three alternative electoral rules are considered: (1) 100 single-member districts using first-past-the-post (FPTP), which in practice yields seat counts A:60, B:30, C:10; (2) a single nationwide party-list proportional representation system that allocates seats exactly proportional to vote share; (3) multi-member districts using the D'Hondt highest-average method — in a representative 10-seat district with votes A:45,000, B:35,000, C:20,000 the D'Hondt allocation is A:5, B:3, C:2. Separately, a single-winner mayoral election in this country uses instant-runoff voting (IRV) with three candidates.\n\nWhich of the following conclusions follows from these facts together with the implications of Arrow's and Gibbard's theorems as described in the article?",
      "options": {
        "A": "FPTP in single-member districts (1) produces a more disproportional seat outcome than the nationwide list-PR (2); D'Hondt in multi-member districts (3) can produce locally proportional-looking results (e.g. 5–3–2 in a 10-seat district) but systematically advantages larger parties relative to exact proportional quotas; and by Gibbard’s result no deterministic single-winner rule (such as IRV) with three or more candidates can be both non-dictatorial and immune to strategic voting, so strategic incentives can exist in the IRV mayoral contest.",
        "B": "Because the D'Hondt example gives A:5, B:3, C:2 in the 10-seat district, D'Hondt always produces exact national proportionality across any seat total; therefore FPTP and PR are functionally equivalent whenever district-level D'Hondt is used, and Gibbard's theorem does not apply to ranked systems like IRV.",
        "C": "Single-member FPTP (1) will always yield a result closer to the parties' vote shares than a nationwide list-PR (2) because geographic concentration translates votes into seats more efficiently; Arrow's theorem guarantees IRV is strategy-proof since it is a ranked method and thus eliminates the spoiler effect.",
        "D": "If a system uses multi-member districts with 10 seats and the vote shares are A:45\\%, B:35\\%, C:20\\%, then any highest-average method (including D'Hondt) must give each party seats exactly equal to $\\text{round}(10\\times\\text{vote share})$, so A:5, B:4, C:1; and Gibbard's theorem implies the only way to avoid strategic voting in single-winner rules is to reduce the ballot to two alternatives."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete three-party, 100-seat scenario comparing FPTP, list-PR, and D'Hondt district outcomes; linked these to Arrow/Gibbard implications about manipulability and proportionality. Distractors violate proportionality math or misstate the theorems.",
      "concepts_tested": [
        "Seat allocation mechanisms and proportionality (party-list PR vs FPTP vs D'Hondt)",
        "Effect of district structure (single-member winner-take-all vs multi-member proportional districts) on representation",
        "Fundamental impossibility results (Arrow's and Gibbard's theorems) and implications for strategic voting and fairness"
      ],
      "source_article": "Electoral system",
      "x": 1.2025469541549683,
      "y": 0.9137877225875854,
      "level": 2,
      "original_question_hash": "e451a136"
    },
    {
      "question": "An NGO, Health4All, pilots a mobile health education app in two districts. After the first 3 months the evaluation team conducts structured interviews, rapid A/B tests of message wording, and weekly usability logs to refine content and delivery. After 12 months, an independent team compares baseline and endline survey scores on pre-specified knowledge and behavior indicators, applies statistical tests to estimate effect sizes, and produces recommendations for national scale-up and funding decisions. Which description best characterizes the two evaluations and their roles?",
      "options": {
        "A": "The 3-month activity is a formative evaluation—iterative, improvement-focused, and responsive to qualitative and rapid-cycle tests—while the 12-month activity is a summative, criteria-based evaluation using predefined indicators and statistical methods to inform decisions about adoption and funding.",
        "B": "The 3-month activity is a summative evaluation because it uses data to judge program merit early, and the 12-month activity is formative because it refines implementation based on endline comparisons to baseline.",
        "C": "Both the 3-month and 12-month assessments are summative evaluations because they each produce definitive judgments about program worth that are valid for decision-making, regardless of timing or methods used.",
        "D": "Both assessments are formative evaluations focused primarily on improving the app; neither needs predefined criteria or statistical analysis because formative work is always qualitative and never used for funding or scaling decisions."
      },
      "correct_answer": "A",
      "generation_notes": "Created a realistic NGO pilot scenario contrasting a mid-project iterative (formative) evaluation with a post-implementation, standards-driven (summative) impact assessment; distractors swap labels or overgeneralize methods.",
      "concepts_tested": [
        "Formative vs summative evaluation (timing and aims: improvement vs assessment)",
        "Criteria-based systematic assessment and use of predefined indicators and statistical methods",
        "Evaluation as decision-support and learning tool (informing scale-up and program refinement)"
      ],
      "source_article": "Evaluation",
      "x": 1.3432819843292236,
      "y": 0.999565839767456,
      "level": 2,
      "original_question_hash": "16106a8d"
    },
    {
      "question": "A coastal state's environmental agency is assessing a proposal to build a chemical processing facility 2 km upstream of an estuarine fishery. Independent studies show a plausible risk of chronic, low‑level releases of a novel organic compound whose long‑term ecotoxicity is incompletely characterized. Local fishers demand protection; the developer argues that full scientific certainty about harm will take years of study and that delaying the project will cause economic loss. Which regulatory package best implements (1) the precautionary principle, (2) the polluter pays principle, and (3) an effective environmental impact assessment (EIA)?",
      "options": {
        "A": "Require a comprehensive, pre‑construction EIA with public participation and independent peer review; set conservative discharge limits and require best available techniques plus a monitoring program and contingency shutdown triggers (precautionary approach despite uncertainty); and mandate that the developer post a legally enforceable financial assurance (bond/insurance) to fund remediation and compensate harmed fishers if damage occurs (internalizing costs).",
        "B": "Grant a provisional permit allowing construction to start immediately, require the developer to submit an EIA within two years of operation, allow interim discharges at industry average levels, and rely on the state to create a compensation fund financed by general taxation if environmental harm later appears.",
        "C": "Ban the entire class of plants and similar chemical processes nationwide until all scientific uncertainties are resolved, and use taxpayer money to compensate any stranded investors; no project‑specific EIA is required because the activity is prohibited.",
        "D": "Approve the project after the company signs a voluntary code of conduct committing to self‑monitoring and periodic public reports; require no independent EIA but ask the company to pay into a voluntary disaster relief pool that may be accessed after demonstrable harm is proven in court."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete siting scenario with scientific uncertainty and socioeconomic stakes; designed four plausible regulatory packages to require students to map actions to the precautionary principle (protect despite uncertainty), polluter pays (developer financial liability), and EIA (prior, participatory, independent assessment).",
      "concepts_tested": [
        "precautionary principle",
        "polluter pays principle",
        "environmental impact assessment"
      ],
      "source_article": "Environmental law",
      "x": 1.3869502544403076,
      "y": 0.8732774257659912,
      "level": 2,
      "original_question_hash": "3d22c2e0"
    },
    {
      "question": "A mid-sized city council enacts a nighttime curfew to reduce youth street violence. Local families adapt by changing household routines; neighborhood parent associations organize petitions and informal patrols; a national NGO mobilizes media campaigns; police intensify street stops; and municipal bureaucrats revise enforcement guidelines. If you were asked to analyze these developments from the perspective of political sociology, which research framing best captures the core analytical commitments of the field described in the article?",
      "options": {
        "A": "Treat politics and society as mutually constitutive: trace how power is distributed and exercised relationally across sites (families, community groups, NGOs, police, bureaucrats, the city state), examining contestation, consent, and coercion at micro and macro levels rather than isolating legal texts or institutions.",
        "B": "Focus primarily on the municipal ordinance and its legal language, measuring compliance and effectiveness through changes in arrest statistics; social responses are secondary background effects best explained by policy implementation failures.",
        "C": "Frame the problem solely in terms of class interests and economic structures, arguing that the curfew is a tool of the capitalist class to discipline a surplus population, with family and community actions reducible to class position.",
        "D": "Analyze the situation mainly through individual leadership and charisma: identify the key charismatic actors (police chiefs, NGO directors, parent leaders) whose personal authority explains the policy's adoption and social reactions, treating institutions as neutral conduits of their will."
      },
      "correct_answer": "A",
      "generation_notes": "Created a realistic municipal curfew scenario and crafted four plausible analytical framings; option A aligns with political sociology by emphasizing interdependence of society and politics, relational/distributive power, and analysis across sites from family to state.",
      "concepts_tested": [
        "Interdependence of society and politics",
        "Power as relational and distributive",
        "Analysis of power across sites from family to state"
      ],
      "source_article": "Political sociology",
      "x": 1.219444990158081,
      "y": 0.9790982007980347,
      "level": 2,
      "original_question_hash": "c4af4d00"
    },
    {
      "question": "A contemporary novel originally published in Language X wins critical acclaim at home. Two years later it is translated into English, French, and Arabic; international publishers issue inexpensive paperback editions; major booksellers in several countries stock it; and public libraries in multiple cities include it on their shelves. Literary journals in different countries publish close readings and scholarly essays treating the book as literature rather than merely reportage. According to dominant formulations of \"world literature\" (e.g., Damrosch, Venkat Mani, and related scholars), which set of developments best explains why this novel can now be considered part of world literature?",
      "options": {
        "A": "The novel's dual trajectory: it has been read and evaluated as literature (critical and scholarly attention) and it has circulated beyond its linguistic and cultural origin through translation and dissemination supported by publishers, booksellers, and public libraries — the combined cultural reception and information-transfer infrastructure that produces world literature.",
        "B": "The novel's commercial success in multiple countries where the original Language X edition was exported intact, relying primarily on tourist bookstores and expatriate networks, shows that wide sales alone are sufficient for world-literary status, even without translation or institutional library support.",
        "C": "The novel's being translated into several languages is the decisive factor: once translated, a text automatically achieves world-literary status regardless of whether critics or scholarly readers treat it as literature or whether affordable editions and libraries make it accessible.",
        "D": "The novel's incorporation into foreign school curricula via cultural diplomacy (government-funded excerpts and summaries) demonstrates that institutional endorsement by states, rather than market-based publishers, booksellers, translators, or libraries, is the primary mechanism for making a work world literature."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete publication-and-reception scenario asking which combination of reception (read as literature) and circulation (translation + print distribution + libraries) matches theoretical definitions by Damrosch and Mani; distractors isolate single factors (commerce, translation alone, state endorsement) to test understanding of the double-process and infrastructural mechanisms.",
      "concepts_tested": [
        "Double process: literary reception plus circulation beyond origin",
        "Role of print culture and institutions (publishers, booksellers, public libraries)",
        "Translation and cross-cultural circulation as necessary conditions for world literature"
      ],
      "source_article": "World literature",
      "x": 0.6793181300163269,
      "y": 1.0791581869125366,
      "level": 2,
      "original_question_hash": "f4ea02fb"
    },
    {
      "question": "A coastal river basin has experienced accelerated bank erosion and higher turbidity in the estuary after a decade of upstream deforestation and expansion of shrimp farms. Local communities report more frequent inundation of low-lying fields and increased health problems from contaminated drinking water. As an integrated geographer tasked with diagnosing the problem and suggesting interventions, which research design best exemplifies an integrated geography (human–environment) approach to this coupled system?",
      "options": {
        "A": "Treat the basin as a coupled human–environment system: quantify land-cover change and river-sediment fluxes using time-series remote sensing, map flood-prone areas and community exposure with GIS, combine these physical datasets with household surveys and land-tenure records to identify socio-economic drivers and feedbacks, and propose interventions that address both biophysical processes and local land-use practices.",
        "B": "Focus exclusively on hydrodynamics and sediment transport: deploy in‑situ gauges and model river flow and turbidity, then recommend engineered channel stabilization and estuarine dredging to reduce turbidity and erosion, without incorporating social data or land-use histories.",
        "C": "Concentrate on cultural perceptions of risk: conduct ethnographic interviews and participatory mapping to document how communities perceive flooding and water quality, then design outreach and education campaigns to change local behavior, without quantitative analysis of physical changes to the landscape.",
        "D": "Use remote sensing only to produce a high-resolution land-cover map and deliver it to local planners; assume that providing better maps is sufficient for decision-making and do not collect field or socio-economic data."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete coastal basin scenario requiring analysis of feedbacks; option A integrates coupled system framing, remote sensing and GIS methods, and synthesis of social and physical data to match integrated geography. Distractors isolate single-discipline approaches or incomplete use of tools.",
      "concepts_tested": [
        "Coupled human–environment systems and feedbacks",
        "Use of remote sensing and GIS to measure human–environment interactions",
        "Integrated geography as synthesis of human and physical geography perspectives"
      ],
      "source_article": "Integrated geography",
      "x": 1.5192360877990723,
      "y": 0.9849785566329956,
      "level": 2,
      "original_question_hash": "07c1c71e"
    },
    {
      "question": "Consider four hypothetical national economies described below. Which one most closely matches the classical definition of state capitalism — i.e., a system in which the state itself functions as the dominant capitalist actor by owning or holding controlling shares in productive enterprises, centrally allocating capital and credit, and extracting surplus from wage labor to accumulate and reinvest for economic and political objectives?",
      "options": {
        "A": "The government owns 85% of heavy industry and energy through corporatized state-owned enterprises (SOEs). SOE profits are retained by the enterprises and channeled by a central planning ministry and state banks into prioritized industrial projects and foreign acquisitions. Workers are wage labor with little factory-level control; the ruling party sets investment priorities to consolidate domestic industry and strengthen geopolitical standing.",
        "B": "Most firms are privately owned. The state uses state-owned banks and sovereign wealth funds to subsidize selected private ‘national champions,’ directs credit to strategic sectors, and negotiates mergers abroad, but private shareholders retain legal ownership and receive the bulk of distributed profits; the state mainly incentivizes rather than directly owning assets.",
        "C": "A mixed-market democracy where the state regulates markets, provides universal welfare and public goods, and operates a handful of utilities as regulated public services; profits from public services are routinely transferred to social programs and citizens via taxation and social benefits rather than being reinvested primarily for accumulation.",
        "D": "A wartime command economy in which the state temporarily nationalizes industries, imposes rationing and central allocations of inputs and credit to sustain the war effort; after the conflict the state restitutes most assets to private owners and restores market mechanisms."
      },
      "correct_answer": "A",
      "generation_notes": "Created a comparative scenario that contrasts direct state ownership/control and surplus extraction (A) with indirect state influence via credit/subsidies (B), welfare/regulation (C), and temporary wartime controls (D), to test definitional scope and the spectrum of state capitalism.",
      "concepts_tested": [
        "State ownership and control of means of production (SOEs, controlling shares, central allocation of credit)",
        "State-led capital accumulation through extraction of surplus value and reinvestment",
        "Definitional ambiguity and spectrum of state capitalism (ownership vs. control; incentives/subsidies vs. direct nationalization)"
      ],
      "source_article": "State capitalism",
      "x": 0.5436872243881226,
      "y": 0.6593111157417297,
      "level": 2,
      "original_question_hash": "befd88c6"
    },
    {
      "question": "Consider a 1 m diameter rigid spacecraft of mass 500 kg in a 400 km low-Earth orbit. Its initial position and velocity are measured to high precision. The dominant forces acting on it are Earth's gravity including the $J_2$ oblateness perturbation, and a small atmospheric drag that can be modelled as $\\mathbf{F}_{\\rm drag}=-\\lambda\\mathbf{v}$ with constant $\\lambda>0$. Which one of the following statements is MOST accurate about predicting its motion, the theoretical formalisms available to derive its equations of motion, and the applicability of more modern theories?",
      "options": {
        "A": "Given sufficiently precise initial conditions, Newtonian mechanics (Newton's second law with the $J_2$ and drag terms) yields deterministic equations of motion that allow prediction of future and past trajectories; however, orbital perturbations can produce sensitive dependence on initial conditions (chaos) that limits long-term practical predictability. The same dynamical laws can be obtained or reformulated using Newtonian force vectors, Lagrangian mechanics (including non-conservative generalized forces to represent the drag), and Hamiltonian mechanics for the conservative part; the latter two are equivalent for conservative dynamics via a Legendre transform. For this spacecraft, relativistic corrections are negligibly small and quantum mechanics is irrelevant.",
        "B": "Because atmospheric drag is non-conservative, only Newton's vector form $\\mathbf{F}=m\\mathbf{a}$ can be used to obtain equations of motion; Lagrangian and Hamiltonian mechanics are inapplicable. Also, because gravity is long-range, special-relativistic corrections dominate motion in low-Earth orbit and must be included for any accurate prediction.",
        "C": "Classical mechanics is inherently statistical: even with exact initial conditions it gives only probabilistic predictions, so quantum mechanics is required to predict the spacecraft's trajectory accurately. Lagrangian and Hamiltonian formulations are merely approximations that do not contain the full information of Newtonian force laws.",
        "D": "Newton's laws guarantee a unique solution from known initial conditions, but Lagrangian and Hamiltonian methods are purely mathematical tricks that cannot represent forces like drag; furthermore, because orbital speeds are significant compared with $c$, special relativity must always replace Newtonian mechanics for any orbital calculation."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete LEO spacecraft scenario including conservative ($J_2$) and non-conservative (linear drag $-\\lambda v$) forces to probe: determinism vs. chaos, equivalence and limits of Newtonian/Lagrangian/Hamiltonian formalisms, and the classical domain of applicability vs. relativity/quantum mechanics.",
      "concepts_tested": [
        "Deterministic prediction from initial conditions and limits from chaos",
        "Equivalence and treatment of dynamics in Newtonian, Lagrangian, and Hamiltonian formalisms",
        "Domain of applicability of classical mechanics and when relativity or quantum mechanics are required"
      ],
      "source_article": "Classical mechanics",
      "x": 1.7249079942703247,
      "y": 1.119308590888977,
      "level": 2,
      "original_question_hash": "3528e161"
    },
    {
      "question": "Midwest Manufacturing, Inc., a corporation incorporated and headquartered in State A, sells electronic toys online to consumers across the United States. Its website uses a standard form contract containing warranty disclaimers and a choice-of-law clause selecting State A law. A consumer resident in State B purchases a toy, which malfunctions and causes injury. The consumer sues in State B alleging (1) breach of contract under the law of sales, (2) violation of State B consumer protection statutes, and (3) violation of a federal product-safety regulation. Which of the following statements best describes the applicable legal framework and the relationship among the different legal regimes?",
      "options": {
        "A": "The sale claim will be governed primarily by state law implementing the Uniform Commercial Code (Article 2) as adopted and possibly modified by the relevant state; however, State B’s consumer protection statute and federal product-safety rules can independently impose obligations and remedies applicable to the transaction. Choice-of-law and forum rules, state variations in UCC adoption, and any expressly preemptive federal statute will determine which rules control, so both private contract law (codified sales law) and public regulatory law are likely to shape the outcome.",
        "B": "Because the transaction involved interstate commerce, federal law exclusively governs all aspects of the dispute: the U.S. Congress’s commerce power and federal product-safety regulation preempt any state sales law or consumer-protection statute, so only federal remedies apply and state contract law is displaced.",
        "C": "Only the parties’ private contract controls the dispute: the standard form contract and its warranty disclaimers are binding, and statutory consumer-protection or federal safety rules cannot be invoked because commercial transactions are governed solely by private ordering under the UCC.",
        "D": "Tort and product-safety claims in State B will supersede any contract claim; commercial law and the UCC are irrelevant once an injury occurs, because public regulatory law always overrides private sales law in consumer injury cases."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete interstate e-commerce scenario to test interplay of UCC as codified state sales law, state consumer-protection statutes, federal safety regulations, choice-of-law issues, and concept of public vs private regulation in commercial law.",
      "concepts_tested": [
        "Role of commercial law (UCC) in creating predictable, enforceable sales rules",
        "Mechanism of codification and state adoption/modification of the Uniform Commercial Code",
        "Interaction between private contract law and public regulatory frameworks (consumer protection, federal regulation, preemption)"
      ],
      "source_article": "Commercial law",
      "x": 1.2792104482650757,
      "y": 0.8497682809829712,
      "level": 2,
      "original_question_hash": "92153257"
    },
    {
      "question": "A municipal planning committee must decide whether to approve construction of a large solar farm on agricultural land (action A) to achieve a citywide reduction in CO2 emissions (goal G). The technical planner argues: “We should build A because it will bring about G and the required funding and grid upgrades are already secured, so the project is practically possible.” The environmental advocate argues: “We should build A because it realizes stewardship and promotes the value of sustainability (V).” A third member insists: “We must build A because protecting future generations is our duty, regardless of local preferences.” Based on the distinctions in practical reason discussed in the article, which option correctly identifies (i) the type of practical reasoning exemplified by the technical planner, (ii) the instrumental critical question that best tests the planner’s claim about practical possibility, and (iii) the normative framework that treats practical reason as law‑abiding and duty‑bound?",
      "options": {
        "A": "(i) Instrumental practical reasoning; (ii) CQ4: “What grounds are there for arguing that it is practically possible for me to bring about A?”; (iii) Kantian account invoking the categorical imperative (duty‑based).",
        "B": "(i) Value‑based practical reasoning; (ii) CQ3: “Among bringing about A and these alternative actions, which is arguably the most efficient?”; (iii) Thomistic first principle (“good is to be done and pursued”).",
        "C": "(i) Instrumental practical reasoning; (ii) CQ3: “Among bringing about A and these alternative actions, which is arguably the most efficient?”; (iii) Utilitarian instrumental account treating reason as an instrument for satisfactions of wants.",
        "D": "(i) Value‑based practical reasoning; (ii) CQ1: “What other goals do I have that should be considered that might conflict with G?”; (iii) Kantian account invoking the categorical imperative (duty‑based)."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete policy scenario to require classification of planner's reasoning as instrumental vs value‑based, selection of the correct instrumental critical question (CQ4 for feasibility), and identification of Kantian duty‑based normative foundation.",
      "concepts_tested": [
        "Instrumental versus value‑based practical reasoning",
        "Instrumental practical reasoning argumentation scheme and critical questions (CQ1–CQ5)",
        "Normative foundations for practical reason (Kantian duty, Thomistic good, utilitarian instrumentality)"
      ],
      "source_article": "Practical reason",
      "x": 1.1966047286987305,
      "y": 1.071159839630127,
      "level": 2,
      "original_question_hash": "1f84b429"
    },
    {
      "question": "Consider two comparable late-18th-century towns, Lowford and Marshbridge. Both sit on fast rivers and have nearby coal deposits. In 1785 Lowford's cotton producers adopt water-frame and mule spinning technology and later install steam engines and power looms; mechanised spinning increases output per worker by about $500\\times$ relative to hand spinning, and the power loom raises a weaver's output by about $40\\times$. Marshbridge has access to the same technical blueprints—British engineers are willing to sell know‑how—but its courts are unreliable in enforcing contracts, there are no local banks to supply capital, and merchants face internal tariffs. Over three decades Lowford industrialises rapidly (mechanised factories, attracting investment, and rising per‑capita incomes); Marshbridge remains a putting‑out, cottage-based textile district. Which explanation best accounts for the divergence?",
      "options": {
        "A": "Technological superiority alone: the $500\\times$ and $40\\times$ productivity multipliers mean that once the machines exist, adoption automatically yields factory growth and higher incomes; institutional differences are secondary.",
        "B": "A joint explanation: the enormous productivity gains from mechanisation (e.g. $500\\times$ in spinning) create the opportunity, but secure property rights, access to credit, integrated markets and entrepreneurial networks are necessary to mobilise capital, build machine tools and factories, and to diffuse British methods—so Lowford industrialised while Marshbridge did not.",
        "C": "Natural‑resource determinism: proximity to coal and water was decisive; since both towns had the same geography, the remaining gap must be due to local variations in raw material quality rather than legal or financial institutions.",
        "D": "Patent and monopoly effects: monopolies (like Watt's steam patent) and restrictive laws prevented diffusion of steam and mill technologies to places such as Marshbridge, so legal restrictions on technology transfer, not local credit or property rights, explain the slow adoption."
      },
      "correct_answer": "B",
      "generation_notes": "Designed a comparative town scenario with numerical multipliers from textile mechanisation to test students' ability to integrate technological productivity effects with institutional prerequisites (property rights, credit, markets) and diffusion mechanisms; distractors reflect technology‑determinist, resource‑determinist, and patent‑focused interpretations.",
      "concepts_tested": [
        "Mechanization and energy shift (water/steam power, machine tools, productivity multipliers)",
        "Institutional framework (property rights, legal enforcement, access to capital, entrepreneurial culture)",
        "Diffusion and sectoral/geographic spread (how British textile innovations and entrepreneurs enabled adoption elsewhere)"
      ],
      "source_article": "Industrial Revolution",
      "x": 1.52251398563385,
      "y": 0.7032520771026611,
      "level": 2,
      "original_question_hash": "8a7b9464"
    },
    {
      "question": "Nation Freeland has a state broadcaster whose board is appointed directly by the president, several large private media conglomerates that rely on government contracts, widespread undeclared product gifts to bloggers, and a 10-year trend where print revenues fell $60\\%$ while digital revenues grew only $30\\%$. A policy task force must recommend a set of reforms that (1) maximize the independence of news media from government control and concentrated private influence, (2) reduce conflicts of interest and improve credibility through ethics and disclosure practices including separation of editorial and advertising, and (3) create realistic economic support for sustaining time‑intensive journalism (e.g., investigative, long‑form) in the digital era. Which single package of reforms below best addresses all three goals simultaneously?",
      "options": {
        "A": "Create statutory protections for freedom of the press and insulate the public broadcaster by changing appointments to an independent, multi‑stakeholder board; strengthen antitrust rules to limit cross‑ownership between media and firms with government contracts; mandate enforceable disclosure rules for gifts and financial ties (including FTC‑style penalties for bloggers) and legally require functional separation of editorial and advertising departments; and offer public‑interest support for digital journalism (competitive grants, tax incentives for subscription/recurrent revenue models, and startup funds for non‑profit investigative outlets).",
        "B": "Provide direct government subsidies to all registered news organizations (including the state broadcaster) to make up for print revenue losses, require registration and licensing of bloggers and online publishers to qualify for funding, and tighten criminal penalties for publishing falsehoods—while leaving ownership concentration and editorial–advertising arrangements to market actors.",
        "C": "Encourage voluntary industry adoption of a unified code of ethics (disclosure, impartiality, editorial–advertising separation) without statutory enforcement; offer a one‑time emergency loan program for legacy newspapers controlled by existing conglomerates; and incentivize paywalls for all outlets to boost digital revenue, but make no changes to public broadcaster governance or media ownership rules.",
        "D": "Impose a targeted tax on digital advertising revenue and transfer the proceeds to the state broadcaster and larger private outlets in proportion to audience reach; require all platforms to pre‑screen user content for disinformation; and allow commercial sponsorship of specific editorial beats provided sponsorship is disclosed in small print."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete national scenario combining governance, ethics, and digital economics; options present plausible policy packages testing effects on independence, conflict mitigation (disclosure, editorial–ad separation), and digital funding models; A integrates statutory protections, enforceable ethics, ownership limits, and diversified digital funding.",
      "concepts_tested": [
        "Media independence and ownership concentration",
        "Journalistic ethics, disclosure, and separation of editorial and advertising",
        "Digital transition economics: monetization challenges and funding models for investigative journalism"
      ],
      "source_article": "Journalism",
      "x": 1.2280534505844116,
      "y": 0.9872515797615051,
      "level": 2,
      "original_question_hash": "e0399927"
    },
    {
      "question": "In the country of Techland, a firm called AutoFab introduces low-cost, programmable manufacturing robots that cut unit production costs in textiles and consumer electronics by 50% within two years. Over five years many incumbent factories become idle and their owners declare bankruptcy; laid-off workers migrate to growing urban tech hubs; venture capital funds new robot-enabled startups producing novel products; and some regions reindustrialize while former industrial towns decline. Policymakers debate whether to subsidize legacy firms or to ease entry and retraining. Which theoretical interpretation of \"creative destruction\" best explains this pattern and which mechanisms does it emphasize?",
      "options": {
        "A": "The Schumpeterian interpretation: an evolutionary process where entrepreneurial innovation and the threat of entry drive dynamic competition. Successful innovations temporarily confer market power, devalue existing capital (bankrupting incumbents), and redistribute wealth so new investment and industries emerge — innovation is the engine of growth even as it destabilizes old institutions.",
        "B": "The Marxian interpretation: capitalism’s internal contradictions periodically produce crises that annihilate accumulated wealth; the bankruptcies and regional decline show the self-destructive tendency of capital which will ultimately lead to capitalism’s demise rather than sustain long-term growth.",
        "C": "A static competition/monopoly view: the introduction of robots primarily increases market concentration as AutoFab secures dominant positions; the bankruptcies are evidence of market failure requiring antitrust and protection to preserve static efficiency and employment.",
        "D": "A classical comparative‑advantage view: the changes reflect Techland reallocating resources to sectors where it has a new comparative advantage; there is no special role for innovation-driven devaluation of capital — markets simply reallocate labor and capital smoothly without net destruction of wealth."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete case (robotic automation) to distinguish Schumpeterian vs Marxian accounts and to test recognition of mechanisms: dynamic competition, devaluation/redistribution of capital, and evolutionary growth effects.",
      "concepts_tested": [
        "Creative destruction as evolutionary driver of capitalism",
        "Dynamic competition mechanism (threat of entry, new technologies, devaluation of existing wealth)",
        "Contrasting Schumpeterian and Marxian interpretations regarding sustainability of capitalism and business cycles"
      ],
      "source_article": "Creative destruction",
      "x": 1.2302429676055908,
      "y": 0.9491304159164429,
      "level": 2,
      "original_question_hash": "115b8a3d"
    },
    {
      "question": "Consider a stylized regional economy with 100 firms and three possible production routines: R, S, and occasionally emerging novelty N. At time 0, $p_0=0.6$ of firms use R and $0.4$ use S. Each discrete period every firm (i) samples a small random subset of peers, (ii) with probability $0.8$ imitates the routine in its sample that yielded the highest observed profit last period (otherwise it retains its routine), and (iii) with probability $\\mu$ an individual firm innovates and adopts a novel routine N. Routine N confers a direct productivity advantage of 10% to adopters but requires complementary skills; if the share of adopters exceeds a threshold $\\theta$ network externalities raise N's productivity further. There is no centralized planner and firms have limited information about the full population. Which pattern of long‑run outcomes best exemplifies the predictions of evolutionary economics as described in the article?",
      "options": {
        "A": "Deterministic convergence to a unique, welfare‑maximizing equilibrium: regardless of $p_0$, imitation drives all firms to adopt the routine with highest true productivity (eventually N once it appears), eliminating heterogeneous routines.",
        "B": "Path dependence and possible lock‑in: historical shares and stochastic innovations can lead the economy to remain stuck with a suboptimal routine (R or S); persistent heterogeneity across firms is possible; occasional innovations and feedbacks drive continual structural change and non‑equilibrium dynamics.",
        "C": "Neutral drift without selection: because imitation is noisy and innovations are rare, shares of routines execute a random walk around initial proportions and no systematic selection or structural change occurs.",
        "D": "Rapid global optimization via perfect learning: firms quickly infer the full payoff structure, coordinate on the socially optimal mix of routines (possibly a stable coexistence of R and N) and thereafter oscillations cease."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete multi‑period imitation–innovation scenario with parameters $p_0$, $\\mu$, $\\theta$ to elicit concepts of path dependence, bounded rationality, selection, heterogeneity, and ongoing innovation consistent with evolutionary economics; options contrast evolutionary vs neoclassical or neutral predictions.",
      "concepts_tested": [
        "Economy as a non‑equilibrium evolving system",
        "Heterogeneous agents with bounded rationality, imitation and learning creating feedbacks",
        "Innovation-driven qualitative and structural change with path dependence and lock‑in"
      ],
      "source_article": "Evolutionary economics",
      "x": 1.2533273696899414,
      "y": 0.9402487277984619,
      "level": 2,
      "original_question_hash": "b51d0428"
    },
    {
      "question": "Consider the following claims derived from a review of research and descriptive material on multilingualism:\n\nI. There is robust, replicated evidence that bilingual or multilingual individuals reliably have superior executive-function performance compared with monolinguals.\n\nII. Children exposed to two languages from early childhood (simultaneous bilinguals) are likely to achieve literacy in both languages and, on average, better pronunciation in a second language if exposure occurs earlier.\n\nIII. Multilingualism confers measurable economic benefits at both individual and national levels (for example, bilingual individuals may earn about $\\$3{,}000 more per year, and a multilingual economy can augment GDP by roughly $10\\%$).\n\nIV. In computing, multilingualisation lies on a continuum: localized systems are adapted for a single locale, multilingualised systems support multiple language content but often a single UI language, and internationalized systems enable co-existence of multiple languages and selectable interface language at runtime.\n\nWhich single answer best reflects the article's presentation of the evidence and descriptions?",
      "options": {
        "A": "All four claims (I, II, III and IV) are accurate summaries of the article.",
        "B": "Claims II, III and IV are supported by the article, while claim I is not robustly supported (evidence for a bilingual executive-function advantage is mixed and not reliably replicated).",
        "C": "Only claims I and II are supported; the article rejects measurable economic benefits and treats computing as unrelated to multilingualism.",
        "D": "Only claim II is supported; claims I, III and IV are contradicted by the article."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a multi-statement diagnostic item where students must evaluate empirical and descriptive claims about cognitive effects, economic/social benefits, developmental acquisition (simultaneous bilingualism), and computing internationalization/localization continuum; distractors reflect plausible misreadings.",
      "concepts_tested": [
        "Cognitive evidence regarding multilingualism and language learning/executive function",
        "Economic and social impacts of multilingualism (trade, GDP, earnings)",
        "Developmental acquisition (L1, simultaneous bilingualism) and computing continuum (localization ↔ multilingualisation ↔ internationalization)"
      ],
      "source_article": "Multilingualism",
      "x": 1.2346574068069458,
      "y": 1.0786006450653076,
      "level": 2,
      "original_question_hash": "e61bca1f"
    },
    {
      "question": "Country Z experiences an unanticipated $10\\%$ increase in the aggregate price level $P$ while nominal wages are fixed by year-long contracts. At the time of the shock many firms run substantially underused capital (idle machines) but some industries already operate near capacity. Which of the following best describes the short-run and long-run effects on real output $Y$ and explains why the aggregate supply curve has flat and steep regions?",
      "options": {
        "A": "Short run: with nominal wages fixed, the $10\\%$ rise in $P$ reduces real wages, firms hire more labour and increase $Y$ (SRAS slopes upward). Because many firms have underused fixed capital, initial increases in demand occur on a relatively flat portion of SRAS; as utilisation rises and bottlenecks appear in capacity-constrained sectors, SRAS becomes much steeper. Long run: nominal wages and other input prices adjust, so output returns to potential determined by capital, labour and technology (LRAS vertical).",
        "B": "Short run: nominal wages immediately rise proportionally to the price level (indexation), so real wages are unchanged and $Y$ does not change; the aggregate supply curve is vertical in both the short and long run. Long run: higher prices permanently raise output because firms always expand capacity when prices rise.",
        "C": "Short run: firms increase output only because producers misperceive the relative price of their product (a misperception effect), not because real wages fall; therefore SRAS is upward sloping solely due to information problems. Long run: the AS curve becomes horizontal because long-run adjustments eliminate capacity constraints.",
        "D": "Short run: because many firms have idle capital, any increase in $P$ is absorbed without substantial change in $Y$, so SRAS is horizontal at all output levels; as demand grows further, firms simply invest immediately and expand capacity so the AS curve never becomes steep. Long run: higher $P$ permanently increases potential output (LRAS shifts right)."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete shock scenario ($10\\%$ price rise, fixed nominal wages, underused capital) to assess understanding of sticky wages causing SRAS slope, capacity bottlenecks producing flat and steep regions, and LRAS verticality determined by factors of production.",
      "concepts_tested": [
        "Sticky nominal wages and short-run aggregate supply slope",
        "Long-run aggregate supply verticality determined by capital, labour, technology",
        "Capacity utilisation and bottlenecks producing flat and steep regions of AS"
      ],
      "source_article": "Aggregate supply",
      "x": 1.2713818550109863,
      "y": 0.9364166259765625,
      "level": 2,
      "original_question_hash": "9019ed6a"
    },
    {
      "question": "A laboratory models a coin as fair, so the theoretical probability of heads is $P(\\text{head})=1/2$ derived by counting symmetric outcomes. The coin is tossed 100 times and 70 heads are observed (empirical frequency $70/100=0.7$). Which one of the following best captures (i) why probability values lie between 0 and 1 and how counting outcomes yields a theoretical probability in simple experiments, (ii) the relation between the theoretical $1/2$ and the observed $0.7$ and how statistical inference connects them, and (iii) why axiomatic formalisms and interpretations (frequentist vs Bayesian) matter for deciding whether the coin is fair?",
      "options": {
        "A": "Correct: Probabilities are measures constrained to the interval $[0,1]$ (so $0\\le P(A)\\le1$ and $P(\\Omega)=1$) and in simple symmetric models are computed by counting favourable outcomes over total outcomes (giving $1/2$ for a fair coin). An empirical frequency $0.7$ is a sample estimate that can deviate from the ideal value because of sampling variability; statistical inference (e.g. a frequentist hypothesis test or a Bayesian update using Bayes' rule) is required to assess whether the deviation is plausibly due to chance (for $n=100$, $70$ heads is highly unlikely under $p=1/2$, two‑sided $p\\approx6\\times10^{-5}$). Axioms (additivity, normalization) guarantee coherent manipulations of probabilities, while interpretations determine whether one treats $1/2$ as a long‑run frequency to be tested or as a prior belief to be updated.",
        "B": "Since probability is fundamentally the long‑run frequency, the observed $0.7$ immediately replaces the theoretical $1/2$ as the 'true' probability; axioms are irrelevant for finite samples and no further statistical inference is needed — the empirical frequency is the probability.",
        "C": "Axiomatic probability requires model probabilities (here $1/2$) to equal observed frequencies; observing $70$ heads in $100$ tosses contradicts the axioms, so the experiment must be flawed and no inference or Bayesian update is appropriate.",
        "D": "Probability is purely subjective, so after observing $70$ heads one must set $P(\\text{head})=0.7$ and discard the counting argument; the normalization and additivity axioms are merely optional bookkeeping and do not constrain updating or hypothesis testing."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete fair‑coin vs observed‑data scenario (100 tosses, 70 heads) to probe understanding of probability bounds, counting-derived theoretical probability, sampling variability and inference (frequentist p‑value and Bayesian updating), and the role of axioms and interpretations.",
      "concepts_tested": [
        "Probability as a measure in [0,1] and counting outcomes for theoretical probability",
        "Difference between theoretical probability and empirical frequency and the role of statistical inference",
        "Axiomatic foundations and interpretations (frequentist vs Bayesian) in reasoning about uncertainty"
      ],
      "source_article": "Probability",
      "x": 1.611629605293274,
      "y": 1.160981297492981,
      "level": 2,
      "original_question_hash": "9674ac1a"
    },
    {
      "question": "Rivertown has 1,000 registered voters and must decide whether to enact a $1{,}000{,}000 annual subsidy to sugar producers. Five sugar producers would each receive $200{,}000 if the subsidy passes; the cost would be spread evenly across all voters (so the per-voter cost is $\\frac{1{,}000{,}000}{1{,}000} = $1{,}000). The mayor (a politician seeking re-election) can either support the subsidy or oppose it; the municipal bureaucracy administers the subsidy and can expand its staff/budget in doing so. City reformers propose replacing the subsidy with a market-like auction requiring producers to buy tradable sugar permits that would raise revenue returned to all taxpayers. According to public choice theory (as described in the article), which of the following outcomes is the most likely prediction, and why?",
      "options": {
        "A": "Without the auction the subsidy is likely to be adopted because the concentrated benefit to the 5 producers ($200{,}000 each) creates strong incentives to lobby and support a re-election-minded mayor and a bureaucracy that can expand its budget; implementing the auction internalizes the cost (producers must pay), reduces rent-seeking, and thus makes the subsidy less likely — a hypothesis that can be empirically tested by measuring lobbying/contribution intensity and policy outcomes.",
        "B": "Because a majority (990 voters) are net losers (each loses $1{,}000), the subsidy will be defeated under normal democratic voting; market-style auctions are irrelevant since collective majorities always block policies that produce net per-capita losses.",
        "C": "Bureaucrats will oppose the subsidy because administering it reduces their discretionary budget and autonomy, so the policy will fail unless the mayor forces it; introducing an auction will increase bureaucracy resistance and therefore make adoption less likely.",
        "D": "Introducing an auction will worsen rent-seeking: producers will spend more on lobbying to shape auction rules and on buying permits, so the auction will increase wasteful expenditure and make passage of the policy more certain rather than less."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete municipal subsidy scenario with numeric per-voter cost and concentrated beneficiaries to test (1) collective outcomes from aggregated individual incentives, (2) application of a market mechanism (auction/permits) to political allocation, and (3) the role of self-interested politicians/bureaucrats and empirical testability via lobbying/contribution measures.",
      "concepts_tested": [
        "Collective outcomes arise from combined individual incentives (concentrated benefits vs diffuse costs)",
        "Use of market-like mechanisms (auctions/permits) in political decision-making to alter incentives and reduce rent-seeking",
        "Self-interested behavior of voters, politicians, and bureaucrats and the empirical testability of public choice hypotheses"
      ],
      "source_article": "Public choice",
      "x": 1.2319661378860474,
      "y": 0.9396737217903137,
      "level": 2,
      "original_question_hash": "98cfc04c"
    },
    {
      "question": "A mid-sized city introduces a costly, complex taxi-licensing regime that requires drivers to hold transferable permits. In the first year after implementation, many informal drivers continue operating illegally; over the next three years drivers take varied actions: some pay for permits when they can, some form cooperative sharing arrangements to rotate a limited number of permits, some learn to use ride-hailing apps, and some organize to lobby the city council for permit reforms. Observed outcomes include higher average fares during enforcement crackdowns, a new cooperative that stabilizes incomes for its members, and a revised permit-transfer rule after council negotiation. Which interpretation best exemplifies the core claims of institutional economics?",
      "options": {
        "A": "The market quickly returned to a neoclassical equilibrium: drivers are rational profit-maximizers who individually calculate permit costs versus expected profits, and the observable changes in fares reflect only price adjustments to clear the market; institutions (permits, cooperatives) are incidental.",
        "B": "The permit law, enforcement, cooperatives, and lobbying are institutions that shape drivers' incentives and market outcomes; drivers exhibit bounded rationality and adaptive learning (experimenting with sharing, apps, lobbying), and purposive collective action and legal negotiation produced institutional change (revised permit-transfer rule) — this process generated the observed dynamic equilibrium.",
        "C": "Technology (ride-hailing apps) alone is the central causal force: apps instantly displaced informal drivers and restored efficient service provision; legal permits were an irrelevant barrier whose effects dissipated without changing institutions or social organization.",
        "D": "Drivers' behavior was fixed by culture and habit; no learning or purposive change occurred — the permit regime simply sorted drivers by pre-existing traits so observed outcomes are driven entirely by historical habituation rather than ongoing institutional interaction or adaptation."
      },
      "correct_answer": "B",
      "generation_notes": "Designed a concrete municipal permit scenario to test how institutions (laws, cooperatives, enforcement, norms) shape market outcomes, highlight bounded rationality and learning (experimentation, apps), and show institutional evolution via purposive collective action and legal change.",
      "concepts_tested": [
        "Institutions as primary shapers of economic behavior and market outcomes",
        "Bounded rationality and adaptive learning in economic agents",
        "Evolution and purposive change of institutions through habituation, collective action, and legal foundations"
      ],
      "source_article": "Institutional economics",
      "x": 1.1209981441497803,
      "y": 0.9325816035270691,
      "level": 2,
      "original_question_hash": "9a28763f"
    },
    {
      "question": "Three island populations of a beetle species are exposed to the same new pesticide. Each beetle carries either allele R (confers resistance) or r (susceptible). Before pesticide application, all three islands have R frequency $p=0.05$. After 8 generations the observed R frequencies are: Island A $p_A=0.60$ with effective population size $N_{e}=10{,}000$ and no migration; Island B $p_B=0.02$ with $N_{e}=50$ and no migration; Island C $p_C=0.04$ with $N_{e}=10{,}000$ but receiving 10% migrants per generation from a mainland population where $p_{main}=0.01$. Assume mutation rates are negligible and R is heritable in a Mendelian fashion. Which explanation best accounts for the observed allele-frequency trajectories in A, B and C?",
      "options": {
        "A": "Island A: strong positive natural selection for R caused the rapid increase in frequency; Island B: random genetic drift in the small effective population caused stochastic loss of R despite selection; Island C: gene flow from the mainland continually introduced r alleles, counteracting selection and keeping R near its initial value.",
        "B": "Island A: genetic drift in the large population randomly drove R to high frequency; Island B: a high mutation rate converted many r to R then selection removed them; Island C: stabilising selection maintained R at low frequency despite migration.",
        "C": "Island A: positive epistasis between R and other loci caused accelerated spread independent of selection; Island B: assortative mating caused loss of R in the small population; Island C: high back-mutation from R to r reduced R frequency despite selection.",
        "D": "Island A: migration from an unsampled population introduced many R alleles; Island B: strong directional selection against R led to its decline; Island C: genetic drift in the large population randomly lowered R frequency to 0.04."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a multi-population scenario with concrete allele frequencies and effective population sizes to test understanding of selection, drift, gene flow, and Mendelian heredity; distractors invoke common misconceptions (mutation, drift in large N, assortative mating).",
      "concepts_tested": [
        "Natural selection as differential survival/reproduction changing allele frequencies",
        "Effects of genetic drift and gene flow on allele frequency dynamics",
        "Heredity and heritable genetic variation (Mendelian transmission enabling evolution)"
      ],
      "source_article": "Evolution",
      "x": 1.8336888551712036,
      "y": 1.1172412633895874,
      "level": 2,
      "original_question_hash": "b5249119"
    },
    {
      "question": "State A conducts a limited cross-border military operation into State B's northeastern province to halt an ongoing campaign of ethnic cleansing, then issues a declaration annexing that province. The UN Security Council remains deadlocked and issues no authorization. Separately, in State B's southwestern province a separatist government issues a unilateral declaration of independence without armed conflict. Which of the following best describes the applicable principles of international law and their contested normative status?",
      "options": {
        "A": "The forcible annexation by State A would violate the territorial integrity principle and Article 2(4) of the UN Charter regardless of its humanitarian rationale; the Responsibility to Protect (R2P) doctrine does not legally authorize unilateral border changes in the absence of Security Council authorization; by contrast, a declaration of independence by the separatist government is not, in itself, a violation of territorial integrity under current international law (per the ICJ advisory opinion). Scholars nonetheless debate the scope and strength of the territorial integrity norm since 1945.",
        "B": "Because State A intervened to stop ethnic cleansing, its annexation is permissible under R2P and customary international law; declarations of independence are always illegal because they necessarily breach territorial integrity; therefore international law favors humanitarian intervention over the prohibition on forcible border changes.",
        "C": "State A's annexation would be lawful as an instance of remedial secession justified by human-rights abuses, and R2P implicitly allows occupying states to alter borders to protect populations; the separatist declaration in the southwest is ipso facto a breach of territorial integrity and must be suppressed.",
        "D": "Territorial integrity applies only to large-scale conquests; small, tactical annexations like State A's are tolerated as long as they address humanitarian emergencies; the ICJ has ruled that all unilateral declarations of independence violate territorial integrity, so the separatist government acted unlawfully."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete two-part scenario (humanitarian intervention with annexation; declaration of independence) to test distinctions: Article 2(4)/territorial integrity prohibition on forcible border changes, limits of R2P without Security Council authorization, ICJ advisory position that declarations of independence are not per se violations, and scholarly debate about the norm's strength.",
      "concepts_tested": [
        "Territorial integrity and Article 2(4) of the UN Charter",
        "Tension between humanitarian intervention/R2P and prohibition on forcible border changes",
        "ICJ advisory opinion on declarations of independence and scholarly debates on the norm's historical development and strength"
      ],
      "source_article": "Territorial integrity",
      "x": 0.6291419863700867,
      "y": 0.4910074770450592,
      "level": 2,
      "original_question_hash": "f69de17d"
    },
    {
      "question": "Consider a federation in which the central government collects $65\\%$ of total tax revenue but the states are assigned $60\\%$ of total public expenditure. Some states are much poorer than others. The central government wants (1) to guarantee a national minimum standard for primary education across all states, and (2) to reduce inter‑state fiscal disparities. It plans two instruments: a matching grant that covers $50\\%$ of any additional state education spending (up to a cap), and an unconditional equalization block grant allocated to poorer states based on need. Which of the following best describes the vertical fiscal situation and the appropriate use and likely effects of these transfers?",
      "options": {
        "A": "This configuration exemplifies vertical fiscal asymmetry (VFA) rather than a prescriptive ‘imbalance’; to enforce national education standards the conditional matching grant is appropriate because it ties central funds to state education spending and lowers the effective price of that local public good (creating income and substitution effects) — although matching grants are distortionary compared with lump‑sum block grants. The unconditional equalization block grant is the right instrument to address horizontal fiscal disparities between rich and poor states.",
        "B": "This is a vertical fiscal imbalance (VFI) that should be cured by giving unconditional equalization grants to all states to enforce national standards — because unconditional grants impose no distortions and therefore ensure states comply with federal objectives without tying funds to specific spending.",
        "C": "There is no vertical fiscal asymmetry here because the centre’s larger revenue share automatically balances the expenditure responsibilities; a conditional block grant will have no effect on state spending unless it equals or exceeds states’ desired education outlays, so matching grants are redundant. Equalization grants should always be conditional to ensure funds are spent on education.",
        "D": "The situation requires centralization of education to eliminate VFA; matching grants are the preferred instrument to reduce horizontal disparities because they require states to spend more, which automatically narrows inter‑state gaps. Unconditional equalization grants are ill‑suited because they create moral hazard and do not enforce spending behavior."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete federation scenario with revenue/expenditure percentages and two transfer instruments; options contrast VFA/VFI terminology and the roles/effects of conditional matching grants versus unconditional equalization block grants, testing theory from the article.",
      "concepts_tested": [
        "Vertical fiscal relations: vertical fiscal asymmetry (VFA) vs vertical fiscal imbalance (VFI)",
        "Intergovernmental transfers: conditional (matching) vs unconditional (equalization block) grants and their incentives/effects",
        "Purpose of fiscal federalism: enforcing national standards and addressing horizontal fiscal disparities"
      ],
      "source_article": "Fiscal federalism",
      "x": 1.2716137170791626,
      "y": 0.9065869450569153,
      "level": 2,
      "original_question_hash": "7d16391b"
    },
    {
      "question": "GalvoTech, a mid-size wearable-electronics firm, has 12 months to introduce a new product. Two parallel opportunities arise: (1) R&D has a completed flexible-display prototype (a pushed technology) and (2) customer support logs persistent complaints about insufficient battery life (a pulled need). According to innovation management principles (cross-level collaboration, the iterative integration cycle search → select → implement → capture, and time-to-market pressures), which program plan best balances speed and quality for both opportunities?",
      "options": {
        "A": "Use a unified, multi-functional development program that treats push and pull with tailored activities along the search→select→implement→capture cycle. For the pushed display: search with technology scouting and internal ideation to map applications; select via portfolio-management criteria and a phase–gate decision with engineering, manufacturing and marketing; implement using concurrent engineering, rapid prototyping, supplier co-development, and rigorous regulatory/testing steps to avoid late rework; capture with PLM, IP strategy and coordinated marketing launch. For the pulled battery need: search via customer ethnography and design-thinking workshops; select with fast concept testing and inclusion in the same phase–gate review; implement with iterative prototypes and pilot production runs; capture by collecting post-launch usage data to feed the next search phase. This plan emphasizes cross-level tools and parallelism to shorten time-to-market without sacrificing quality.",
        "B": "Prioritise the pushed flexible display and fast-track it through a purely R&D-led pipeline: search is internal technology assessment; select by R&D push to production; implement with a single engineering team attempting to transfer the prototype directly to mass production to meet 12 months; capture by immediate market release. Defer customer battery work to a later cycle. Rely on TRIZ sessions and limited marketing input to accelerate decisions.",
        "C": "Treat both opportunities as market-driven (pull). Begin with broad external idea-sourcing and open contests to customers for battery and display preferences (search), then immediately prototype the winning ideas and release beta products within 6–9 months to capture market share. Skip formal phase–gate reviews and minimize testing to meet aggressive timing; use rapid online feedback to correct quality problems after launch.",
        "D": "Manage both projects separately using lightweight tools: for display, run brainstorming and ideation then hand off to manufacturing; for battery, conduct a short marketing survey then task a contractor to redesign cells. Use a single flat review meeting at month 6 to decide which to continue. Emphasize speed over structured selection or integrated testing, and use product-lifecycle management only after launch."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete firm scenario requiring mapping of innovation tools to the iterative search→select→implement→capture cycle, distinguishing push vs pull approaches and showing how cross-functional teams and concurrent engineering reduce time-to-market while preserving quality.",
      "concepts_tested": [
        "Tools enabling cross-level collaboration and common understanding (multi-functional teams, PLM, phase–gate, prototyping)",
        "Iterative evolutionary cycle: search → select → implement → capture and mapping of tools to stages",
        "Push vs pull dynamics and strategies to balance reduced time-to-market with quality (concurrent engineering, supplier co-development, testing)"
      ],
      "source_article": "Innovation management",
      "x": 1.331786036491394,
      "y": 0.9895422458648682,
      "level": 2,
      "original_question_hash": "0205048c"
    },
    {
      "question": "Country X invests in container terminals and logistics, cutting average freight costs by $30\\%$ and halving delivery times; it also deploys nationwide broadband, reducing information costs for exporters. Within two years it accedes to the WTO, cutting its MFN tariffs from $15\\%$ to $5\\%$ and signaling gradual capital‑account openness. Based on the mechanisms of economic globalization, which of the following outcome bundles is most likely over the next decade?",
      "options": {
        "A": "Significant increase in cross‑border trade volumes—especially of intermediate goods—greater vertical specialization (fragmentation of production), a rise in foreign direct investment into export‑oriented manufacturing and services, expanded cross‑border capital flows and deeper economic interdependence (and associated supply‑chain exposure).",
        "B": "A decline in international trade because lower freight and information costs make domestic production cheaper, leading firms to vertically integrate domestically; WTO accession will primarily cause capital to exit the country due to stricter rules. ",
        "C": "Immediate equalization of wages across sectors and regions within Country X, because tariff cuts and better technology remove all comparative advantages and distribute gains uniformly; FDI will be negligible because investors avoid newly open markets. ",
        "D": "A short‑term spike in exports followed by rapid deindustrialization as cheaper imported final goods overwhelm domestic producers; improved telecoms will have little effect on trade in intermediate goods or services. "
      },
      "correct_answer": "A",
      "generation_notes": "Created a policy‑scenario question linking reduced transport/telecom costs and WTO accession to predicted increases in trade, fragmentation of production, FDI, capital flows, and interdependence; distractors include common misconceptions (import substitution, wage equalization, FDI avoidance).",
      "concepts_tested": [
        "Transport and communication cost reductions enabling trade and fragmentation of production",
        "Role of GATT/WTO in lowering trade barriers and signalling commitment to open trade",
        "How FDI and cross‑border capital flows integrate economies into global production networks"
      ],
      "source_article": "Economic globalization",
      "x": 1.2434381246566772,
      "y": 0.9006946086883545,
      "level": 2,
      "original_question_hash": "92afc683"
    },
    {
      "question": "Consider the gas-phase reversible dimerization 2 NO2 ⇌ N2O4. At a fixed temperature the forward and reverse rate constants are measured to be $k_{+}=0.10\\ \\mathrm{M^{-1}s^{-1}}$ and $k_{-}=0.05\\ \\mathrm{s^{-1}}$, respectively. A sample at equilibrium has a measured nitrogen dioxide concentration $[\\mathrm{NO_2}]=0.020\\ \\mathrm{M}$. Using the law of mass action, which of the following statements is correct about the equilibrium composition and the microscopic processes occurring in the sample?",
      "options": {
        "A": "The equilibrium constant $K_{c}=k_{+}/k_{-}=2$, so $[\\mathrm{N_2O_4}] = K_{c}[\\mathrm{NO_2}]^{2}=2(0.020)^{2}=8.0\\times10^{-4}\\ \\mathrm{M}$. Forward and reverse reaction rates are equal and nonzero, so molecules continue to interconvert (dynamic equilibrium) while macroscopic concentrations remain constant.",
        "B": "$[\\mathrm{N_2O_4}]=8.0\\times10^{-4}\\ \\mathrm{M}$, but because the forward and reverse rates are nonzero the concentrations must still be changing; the system is not at equilibrium.",
        "C": "$K_{c}=2$ and therefore $[\\mathrm{N_2O_4}]=4.0\\times10^{-3}\\ \\mathrm{M}$; at this composition the forward and backward rates are equal.",
        "D": "$[\\mathrm{N_2O_4}]=8.0\\times10^{-4}\\ \\mathrm{M}$ and forward and reverse rates are equal; equality of rates implies both microscopic reactions have stopped (no molecular interconversion)."
      },
      "correct_answer": "A",
      "generation_notes": "Used a concrete reversible dimerization (2 NO2 ⇌ N2O4), applied the law of mass action to compute Kc = k+/k- and solved for product concentration; distractors target common misconceptions about nonzero rates at equilibrium and arithmetic errors.",
      "concepts_tested": [
        "Equality of forward and reverse reaction rates at equilibrium (dynamic equilibrium)",
        "Application of the law of mass action to relate rate constants and concentrations to compute equilibrium composition",
        "Distinction between microscopic activity (ongoing molecular conversion) and macroscopic constancy of concentrations"
      ],
      "source_article": "Chemical equilibrium",
      "x": 1.8530352115631104,
      "y": 1.0502291917800903,
      "level": 2,
      "original_question_hash": "c5f81af2"
    },
    {
      "question": "Methane combusts according to the balanced equation CH4 + 2 O2 → CO2 + 2 H2O (liquid). You place 1.50 g of CH4 (g) and 3.36 L of O2 (g) in a closed vessel at 298 K and 1.00 atm and allow the reaction to go to completion. Using R = 0.08206 L·atm·K−1·mol−1, and molar masses CH4 = 16.04 g·mol−1 and CO2 = 44.01 g·mol−1, determine (i) the limiting reagent, (ii) the mass of CO2 produced (g), and (iii) the volume of CO2 (L) at the same T and P. Show your reasoning by computing moles where appropriate.",
      "options": {
        "A": "O2 is limiting; mass CO2 = 3.02 g; volume CO2 = 1.68 L",
        "B": "CH4 is limiting; mass CO2 = 4.12 g; volume CO2 = 2.29 L",
        "C": "O2 is limiting; mass CO2 = 1.51 g; volume CO2 = 0.84 L",
        "D": "O2 is limiting; mass CO2 = 3.02 g; volume CO2 = 1.21 L"
      },
      "correct_answer": "A",
      "generation_notes": "Compute moles: n(CH4)=1.50/16.04, n(O2)=PV/RT with given P,V,T. Use stoichiometric coefficients (1:2:1) to find limiting reagent (O2), then n(CO2)=n(O2)/2, convert to mass via molar mass and to volume via ideal gas law.",
      "concepts_tested": [
        "Conservation of mass and integer molar ratios in balanced equations",
        "Stoichiometric coefficients as mole-to-mole conversion factors and mass conversions",
        "Application of the ideal gas law to relate gas moles and volumes in stoichiometric calculations"
      ],
      "source_article": "Stoichiometry",
      "x": 1.8618733882904053,
      "y": 1.025783896446228,
      "level": 2,
      "original_question_hash": "8789fcb7"
    },
    {
      "question": "A 10,000 ha mixed temperate forest originally experienced low-intensity surface fires at a frequency of about $f_0=1/25\\ \\text{yr}^{-1}$ (one fire every ~25 years), which maintained a dominance of a shade-intolerant pine (species P) and allowed periodic succession to early- and mid-successional species. Over the last 30 years human activities converted large patches to pasture, instituted frequent logging and mowing, and introduced a non-native, fast-reproducing grass (species W). The new disturbance regime in managed patches is approximately $f_1=1/3\\ \\text{yr}^{-1}$ (one severe soil-disturbing event every ~3 years) with higher intensity and timing that repeatedly disrupts seedling establishment. Based on principles of disturbance ecology, which of the following outcomes is most likely over the next 50 years for the managed patches?",
      "options": {
        "A": "Managed patches will become a novel, disturbance-dominated community where species W and other disturbance-adapted ‘‘weeds’’ dominate; overall species richness declines relative to the original forest, succession is repeatedly arrested at early stages, and evolutionary pressures favor traits like rapid reproduction and persistent seed banks.",
        "B": "Shade-tolerant, long-lived climax species (species C) will outcompete the pines and replace them across managed patches because increased disturbance reduces competition and accelerates succession toward late-successional dominance.",
        "C": "The original pine-dominated community will re-establish because periodic fires are cyclic and the system will return to its pre-disturbance successional trajectory despite anthropogenic activities.",
        "D": "Biodiversity in managed patches will increase due to the intermediate-disturbance effect, producing a more heterogeneous community with higher species richness than either the original forest or intensely disturbed sites."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete landscape scenario changing disturbance frequency/intensity and introducing non-native species to test effects on population sizes, succession, and anthropogenic selection; correct choice predicts dominance by disturbance-adapted species and novel ecosystem development.",
      "concepts_tested": [
        "Disturbance-driven shifts in population sizes, species richness, and successional trajectories",
        "Role of disturbance regime characteristics (frequency, intensity, timing) in determining ecological outcomes",
        "Anthropogenic disturbance creating novel ecosystems and selecting for disturbance-adapted species (weeds)"
      ],
      "source_article": "Disturbance (ecology)",
      "x": 1.6945489645004272,
      "y": 0.9591797590255737,
      "level": 2,
      "original_question_hash": "77a7324c"
    },
    {
      "question": "Two collaborators, Dr. X and Dr. Y, decide whether to Contribute (C) effort to a joint paper or Free-ride (F). The reward–cost payoff matrix (X payoff, Y payoff) is: if both C: $(8,8)$; X C, Y F: $(2,10)$; X F, Y C: $(10,2)$; both F: $(3,3)$. According to social exchange theory (cost–benefit evaluation, interdependence/power, and exchange structures), which of the following is the best prediction about the dyadic interaction?",
      "options": {
        "A": "The outcomes are noncorrespondent (each prefers to free-ride while the other contributes); each actor can unilaterally improve their own outcome by switching to F (behavior control); because the payoff structure is symmetric, neither has greater power; thus the dyadic exchange is unstable absent mechanisms (e.g., reciprocity norms or third‑party generalized exchange) to deter defection.",
        "B": "The outcomes are correspondent (both share the same ranking of joint outcomes), so mutual contribution (C,C) will be stable; Dr. Y has fate control over X because Y attains a higher maximum payoff $(10)$ when X contributes, giving Y decisional dominance.",
        "C": "Both actors are better off not engaging (both F yields $(3,3)$) so the relationship will terminate quickly; Dr. X holds more power because the highest individual payoff $(10)$ occurs when X free‑rides and Y contributes, indicating asymmetric dependence in X's favor.",
        "D": "Because the mutual‑cooperate payoff $(8,8)$ exceeds unilateral defection payoffs for the defector, actors will rationally avoid free‑riding; the matrix therefore predicts strong, stable dyadic reciprocity without need for external enforcement or generalized exchange."
      },
      "correct_answer": "A",
      "generation_notes": "Created a symmetric 2x2 payoff matrix to probe students' understanding of correspondence vs noncorrespondence, unilateral behavior control, power/dependence symmetry, and how dyadic vs generalized exchange and reciprocity affect stability under social exchange theory.",
      "concepts_tested": [
        "Cost–benefit evaluation and relationship stability",
        "Interdependence and power dynamics (behavior control, correspondence vs noncorrespondence)",
        "Exchange structures (dyadic reciprocity vs generalized exchange) and their influence on predicted interaction patterns"
      ],
      "source_article": "Social exchange theory",
      "x": 1.2603685855865479,
      "y": 1.0199640989303589,
      "level": 2,
      "original_question_hash": "1fe1e51d"
    },
    {
      "question": "A 62-year-old patient presents with sudden right middle cerebral artery (MCA) infarction. Neuroimaging shows focal neuronal necrosis in the right precentral gyrus, and clinically the patient has left-sided hemiparesis. Which statement best characterizes the pathophysiology of this presentation and aligns with the methodological foundations of pathophysiology described historically (reductionism, cell-focused study, clinical observation and experimentation)?",
      "options": {
        "A": "Pathophysiology is limited to the anatomic description of the infarct (the pathology); therefore the appropriate study approach is large-scale clinical case series and descriptive autopsy reports without experimental manipulation.",
        "B": "Pathophysiology explains how the pathological lesion (neuronal necrosis) produces altered function (reduced corticospinal output producing hemiparesis). It frames the relation between pathology and physiology (e.g., motor output $M$ scales with the number of functional motor neurons $N$, $M \\propto N$) and is best investigated by reductionist, cell-focused experimental methods (neuronal cell culture, intravital or animal models) combined with careful clinical observation and controlled experiments.",
        "C": "Pathophysiology studies only normal physiological processes and so should be investigated exclusively with healthy volunteer physiological experiments rather than using disease models or pathological specimens.",
        "D": "Pathophysiology primarily aims to identify a single external causal agent (as in classical germ theory), so the most appropriate method is isolation and culture of a microbe from the infarcted tissue."
      },
      "correct_answer": "B",
      "generation_notes": "Created a clinical vignette (MCA infarct → hemiparesis) and provided four options that distinguish pathology vs physiology vs pathophysiology and align with historical methodological foundations (reductionism, cell-focused experimental work, clinical observation/experimentation).",
      "concepts_tested": [
        "Pathophysiology links pathological lesions to altered physiological function",
        "Distinction and integration of pathology, physiology, and pathophysiology",
        "Methodological foundations: reductionism, cell-focused experiments, clinical observation and experimentation"
      ],
      "source_article": "Pathophysiology",
      "x": 1.2735326290130615,
      "y": 0.9277442693710327,
      "level": 2,
      "original_question_hash": "52b29360"
    },
    {
      "question": "In the postcolonial state of Kasiya the Ministry of Education mandates that 70% of primary-school instruction be delivered in the former colonial language; new state funds build teacher-training colleges teaching only that language and subsidize television and radio broadcasting exclusively in it. Simultaneously, community-run schools for local languages lose state grants and most close; national advertising campaigns describe the colonial language as the \"language of progress\" and scholarships require proficiency in it. Which interpretation best applies to the mechanisms at work in Kasiya, using Phillipson's framework of linguistic imperialism?",
      "options": {
        "A": "This exemplifies linguistic imperialism: a structural/resource mechanism (state redistribution of funding and institutional support), an ideological mechanism (normalization and prestige rhetoric), and subtractive effects on local languages (closure of community schools and reduced media presence).",
        "B": "This is primarily an ideological campaign: the government only tries to change attitudes toward the colonial language, but since local-language schools remain autonomous and funding is not reallocated, it does not represent structural linguicism or subtractive loss.",
        "C": "This is mainly a neutral modernization policy: the state is investing in a single instructional language for efficiency and economic utility without creating inequality, so it cannot be characterized as linguistic imperialism under Phillipson's criteria.",
        "D": "This is best described as appropriation: local populations will adopt the colonial language for pragmatic reasons while retaining their native tongues, so state intervention simply reflects grassroots demand rather than hegemonic imposition."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete postcolonial policy scenario mapping directly to Phillipson's characteristics: resource/institutional bias, ideological prestige rhetoric, and subtractive impacts on local languages; options contrast structural vs ideological vs appropriation interpretations.",
      "concepts_tested": [
        "Structural/resource-based mechanisms favoring a dominant language",
        "Ideological mechanisms of prestige and normalization of a dominant language",
        "Subtractive effects and power relations leading to language loss and resistance"
      ],
      "source_article": "Linguistic imperialism",
      "x": 1.2158187627792358,
      "y": 1.073871374130249,
      "level": 2,
      "original_question_hash": "f5809dbf"
    },
    {
      "question": "GreenLeaf Ltd purchases manufacturing equipment for $100{,}000 on credit and records the transaction using double-entry bookkeeping. Which of the following best describes how that bookkeeping entry, management accounting reports, financial accounting requirements, and auditing interact to produce reliable information for different stakeholders?",
      "options": {
        "A": "The purchase is recorded by double-entry bookkeeping (Inventory or Equipment debit $100{,}000; Accounts Payable credit $100{,}000), which provides the transaction-level records used to prepare both internal management analyses and external financial statements. Management accounting may reallocate costs and produce forward-looking, non-GAAP measures (e.g., product-line contribution margins using internal overhead allocations) to support managerial decisions. External financial reporting must follow GAAP/IFRS, produce comparable historical statements, and is subject to independent audit that provides reasonable assurance about the absence of material misstatement and thereby enhances credibility for investors and creditors.",
        "B": "Because management accounting drives all reporting, its internal cost allocations (which need not be documented as debits and credits) are the basis for external financial statements; auditors simply check that management reports are reasonable, and GAAP/IFRS are optional guidelines rather than binding standards for external users.",
        "C": "Double-entry bookkeeping is only required for external financial reporting; management accounting uses separate single-entry records and cannot rely on the company’s double-entry ledger. Therefore, auditors evaluate only the double-entry books and disregard internal management reports when forming an audit opinion.",
        "D": "Auditors guarantee that financial statements are absolutely correct and fraud-free; as a result, companies can use any internal (non-GAAP) measures for external reporting because the auditor will detect and correct all differences between management measures and GAAP/IFRS-compliant statements."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete purchase-on-credit scenario to require recognition of double-entry entries and contrast internal management reports with GAAP/IFRS-based external statements and the auditor’s role; wrong options present common misconceptions about GAAP, auditing, and bookkeeping.",
      "concepts_tested": [
        "Double-entry bookkeeping and transaction recording",
        "Distinction between financial accounting (external, GAAP/IFRS) and management accounting (internal, non-GAAP, forward-looking)",
        "Role of auditing and accounting standards in ensuring reliability and credibility (reasonable assurance, material misstatement)"
      ],
      "source_article": "Accounting",
      "x": 1.3082280158996582,
      "y": 0.9188271164894104,
      "level": 2,
      "original_question_hash": "8e1b8633"
    },
    {
      "question": "A mid-sized manufacturing firm currently runs separate legacy applications for purchasing, inventory, payroll and accounts payable. They replace these with a cloud-based ERP that integrates an accounting information system (AIS) module. The new system performs automated three-way matching (purchase order, goods receipt, supplier invoice), posts inventory and payroll transactions to the same centralized ledger, and produces consolidated monthly financial reports within 48 hours. After implementation the firm reduces the number of data-entry clerks and hires two ERP support specialists. Which of the following best summarizes the combined effects of ERP integration, AIS automation, and the deployment shift described?",
      "options": {
        "A": "Integration of ERP modules enables coordinated cross-functional processes by sharing master and transactional data (so three-way matching and automatic ledger postings occur without manual interfaces); automation improves timeliness and accuracy of reports used by managers and external stakeholders; moving to a cloud-packaged ERP lowers upfront capital costs and increases scalability while reducing low-skill transactional roles and shifting workforce demand toward higher-skilled support.",
        "B": "Because the ERP integrates modules and centralizes data, the firm no longer needs internal controls or reconciliations; automation produces error-free reports that eliminate the need for managerial review; cloud deployment guarantees lower total lifetime cost, so no long-term budgeting for IT support is necessary.",
        "C": "Integration requires complex point-to-point interfaces between modules, which increases reconciliation work; automation slows reporting due to batch processes; cloud-based packaged ERP always increases costs and prevents the firm from reducing transactional staffing.",
        "D": "The AIS module centralizes data but cannot perform cross-functional processes like three-way matching; automation mainly benefits external auditors rather than managers; moving to cloud deployment primarily increases security risks and typically causes firms to hire more low-skill clerks rather than reduce them."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete ERP migration scenario requiring synthesis: how integrated modules share data enabling cross-functional automation (three-way match), how automation improves timeliness/accuracy for decision-making, and how cloud packaged ERP affects costs, scalability, and workforce composition.",
      "concepts_tested": [
        "ERP integration and shared data enabling cross-functional processes",
        "Automation improving timeliness and accuracy of financial information for decision-making",
        "Deployment shifts (packaged/ERP/cloud) affecting costs, scalability, and workforce roles"
      ],
      "source_article": "Accounting information system",
      "x": 1.406232476234436,
      "y": 1.023650884628296,
      "level": 2,
      "original_question_hash": "46f605f5"
    },
    {
      "question": "A 56-year-old man presents with acute onset fever, pleuritic chest pain and productive cough. Chest radiograph shows a lobar consolidation. Sputum culture grows alpha-hemolytic, optochin-sensitive Gram-positive cocci; laboratory studies show a neutrophil-predominant leukocytosis and elevated CRP. Which of the following best explains the pathogenesis of his illness, the expected sequence of host immune responses, and the rationale for the initial treatment?",
      "options": {
        "A": "Colonization of the alveolus by Streptococcus pneumoniae with local multiplication and release of pneumolysin that damages epithelium; an immediate innate inflammatory response with neutrophil influx and cytokine-mediated symptoms (within $<48$ h) followed by an adaptive antibody-mediated response (opsonization and complement activation over $\\u223c 7$–$14$ d) that aids clearance; initial therapy is a beta-lactam antibiotic active against S. pneumoniae.",
        "B": "Primary viral infection of alveolar epithelial cells producing cytopathic effects; the adaptive T-cell response predominates early while the innate response is minimal; therefore an antiviral (e.g., neuraminidase inhibitor) is the appropriate initial therapy.",
        "C": "This presentation represents harmless colonization with an immune hypersensitivity reaction causing radiographic consolidation; because bacteria are not multiplying, immunosuppression with corticosteroids rather than antibiotics is indicated.",
        "D": "The clinical syndrome is due entirely to a preformed exotoxin released elsewhere and transported to the lung, so antibiotics are unnecessary; immediate treatment should be antitoxin and purely supportive care because adaptive immunity has no role in clearance."
      },
      "correct_answer": "A",
      "generation_notes": "Clinical pneumonia vignette used to test (1) tissue invasion and toxin-mediated damage (pneumolysin), (2) timeline and roles of innate (neutrophils/inflammation) then adaptive (antibody/opsonization) responses, and (3) pathogen-directed therapy—antibiotic choice for bacterial pneumonia.",
      "concepts_tested": [
        "Pathogenesis of infection (invasion, replication, toxin-mediated tissue damage)",
        "Host immune response sequence (innate inflammation then adaptive antibody-mediated clearance)",
        "Pathogen-specific treatment rationale (antibiotics for bacterial infections)"
      ],
      "source_article": "Infection",
      "x": 2.174764394760132,
      "y": 1.1224033832550049,
      "level": 2,
      "original_question_hash": "803e2050"
    },
    {
      "question": "Orion Systems is a mid‑sized software supplier whose recent projects have been late, over budget, and fragile in maintenance. Management argues for a single strategic change to turn performance around. Which of the following initiatives best embodies the principles and historical lessons of software engineering — integrating engineering discipline with software development practices and using established standardization and process‑maturity mechanisms to improve reliability and maintainability?",
      "options": {
        "A": "Immediately double the development staff and mandate overtime to meet delivery dates; let senior programmers decide design and testing practices ad hoc so implementation speed increases.",
        "B": "Institute a disciplined development lifecycle: formal requirements engineering (functional and non‑functional), layered design (interface, architecture, detailed), systematic testing and maintenance practices; adopt a process‑maturity improvement program (e.g., CMMI‑DEV inspired roadmap) and align training and process documentation to SWEBOK guidance, taking lessons from the historical software crisis and SEI initiatives.",
        "C": "Outsource all coding to the lowest‑cost vendor and invest only in vendor‑specific tool certifications and proprietary frameworks; retain minimal in‑house oversight to reduce payroll.",
        "D": "Rebrand development as creative programming: remove formal processes, allow individual programmers complete freedom to refactor and redesign continuously, and avoid benchmarking against external standards to foster innovation."
      },
      "correct_answer": "B",
      "generation_notes": "Created a concrete company scenario requiring selection of an approach that integrates engineering practices (requirements, design, testing, maintenance) with process‑maturity standards (CMMI‑DEV, SWEBOK) and references historical catalysts (software crisis, SEI/NATO). Distractor options reflect common but incorrect alternatives (scale up labor, outsource, or eschew standards).",
      "concepts_tested": [
        "Integration of engineering principles with software development lifecycle (requirements, design, testing, maintenance)",
        "Use of standardization and maturity frameworks (CMMI‑DEV, SWEBOK, SEI) to guide and evaluate process improvement",
        "Historical drivers of software engineering formalization (software crisis, NATO conference, SEI) and their influence on modern practices"
      ],
      "source_article": "Software engineering",
      "x": 1.433575987815857,
      "y": 1.0582112073898315,
      "level": 2,
      "original_question_hash": "7c90f0a0"
    },
    {
      "question": "Riverton is a small industrial town where a single firm employs 2,000 workers. A union organizes and a scholar measures employer bargaining power as $\\beta=0.7$, meaning employers appropriate roughly 70% of any negotiated surplus. Which of the following choices best synthesizes (i) the employment relationship as a system of interrelated actors and institutions, (ii) the implications of $\\beta=0.7$ for bargaining outcomes and policy, and (iii) correct examples of the three faces of industrial relations (science building, problem solving, ethical)?",
      "options": {
        "A": "The employment relationship is a system of interacting actors (employer, employees, union, state) and institutions whose interactions determine outcomes; $\\beta=0.7$ implies a substantial employer advantage that will depress wages and tilt bargaining outcomes toward management, creating a case for institutional remedies such as collective bargaining frameworks, minimum wage laws, works councils, and stronger labour law; science building = rigorous empirical research measuring $\\beta$ and institutional effects, problem solving = designing grievance procedures, mediation and collective-bargaining protocols for Riverton, ethical = normative advocacy for worker protections and, where appropriate, deeper structural reforms to redress power asymmetries.",
        "B": "The employment relationship is primarily a relationship between a benevolent manager and employees, so institutional actors like unions and the state are peripheral; $\\beta=0.7$ is a technicality that does not justify intervention because competitive market forces will quickly restore fair wages; science building = developing managerial best-practices, problem solving = managers imposing productivity targets, ethical = treating employees as part of one firm ‘family’.",
        "C": "The employment relationship consists only of employer and individual employees (institutions are irrelevant); $\\beta=0.7$ indicates the union has strong leverage and therefore wages will exceed competitive levels; science building = advocacy for union militancy, problem solving = strikes to force higher pay, ethical = scholarly neutrality that avoids recommending policy (scholars should only describe, not prescribe).",
        "D": "The employment relationship is driven solely by market-clearing prices so institutional actors only distort outcomes; $\\beta=0.7$ therefore simply signals a temporary imbalance that will vanish without policy; science building = theoretical models assuming perfect competition, problem solving = relying on firm-level HR training to resolve disputes, ethical = maximizing shareholder value as the primary normative goal."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete town scenario with a measured bargaining-power parameter $\\beta=0.7$ and designed options to test (1) conception of employment relationship as interacting actors/institutions, (2) consequences of non‑perfect competition and bargaining asymmetry for wages and policy, and (3) mapping of the three faces (science building, problem solving, ethical) to concrete activities.",
      "concepts_tested": [
        "Employment relationship as system of interrelated actors and institutions",
        "Three faces of industrial relations: science building, problem solving, ethical",
        "Labor market imperfect competition and bargaining power asymmetries (implications for policy and bargaining outcomes)"
      ],
      "source_article": "Industrial relations",
      "x": 1.2441191673278809,
      "y": 0.9161280989646912,
      "level": 2,
      "original_question_hash": "216d98cf"
    },
    {
      "question": "Country Z recorded $15$ million international tourist arrivals in 2019 with an average spending of $900$ per arrival. Tourism receipts therefore represented $10\\%$ of GDP and $60\\%$ of the country's service exports that year. A pandemic in 2020 reduced international arrivals by $70\\%$. Which of the following statements correctly (1) computes the new tourism receipts and the absolute receipts loss (in USD) and loss as a percentage of GDP, and (2) proposes the most appropriate policy package that balances restoring macroeconomic stability (including balance-of-payments concerns), building resilience to external shocks, and aligning tourism development with sustainable tourism principles and the SDGs?",
      "options": {
        "A": "Correct computations: new receipts $=15\\times(1-0.70)\\times900=4.05$ billion USD; absolute loss $=13.5-4.05=9.45$ billion USD, which equals $9.45/135\\approx7\\%$ of GDP. Best policy package: time-limited income/support for affected workers and SMEs + public investment in low-carbon, nature- and community-based tourism infrastructure (eco-certification, waste/water systems), active diversification toward resilient source markets and domestic tourism, targeted retraining programs, and conditional support for firms that adopt sustainability standards (aligned with SDG 8 and SDG 12).",
        "B": "Incorrect computations: new receipts $=5.4$ billion USD and absolute loss $=8.1$ billion USD (about $6\\%$ of GDP). Recommended policy: rapid deregulation and aggressive international marketing plus removing taxes on aviation fuel to restore arrivals quickly — prioritises fast recovery over environmental or social safeguards.",
        "C": "Incorrect computations: new receipts $=3.0$ billion USD and absolute loss $=10.5$ billion USD (about $7.8\\%$ of GDP). Recommended policy: an extended ban on international arrivals until full global vaccination plus universal subsidies to all tourism firms irrespective of green or social conditions.",
        "D": "Incorrect computations: new receipts remain unchanged at $13.5$ billion USD (no loss). Recommended policy: rely solely on stimulating domestic demand through temporary travel vouchers without structural changes toward sustainability or market diversification."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a numeric scenario using arrivals, average spend, and a 70% shock; combined arithmetic on receipts and GDP share with policy choices that reflect sustainable tourism, balance-of-payments impacts, and resilience.",
      "concepts_tested": [
        "Tourism's contribution to macroeconomic indicators (receipts, balance of payments, % of GDP)",
        "Sustainable tourism policy measures and alignment with UN SDGs",
        "Vulnerability to external shocks and resilience policy responses (diversification, domestic tourism, targeted support)"
      ],
      "source_article": "Tourism",
      "x": 1.2884535789489746,
      "y": 0.9327518343925476,
      "level": 2,
      "original_question_hash": "d3ee430a"
    },
    {
      "question": "Marina Bay is a coastal municipality with a disused 19th-century shipyard, an active indigenous craft community, and fragile beach-front archaeological features. Recent market research shows a rising cohort of visitors whose primary motivation is participatory cultural experiences (e.g., craft workshops, guided heritage walks, culinary classes). The municipal council must adopt a 5-year cultural tourism strategy that: (1) aligns supply with this visitor motivation, (2) captures economic and conservation benefits while minimizing negative externalities such as rising housing costs, pollution, and social disruption for residents, and (3) leverages experiential cultural tourism to contribute to regional development. Which of the following policy packages best accomplishes these objectives?",
      "options": {
        "A": "Prioritize rapid expansion of large-scale accommodations and a waterside theme park financed by external investors; run mass-marketing campaigns to attract high volumes of recreational tourists; allocate a small percentage of tax revenues to a preservation fund but allow market rents to dictate housing and retail development.",
        "B": "Contract national tour operators to run standardized heritage bus tours and manage gift shops; use most ticket revenue for municipal tourism marketing; implement minimal environmental controls and permit new hotels on the waterfront; encourage local artisans to sell through operator-managed outlets for scale.",
        "C": "Establish a community-led creative tourism program that subsidizes local craft and culinary workshops, channels a modest % of visitor fees into a heritage conservation trust, enacts zoning and rent-stabilization measures to limit displacement, implements pollution and visitor-capacity limits on sensitive sites, and forges a regional partnership to integrate skills training and market linkages.",
        "D": "Ban all tourist activity in the shipyard and beachfront archaeological zones to prevent damage; invest exclusively in digital virtual tours curated by outside historians; prohibit any new tourism enterprises in the town to avoid social change and keep preservation absolute."
      },
      "correct_answer": "C",
      "generation_notes": "Created a realistic municipal scenario requiring evaluation of policy trade-offs; options contrast mass-market, operator-driven, community-led experiential, and protectionist strategies to test understanding of motivation→supply, positive/negative impacts, and strategic development framing.",
      "concepts_tested": [
        "Tourist motivation shaping supply and product design",
        "Positive and negative impacts of cultural tourism on host communities",
        "Policy and planning to leverage experiential cultural tourism for regional development"
      ],
      "source_article": "Cultural tourism",
      "x": 1.3497066497802734,
      "y": 0.9341771602630615,
      "level": 2,
      "original_question_hash": "4a56b9f6"
    },
    {
      "question": "Let Σ = {0,1,2,3,4,5,6,7,8,9,+,=} and let L be the set of strings over Σ defined by the rules: (i) every nonempty string that contains neither '+' nor '=' and does not start with '0' is in L, and '0' is in L; (ii) a string containing '+' but not '=' is in L iff every '+' separates two strings in L; (iii) a string containing '=' is in L iff it contains exactly one '=' and the substring on each side is in L. Which of the following statements is fully correct?",
      "options": {
        "A": "The set L is a formal language generated by a grammar over Σ and is regular (so membership w ∈ L can be decided by a finite automaton). The subset T = {w ∈ L | the arithmetic equality in w is numerically true} is also a formal language (a decision problem) decidable by a Turing machine but not generally by a finite automaton. Formal language theory studies these syntactic distinctions (not semantics) and historically arose from linguistics.",
        "B": "L is not a formal language because it mixes syntactic and semantic constraints; therefore membership w ∈ L is undecidable. The set of true equalities T is regular and can be recognized by a finite automaton.",
        "C": "L is a formal language but neither regular nor context-free; the membership problem w ∈ L requires a Turing machine with unbounded memory. The distinction between L and T is semantic only and is not relevant to complexity classes.",
        "D": "L is finite because each use of '+' or '=' restricts length, so L can be enumerated explicitly; complexity classes are about algorithms, not sets of languages, so T cannot be placed into a complexity class."
      },
      "correct_answer": "A",
      "generation_notes": "Used the concrete arithmetic-equality grammar from the article to test (1) formal grammar → formal language and well-formed strings, (2) decision problems as languages and machine-based complexity distinctions (finite automata vs Turing machines), and (3) emphasis on syntactic focus and historical linguistics origin.",
      "concepts_tested": [
        "formal grammar defines a formal language and well-formed strings",
        "decision problems as formal languages and complexity classes defined by machine recognition power",
        "formal language theory focuses on syntax and its historical link to linguistics"
      ],
      "source_article": "Formal language",
      "x": 1.4941296577453613,
      "y": 1.1621023416519165,
      "level": 2,
      "original_question_hash": "c1ddb54b"
    },
    {
      "question": "You have been commissioned to produce two printed maps of the same coastal region (approx. 50 km × 30 km) that lies between $55^\\circ\\text{N}$ and $56^\\circ\\text{N}$. Map H is for day-hikers and park managers; Map N is for coastal navigation by small craft. For each map you must choose a projection, the appropriate level of generalization (what to omit or simplify), and which cartographic elements to emphasize. Which one of the following combinations best applies the principles of purpose-driven feature selection, suitable projection trade-offs, and integrated map design?",
      "options": {
        "A": "Map H (hikers): use a local Transverse Mercator/UTM projection to minimize linear and angular distortion at the map scale, produce a large-scale map ($1:25{,}000$) with dense detail (contour lines every ~10 m, footpaths, campsites, small streams), hillshade and precise typography; omit detailed marine bathymetry and major administrative labels that clutter the hiking layout. Map N (navigation): use a Mercator projection (preserves rhumb-line bearings), medium scale ($1:50{,}000$) emphasizing bathymetry and depth contours, lighthouses, aids to navigation, compass rose and rhumb lines; generalize and simplify inland topographic detail and minor footpaths to reduce clutter.",
        "B": "Map H (hikers): use an equal-area projection (e.g., Albers) so park managers can compare areas of habitats, produce a small-scale $1:200{,}000$ map but include detailed trails and contour lines by overlapping symbols; Map N (navigation): use an equal-area projection so vessel operators can compare areas of marine habitats, include dense inland detail and municipal boundaries for context, and omit compass roses because bearings are preserved by the equal-area property.",
        "C": "Map H (hikers): use a schematic topological layout (like a transit map) to make trail connections obvious, ignore scale and precise contouring, emphasize station-like symbols for camps; Map N (navigation): use a local orthographic (globelike) projection that preserves visual appearance, include exhaustive inland detail and political labeling to satisfy chart users, and retain all small shoreline rocks and drowned features without generalization.",
        "D": "Map H (hikers): use a global compromise projection (e.g., Robinson) so the map looks familiar, print at $1:100{,}000$ with both detailed marine and inland features to serve all audiences; Map N (navigation): use the same Robinson projection and identical content so users get consistent cartography, include full decorative elements (cartouches, heraldry) and no special navigational elements like rhumb lines or depth-soundings."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete two-map commissioning scenario requiring choices about projection, generalization, and design elements; options contrast appropriate and inappropriate projection/feature decisions for the two audiences.",
      "concepts_tested": [
        "Purpose-driven selection and generalization of map features (map editing and abstraction)",
        "Map projection trade-offs for representing 3D Earth on 2D media (distortion, choice by purpose)",
        "Map design process that integrates editing, generalization, symbology, and typography to communicate to a specific audience"
      ],
      "source_article": "Cartography",
      "x": 1.5914756059646606,
      "y": 1.0584697723388672,
      "level": 2,
      "original_question_hash": "85a6ca81"
    },
    {
      "question": "You are designing a geographically distributed order-payment system (nodes in NY, Frankfurt, and Singapore). Requirements: (1) payments must be processed in a causally consistent order even when events are generated concurrently at different nodes, (2) the system must continue to operate when one or two nodes fail, and (3) the architecture must scale under bursty load. Which combination of design choices best addresses the lack of a global clock, partial node failures, and the choice of message-passing mechanism?",
      "options": {
        "A": "Use logical clocks (Lamport or vector timestamps) to provide causal ordering; replicate services with leader election and health checks plus circuit breakers to isolate failed nodes; employ asynchronous message queues (pub/sub for events, point-to-point for commands) with idempotent handlers and at-least-once delivery (deduplication) to enable scalability.",
        "B": "Synchronize physical clocks across datacenters (NTP/PTP) and use timestamps from those clocks for global ordering; handle failures by retrying the failed node until it comes back online; use synchronous HTTP request/reply for all inter-node communication to ensure immediate consistency and simplify ordering.",
        "C": "Centralize ordering and coordination in a single shared-disk database that all nodes query for sequence numbers; assume node failures are rare so no replication is necessary; use RPC-like connectors for low-latency calls directly to the coordinator to preserve strong consistency.",
        "D": "Assign a monotonically increasing counter at each node for local ordering and merge orders by comparing counters; when a node fails, pause the entire system until the node recovers to avoid split-brain; use UDP multicast to broadcast events for maximum throughput without delivery guarantees."
      },
      "correct_answer": "A",
      "generation_notes": "Provided a concrete distributed payment scenario and offered four architecture choices combining ordering, fault-tolerance, and messaging; option A correctly matches logical clocks, replication+isolation, and asynchronous message queues with idempotency.",
      "concepts_tested": [
        "lack of global clock and logical clock mechanisms",
        "partial component failures and fault-tolerant design",
        "message passing mechanisms (queues, RPC, pub/sub) and delivery/idempotency implications"
      ],
      "source_article": "Distributed computing",
      "x": 1.4551002979278564,
      "y": 1.0955464839935303,
      "level": 2,
      "original_question_hash": "fce2bc1b"
    },
    {
      "question": "Consider the following scenario. In the highland valley of Valoria, small pastoral bands historically conducted occasional, short-lived raids for livestock: these raids were ad hoc, lacked a sustained logistics base or centralized command, and caused limited casualties. A mineral discovery creates a sustained surplus in the valley; one band (Group R) uses surplus wealth to establish training depots, supply chains, and a centralized command able to conduct multi-season campaigns. The neighboring state, the Republic of North Valoria (RN), mobilizes regular forces in response. After three years of fighting, both sides deliberately attack tax collectors, markets, and transport hubs in populated towns, producing widespread civilian deaths, marketplace collapse, and mass displacement. Which statement best integrates the role of organizational capacity, economic surplus, and the concept of total war for this case?",
      "options": {
        "A": "This scenario illustrates a transition from raid-based conflict to war: Group R's development of centralized command, logistics, and capacity for sustained operations meets criteria for organized armed conflict (war). The mineral surplus plausibly incentivized escalation (raiding became profitable and fundable). The deliberate targeting of marketplaces, tax collectors, and transport infrastructure constitutes features of total war because it expands legitimate targets beyond military forces, explaining the high civilian casualties and economic collapse.",
        "B": "The discovery of surplus should reduce violence by creating trade links and mutual dependence; therefore the persistent fighting indicates the surplus was not a causal factor. Because armies remained focused on conventional military targets, the attacks on markets are tactical anomalies and do not meet the threshold for total war, so civilian casualties should be marginal relative to battlefield deaths.",
        "C": "Formation of Group R's centralized force still does not qualify the conflict as war under standard definitions because only states can wage war; conflicts involving non-state actors remain raids or insurgencies regardless of logistics. Economic surplus is incidental, and attacking marketplaces is merely counterinsurgency or criminal violence, not total war, so the classification and expected societal impact remain limited.",
        "D": "This remains asymmetric warfare because the original power imbalance persists; total war requires industrial-scale mobilization and formal declarations between states, so deliberate attacks on civilian economic nodes do not amount to total war. Civilian casualties depend primarily on weapon lethality rather than the expansion of legitimate targets, so the scenario does not demonstrate total-war dynamics."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete Valoria scenario showing progression from ad hoc raids to organized sustained operations funded by surplus, then deliberate attacks on civilian economic targets to test recognition of transition to war, economic drivers (surplus -> raiding/organization), and definition/significance of total war.",
      "concepts_tested": [
        "Organizational capacity and classification of conflict (command, logistics, ability to sustain operations)",
        "Economic surplus as a driver of escalated raiding and warfare",
        "Total war definition and its effects on civilians/infrastructure"
      ],
      "source_article": "War",
      "x": 0.9292612671852112,
      "y": 0.5996537804603577,
      "level": 2,
      "original_question_hash": "534be9a3"
    },
    {
      "question": "Consider the following scenario: the insurgent group \"Mountain Dawn\" operates in a mountainous province of the state Freedonia. Freedonia fields a modern army with air power, precision munitions, and advanced surveillance, while Mountain Dawn lacks heavy weapons and relies on small arms, tunnels, ambushes, and occasional rocket attacks. Mountain Dawn receives covert materiel and training from a neighboring rival state, enjoys partial local civilian support (information and limited shelter), and uses hit-and-run raids that avoid set-piece battles. Freedonia is a democracy with a divided legislature and a public sensitive to casualty reports. After two years, Mountain Dawn has not been destroyed and has forced Freedonia to withdraw some units and enter negotiations. Which option best explains (1) why Mountain Dawn chose to fight despite its material inferiority and (2) how it managed to impose costs and achieve partial success — using mechanisms grounded in asymmetric warfare literature and linking the case to related irregular-warfare concepts?",
      "options": {
        "A": "Mountain Dawn fought because it had external sponsorship and because Freedonia could not credibly sustain coercive threats without domestic political costs; it succeeded by exploiting difficult terrain and civilian information networks with guerrilla hit-and-run tactics, leveraging external logistics and strategic interaction that made escalation politically costly for Freedonia. This case exemplifies asymmetric warfare — an umbrella for guerrilla/insurgency/irregular methods used to offset material inferiority.",
        "B": "Mountain Dawn fought because it secretly possessed advanced weapons that made it militarily equal to Freedonia; it succeeded by winning conventional, set-piece battles in mountains where technology gave it parity, showing that material superiority is irrelevant in modern conflicts.",
        "C": "Mountain Dawn continued fighting solely because of ideological fanaticism and a higher willingness to suffer casualties, and it prevailed only because Freedonia's military competence was low; civilian support and external aid were negligible and irrelevant to the outcome.",
        "D": "Mountain Dawn was provoked into rebellion by Freedonia's extreme demands and aimed to trigger a regional war to gain support; it prevailed by escalating proportionally and matching Freedonia's conventional capabilities, so the conflict is best understood as interstate conventional warfare rather than guerrilla or insurgent action."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete insurgency scenario testing why weaker actors initiate and sustain conflict and how they can defeat stronger actors, requiring recognition of external support, credibility/domestic-political constraints, terrain and guerrilla tactics, and the umbrella relation to irregular warfare.",
      "concepts_tested": [
        "Power asymmetry driving offset strategies and unconventional methods",
        "Two core puzzles of asymmetric conflict (why weak fight; how weak can win) and mechanisms (external support, credibility, strategic interaction, political constraints)",
        "Relationship between asymmetric warfare and related forms (guerrilla warfare, insurgency, terrorism) as irregular warfare"
      ],
      "source_article": "Asymmetric warfare",
      "x": 1.2319148778915405,
      "y": 0.8319642543792725,
      "level": 2,
      "original_question_hash": "37d53533"
    },
    {
      "question": "You are part of an interdisciplinary team studying a longitudinal coauthorship dataset of $N=200$ researchers over 10 years. For each year you have an adjacency matrix and nodal attributes (institution, seniority). Your goals are: (1) build a predictive model that gives the probability a dyad will form a coauthorship tie in year $t+1$ given past years; (2) quantify the effects of triadic closure and attribute-based homophily on tie probabilities; and (3) visualize evolving community structure and identify emerging hubs. Which methodological pipeline best satisfies these goals while reflecting the interdisciplinary foundations of network science?",
      "options": {
        "A": "Represent each year as a graph (adjacency matrices) and compute network summaries (degrees, clustering). Fit a temporal Exponential Random Graph Model (ERGM/TERGM) where P(Y=y|\\theta)=\\frac{\\exp(\\theta^{T}s(y))}{c(\\theta)} and include sufficient statistics for triangles and attribute-match terms to estimate triadic closure and homophily; compare predictive performance to a network probability matrix derived from historical edge frequencies as a baseline; and apply data-mining and information-visualization methods to display communities and hubs. (Correct)",
        "B": "Treat the dataset as independent snapshots and model tie formation with an Erdős–Rényi model $G(N,p)$ in each year (estimate a yearly $p$), then use principal component analysis on node attributes for visualization. This yields yearly tie probabilities and low-dimensional plots of researchers.",
        "C": "Preserve only the observed degree sequence by fitting a configuration model for each year and use PageRank to identify hubs; infer triadic closure indirectly since degree preservation generates realistic hubs, and visualize networks with basic force-directed layouts.",
        "D": "Ignore network generative models and run separate dyadic logistic regressions for each potential pair using only nodal covariates (no network statistics), then apply hierarchical clustering on predicted pairwise affinities to identify communities and label top-degree nodes as hubs."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a realistic longitudinal coauthorship scenario requiring representation, interdisciplinary methods, and probabilistic modeling. Option A integrates graph representation, ERGM (temporal) for tie probabilities and sufficient statistics, comparison via network probability matrix, plus data-mining/visualization. Other options are plausible but fail to capture triadic/structural probabilistic modeling or interdisciplinarity.",
      "concepts_tested": [
        "networks as relational data enabling predictive models",
        "interdisciplinary integration of graph theory, statistical mechanics, data mining/visualization, and statistics",
        "probabilistic modeling of network structure (ERGMs and network probability matrices)"
      ],
      "source_article": "Network science",
      "x": 1.5986077785491943,
      "y": 1.1664894819259644,
      "level": 2,
      "original_question_hash": "9ae2ca15"
    },
    {
      "question": "Ryan tells his long-term partner Maya that he worked late with a colleague, when in fact he met his ex for dinner. He says this to avoid provoking Maya’s jealousy and to keep the relationship calm. According to Interpersonal Deception Theory and the research on motivations and relational outcomes described in the article, which interpretation best explains Ryan’s motive and the most likely short- and longer-term consequences for both partners?",
      "options": {
        "A": "Ryan’s deception is primarily relationally motivated (to manage Maya’s feelings). Interpersonal Deception Theory predicts a dynamic, mutual process in which Ryan (the sender) manipulates information while Maya (the receiver) attempts to validate it; Ryan’s cognitive load increases the chance of verbal and nonverbal leakage (speech disturbances, nonimmediacy cues), and Ryan will tend to perceive less intimacy and empathy from Maya and experience growing distress the longer the deception is maintained. If discovered, the deception is likely to produce detachment, reduced relationship satisfaction and commitment for both partners (although a successfully undetected, benevolent lie can temporarily increase the deceived partner’s positive feelings).",
        "B": "Ryan’s deception is best characterized as instrumental (avoiding punishment). Under Interpersonal Deception Theory this is a one-way, strategic tactic that produces no cognitive load or leakage; Ryan will feel relieved and the relationship’s intimacy will increase because conflict was avoided, and discovery is unlikely to change satisfaction.",
        "C": "Ryan’s motive is identity-preserving (protecting his self-image). Interpersonal Deception Theory predicts that identity lies cause immediate increases in perceived intimacy because the deceiver projects confidence; the deceiver feels no distress and the receiver typically remains unaware and unaffected even if the deception is later revealed.",
        "D": "Ryan’s behaviour is equally instrumental and identity-driven; Interpersonal Deception Theory treats deception as a static mismatch of messages (not a dynamic interaction). Consequently deception is easy to detect, leading rapidly to relationship dissolution and no intermediate changes in perceived intimacy or distress."
      },
      "correct_answer": "A",
      "generation_notes": "Created a vignette where motivation (avoid jealousy) maps to relational motive; asked for interpretation using Interpersonal Deception Theory and documented relational outcomes (cognitive load, leakage, decreased perceived intimacy, distress, discovery effects). Distractors misstate motives, IDT dynamics, and empirical outcomes.",
      "concepts_tested": [
        "Interpersonal Deception Theory dynamics",
        "Motivations for deception: instrumental vs relational vs identity",
        "Relational consequences of deception (intimacy, empathy, distress, discovery effects)"
      ],
      "source_article": "Deception",
      "x": 1.2824962139129639,
      "y": 1.0036160945892334,
      "level": 2,
      "original_question_hash": "8877b407"
    },
    {
      "question": "You prepare three thin soft-matter films at 300 K on identical substrates: (A) a melt of flexible polymer chains (chain entanglements but no covalent bonding to the substrate), (B) a dense colloidal monolayer of 100 nm particles interacting by weak van der Waals attractions, and (C) a lipid bilayer vesicle flattened into a quasi-2D film. In all three systems the characteristic interaction energies between the mesoscopic building blocks are about $5\\,k_B T$ at 300 K, and the chemical compositions differ substantially. The substrate surface energy is tunable from neutral to weakly attractive for the film constituents. Based on the principles of soft matter, which statement most correctly predicts the expected qualitative responses when the substrate is made weakly attractive and when temperature is modestly increased?",
      "options": {
        "A": "All three films will tend to adsorb/flatten onto the weakly attractive substrate without an external compressive load (i.e., become \"squashed\"); because their interaction energies are only a few $k_B T$, modest increases in thermal energy that change the balance relative to $k_B T$ will disorder mesoscopic structure and increase fluidity. Thus their macroscopic response is governed more by mesoscopic organization and thermal (entropic) effects than by detailed chemistry.",
        "B": "Only the polymer melt will flatten appreciably because polymer chains form covalent backbones that make them mechanically distinct; the colloidal and lipid films will remain essentially unchanged unless large external forces are applied. Small temperature changes much less than $k_B T$ are irrelevant for structure.",
        "C": "All three systems will preserve long-range crystalline order and resist deformation until a large external stress is applied, because mesoscopic assemblies in condensed materials are rigid and governed primarily by strong interparticle potentials rather than thermal fluctuations.",
        "D": "The lipid bilayer will alone exhibit strong adsorption and flattening because of amphiphilic chemistry; polymers and colloids will not interact significantly with the substrate. Entropy is a minor factor and thermal changes do not alter adsorbed morphology."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete three-sample scenario (polymer, colloid, lipid) with interaction energies ~5 k_BT and tunable substrate affinity to probe (1) energy scale ~k_BT and thermal sensitivity, (2) universality via mesoscopic structure over chemistry, and (3) interfacial-induced deformation (squashing). Distractors violate these principles.",
      "concepts_tested": [
        "Energy scale near k_BT and entropy-dominated behavior",
        "Universality of soft matter via mesoscopic structure rather than chemical detail",
        "Interfacial effects causing deformation/adsorption without external load"
      ],
      "source_article": "Soft matter",
      "x": 1.8124418258666992,
      "y": 1.0377041101455688,
      "level": 2,
      "original_question_hash": "ebda4410"
    },
    {
      "question": "A mid-sized U.S. city has been awarded a federal grant to renovate a downtown block. The proposed project would demolish a two-story contributing building that is listed on the local historic register and adjacent to a building already on the National Register of Historic Places. Residents cite the philosophical obligation to protect the built heritage formed over centuries. Which of the following sequences best describes the legally required regulatory steps and the coordinated roles of preservation professionals and advocates that would most likely occur before demolition can lawfully proceed?",
      "options": {
        "A": "Because federal funds are involved, the lead federal agency must initiate a Section 106 review and consult with the State Historic Preservation Officer (SHPO) and the Advisory Council on Historic Preservation (ACHP); the National Register status will be a key factor in assessing adverse effects. At the same time the developer must apply to the local historic preservation commission for a certificate of appropriateness under the city preservation ordinance. Preservation architects/contractors prepare alternatives and a rehabilitation design that follow the Secretary of the Interior's Standards (and may enable federal rehabilitation tax credits). Advocacy groups and downtown revitalization stakeholders participate in public consultation and can negotiate mitigation (adaptive reuse, documentation, partial retention) to balance development with the city's patrimonial obligation.",
        "B": "The National Register listing of the adjacent building automatically forbids any nearby demolition, so no federal review or local certificate is needed; the developer must instead seek permission only from the National Park Service, which has veto power over demolition within historic districts.",
        "C": "Local law is determinative: because the building is on the local register the developer only needs a certificate of appropriateness from the municipal commission; federal Section 106 review is irrelevant even though federal grant money is used, and preservation architects are optional consultants retained only if the owner wants tax credits.",
        "D": "The developer may obtain standard demolition permits from the city building department and proceed; after demolition the preservation community can request mitigation such as archival documentation or design modifications, and any involvement by SHPO, ACHP, or preservation architects is discretionary and primarily ceremonial."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete federal-funding downtown redevelopment scenario to test knowledge of Section 106, National Register influence, local certificates of appropriateness, Secretary of the Interior's Standards, and the roles of regulatory compliance, architects, advocacy, and revitalization programs.",
      "concepts_tested": [
        "Preservation as a civic/philosophical obligation in urban development",
        "U.S. legal/regulatory mechanisms (Section 106, National Register, local certificates of appropriateness)",
        "Interrelationship of practice areas: regulatory compliance, architecture/construction, advocacy, downtown revitalization"
      ],
      "source_article": "Historic preservation",
      "x": 1.2223414182662964,
      "y": 0.42905256152153015,
      "level": 2,
      "original_question_hash": "929885a4"
    },
    {
      "question": "Region X is a mid-sized coastal food-producing region that supplies both local markets and export. Current characteristics: 60% of arable land is used for a single staple monoculture; 20% of agricultural value derives from pollinator-dependent horticulture; pollinator populations have declined by 40% in the last decade; fertilizer runoff causes seasonal coastal eutrophication; post-harvest losses average 30% of production; and the transport network is frequently disrupted by storms. The regional government may adopt one of the following single policy packages to increase long-term resilience of its food system, reduce dependence on degrading ecosystem services, and minimize negative environmental feedbacks. Which option is most likely to achieve those objectives?",
      "options": {
        "A": "Expand subsidies and mechanization for the existing monoculture to maximize yield per hectare using synthetic fertilizers and large-scale storage facilities, accepting continued reliance on fossil-fuel-based inputs.",
        "B": "Invest in a coordinated agroecological transition: restore pollinator habitats, promote crop diversification and intercropping, upgrade cold-chain and storage to cut post-harvest loss from 30% to 15% (increasing effective supply from $P(1-0.30)$ to $P(1-0.15)$), support smallholder cooperatives, and strengthen local transport nodes.",
        "C": "Implement high eco-tariffs on imported foods to protect domestic producers and mandate higher retail prices for imports, without changing domestic production practices or infrastructure.",
        "D": "Enter public–private partnerships to build large-scale, energy-intensive vertical farms and automated processing centers in urban areas to replace a portion of regional primary production."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a realistic regional scenario synthesizing ecosystem-service dependence (pollinators, runoff), interconnected supply-chain vulnerabilities (post-harvest loss, transport), and policy choices; options reflect trade-offs from the article and the correct choice emphasizes diversification, ecosystem restoration, reduced waste, and local resilience.",
      "concepts_tested": [
        "Interconnectedness of food system components (production, storage, transport, markets)",
        "Dependence on ecosystem services and environmental context (pollinators, runoff, eutrophication)",
        "System resilience, policy interventions, and feedback loops (diversification, waste reduction, externalities)"
      ],
      "source_article": "Food system",
      "x": 1.5242167711257935,
      "y": 0.892022967338562,
      "level": 2,
      "original_question_hash": "956ad2e7"
    },
    {
      "question": "A multinational consulting firm (Firm X) historically enforced a strict appearance-based professionalism policy: visible tattoos prohibited and restrictive rules on natural hairstyles. In response to employee complaints and changing social norms, Firm X implemented an inclusive-professionalism policy that removed the tattoo ban, allowed natural hairstyles, and introduced an ethics training program emphasizing respect and equitable treatment. Over the subsequent 12 months the firm reported: hiring rate for applicants with visible tattoos in the U.S. rose from 12% to 20% (an absolute increase of $8\\%$); client-rated perceived competence in the U.S. changed from 4.2 to 4.0 (on a 5-point scale), in New Zealand remained 4.1, and in Japan increased from 3.7 to 3.9; employee-reported trust in management rose from 3.5 to 4.1. Firm researchers cautioned that the hiring increase is correlative, noting contemporaneous factors such as a local labor shortage and targeted recruitment campaigns aimed at tattooed applicants. Which of the following conclusions is best supported by these results?",
      "options": {
        "A": "The inclusive-professionalism policy directly caused the $8\\%$ increase in hires of tattooed applicants; thus removing appearance bans universally will reliably increase employment for tattooed people across all contexts.",
        "B": "The combined policy and ethics training plausibly improved internal trust and signaled a broader professionalism norm, but the observed hiring increase is correlative and likely influenced by labor-market and recruitment factors; complementary legal or organizational protections (e.g., laws like the CROWN Act) can further address hair-based discrimination.",
        "C": "Because client-rated perceived competence declined in the U.S., removing tattoo bans demonstrably harms client perceptions and the firm should reinstate appearance restrictions to protect perceived competence.",
        "D": "The divergent client responses (improvement in Japan but decline in the U.S.) show that cultural norms alone determine the effects of appearance policies, so firms need only adopt region-specific appearance rules and can omit ethics training."
      },
      "correct_answer": "B",
      "generation_notes": "Created a concrete multisite scenario with numeric outcomes to assess causal inference, interplay of ethics/presentation on trust and competence perceptions, cultural variability, and policy vs correlation; correct option emphasizes cautious interpretation of correlation and role of policy interventions.",
      "concepts_tested": [
        "Professionalism as combination of ethics and appearance affecting trust and perceived competence",
        "Cultural and organizational variability in professional norms and mechanisms of policy change (e.g., training, laws)",
        "Limits of correlational findings and the role of policy interventions (e.g., CROWN Act) in promoting equity"
      ],
      "source_article": "Professionalism",
      "x": 1.3111493587493896,
      "y": 0.9953369498252869,
      "level": 2,
      "original_question_hash": "7f1ff484"
    },
    {
      "question": "A team studies a clade of salamanders historically placed in the genus Hapturus. Their morphological and molecular phylogenetic analysis (including fossil taxa) shows that species H1, previously assigned to Hapturus, is nested within the clade of genus Neotrix, while H2–H4 form a separate monophyletic group. The researchers (i) publish the tree and argue H1 should be transferred to Neotrix to restore monophyly, (ii) describe a new species H5, designate a holotype, and provide a diagnosis, (iii) propose elevating the subgenus Hapturus to full genus rank, and (iv) publish all nomenclatural changes following the International Code of Zoological Nomenclature (ICZN). Which statement best characterizes which activities are taxonomy versus systematics/nomenclature and whether the proposed taxonomic changes align with modern phylogenetic principles?",
      "options": {
        "A": "Correct: Inferring the tree and concluding H1 belongs in Neotrix is systematics (investigating evolutionary history); describing H5, designating its holotype, providing a diagnosis, and producing the classification (including rank changes) are taxonomic acts (alpha taxonomy/classification) that invoke nomenclature; transferring H1 to Neotrix to restore monophyly conforms to modern (cladistic) principles, and making the transfer requires formal nomenclatural acts under the ICZN (a new combination published with evidence).",
        "B": "Because taxonomy only concerns naming, steps (ii) and (iii) are taxonomy but (i) is purely nomenclature; changing H1’s placement does not require phylogenetic evidence, and the ICZN does not regulate transfers between genera, so no formal nomenclatural act is necessary.",
        "C": "Systematics is limited to specimen curation and collection management, while taxonomy includes phylogenetic reconstruction and rank decisions; therefore (i) and (iii) are taxonomic, and designating a holotype is unrelated to nomenclature and need not be published to be valid.",
        "D": "Modern taxonomy prohibits any change that would create paraphyly, so the researchers must split Hapturus into multiple genera rather than transfer H1 to Neotrix; describing H5 without molecular data would be invalid under current codes, so the team should delay publication until genomic data are available."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete taxonomic scenario with paraphyly and mixed actions (phylogenetic inference, species description, rank change, nomenclatural publication) to require distinguishing systematics vs taxonomy vs nomenclature and evaluating monophyly/cladistic principles.",
      "concepts_tested": [
        "Hierarchical classification and taxonomic ranks",
        "Phylogenetic (evolutionary) basis for modern taxonomy and monophyly",
        "Distinction and overlap between taxonomy, systematics, and nomenclature (ICZN role)"
      ],
      "source_article": "Taxonomy (biology)",
      "x": 3.1630420684814453,
      "y": 1.0612657070159912,
      "level": 2,
      "original_question_hash": "ee520f24"
    },
    {
      "question": "A university is building an institutional repository containing research articles, datasets, software, and legal case documents. They require: (1) an interface that lets users browse from the most general categories down to very specific items (general-to-specific progression); (2) the ability for some items (e.g., a \"Climate model code\") to appear under both 'Software' and 'Climate Science' without duplicating records; (3) an explicit way to represent that \"Dataset A is part of Project X\" (a part–whole relation); and (4) a legal-document classification where the meaning of a term like \"contract\" has a stable core but an indeterminate penumbra that depends on case context. Which of the following design choices best satisfies all four requirements?",
      "options": {
        "A": "Implement a hierarchical containment taxonomy (root → more specific nodes) to support browsing; allow controlled polyhierarchy/cross-listing so items can have multiple parents (e.g., Software and Climate Science); represent part–whole relations using mereological \"has-a\" relations (not as is-a/subtype) and capture those richer relations in an ontology layer (since taxonomies alone are narrower); and for legal materials employ an open-ended contextual taxonomy that models core and penumbra meanings.",
        "B": "Build a strict single-parent tree taxonomy (each node has exactly one parent) to preserve mutually exclusive classes; encode all relations as is-a (treat \"Dataset A is part of Project X\" as a subtype); forbid cross-listing to avoid ambiguity; and use the same fixed taxonomy for legal documents so definitions remain uniform.",
        "C": "Use a flat folksonomy based on user tags so items can appear in multiple tag collections and users can browse by tag frequency; simulate part–whole by co-assigned tags (e.g., tags \"Dataset A\" and \"Project X\"); and handle legal terms by adding authoritative tags for their intended meanings, without a specialized contextual taxonomy.",
        "D": "Replace taxonomies with a comprehensive ontology only: model everything as arbitrary relations (no explicit hierarchical containment for browsing), represent part–whole and subtype relations uniformly in the ontology, and define legal terms with fixed ontology classes so their meanings do not vary by context."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a practical repository design scenario requiring hierarchical browsing, polyhierarchy, part–whole (mereology) vs is-a distinction, and context-sensitive legal taxonomy; provided one option that combines containment taxonomy, controlled cross-listing, ontology layer for relations, and open-ended contextual taxonomy for law.",
      "concepts_tested": [
        "hierarchical containment (root to specific progression)",
        "polyhierarchy / cross-listing (multi-parent relationships)",
        "is-a versus has-a (hyponymy vs mereology)",
        "difference between taxonomy and ontology (relation expressivity)",
        "open-ended contextual taxonomy (legal core and penumbra)"
      ],
      "source_article": "Taxonomy",
      "x": 1.2943171262741089,
      "y": 1.0302449464797974,
      "level": 2,
      "original_question_hash": "ed3e773f"
    },
    {
      "question": "A public university admits $100$ students each year from an applicant pool of $800$ majority-group applicants (Group A) and $200$ underrepresented-group applicants (Group B). Under baseline meritocratic admissions (no special measures) the university admits $90$ students from Group A and $10$ from Group B; graduation probabilities are $0.90$ for admitted Group A students and $0.70$ for admitted Group B students. Two policy alternatives are proposed:\n\nPolicy 1 (Quota): reserve $20$ of the $100$ seats for Group B (the other $80$ filled by merit); assume Group B admitted under the quota have a lower graduation probability of $0.60$ due to preparedness mismatch, while Group A graduates at $0.90$.\n\nPolicy 2 (Positive action + supports): no rigid quota, but targeted outreach increases Group B applicants so that the university admits $25$ Group B and $75$ Group A; the university also adds mentoring and bridge programs that raise Group B admitted students' graduation probability to $0.85$, while Group A graduation remains $0.90$.\n\nUsing these numbers, which policy best advances the normative goal of substantive equality (bridging inequality in both participation and successful outcomes) while minimizing the principal trade-offs discussed in debates (reverse discrimination and mismatch)?",
      "options": {
        "A": "Policy 2. It yields $75\\times0.90=67.5$ Group A graduates and $25\\times0.85=21.25$ Group B graduates (total $88.75$ graduates), raises Group B representation among admitted students to 25% and among graduates to ~24%, and mitigates mismatch through supports while avoiding a rigid quota that could trigger stronger reverse-discrimination objections.",
        "B": "Policy 1. The quota raises Group B admissions to 20 and produces $80\\times0.90=72$ Group A graduates plus $20\\times0.60=12$ Group B graduates (total $84$ graduates). It clearly increases representation relative to baseline but reduces overall successful outcomes and risks mismatch and stronger claims of reverse discrimination.",
        "C": "Baseline meritocratic admissions. It produces $90\\times0.90=81$ Group A graduates and $10\\times0.70=7$ Group B graduates (total $88$ graduates), maximizes per-student preparedness and avoids reverse-discrimination, and therefore best satisfies substantive equality by preserving academic standards.",
        "D": "Both Policy 1 and Policy 2 are effectively equivalent for substantive equality because both increase Group B admissions relative to baseline and therefore achieve the same normative outcome regarding participation and fairness."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a numerical admissions/graduation scenario comparing a rigid quota versus positive-action plus supports; computed admitted shares and expected graduates to evaluate substantive equality, mismatch risk, and reverse-discrimination trade-offs.",
      "concepts_tested": [
        "Substantive equality (representation plus successful outcomes)",
        "Mechanisms: quotas versus positive-action (outreach + supports)",
        "Trade-offs: mismatch effects, reverse discrimination, and policy design impact on real-world outcomes"
      ],
      "source_article": "Affirmative action",
      "x": 1.1626174449920654,
      "y": 0.8058608770370483,
      "level": 2,
      "original_question_hash": "c73c7bb1"
    },
    {
      "question": "Orion Textiles, a 600-employee firm, has observed a steady decline in customer satisfaction and rising defect rates. The CEO wants to implement a quality initiative aligned with Total Quality Management (TQM). She proposes four alternative action plans. Which plan best embodies the core principles of TQM (cross-functional, organization-wide ownership of quality; executive commitment and resource allocation for continuous improvement; use of established quality-control tools while recognizing no single universal methodology)?",
      "options": {
        "A": "Mandate that the production department alone is accountable for product quality, introduce a short-term bonus tied to monthly defect-reduction targets for production supervisors, and avoid investing in company-wide training to save costs.",
        "B": "Form standing cross-functional teams that include sales, engineering, accounting, and suppliers; allocate a dedicated budget for company-wide training and staffing; set long-term improvement goals; and use PDCA cycles and basic statistical process-control tools to drive continuous improvements.",
        "C": "Hire an external consultant to obtain ISO 9001 certification as the sole measure of quality; centralize quality authority in a single quality department; freeze process changes until certification is achieved.",
        "D": "Prioritize immediate cost reduction by outsourcing several production lines and implementing headcount reductions; measure success only by quarterly profit KPIs; postpone any training or process-improvement projects."
      },
      "correct_answer": "B",
      "generation_notes": "Created a scenario comparing four realistic managerial responses; correct option emphasizes cross-functional ownership, leadership resource allocation, continuous improvement with PDCA and SPC, and supplier involvement—core TQM elements. Distractors reflect common misunderstandings (isolating production, relying solely on certification, and cost-cutting without improvement).",
      "concepts_tested": [
        "Cross-functional, organization-wide ownership of quality",
        "Leadership commitment and resource allocation for continuous improvement",
        "Continuous improvement culture using quality-control tools and relation to standards/frameworks"
      ],
      "source_article": "Total quality management",
      "x": 1.418117642402649,
      "y": 1.0055464506149292,
      "level": 2,
      "original_question_hash": "15ed3f08"
    },
    {
      "question": "GreenTech Inc needs $50\\ \\text{million}$ to expand its factory. In the same region, household savers hold deposits at commercial banks and invest in mutual funds; there is also an investment bank and a listed stock exchange. Which of the following sequences best describes how the financial system components cooperate to mobilize those household savings and allocate capital to GreenTech while preserving liquidity and allowing risk sharing?",
      "options": {
        "A": "Households place savings with commercial banks and mutual funds (financial institutions mobilize funds); the investment bank underwrites GreenTech's new equity/bond issue in the primary market (financial markets and instruments allocate funds to the firm); those securities trade on the secondary market providing liquidity; financial services (asset managers, insurers) and derivative contracts are used to diversify exposures and hedge firm/lender risks.",
        "B": "Households directly buy GreenTech's securities on the secondary market, which immediately provides the $50\\ \\text{million}$; banks only provide settlement services, while derivatives are unnecessary for liquidity or risk sharing.",
        "C": "Derivatives and insurance companies are the primary allocators of capital: they buy GreenTech's debt in the primary market and redistribute cash flows to savers, while banks and secondary markets play only a minor custodial role.",
        "D": "The central bank uses open market operations to transfer $50\\ \\text{million}$ of government securities to GreenTech, thereby reallocating household savings directly from banks to the firm without engaging primary markets, investment banks, or mutual funds."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete $50 million$ expansion scenario to test mapping of savers→financial institutions→primary market (underwriting)→secondary market liquidity and role of financial services/derivatives in risk sharing; distractors introduce common misconceptions about secondary markets, derivatives, and central bank roles.",
      "concepts_tested": [
        "Financial intermediation mobilizing savings",
        "Allocation of funds via primary and secondary markets and financial instruments",
        "Roles of financial institutions and services in liquidity provision and risk sharing"
      ],
      "source_article": "Financial system",
      "x": 1.2916784286499023,
      "y": 0.9061273336410522,
      "level": 2,
      "original_question_hash": "318ef462"
    },
    {
      "question": "A field botanist observes a honey bee carrying dehydrated pollen from species X onto the stigma of species Y. Which of the following sequences and outcomes correctly describes the cellular events that must occur for a viable hybrid seed to form, and explains why the pollen did not germinate while still in the anther?",
      "options": {
        "A": "The dehydrated pollen must rehydrate on the stigma (restoring the bilayer plasma membrane), undergo activation with actin filament reorganisation, and then produce a pollen tube that grows down the style delivering two male gametes; one fuses with the egg to form the embryo and the other fuses with the polar nuclei to produce the nutritive endosperm — thus a bee-mediated cross can yield a hybrid seed only if X and Y are reproductively compatible. Premature germination in the anther is prevented by the pollen’s dehydrated state.",
        "B": "Because angiosperm pollen already contains two male nuclei, the pollen grain germinates inside the anther and the pollen tube grows autonomously; both male nuclei fuse with the egg to make a triploid embryo while the endosperm is produced later by maternal tissues. Biotic vectors merely move already-germinated pollen.",
        "C": "After deposition on the stigma the pollen immediately divides to give two pollen tubes that independently fertilize two separate eggs; pollen does not need to rehydrate because anther conditions are equivalent to the stigma, and hybrid seed formation is guaranteed whenever a pollinator transfers pollen between species.",
        "D": "The pollen must first be taken into the ovary intact (without pollen-tube growth); then a single male gamete enters and fuses with egg to make the embryo while the endosperm develops from unfertilised maternal cells; the pollen stayed ungerminated in the anther because insect vectors chemically inhibit germination until they reach another flower."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a realistic bee-mediated interspecific pollination scenario to test (1) double fertilization and fates of the two male nuclei, (2) role of pollinators in cross- vs self-pollination and hybridization constraints, and (3) the hydration→activation→pollen-tube emergence sequence and prevention of premature anther germination.",
      "concepts_tested": [
        "Double fertilization in angiosperms (one male nucleus → embryo, one → endosperm)",
        "Pollinator-mediated gene flow (cross- vs self-pollination and interspecific compatibility)",
        "Pollen germination stages: dehydration prevents premature germination; rehydration, activation, pollen tube emergence"
      ],
      "source_article": "Pollination",
      "x": 3.09346866607666,
      "y": 1.0529135465621948,
      "level": 2,
      "original_question_hash": "cd23715a"
    },
    {
      "question": "Consider a plant species P with a range spanning three connected habitat patches. In Patch A a specialist bee B is abundant; measurements show directional selection on P for increased nectar volume and longer corolla length, and concurrent morphological change in B that improves handling of long corollas. In Patch B a fungal pathogen F infects P; both host resistance alleles and pathogen virulence alleles rise and fall over generations. In Patch C P occurs without B or F. Which one of the following sets of interpretations best accords with coevolutionary principles (reciprocal selection and arms races), the diversity of pairwise vs diffuse interactions, and the geographic mosaic theory?",
      "options": {
        "A": "Patch A is a mutualistic hot spot: strong reciprocal selection (coevolution/covariation of plant and bee traits) drives local trait escalation; Patch B is an antagonistic hot spot exhibiting a Red Queen arms race with fluctuating resistance and virulence alleles; Patch C is a cold spot with no reciprocal selection. Gene flow and drift (trait remixing) among patches will produce a geographic mosaic of trait values and may spread adaptations from hot spots to cold spots. Diffuse coevolution may operate where multiple pollinators are involved, but the observed pattern still fits the geographic mosaic model.",
        "B": "Because mutualisms reduce conflict, Patch A should show trait stasis (no escalation), so the plant and bee changes are probably nonadaptive plasticity; Patch B must show steady directional increases in resistance only (pathogens cannot coevolve rapidly), and Patch C should evolve the same traits as A because selection acts uniformly across the species range.",
        "C": "The observations imply only pairwise coevolution: if the plant coevolves with the bee in Patch A it cannot also coevolve with the fungus in Patch B; therefore the host's genetic changes in Patch B are spillover effects from selection in Patch A, and geographic structure is irrelevant.",
        "D": "Coevolutionary arms races always lead to species-wide uniform trait escalation, so all three patches should converge on the same high-nectar, long-corolla phenotype and high resistance alleles regardless of local presence of B or F; any observed spatial variation reflects measurement error."
      },
      "correct_answer": "A",
      "generation_notes": "Created a three-patch scenario (mutualistic hot spot, antagonistic hot spot, cold spot) to probe predictions of reciprocal selection/arms races, pairwise vs diffuse interactions, and geographic mosaic theory (hot/cold spots, trait remixing).",
      "concepts_tested": [
        "reciprocal selective pressures and evolutionary arms races",
        "pairwise versus diffuse (multi-species) coevolution and types of interactions (mutualism, host–parasite, predator–prey)",
        "geographic mosaic theory: hot spots, cold spots, and trait remixing via gene flow and drift"
      ],
      "source_article": "Coevolution",
      "x": 1.8371628522872925,
      "y": 1.116415023803711,
      "level": 2,
      "original_question_hash": "2b2e3f49"
    },
    {
      "question": "A mid-sized, family-owned manufacturing firm with a ten-year-old legacy ERP, limited IT budget, and a workforce skeptical of new systems wants to pursue digital transformation to increase operational efficiency, enable product-service innovation, and improve customer experience. Senior management is risk-averse and concerned about cost and disruption. According to the principles of discovery-driven planning (DDP) and the enablers/barriers discussed in the article, which of the following strategies is most likely to maximize the probability of successful, value-creating digital transformation for this firm?",
      "options": {
        "A": "Adopt a DDP approach: launch a small portfolio of prioritized pilot projects (e.g., digitize invoicing, deploy analytics for predictive maintenance) with clear success metrics, integrate new modules incrementally via APIs to the legacy ERP, secure visible top-management sponsorship, invest in targeted worker training and communication, and reinvest measured efficiency and customer-experience gains into scaling.",
        "B": "Execute a one-time, company-wide ERP replacement within six months to realize efficiency gains quickly; delay extensive training and cultural change until after the new system is deployed to minimize upfront costs and speed to value.",
        "C": "Purchase and deploy multiple advanced technologies immediately (AI, IoT sensors, blockchain) from leading vendors, assume technology alone will drive innovation and customer improvements, and plan to address legacy integration and staff skills only if benefits fail to materialize.",
        "D": "Outsource the entire digital transformation to an external managed-services provider to avoid internal resistance and legacy complications; let the vendor own rollout, change-management, and long-term strategy so the firm can focus on core manufacturing."
      },
      "correct_answer": "A",
      "generation_notes": "Created a realistic firm-level scenario requiring application of DDP (incremental, risk-mitigating) and evaluation of barriers/enablers (change management, leadership, legacy systems, skills) and value-creation goals. Options contrast DDP with common incorrect approaches.",
      "concepts_tested": [
        "Discovery-driven planning (DDP) as an incremental, risk-mitigating approach",
        "Barriers and enablers of digital transformation (change management, leadership support, legacy systems, resources, worker skills)",
        "Value creation through digital transformation (innovation, customer experience, efficiency) and how strategy choices affect it"
      ],
      "source_article": "Digital transformation",
      "x": 1.3778996467590332,
      "y": 1.0010021924972534,
      "level": 2,
      "original_question_hash": "1c39dd18"
    },
    {
      "question": "An anthropologist, Dr. Sato, conducts a two-year participant-observation study of the Nuru people, who practice a ritual that includes killing a small number of newborns deemed \"spirit-afflicted.\" Dr. Sato learns the Nuru language, documents the ritual's ecological, kinship, and symbolic functions, and emphasizes how the practice fits coherently within Nuru cosmology. International human-rights groups demand immediate intervention. Which of the following best captures (1) the epistemological claim of cultural relativism, (2) its methodological/heuristic application in Dr. Sato’s fieldwork, and (3) the central ethics/universalism tension that arises in the human-rights dispute?",
      "options": {
        "A": "(1) Epistemological: claims that what counts as knowledge, truth, or moral right is determined within cultural frameworks, so judgments must be intelligible in terms of Nuru categories. (2) Methodological/heuristic: entails suspending ethnocentric condemnation and using long-term enculturation and participant-observation to interpret the practice in its systemic context. (3) Ethics/universalism tension: cultural relativism does not automatically oblige moral endorsement nor logically preclude human-rights critique; it is a procedural reminder to account for enculturation while recognizing that applying universal rights remains contested and requires careful justification.",
        "B": "(1) Epistemological: asserts there are universal moral truths discoverable by detached scientific analysis, independent of cultural context. (2) Methodological/heuristic: implies researchers should prioritize cross-cultural generalizations over deep local immersion. (3) Ethics/universalism tension: therefore international human-rights norms override local practices and intervention is straightforwardly justified.",
        "C": "(1) Epistemological: holds that truth is purely subjective and unverifiable between cultures, so no intersubjective claims are possible. (2) Methodological/heuristic: requires anthropologists never to criticize or attempt to change harmful practices. (3) Ethics/universalism tension: consequently all human-rights interventions are illegitimate because they impose external values.",
        "D": "(1) Epistemological: presumes all cultures share identical conceptual categories but differ only in surface expressions. (2) Methodological/heuristic: favors classifying artifacts and practices into evolutionary stages to explain the ritual. (3) Ethics/universalism tension: implies cultural relativism actually supports a search for absolute, universal moral laws by comparative typology."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete ethnographic scenario (ritual killing) to require students to distinguish (a) epistemological relativism, (b) methodological use of enculturation/participant-observation, and (c) the normative tension about human rights; distractors reflect common misconceptions (universalism, radical subjectivism, 19th-century evolutionism).",
      "concepts_tested": [
        "Epistemological relativism",
        "Cultural relativism as methodological/heuristic approach",
        "Ethics versus universalism (human-rights tension)"
      ],
      "source_article": "Cultural relativism",
      "x": 1.232095718383789,
      "y": 0.9906306862831116,
      "level": 2,
      "original_question_hash": "51e4c154"
    },
    {
      "question": "A systems engineering firm must integrate $n=6$ discrete subsystems that currently expose heterogeneous, proprietary interfaces. The firm’s priorities are (1) delivering improved customer-facing performance and reduced operational cost, and (2) supporting future growth (adding more subsystems within 2–3 years) with minimal rework. Estimated costs are as follows: a point-to-point interface between two heterogeneous subsystems costs $\\$20{,}000$ to implement and $\\$3{,}000$/year to maintain; an ESB-based scheme requires one adapter per subsystem at $\\$10{,}000$ each plus an ESB middleware procurement of $\\$100{,}000$, with adapter maintenance $\\$1{,}000$/adapter/year and ESB maintenance $\\$10{,}000$/year. A vertical (silo) approach grouping subsystems by functionality is estimated at $\\$150{,}000$ initial and $\\$25{,}000$/year, but every time new functionality is required a new silo costing $\\$80{,}000$ must be created. Using the formula for full-mesh point-to-point interfaces $\\frac{n(n-1)}{2}=15$, determine which integration method best satisfies the firm’s priorities and why.",
      "options": {
        "A": "Adopt star (full-mesh) integration: initial cost $15\\times\\$20{,}000=\\$300{,}000$ and maintenance $15\\times\\$3{,}000=\\$45{,}000$/year. Although initially more expensive, it maximizes direct flexibility and reuse of functionality across subsystems, so it best delivers customer value despite high scaling costs.",
        "B": "Use vertical (silo) integration: initial cost $\\$150{,}000$ and $\\$25{,}000$/year. This minimizes short-term expense and isolates vendors, so it best meets the firm’s priorities because it reduces initial outlay even if future new functionality requires additional silos.",
        "C": "Implement a horizontal ESB-based integration: initial cost $6\\times\\$10{,}000 + \\$100{,}000 = \\$160{,}000$ and maintenance $6\\times\\$1{,}000 + \\$10{,}000 = \\$16{,}000$/year. This minimizes initial and long-term interface complexity, makes heterogeneous interfaces manageable via adapters and translations, and scales cheaply when adding subsystems, so it best satisfies both performance/cost and growth objectives.",
        "D": "Standardize on a common data format without an ESB: initial cost roughly $\\$200{,}000$ and maintenance comparable to ESB. This eliminates heterogeneity at source and avoids middleware overhead, therefore it is superior because once data formats are standardized no adapters or middleware are required for future subsystems."
      },
      "correct_answer": "C",
      "generation_notes": "Created a numerical scenario (n=6) with explicit costs for point-to-point, ESB adapters/middleware, and silos to force quantitative comparison of initial vs long-term costs and to illustrate how interface heterogeneity increases star integration cost; asked which method aligns with objectives of customer value, cost reduction, and scalability.",
      "concepts_tested": [
        "System integration as unifying subsystems to deliver overarching functionality",
        "Trade-offs among integration methods (vertical, star/spaghetti, horizontal/ESB) affecting cost, scalability, and maintenance",
        "Impact of interface heterogeneity on integration effort and how value-driven objectives guide method selection"
      ],
      "source_article": "System integration",
      "x": 1.432637333869934,
      "y": 1.0513474941253662,
      "level": 2,
      "original_question_hash": "773b7783"
    },
    {
      "question": "In the mid-sized city of Baywood, the electorate comprises 40% homeowners (typical turnout $65\\%$), 35% low‑income renters (turnout $18\\%$), and 25% young professionals (turnout $40\\%$). The city council historically prioritizes proposals backed by high‑turnout constituencies. Low‑income renters collectively demand a $2 million allocation for affordable housing. The municipal government runs a participatory budgeting process and regular public council hearings; renters also run well‑attended volunteer programs (food pantry, neighborhood cleanups). Which strategy is most likely to increase the probability that the council will approve the $2 million for affordable housing, given the dynamics of representation, collective action, and forms of civic engagement?",
      "options": {
        "A": "Rely on the renters' existing volunteer programs to demonstrate community need and hope council members respond to the demonstrated civic contributions without changing electoral behavior.",
        "B": "Conduct a targeted voter registration and turnout campaign that raises renters' turnout from $18\\%$ to above $50\\%$, and simultaneously recruit and support council candidates who explicitly pledge the $2 million allocation.",
        "C": "Launch a citywide online petition and social media campaign highlighting renters' stories and use viral visibility to pressure the council to reallocate $2 million from other line items.",
        "D": "Combine a renters' voter mobilization effort (to reduce underrepresentation), build a cross‑group coalition with local NGOs and sympathetic homeowners, and use participatory budgeting meetings and formal public hearings to present a concrete, evidence‑based proposal for the $2 million allocation."
      },
      "correct_answer": "D",
      "generation_notes": "Created a concrete municipal case study with turnout percentages to require integration of concepts: representation effects, collective action linking communities to institutions, and interaction of civic/electoral/political‑voice forms. Option D tests multifaceted strategy; other options are plausible but incomplete.",
      "concepts_tested": [
        "Civic engagement as collective action connecting individuals, communities, and institutions",
        "Underrepresentation and its effect on policy attention and governance",
        "Different forms of engagement (civic volunteerism, electoral participation, and political voice) and their interaction with formal institutions (participatory budgeting, council hearings)"
      ],
      "source_article": "Civic engagement",
      "x": 1.2232111692428589,
      "y": 0.919245183467865,
      "level": 2,
      "original_question_hash": "10691dce"
    },
    {
      "question": "Jordan, aged 32, has prioritized work–life balance and over the last 8 years has taken a string of short-term, non-overlapping jobs (barista, entry-level data clerk, freelance event assistant). These roles did not build on a single specialized technical skill. Jordan’s current employer, however, expects traditional vertical promotion (greater hours, managerial responsibility). Which choice best classifies Jordan’s career archetype and prescribes the most effective combined individual and organizational interventions to align Jordan’s long-term learning and personal fulfillment with realistic advancement opportunities?",
      "options": {
        "A": "Jordan’s pattern is a transitory career (frequent unrelated job changes). Effective alignment requires Jordan to clarify personal values and invest in portable, marketable skills (e.g., project coordination, digital literacy) while the organization implements flexible work arrangements, recognizes transferable experience, and offers lateral development paths (job rotation, role breadth) instead of only vertical promotions.",
        "B": "Jordan exemplifies a steady-state career and should therefore commit to a long-term specialization; the organization should create deep apprenticeship programs and a single-track promotion ladder that rewards tenure and technical mastery.",
        "C": "Jordan is on a linear career trajectory; the immediate remedy is for Jordan to accept increased responsibility and seek formal managerial training while the employer formalizes a strict hierarchical promotion process tied to performance metrics.",
        "D": "Jordan’s pattern is best described as a spiral career; the optimal approach is for Jordan to consolidate disparate roles into a single professional narrative and for the organization to discourage job-hopping by tying benefits and advancement strictly to tenure."
      },
      "correct_answer": "A",
      "generation_notes": "Used a concrete worker scenario to require classification among steady-state/linear/transitory/spiral and selected combined individual vs organizational interventions that preserve work–life balance while enabling career advancement.",
      "concepts_tested": [
        "Alignment principle (matching learning and personal fulfillment to advancement)",
        "Career-path archetypes and progression (steady-state, linear, transitory, spiral)",
        "Levels of analysis and interaction (individual vs organizational interventions)"
      ],
      "source_article": "Career development",
      "x": 1.3030133247375488,
      "y": 1.0009318590164185,
      "level": 2,
      "original_question_hash": "9df358fb"
    },
    {
      "question": "A school district is deciding between three reading curricula. Which of the following policy claims would legitimately be described as an evidence-based practice according to the three conditions described in the article (comparative evidence against an alternative, support given district preferences, and a sound account explaining the evidence and preferences)?",
      "options": {
        "A": "The district adopts Curriculum X after a randomized controlled trial (RCT) comparing Curriculum X to Curriculum Y across 20 schools showed mean reading scores increased from $62$ to $74$ (statistically significant). The district's stated priority is to raise standardized reading scores, and the curriculum committee publishes the trial methods, statistical analyses, and a rationale linking the trial results to the district preference for test-score improvement.",
        "B": "The district adopts Curriculum Z because archival data show schools that implemented Z tended to have higher reading scores than those that did not; no direct comparative trial was conducted, and the decision memo cites teachers' anecdotal reports without providing a formal analysis or an explicit statement of district priorities.",
        "C": "The district continues using the long-established Curriculum T because most veteran teachers prefer it and report ‘‘it works for our students’’; the district issues a short justification based on tradition and collective teacher experience but provides no comparative data or written explanation connecting evidence to district goals.",
        "D": "The district adopts Curriculum W after an RCT shows modest gains in decoding skills versus a control; however, the district's formal policy emphasizes holistic socio-emotional development rather than test scores, and the adoption memo contains only a brief reference to the trial without explaining how the trial evidence relates to the district's stated priorities."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete education policy scenario testing the three formal conditions for an evidence-based claim; options vary which condition(s) are missing to create plausible distractors.",
      "concepts_tested": [
        "Comparative evidence from properly designed evaluations (RCTs versus observational/anecdotal evidence)",
        "Three necessary conditions for an evidence-based claim: comparative evidence, alignment with preferences/goals, and a sound explanatory account",
        "Limitations of evidence-based practice including the need to relate population-level trial results to specific institutional preferences and decision rationales"
      ],
      "source_article": "Evidence-based practice",
      "x": 1.333014965057373,
      "y": 1.001874566078186,
      "level": 2,
      "original_question_hash": "b7cfdafe"
    },
    {
      "question": "You are advising a multidisciplinary team planning restoration of a 200‑ha riparian wetland that was converted to pasture 60 years ago. The site is now dominated by invasive common reed (Phragmites australis), hydrology is seasonally altered by drainage ditches, and a native frog species has shown a 70% decline due to loss of shallow breeding pools and leaf‑litter habitat. The team must set restoration goals, design interventions, and engage stakeholders. Which of the following restoration plans best exemplifies the integrated principles of (i) diagnosing and restoring life cycles, species interactions, and essential resources; (ii) choosing goals that balance historic‑baseline targets with realistic, function‑based outcomes shaped by political and cultural context; and (iii) practicing social‑ecological restoration with meaningful stakeholder inclusion?",
      "options": {
        "A": "Reestablish the 1950s baseline by removing all non‑native plants, recontouring channels to their historical geometry, planting historically dominant tree species, and setting a strict no‑human‑use policy determined by biologists—without altering regional water management or consulting local communities.",
        "B": "Maximize rapid ecosystem services (flood attenuation and carbon uptake) by excavating few deep ponds and planting a dense monoculture of fast‑growing willows across the site, minimizing initial costs and long‑term maintenance, with occasional contractors controlling Phragmites by herbicide.",
        "C": "Co‑develop goals with local landowners and Indigenous groups; combine targeted Phragmites control with reestablishment of a mosaic of shallow breeding pools and native plant assemblages selected for their roles in the frog life cycle and food webs; restore natural seasonal hydrology where feasible, source propagules from nearby adapted populations, and implement an adaptive monitoring program tied to social objectives (e.g., culturally important species, subsistence uses).",
        "D": "Fence the site to exclude livestock and allow passive natural regeneration, supplemented by a one‑time seeding of a generic native seed mix sourced from a commercial supplier 500 km away; delay stakeholder engagement until after vegetation recovery is evident."
      },
      "correct_answer": "C",
      "generation_notes": "Created a realistic wetland restoration scenario and four plans that each emphasize different trade‑offs. Option C integrates life‑history needs, functional targets sensitive to climate and politics, and explicit stakeholder/TEK inclusion—testing all three target concepts.",
      "concepts_tested": [
        "life cycles and species interactions in restoration design",
        "historic‑baseline vs function‑based goal selection influenced by political and cultural context",
        "social‑ecological restoration and stakeholder inclusion"
      ],
      "source_article": "Ecological restoration",
      "x": 1.5599002838134766,
      "y": 0.9064228534698486,
      "level": 2,
      "original_question_hash": "a45cf21f"
    },
    {
      "question": "Two states provide contrasting constitutional arrangements. Civitas has a codified constitution that (a) includes an entrenched clause declaring “freedom of assembly” protected against amendment, (b) vests a constitutional court with power to strike down legislation as ultra vires, and (c) requires a 3/5 parliamentary supermajority plus a national referendum to amend fundamental rights. Parlia has an uncodified constitution based on parliamentary sovereignty: constitutional rules are dispersed across statutes and conventions, Parliament may alter constitutional rules by ordinary statute (simple majority), and courts lack power to invalidate Acts of Parliament. Both governments enact time‑limited emergency regulations curtailing street protests during a security crisis. Six months later the governing party in Parlia introduces an ordinary statute to permanently abolish freedom of assembly; in Civitas a similar proposal to entrench the abolition is put to a referendum and is rejected. Which of the following best predicts the legal and political consequences, given constitutional design, judicial review, and amendment mechanisms?",
      "options": {
        "A": "In Civitas the constitutional court would likely declare any later ordinary statute purporting to abolish freedom of assembly ultra vires and therefore invalid ab initio; the failed referendum and the entrenched clause mean the right remains constitutionally protected and emergency regulations remain temporally limited and subject to judicial review. In Parlia, by contrast, parliamentary sovereignty and the lack of a court able to strike down Acts mean the simple‑majority statute can validly abolish the right as ordinary law, making the right vulnerable to majoritarian change and reducing long‑term stability of rights protections.",
        "B": "Because both states invoked emergency powers, governments in Civitas and Parlia can permanently abolish freedom of assembly: courts normally defer during crises and any later judicial review will validate the permanent restrictions as necessary for security, so neither constitutional structure prevents the abolition.",
        "C": "In Parlia parliamentary conventions and political norms will prevent the majority from actually abolishing freedom of assembly despite the legal capacity to do so; in Civitas the failed referendum is only a political setback, and Parliament could enact an ordinary statute that achieves the same result without triggering constitutional review.",
        "D": "The entrenched clause in Civitas protects freedom of assembly only against ordinary statutes but not against emergency laws, so the government there can convert emergency regulations into permanent law; in Parlia, ordinary courts would nevertheless strike down any statute abolishing the right because basic rights are implied limits of parliamentary power under common law traditions."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a comparative scenario (Civitas vs Parlia) to require application of concepts: entrenched clauses, judicial review, parliamentary sovereignty, amendment thresholds, emergency powers. Options contrast likely legal outcomes and political stability.",
      "concepts_tested": [
        "constitutional power limitation and rights protection",
        "written (codified) vs unwritten (uncodified) constitutions and governance implications",
        "amendment/change mechanisms (supermajority, referendum) and effects on stability/adaptability"
      ],
      "source_article": "Constitution",
      "x": 1.1274405717849731,
      "y": 0.7571963667869568,
      "level": 2,
      "original_question_hash": "df9555f5"
    },
    {
      "question": "Consider these three concrete scenarios in a contemporary state governed by the rule of law: (i) A national environmental agency issues an administrative order closing a factory and imposing fines after an on‑site inspection; the factory owner alleges the agency misapplied its statutory powers and asks a court to set the order aside. (ii) The same factory owner has a written supply contract with that agency to process industrial waste; he sues for breach of contract when the agency fails to pay. (iii) A workplace safety inspector (an arm of the government) enters the factory and issues mandatory remedial directions under occupational health statutes. For each scenario, decide whether it is most appropriately characterized as public law, private law, or both, and whether a remedy by judicial review (i.e., a court reviewing the lawfulness of an administrative act) is ordinarily available. Which single answer best describes the legal classification and availability of judicial review for the three situations?",
      "options": {
        "A": "(i) Public law and subject to judicial review because the agency acted by exercising statutory powers; (ii) Private law (contractual dispute) and normally litigated in civil courts rather than by judicial review; (iii) Public law (enforcement using statutory imperium) and not resolved by contract remedies but by administrative enforcement and review.",
        "B": "(i) Private law because it concerns the factory owner's rights against another party; judicial review is unavailable; (ii) Public law because the counterparty is the State so all disputes with the State are public law; (iii) Private law because workplace instructions arise from employment relationships, so administrative review is irrelevant.",
        "C": "(i) Public law but judicial review is only available for constitutional claims, so ordinary procedural or statutory errors are resolved in civil courts; (ii) Public law because any dispute involving a governmental body is public law; (iii) Ambiguous—workplace inspections are private when the inspector negotiates with the employer and public when issuing mandatory orders.",
        "D": "(i) Public law and subject to judicial review as a check that the agency acted secundum et intra legem; (ii) Private law if the agency acted as a contracting party, though some contract-related matters may also entail public-law considerations; (iii) Usually public law because the inspector exercises unilateral regulatory power—remedies include administrative enforcement and judicial review of legality."
      },
      "correct_answer": "A",
      "generation_notes": "Created a three-part scenario to test (1) judicial review as a rule-of-law mechanism constraining administrative power, (2) asymmetry of government enforcement powers, and (3) the functional boundary where government contracting is typically private law. Option A reflects textbook public/private classifications and remedies.",
      "concepts_tested": [
        "Rule of law and judicial review (secundum et intra legem)",
        "Asymmetric public-law relationships and government imperium",
        "Functional distinction between public and private law when the State acts as regulator vs. contractor"
      ],
      "source_article": "Public law",
      "x": 1.2581560611724854,
      "y": 0.851123034954071,
      "level": 2,
      "original_question_hash": "7a90a065"
    },
    {
      "question": "A university network has two departmental LANs, LAN-A and LAN-B, connected by an optical-fiber backbone. Each LAN is a switched star topology: end hosts connect to a local switch using Cat5e twisted-pair copper. Host alice in LAN-A has hostname alice.example.edu and IP 10.0.1.5; host bob in LAN-B has hostname bob.example.edu and IP 10.0.2.20. The DNS server maps bob.example.edu → 10.0.2.20. When alice sends an IP packet to bob, which of the following descriptions most accurately explains the roles of name resolution, addressing (IP vs MAC), devices (switch vs router), topology, and physical media used during the end-to-end transfer?",
      "options": {
        "A": "Alice resolves bob.example.edu to IP 10.0.2.20 via DNS. Because that IP is in a different subnet, Alice encapsulates the IP packet in an Ethernet frame whose destination MAC is Alice's default gateway (the router) and whose source MAC is Alice's NIC. The LAN-A switch (star topology over copper UTP) forwards the frame to the router, which uses the IP address and its routing table to forward the packet across the optical-fiber backbone toward LAN-B. At LAN-B the router rewrites the Ethernet destination MAC to Bob's MAC and the LAN-B switch delivers the frame over copper to Bob.",
        "B": "Alice asks the router to resolve bob.example.edu to Bob's MAC address; the router returns Bob's MAC, so Alice sends an Ethernet frame with destination MAC=Bob's MAC directly. Switches in the two LANs forward frames based on IP addresses, while the optical-fiber backbone is only used for control-plane traffic.",
        "C": "Name resolution (DNS) maps hostnames to MAC addresses, so Alice obtains Bob's MAC from DNS and sends a frame directly to Bob; because switches operate at the network layer they route frames between LAN-A and LAN-B, and the physical media type (copper vs fiber) does not affect which device forwards packets.",
        "D": "Alice broadcasts the packet to all hosts in the campus because the tree-like topology forces broadcast flooding; the packet is copied across both copper and fiber media to reach Bob, and address resolution is unnecessary since broadcasts reach every host."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete campus LAN-to-LAN scenario to test DNS-based name resolution, ARP/gateway behavior, distinction between MAC (link layer) and IP (network layer), roles of switches versus routers, and the use of copper UTP for LAN links and optical fiber for backbone.",
      "concepts_tested": [
        "Addressing and name resolution (hostnames, DNS → IP)",
        "Link-layer vs network-layer addressing (MAC vs IP) and device responsibilities (switch vs router)",
        "Network topology and physical media (star topology over copper UTP; fiber backbone)"
      ],
      "source_article": "Computer network",
      "x": 1.4587526321411133,
      "y": 1.0729916095733643,
      "level": 2,
      "original_question_hash": "98dfe6ba"
    },
    {
      "question": "Acme Instruments deploys 10,000 battery-powered air-quality sensors across three cities. Each sensor periodically sends a 200‑byte reading to a cloud aggregator over third‑party networks that provide only best‑effort packet delivery. The engineering team must design the communication solution to (1) minimize per‑sensor energy use, (2) maximize end‑to‑end data reliability, and (3) ensure that future third‑party services and analytics platforms can interoperate with minimal rework. Which of the following design choices best embodies the protocol design principles of layered protocol stacks, the end‑to‑end principle for assigning reliability responsibilities, and the use of standards/semantics to achieve interoperability?",
      "options": {
        "A": "Keep the network simple (best‑effort) and implement end‑to‑end reliability at the sensor/cloud endpoints (e.g., lightweight transport or application ACKs and sequence numbers); use an open, standardized message format and schema (e.g., JSON with a published schema) so other platforms can parse semantics without changing lower layers.",
        "B": "Ask each network operator to provide guaranteed delivery by deploying link‑level ARQ and virtual circuits across the path; sensors transmit raw binary datagrams without sequence numbers to save energy, relying on the network to mask loss; define a proprietary compact binary payload to minimize bytes on the air.",
        "C": "Implement reliability redundantly by doing per‑link retransmissions in middleboxes and also end‑to‑end ACKs at the cloud; use a closed, highly optimized binary format with nonstandard field encodings so the sensor payloads are shortest possible, accepting that third‑party platforms must adapt their parsers.",
        "D": "Design strict layering where the link layer provides automatic recovery and reordering so endpoints assume perfect delivery; use an ad hoc plain‑text format chosen by the first deployment team, and defer standardization until later when traffic grows."
      },
      "correct_answer": "A",
      "generation_notes": "Presented a concrete IoT scenario requiring tradeoffs; each option maps to different allocations of reliability and standardization choices. Option A aligns with layering, end‑to‑end reliability at endpoints, and use of open standards for interoperability.",
      "concepts_tested": [
        "Protocol layering and modularity",
        "End‑to‑end principle and allocation of reliability",
        "Standards, semantics, and shared formats for interoperability"
      ],
      "source_article": "Communication protocol",
      "x": 1.4922618865966797,
      "y": 1.09172785282135,
      "level": 2,
      "original_question_hash": "679f1fc9"
    },
    {
      "question": "A mid-sized manufacturing firm reports that annual voluntary turnover rose from $12\\%$ to $20\\%$ over two years, near-miss safety incidents increased by $30\\%$, and employee survey scores indicate high work–family conflict and low job satisfaction. The firm's new I-O psychologist is asked to design a program that (1) integrates scientific research with practice, (2) uses interventions such as recruitment, training, feedback, and change management to improve performance, motivation, satisfaction, health, and safety, and (3) addresses spillover between work and nonwork life. Which of the following action plans best exemplifies the scientist–practitioner model while targeting the described outcomes and the work–nonwork interface?",
      "options": {
        "A": "Conduct a mixed-methods diagnostic (quantitative surveys, archival turnover/safety data, and focus groups) and a job analysis; select evidence-based interventions (validate selection tools, implement task- and affective-focused training with formative/summative evaluation, introduce 360-degree feedback, and redesign schedules to improve safety climate and reduce work–family conflict); pilot the interventions in two plants; evaluate effects with pre–post comparisons and hierarchical linear modeling to estimate individual- and unit-level changes in job performance, satisfaction, burnout, and incident rates; iterate based on findings.",
        "B": "Design a controlled laboratory study to test cognitive mechanisms of stress using student participants, publish findings in academic journals about stress physiology, and then recommend general stress-reduction pamphlets for employees without assessing the firm's specific job demands or measuring organizational outcomes.",
        "C": "Immediately tighten absence policies and increase monitoring to reduce turnover and safety incidents, centralize scheduling to eliminate overtime, and rely on exit interviews to infer causes; treat work–family conflict as an individual time-management problem and require employees to attend mandatory workshops without baseline measurement or follow-up evaluation.",
        "D": "Refer all affected employees to the employee assistance program (EAP) for individual counseling focused on coping and work–life balance, supplement with optional stress-management seminars, but make no changes to recruitment, selection, training, feedback systems, or work design at the organizational level."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete organizational scenario and four plausible plans; correct option integrates research and practice (mixed methods, job analysis), applies evidence-based interventions (selection, training, 360-feedback, work redesign), evaluates outcomes with multilevel analysis, and addresses work–nonwork spillover.",
      "concepts_tested": [
        "Scientist–practitioner model (integration of research and applied interventions)",
        "Interventions and outcomes (recruitment, training, feedback, change management affecting performance, motivation, satisfaction, health, safety)",
        "Work–nonwork interface (addressing work–family conflict, burnout, and organizational factors influencing nonwork outcomes)"
      ],
      "source_article": "Industrial and organizational psychology",
      "x": 1.2569382190704346,
      "y": 0.9790945649147034,
      "level": 2,
      "original_question_hash": "062aec83"
    },
    {
      "question": "In 1928 a group of painters and sculptors in an industrial European city begin producing work that emphasizes machine parts, dynamic motion and geometric abstraction. They publish a manifesto declaring an explicit program to integrate new engineering technologies and scientific ideas into visual form, exhibit together under the label \"Mechanism,\" and critics adopt the name and defend their program in periodicals. Fifty years later art historians debate whether \"Mechanism\" should be classified as an art movement. Which assessment best applies given theories about art movements, modernism and postmodernism?",
      "options": {
        "A": "This is a prototypical modernist art movement: its origin responds to technological and scientific changes, it manifests a common visual language and an explicit rhetorical program (the manifesto and critical discourse) that unify practitioners; however, postmodern theory would problematize the category by arguing that such coherent movements become less applicable or discernible in the contemporary/postmodern era.",
        "B": "It cannot be called an art movement because artists must produce visually identical works; the existence of a manifesto and critical support is irrelevant unless every practitioner uses identical formal techniques.",
        "C": "The label is inappropriate because art movements refer only to literature and architecture; visual arts developments are described exclusively as genres rather than movements.",
        "D": "Postmodern theory would unambiguously affirm \"Mechanism\" as a movement because postmodernism extends modernist grand narratives and thereby strengthens the applicability of movement-based categorization to all subsequent art practices."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete historical vignette testing: (1) movements arise from responses to technology/science/philosophy, (2) shared visual language plus manifesto/critical rhetoric unify a movement, and (3) modern vs postmodern theoretical debates about the applicability of the 'art movement' concept.",
      "concepts_tested": [
        "Origins of movements in response to technological, scientific and philosophical change",
        "Shared visual language and explicit rhetoric (manifestos, critics) as unifying features of movements",
        "Theoretical distinction between modernist endorsement of movements and postmodern skepticism about their applicability"
      ],
      "source_article": "Art movement",
      "x": 0.47039783000946045,
      "y": 1.0836328268051147,
      "level": 2,
      "original_question_hash": "f3187b4b"
    },
    {
      "question": "Three real-world vignettes examine prosocial behavior. For each, identify the primary proximate motive, the most applicable evolutionary explanation (if any), and which organizational or developmental outcome is most consistent with the literature:\n\nVignette 1 — A daughter spends 10 hours per week providing unpaid care to her ailing parent. The subjective cost to her equals 6 fitness units and the benefit to the parent is estimated at 20 fitness units; genetic relatedness $r=0.5$.\n\nVignette 2 — A high school adopts a \"self‑transcendent purpose for learning\" curriculum; teachers report fewer dropouts, higher persistence on tedious tasks, and improved math/science GPAs.\n\nVignette 3 — A company introduces a small monetary bonus plus public leaderboard for employees who donate to a charity; donations rise sharply the first quarter but plateau thereafter.\n\nWhich answer correctly matches each vignette to (i) the dominant proximate motive, (ii) the best evolutionary account (where applicable, use Hamilton's rule $rB>C$ if kin selection applies), and (iii) the most likely interpretation of the observed outcome?",
      "options": {
        "A": "V1: Motivation = empathy plus kin‑directed altruism; evolutionary account = kin selection applies because $rB=0.5\\times20=10>6=C$, so helping can be favored by inclusive fitness; Outcome interpretation = kin care driven by both proximate empathy and ultimate inclusive fitness. V2: Motivation = internalized self‑transcendent prosocial purpose (intrinsic motivation); evolutionary account = cultural evolution/socialization more relevant than genetic selection; Outcome interpretation = durable improvements in motivation and learning. V3: Motivation = extrinsic/egoistic incentives (status, reciprocity); evolutionary account = reciprocal/strategic helping or social signaling rather than kin selection; Outcome interpretation = short‑term increase due to incentives that may not be durable.",
        "B": "V1: Motivation = generalized reciprocity (expectation of return); evolutionary account = reciprocal altruism explains it even with $rB>C$ irrelevant; Outcome interpretation = unpaid care is primarily investment expecting future payback. V2: Motivation = conformity to social norms; evolutionary account = kin selection explains group success in schools; Outcome interpretation = curriculum effects are due to peer pressure, not intrinsic purpose. V3: Motivation = pure altruism; evolutionary account = inclusive fitness explains corporate prosociality; Outcome interpretation = monetary incentives reveal genuine altruism because donations increased.",
        "C": "V1: Motivation = pure, selfless altruism with no proximate emotional basis; evolutionary account = no evolutionary explanation needed because altruism is spontaneous; Outcome interpretation = caregiving is an expression of moral duty independent of fitness. V2: Motivation = short‑lived positive mood effects (\"feel good‑do good\"); evolutionary account = sexual selection explains improved grades; Outcome interpretation = the curriculum temporarily boosts mood which improves grades but not long‑term outcomes. V3: Motivation = intrinsic warm‑glow only; evolutionary account = kin selection because employees identify as a work family; Outcome interpretation = monetary bonus simply amplified existing intrinsic motives and will remain stable.",
        "D": "V1: Motivation = selfish strategic calculation for reputation and material support; evolutionary account = reciprocal altruism best explains it (not kin selection), so Hamilton's rule is inapplicable; Outcome interpretation = care persists only while strategic benefits continue. V2: Motivation = operant conditioning (reward/praise) alone; evolutionary account = group selection explains schoolwide benefits; Outcome interpretation = gains are due to external reinforcement rather than meaningful purpose and will dissipate without continued rewards. V3: Motivation = anonymity reduces helping, so public leaderboard actually suppressed intrinsic motives; evolutionary account = psychopathology explains donor behavior; Outcome interpretation = plateau reflects erosion of prosocial norms under market incentives."
      },
      "correct_answer": "A",
      "generation_notes": "Created a three‑vignette scenario requiring application of proximate motives (empathy, intrinsic vs extrinsic incentives, reciprocity), the evolutionary model (Hamilton's rule for kin selection), and interpretation of organizational/educational outcomes (durability of intrinsic motivation vs short‑term incentive effects). Option A aligns with literature: kin selection for kin care, self‑transcendent purpose producing durable educational gains, and monetary/status incentives giving short‑term increases.",
      "concepts_tested": [
        "Proximate motives for prosocial behavior (empathy, intrinsic vs extrinsic incentives, reciprocity)",
        "Evolutionary explanations (kin selection, inclusive fitness, Hamilton's rule)",
        "Effects of prosocial interventions on educational and workplace outcomes (durability of intrinsic motivation vs short‑term incentives)"
      ],
      "source_article": "Prosocial behavior",
      "x": 1.279922604560852,
      "y": 1.0206407308578491,
      "level": 2,
      "original_question_hash": "01518a2a"
    },
    {
      "question": "A new small body is discovered whose path will take it to 0.2 AU from the Sun. Three observatories report only angular measurements (right ascension and declination) at known times t1, t2, t3. The team wants to (1) determine the object's six orbital elements and (2) predict its trajectory near perihelion with meter-level accuracy. Which of the following statements is most accurate?",
      "options": {
        "A": "Because the three observations provide $3\\times(\\mathrm{RA},\\mathrm{Dec})=6$ scalar data points at known epochs, Gauss-style orbit determination can solve for the three line-of-sight ranges and thus the six orbital elements under the two-body (Newtonian) assumption; the resulting Keplerian solution can be propagated, but achieving meter-level accuracy at 0.2 AU requires adding general-relativistic corrections to the equations of motion during propagation (and careful treatment of light-time and observational reduction).",
        "B": "Three angular measurements alone are fundamentally insufficient to determine six orbital elements: without independent range or Doppler measurements one cannot recover the orbit, so Newtonian methods cannot be applied until ranges are measured; general relativity is irrelevant in this context.",
        "C": "Gauss's method can produce initial orbital elements from three angular observations, but because the perihelion is at 0.2 AU Keplerian two-body dynamics completely fail and one must immediately use only full n-body numerical integration with relativistic gravity even for the initial determination.",
        "D": "The three angular observations suffice to determine an orbit using Newtonian two-body mechanics and no relativistic corrections are necessary either in the orbit determination or in subsequent propagation; any needed precision can be obtained by refining Newtonian perturbations (e.g., planetary perturbations and radiation pressure) alone."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete near-Sun comet scenario to test knowledge of Gauss three-observation orbit determination (three RA/Dec pairs = six scalars), the Newtonian two-body assumption for initial orbit solution, and when general-relativistic corrections are required for high-precision propagation.",
      "concepts_tested": [
        "Gauss orbit determination using three angular observations",
        "Applicability of Newtonian two-body dynamics versus general relativity for high-precision orbital propagation"
      ],
      "source_article": "Orbital mechanics",
      "x": 1.7468960285186768,
      "y": 1.1042194366455078,
      "level": 2,
      "original_question_hash": "10df86fa"
    },
    {
      "question": "A regional hospital is designing a hypertension program around a newly approved drug Atenol‑X. Clinical trials show that patients carrying the G allele at biomarker SNP rs123 derive a large blood‑pressure benefit from Atenol‑X, while carriers of the A allele have a high rate of serious adverse events. The hospital proposes: (1) routine pre‑prescription genotyping of rs123 to decide whether to prescribe Atenol‑X or an alternative; (2) genotype‑guided dose adjustments based on CYP3A5 variants to reduce side effects; (3) calculation of a polygenic risk score (PRS) for progression to resistant hypertension and offering high‑risk patients a lifestyle prevention bundle; and (4) a patient portal that shares genomic results and invites patients to opt into monitoring and trial enrollment. Which one of the following statements best characterizes how this program relates to precision medicine, personalized medicine (including pharmacogenomics), and the P4 medicine framework?",
      "options": {
        "A": "This program exemplifies precision medicine by using a biomarker (rs123) to classify patients into subgroups for targeted therapy (not by creating patient‑unique drugs); the genotype‑guided dosing is an application of personalized medicine/pharmacogenomics; the PRS and lifestyle bundle implement the predictive and preventive aspects; and the patient portal provides the participatory element — together constituting a P4 approach.",
        "B": "Because the hospital plans to change dosing and prescribe only to G‑allele carriers, the program is personalized medicine in the strict sense because it aims to create unique, patient‑specific drugs and treatments for each individual rather than subgroup classification.",
        "C": "Using rs123 only to select trial participants is not precision medicine; precision medicine requires whole‑genome sequencing for every patient and cannot rely on single biomarkers or pharmacogenomic dosing adjustments.",
        "D": "The program violates the P4 framework because predictive and preventive strategies (like PRS and lifestyle bundles) are incompatible with participatory practices; P4 requires either population‑level prevention or individualized participation but not both."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a clinical scenario linking biomarker stratification, pharmacogenomic dosing, PRS preventive actions, and patient engagement; options test correct distinction that precision medicine stratifies into subgroups, personalized medicine includes pharmacogenomics, and P4 comprises predictive, preventive, personalized, participatory elements.",
      "concepts_tested": [
        "Biomarker/genomic-guided stratification for therapy selection",
        "Distinction and interplay between precision medicine and personalized medicine/pharmacogenomics",
        "P4 medicine framework and its practical implications (predictive, preventive, personalized, participatory)"
      ],
      "source_article": "Personalized medicine",
      "x": 1.3430416584014893,
      "y": 0.9912683367729187,
      "level": 2,
      "original_question_hash": "589bc56b"
    },
    {
      "question": "A pharmaceutical team discovers compound X that potently inhibits oncogenic BRAF (IC50 = 10 nM) and also inhibits MEK1 (IC50 = 100 nM) and PI3Kα (IC50 = 200 nM). In murine xenograft models of metastatic melanoma, X produces deeper tumor regressions and a longer delay to acquired resistance than a highly selective BRAF inhibitor. Given the principles of polypharmacology and systems-level drug design, which of the following interpretations and next steps is most appropriate?",
      "options": {
        "A": "Interpret X as a candidate 'magic shotgun'—a selectively non-selective agent that engages multiple network nodes (BRAF, MEK, PI3K) to increase efficacy and delay resistance. Proceed to apply chemoproteomics to map proteome-wide interactions, then optimize the molecule to preserve therapeutically beneficial multi-target engagement while minimizing off-target interactions that could cause toxicity; this follows the SAMT/multitarget design rationale.",
        "B": "Conclude that multi-target activity is undesirable and that X's additional activities are artifacts; focus exclusively on increasing BRAF selectivity (reduce MEK and PI3K activity) because single-target ‘magic bullet’ drugs always minimize side effects and are therefore preferable.",
        "C": "Treat X as a selective chemical probe for BRAF and ignore the MEK and PI3K data as biologically irrelevant; next step is to run single-cell transcriptomics to validate BRAF-specific signaling changes, without performing proteome-wide off-target profiling.",
        "D": "Assume multi-target benefits occur only when targets belong to the same enzyme family, so the observed efficacy must be due to BRAF inhibition alone; therefore combine X with a separate MEK inhibitor rather than designing a single agent to hit multiple pathways."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete drug-discovery scenario illustrating multi-target inhibition (BRAF, MEK, PI3K) and asked for the interpretation and next steps consistent with polypharmacology, magic shotgun/SAMT concepts and use of chemoproteomics.",
      "concepts_tested": [
        "Polypharmacology as intentional multi-target drug design to improve efficacy and manage toxicity",
        "Network/systems rationale for treating complex diseases by hitting multiple interacting targets",
        "Methodological shift to systems-level approaches (chemoproteomics, SAMT, magic shotgun) for profiling and optimizing multi-target agents"
      ],
      "source_article": "Polypharmacology",
      "x": 1.9260129928588867,
      "y": 1.0622859001159668,
      "level": 2,
      "original_question_hash": "88971322"
    },
    {
      "question": "You are investigating why a single-point mutation (Ala172Val) in a 420-residue enzyme reduces its catalytic rate by ~80% despite normal expression levels. Available data are: (1) a 2.0 Å X-ray crystal structure of the wild-type (WT) enzyme that lacks electron density for residues 165–180 (a flexible loop near the active site), (2) an NMR HSQC of a 25 kDa monomeric fragment showing substantial chemical-shift perturbations for residues 168–176 and evidence of millisecond–second exchange, (3) a 4.5 Å cryo-EM reconstruction of the tetrameric mutant showing a shifted relative orientation of subunits compared with WT, and (4) molecular dynamics (MD) simulations of the mutant starting from the WT X-ray model that sample a loop conformation in which residues 165–180 close over the active site and reduce solvent accessibility. Which interpretation best integrates these datasets to explain the loss of catalytic activity?",
      "options": {
        "A": "The Ala172Val mutation globally destabilizes the enzyme tertiary fold, causing partial unfolding and aggregation; the reduced activity is a consequence of the misfolded population and is supported by the missing loop density in the X-ray structure and the NMR exchange.",
        "B": "The mutation perturbs the local primary-to-tertiary relationship of the loop (residues 165–180), shifting the conformational ensemble toward a loop-closed state that occludes the active site; NMR reports increased loop dynamics, MD demonstrates loop closure and reduced solvent access, and cryo-EM reveals a concomitant change in quaternary packing that further impairs catalysis—together explaining the activity loss while the core fold remains intact.",
        "C": "The NMR perturbations and MD loop closure are artefacts of studying a fragment and an in silico model; only the cryo-EM tetramer change is relevant, so the activity drop is solely due to altered subunit–subunit contacts at the quaternary level rather than any local loop or active-site effect.",
        "D": "Ala172Val directly alters the chemical mechanism by replacing a catalytic residue, so the decrease in rate arises from loss of catalytic chemistry; structural methods only incidentally detect unrelated dynamics and packing changes."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete enzyme-mutation scenario combining X-ray, NMR, cryo-EM, and MD; options contrast local tertiary/dynamic effects vs global unfolding, quaternary-only explanations, and direct catalytic substitution to test integration of methods and structure–function reasoning.",
      "concepts_tested": [
        "Structure–function relationship (primary → tertiary → function)",
        "Complementary roles of X-ray crystallography, NMR, and cryo-EM in resolving static structure, dynamics, and assembly",
        "Integration of experimental data with molecular dynamics to explain conformational ensembles and functional effects"
      ],
      "source_article": "Structural biology",
      "x": 1.9546613693237305,
      "y": 1.095090627670288,
      "level": 2,
      "original_question_hash": "c2f7bc96"
    },
    {
      "question": "An engineering team is designing remote-control interfaces for an underwater ROV used for maintenance. Four prototype HMIs are defined:\n\nPrototype A: a capacitive touchscreen monitor displaying graphical controls, a gamepad joystick, and stereo speakers for alerts.\n\nPrototype B: an immersive head-mounted display (HMD) that blocks the real world, haptic gloves providing force feedback, spatialized audio, and an olfactory dispenser synchronized to environmental events.\n\nPrototype C: augmented-reality (AR) glasses that overlay telemetry onto the operator's physical console, supplemented by physical knobs and buttons and audio cues.\n\nPrototype D: a brain–computer interface that uses scalp electrodes (EEG) to extract high-level intent; no physical controllers or visual displays are required except occasional system-generated audio prompts.\n\nWhich prototype best satisfies all three of the following requirements: (1) it is implemented using human interface devices (HIDs) forming the HMI layer between human and machine; (2) it is classified as a 4-sense (4S) virtual reality composite user interface (CUI) according to the taxonomy (i.e., virtual reality that interfaces with four senses); and (3) relative to a standard GUI+joystick setup, it materially reduces the operator's need for gross physical movement while still delivering rich multisensory feedback to support operator decision-making?",
      "options": {
        "A": "Prototype A — uses HIDs (touchscreen and joystick) and provides visual/tactile/audio feedback, so it is a standard CUI and reduces some movement compared to keyboard-only controls.",
        "B": "Prototype B — uses HIDs (HMD, haptic gloves, audio and scent devices) and, by blocking the real world while interfacing sight, touch, sound and smell, is a 4-sense virtual reality CUI that reduces gross physical locomotion while offering rich multisensory feedback.",
        "C": "Prototype C — uses HIDs (AR glasses and physical controls), interfaces visual, tactile and auditory senses and is therefore a 3-sense augmented-reality CUI; it reduces head movement but not gross hand manipulation.",
        "D": "Prototype D — is a direct brain–computer interface (no conventional HIDs), minimizes physical movement the most, but does not qualify as a 4-sense virtual-reality CUI because it lacks the multimodal sensory rendering required by that taxonomy."
      },
      "correct_answer": "B",
      "generation_notes": "Created a concrete ROV-control scenario with four prototypes; each option maps to HMI layer membership, CUI taxonomy (standard/virtual/augmented/BCI), and usability tradeoffs about physical input and feedback. Only Prototype B meets all three listed criteria.",
      "concepts_tested": [
        "Usability-driven design and feedback (minimize input, provide effective multisensory feedback)",
        "Layered HMI architecture and HID role; classification of composite UIs (standard/virtual/augmented/qualia/BCI)"
      ],
      "source_article": "User interface",
      "x": 1.3844332695007324,
      "y": 1.0673096179962158,
      "level": 2,
      "original_question_hash": "59e55879"
    },
    {
      "question": "On a single-CPU, preemptive multitasking system with virtual memory (page size $4\\,\\text{KiB}$) and a DMA-capable disk controller, a process P issues a blocking system call to write a contiguous $1\\,\\text{MB}$ user buffer to disk. Some pages of that buffer are currently swapped out. Which chronological sequence below correctly describes what the operating system (kernel) and hardware must do from the moment P makes the write system call until P resumes after the write completes?",
      "options": {
        "A": "P executes the write syscall and traps to kernel; kernel validates the request and handles any page faults, bringing pages into RAM and pinning them; kernel creates a device-status table entry and programs the DMA controller with the physical addresses and size; kernel context-switches to the next ready process; DMA transfers the data to disk; disk controller raises an interrupt on completion; CPU invokes the interrupt handler which saves state, reads the interrupt vector, consults the device-status table, marks P as ready; scheduler runs and context-switches back to P, the kernel restores P's registers and the syscall returns.",
        "B": "P executes the write syscall and kernel immediately programs the DMA controller and starts transfer; DMA begins transferring while swapped-out pages cause page faults later and the kernel handles them mid-transfer; when DMA finishes the disk interrupts and the kernel completes the syscall and resumes P.",
        "C": "P executes the write syscall and kernel validates the buffer but, instead of using DMA, kernel busy-waits polling the device status until the entire transfer completes; after polling finishes kernel unblocks P and returns to user mode.",
        "D": "P executes the write syscall and kernel returns immediately to user mode (non-blocking); user process continues executing while the DMA copies data; when the disk interrupts, the interrupt handler directly writes into P's user-space stack to signal completion without consulting process control blocks or the scheduler."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete single-CPU DMA+virtual-memory scenario to test sequencing of syscalls, page faults/pinning, DMA setup, context switching, interrupts and scheduler behavior; provided four chronological options with one correct order.",
      "concepts_tested": [
        "Resource allocation and scheduling (CPU time, context switch)",
        "OS as abstraction layer and kernel responsibilities (validation, pinning, device-status table)",
        "Interaction mechanisms: system calls, page-fault handling, DMA, hardware interrupts and ISR/scheduler coordination"
      ],
      "source_article": "Operating system",
      "x": 1.4570271968841553,
      "y": 1.125169038772583,
      "level": 2,
      "original_question_hash": "e4c622bf"
    },
    {
      "question": "You are an HR manager evaluating three applicants for a software engineering role. Applicant X holds an exam-based certification from an independent, accredited body that requires renewal every 3 years. Applicant Y completed an instructor-led, education-based certification that is granted for life after satisfactory course completion. Applicant Z holds a certificate issued internally by their previous employer with no independent assessment. Based on the principles of certification, which of the following statements is MOST accurate?",
      "options": {
        "A": "An exam-based certification with independent accreditation and periodic renewal (Applicant X) guarantees complete mastery of the subject and therefore removes the need for any further assessment by the employer.",
        "B": "An education-based, instructor-led certification granted for life (Applicant Y) necessarily indicates deeper, more reliable knowledge than any exam-based certification, and is therefore a better predictor of higher salary outcomes than exam-based credentials.",
        "C": "A certificate issued internally without independent assessment (Applicant Z) can function as a marketing gimmick ('cheap talk') and provide little independent assurance; by contrast, third-party, accredited certifications with renewal requirements give stronger independent attestation of competence. Empirical studies also show certifications can correlate with average salary increases (e.g., about $15\\%$ to $20\\%$), but such outcomes are associations rather than guarantees of individual capability.",
        "D": "Licensure and certification are interchangeable: both are normally administered by the same governmental body, carry legal authority to restrict practice, and thus should always be prioritized over private or third-party certificates."
      },
      "correct_answer": "C",
      "generation_notes": "Constructed a hiring scenario contrasting exam-based vs education-based vs internally issued certificates; options probe renewal, external assessment, attestation limits, salary correlations, and licensure vs certification distinctions.",
      "concepts_tested": [
        "Certification as credentialing requiring external assessment and renewal",
        "Difference between exam-based and education-based certifications and limits of what certification attests",
        "Relationship between certification, accreditation/credibility, and correlated outcomes (salary) plus caveats about marketing-only certificates"
      ],
      "source_article": "Certification",
      "x": 1.4392492771148682,
      "y": 1.0226426124572754,
      "level": 2,
      "original_question_hash": "ea4454c2"
    },
    {
      "question": "A 6 mm-thick steel plate contains a central through-thickness crack of total length 20 mm (so half-crack length $a=10\\ \\text{mm}=0.01\\ \\text{m}$). The plate is subjected to cyclic uniaxial tensile loading between 0 and 200 MPa (so stress range $\\Delta\\sigma=200\\ \\text{MPa}$). Material properties: yield strength $\\sigma_Y=400\\ \\text{MPa}$, fracture toughness $K_{Ic}=80\\ \\text{MPa}\\sqrt{\\text{m}}$, and fatigue threshold $\\Delta K_{th}=10\\ \\text{MPa}\\sqrt{\\text{m}}$. For a central crack in a large plate use $Y=1$ (so $K=\\sigma\\sqrt{\\pi a}$). Which one of the following conclusions is correct?",
      "options": {
        "A": "Loading is Mode I (opening). $K_{max}=200\\sqrt{\\pi(0.01)}\\approx35.45\\ \\text{MPa}\\sqrt{\\text{m}}<K_{Ic}$ so immediate unstable fracture will not occur; however $\\Delta K=\\Delta\\sigma\\sqrt{\\pi a}\\approx35.45\\ \\text{MPa}\\sqrt{\\text{m}}>\\Delta K_{th}$ so the crack will grow by fatigue. The estimated plastic-zone radius $r_p=K_{Ic}^2/(2\\pi\\sigma_Y^2)\\approx6.37\\ \\text{mm}\\approx0.64a$, i.e. not small compared with $a$, so small-scale yielding (LEFM) is not strictly valid and elastic–plastic parameters (e.g. $J$ or CTOD) should be used for predicting unstable fracture (though $\\Delta K$ still indicates fatigue driving force).",
        "B": "Because $K_{max}=200\\sqrt{\\pi a}\\approx35.45\\ \\text{MPa}\\sqrt{\\text{m}}<K_{Ic}=80\\ \\text{MPa}\\sqrt{\\text{m}}$, neither fatigue nor static crack growth will occur; the crack is safe under the given loading.",
        "C": "The plastic-zone radius $r_p=K_{Ic}^2/(2\\pi\\sigma_Y^2)\\approx6.37\\ \\text{mm}$ is much smaller than $a=10\\ \\text{mm}$, hence small-scale yielding holds and LEFM with $K$ fully governs both fatigue and final fracture prediction.",
        "D": "The maximum stress intensity $K_{max}\\approx35.45\\ \\text{MPa}\\sqrt{\\text{m}}$ exceeds $K_{Ic}=80\\ \\text{MPa}\\sqrt{\\text{m}}$, so the plate will experience immediate fast fracture; fatigue effects are negligible."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete cyclic-loading scenario for a center crack; computed $K_{max}$, $\\Delta K$, and plastic-zone radius $r_p$ to test recognition of Mode I, fatigue threshold, LEFM validity, and when to use elastic–plastic parameters (J/CTOD).",
      "concepts_tested": [
        "Stress intensity factor (K) and Mode I loading",
        "LEFM vs elastic–plastic fracture mechanics (small-scale yielding criterion, J/CTOD)",
        "Fatigue crack driving parameter $\\Delta K$ and fracture threshold $K_{Ic}$ (failure criteria)"
      ],
      "source_article": "Fracture mechanics",
      "x": 1.7746047973632812,
      "y": 1.0256531238555908,
      "level": 2,
      "original_question_hash": "74ae6559"
    },
    {
      "question": "Two university research groups formalize mathematical frameworks to model physical space for competing physical theories. Group 1 builds theory T1 by taking: (i) the usual axioms of Euclidean geometry, (ii) a construction of the real numbers inside Zermelo–Fraenkel set theory with Choice (ZFC) via Dedekind cuts, and (iii) standard analysis developed from those reals. Group 2 builds theory T2 by replacing Euclid's parallel postulate with its negation (producing hyperbolic geometry) but otherwise using the same ZFC-based construction of the reals and analysis. Which of the following best captures the relation between axioms, mathematical truth, the historical foundational crisis, and the role of empirical reality in choosing axioms?",
      "options": {
        "A": "Physical measurement alone determines which axioms are ‘‘true’’. Therefore mathematicians must discard any axiom system whose theorems disagree with experiment; the axiomatic method is invalidated because truth is empirical rather than deductive.",
        "B": "Both T1 and T2 are legitimate formal theories: the axiomatic method makes objects and theorems depend on chosen axioms, and each theory yields provable statements from its axioms. The late-19th/early-20th century foundational crisis prompted the development of mathematical logic and axiomatic set theory (e.g. ZFC) to formalize and compare such systems. Empirical physics can guide the choice of axioms for modeling nature, but it does not change formal derivability inside a given axiomatic system.",
        "C": "Because ZFC is incomplete by Gödel, any geometric theorem about triangles cannot be proved in ZFC; therefore geometry must be recast in a second-order logic if one wants absolute mathematical truth about space.",
        "D": "Russell-style paradoxes show that any set-theoretic formalization of space is inconsistent unless one rejects infinite sets outright. Thus the only consistent foundations for mathematics applicable to physics are strictly intuitionistic, finitist systems that ban actual infinity."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete comparative scenario (Euclidean vs hyperbolic with ZFC-built reals) to probe: (1) axiomatic method defines objects and proves theorems; (2) historical foundational crisis led to mathematical logic and set theory; (3) empirical reality guides axiom choice but does not alter formal derivability. Options C and D present common misconceptions; A overstates empirical determination.",
      "concepts_tested": [
        "axiomatic method and axioms defining mathematical objects",
        "foundational crisis and rise of mathematical logic/set theory (ZFC)",
        "interplay between empirical guidance and axiomatic abstraction (role of reality in choosing axioms)"
      ],
      "source_article": "Foundations of mathematics",
      "x": 1.6007660627365112,
      "y": 1.1809203624725342,
      "level": 2,
      "original_question_hash": "d0de2ab6"
    },
    {
      "question": "You are a member of a university team preparing a policy brief to advise your city council on reducing the impact of election-related misinformation. Your dataset includes tweets, a peer-reviewed public‑policy article, a government press release, and a partisan blog. Which sequence of actions best exemplifies information literacy as an integrated set of abilities that (1) reflects on and discovers what information is needed, (2) understands how different information is produced and valued, (3) locates, evaluates and uses information effectively, and (4) participates ethically in the civic community?",
      "options": {
        "A": "Quickly aggregate as many items as possible from social media and blogs, quantify the volume of claims supporting each side, and present the highest‑frequency claims as indications of public concern to the council.",
        "B": "Run all sources through automated credibility‑scoring software, discard items below the software threshold, and use the remaining sources to draft recommendations—relying on the tool's ranking rather than manual assessment.",
        "C": "Start by articulating the specific policy questions and stakeholder needs; map each source's origin, purpose and audience; search intentionally for diverse, authoritative evidence; evaluate each source for authority, bias, currency and method; synthesize findings into evidence‑based recommendations; and disclose methods and limitations when presenting to the council and community.",
        "D": "Focus on teaching the council critical thinking heuristics—such as asking 'who benefits'—and let them decide which of the available sources to trust, without producing a synthesized brief or explicit source evaluation."
      },
      "correct_answer": "C",
      "generation_notes": "Designed a practical civic scenario requiring application of definitions and standards from the article; options isolate single aspects (volume metrics, automated tools, critical heuristics) versus the integrated, process‑oriented, socially responsible approach that is correct.",
      "concepts_tested": [
        "Integrated definition of information literacy (reflective discovery, production/valuation, creation of knowledge, ethical participation)",
        "Process model: recognize need, locate, evaluate, use information effectively",
        "Social and political role: democracy, civic participation, critical thinking, combating misinformation"
      ],
      "source_article": "Information literacy",
      "x": 1.204840064048767,
      "y": 0.9914726614952087,
      "level": 2,
      "original_question_hash": "c0dad26d"
    },
    {
      "question": "A startup's board (the principal) hires a CEO (the agent). There are two investor groups: venture-capital equity holders who gain from higher return volatility and a bank that provided debt and prefers lower volatility. The CEO privately knows her managerial ability, modeled as \\(\\theta\\in\\{\\theta_H,\\theta_L\\}\\) (adverse selection), and after hiring she chooses a project risk level \\(r\\) that is unobservable to the board (hidden action / moral hazard). The board cannot perfectly monitor \\(r\\). Which contractual package most directly (i) screens or mitigates adverse selection about \\(\\theta\\), (ii) reduces hidden-action risk-taking incentives over \\(r\\), and (iii) reconciles the conflicting preferences of multiple principals (equity and debt)?",
      "options": {
        "A": "Grant the CEO a very large, short‑dated stock‑option grant (high-powered equity incentives) with no probation period; rely on equity upside to discipline selection and effort.",
        "B": "Use a staggered contract that begins with a probationary/trial period and observable performance milestones (screening), require the CEO to post personal stake or deferred compensation (bonding/deferred pay), set moderate performance‑sensitive pay combined with independent monitoring and explicit debt covenants that limit excessive risk shifting.",
        "C": "Run an internal tournament among senior managers where promotion to CEO depends on relative rank; set pay so only the tournament prize differs—no separate monitoring or covenants.",
        "D": "Pay the CEO a fixed high salary (no performance pay) and leave project choice unconstrained, arguing that removing performance incentives prevents opportunistic risk‑taking."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete startup scenario combining adverse selection (private ability \\theta), moral hazard (hidden risk choice r), and multiple principals (equity vs debt). Provided four plausible contractual packages; answer B bundles screening, bonding/deferred pay, monitoring and covenants which together address the three core issues.",
      "concepts_tested": [
        "information asymmetry (adverse selection and hidden action)",
        "moral hazard vs adverse selection",
        "incentive alignment mechanisms (screening, bonding, monitoring, deferred compensation, debt covenants) and multiple-principal coordination"
      ],
      "source_article": "Principal–agent problem",
      "x": 1.3118821382522583,
      "y": 0.9875099062919617,
      "level": 2,
      "original_question_hash": "b9c9c077"
    },
    {
      "question": "A mid-sized country, Lalandia, has the following indicators: women do 70% of unpaid domestic labor, occupy 22% of parliamentary seats, and are frequently portrayed in popular media as sexualized objects. A ten-year program doubled girls' secondary school completion but the gender wage gap and occupational segregation remain large. The government must choose a policy package to reduce gender inequality. Which option best reflects the diagnosis and remedies emphasized by feminist theory in the provided article?",
      "options": {
        "A": "Continue and expand only girls’ formal education (scholarships, more schools), expecting higher attainment alone will eliminate discrimination and occupational segregation.",
        "B": "Pair sustained investment in girls’ education with legal reforms that dismantle patriarchal institutions (e.g., inheritance, custody, and employment law), public campaigns to challenge sexual objectification, and social supports (affordable childcare) to redistribute unpaid domestic labor.",
        "C": "Rely primarily on market incentives (tax breaks for firms hiring women, subsidies for female entrepreneurs) while avoiding changes to cultural norms or legal structures, assuming economic growth will erase gendered outcomes.",
        "D": "Promote assimilation strategies: train women to adopt typically male occupations and discourage ‘feminine’ cultural expressions so women emulate men’s behavior and thereby become equally competitive in the labor market."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete policy-choice scenario (Lalandia) to test assessing gender inequality, the role of patriarchy as structural mechanism, and education as part of a multi-pronged remedy. Options contrast isolated education, structural/legal reforms plus cultural change (correct), market-only fixes, and assimilationist strategies.",
      "concepts_tested": [
        "Gender inequality as central analytic problem linking social roles/experiences to unequal outcomes",
        "Patriarchy as a structural mechanism sustaining inequality (legal/cultural power relations, objectification)",
        "Education as a pathway for social change but insufficient without institutional and cultural reforms"
      ],
      "source_article": "Feminist theory",
      "x": 0.9060330986976624,
      "y": 0.8721317648887634,
      "level": 2,
      "original_question_hash": "fe6e66b5"
    },
    {
      "question": "Two neighbouring coastal states, X and Y, dispute a fishery worth V = 100 utility units. Military capabilities give X a probability of victory p = 0.6; Y's probability is 1 - p = 0.4. If they fight, the expected utility from war for X is $EU_X = 0.6\\times100 - 30 = 30$ (war cost $c_X=30$) and for Y is $EU_Y = 0.4\\times100 - 50 = -10$ (war cost $c_Y=50$). Instead of fighting, they sign a bilateral treaty establishing arbitration and monitoring; the negotiated agreement allocates 50 units to each state. Which theoretical explanation most plausibly accounts for why both states accepted the arbitration treaty rather than initiating war?",
      "options": {
        "A": "Neorealism: Because X is materially superior (p = 0.6), the anarchic structure compels X to consolidate power via binding agreements that formalize its dominance; Y accepted the treaty out of fear of future coercion rather than immediate material calculations.",
        "B": "Neoliberal institutionalism combined with the bargaining (rational choice) model: The expected-utility calculation shows a bargaining range (both get more from settlement: X gets 50 > EU_X = 30, Y gets 50 > EU_Y = -10) and the institution (arbitration + monitoring) reduces information asymmetries and enforcement costs, making a negotiated settlement mutually preferable under anarchy.",
        "C": "Constructivism: A rapid identity transformation between X and Y (they redefined themselves as maritime partners) explains the settlement; shared norms made the 50/50 split appropriate even though material power favoured X.",
        "D": "Selectorate/rent-seeking logic: X's leader signed the treaty because a small winning coalition demanded the short-term domestic rents and prestige that come from appearing cooperative, so the treaty is primarily an outcome of domestic political incentive structures rather than interstate bargaining or institutions."
      },
      "correct_answer": "B",
      "generation_notes": "Designed a concrete two-state dispute with numerical expected-utility calculations to force application of neorealist, neoliberal institutionalist, constructivist, and selectorate/rational-choice reasoning; correct option links bargaining-range math and institutional functions.",
      "concepts_tested": [
        "Competing IR frameworks: realism vs liberalism vs constructivism",
        "Structural vs institutional explanations (neorealism vs neoliberal institutionalism) and bargaining under anarchy",
        "Rational choice models: expected utility and bargaining model (selectorate referenced as alternative)"
      ],
      "source_article": "International relations theory",
      "x": 1.1946483850479126,
      "y": 0.9351183176040649,
      "level": 2,
      "original_question_hash": "5656c1cc"
    },
    {
      "question": "Four island constitutions are proposed after decades of inter-tribal violence used to justify leaving the \"state of nature.\" Which proposal best matches John Locke's social-contract theory regarding (1) why people leave the state of nature, (2) the source and form of political legitimacy (consent), and (3) the relation between natural rights and legal rights?",
      "options": {
        "A": "The inhabitants transfer all their rights to a single sovereign with absolute power so long as that sovereign secures order; arbitrary edicts are tolerated because any alternative would return them to the violent state of nature.",
        "B": "Each person claims they already possess natural rights; they agree to form a government that acts as a neutral judge to protect life, liberty and property, and they retain the right to remove rulers who violate those protections—consent to government legitimates its authority.",
        "C": "All individuals collectively alienate their individual rights to the community’s \"general will,\" which alone legislates; laws are binding because they express the common good and thus are the true expression of individual freedom.",
        "D": "Voluntary associations are formed in which each individual keeps full personal sovereignty and merely agrees not to coerce others; there is no surrender of authority to a centralized government and no presumption of collective legislation."
      },
      "correct_answer": "B",
      "generation_notes": "Presented four concrete constitutional proposals modeled on Hobbes, Locke, Rousseau, and Proudhon to test students' ability to map state-of-nature justification, consent-based legitimacy (including right to remove rulers), and the primacy of natural rights in Locke's theory.",
      "concepts_tested": [
        "state of nature as justificatory device",
        "consent-based legitimacy (explicit/tacit and right to revolt)",
        "natural rights versus legal rights (Locke's protection of life, liberty, property)"
      ],
      "source_article": "Social contract",
      "x": 1.207098126411438,
      "y": 0.9414944052696228,
      "level": 2,
      "original_question_hash": "d6515fe1"
    },
    {
      "question": "You are editing a 12th-century narrative surviving in seven medieval manuscripts (M1–M7) and an early printed edition (P, 16th c.) that claims to incorporate authorial revisions. Collation reveals: (a) M1–M3 share several identical omissions and a common misspelling not found elsewhere; (b) M4–M6 share a different set of identical errors plus a paragraph present only in these three (likely an interpolation); (c) M7 contains a mixture of readings from both groups; (d) phylogenetic analysis of variant loci clusters M1–M3 and M4–M6 into distinct clades but flags M7 as contaminated (mixed ancestry); and (e) there is no known autograph. Which editorial strategy best fits the evidence to reconstruct the archetype (or closest attainable exemplar) while documenting variants appropriately?",
      "options": {
        "A": "Apply strict stemmatics alone: treat the two manuscript clades as independent branches descended from a single archetype, select the dominant reading at each node to reconstruct the archetype, and emend where no witness preserves a plausible reading.",
        "B": "Choose the printed edition P as the copy-text and perform copy-text editing: correct its obvious typographical errors by consulting the manuscripts, but otherwise let P govern substantive and accidental readings because it claims authorial revision.",
        "C": "Use a hybrid method: employ phylogenetic results to build a stemma where relationships are clear, identify and treat M7 as a contaminated witness (exclude or weight it appropriately), use eclectic judgement (external/internal criteria such as lectio difficilior and lectio brevior) to choose between competing readings, record interpolations and variants in a full critical apparatus, and avoid privileging P unless independent evidence shows its revisions are genuinely authorial.",
        "D": "Adopt a Best-text (single-witness) approach: select the single manuscript judged ‘best’ (for example M1) and emend it minimally for obvious scribal corruptions, ignoring the printed edition and other manuscripts unless they supply manifest corrections."
      },
      "correct_answer": "C",
      "generation_notes": "Constructed a realistic manuscript-collation scenario with contamination and a later printed witness; options contrast stemmatics, copy-text, eclectic/phylogenetic hybrid, and best-text. Correct answer advocates combining phylogenetic detection of relationships/contamination with stemmatic/eclectic selection and full apparatus.",
      "concepts_tested": [
        "variation mechanisms in manuscript transmission (omissions, interpolations, contamination)",
        "methods of textual reconstruction (stemmatics, eclecticism, copy-text editing) and use of quantitative/phylogenetic techniques to detect relationships and guide editing"
      ],
      "source_article": "Textual criticism",
      "x": 0.7392001152038574,
      "y": 0.9891819953918457,
      "level": 2,
      "original_question_hash": "eec6b816"
    },
    {
      "question": "A national ethics committee must choose between two proposals. Proposal I: adopt a pro‑natalist policy that increases population from $1{,}000{,}000$ people with average utility $10$ to $11{,}000{,}000$ people by adding $10{,}000{,}000$ new lives each with average utility $1$. (Current total utility $=1{,}000{,}000\\times10=10{,}000{,}000$; after the change total utility $=20{,}000{,}000$ and average utility $\\approx1.82$.) Proposal II: in a single crisis, framing one innocent person would almost certainly avert riots that would otherwise kill $100$ people and produce long‑term social harm. Which of the following descriptions best matches the judgments and theoretical rationales given by the utilitarian variants discussed in the article?",
      "options": {
        "A": "Total utilitarianism would endorse Proposal I because it increases aggregate utility even though it lowers average utility; average utilitarianism would reject Proposal I because it reduces mean welfare. For Proposal II, act utilitarianism might permit framing the innocent if the immediate consequences maximize utility, whereas rule utilitarianism would forbid framing because a general rule against punishing innocents produces greater long‑run utility (avoids second‑order evils). Classical/egalitarian utilitarianism treats the suffering of all sentient beings as morally relevant and demands equal consideration.",
        "B": "Average utilitarianism would endorse Proposal I (it prioritizes population growth because more people means more total well‑being), and total utilitarianism would reject it (it prioritizes mean quality over quantity). For Proposal II, rule utilitarianism would allow framing in exceptional cases, while act utilitarianism would always prohibit framing to preserve justice. Utilitarianism gives stronger moral weight to human beings than to nonhuman animals.",
        "C": "Act utilitarianism would reject Proposal I because adding many low‑wellbeing lives dilutes the moral worth of existing people; rule utilitarianism would endorse Proposal I because a pro‑natalist rule increases aggregate economic productivity. For Proposal II, both act and rule utilitarians would categorically forbid framing an innocent because justice is a non‑consequential constraint. Utilitarianism excludes nonhuman animals from moral consideration.",
        "D": "Egoistic utilitarianism (a form of consequentialism) would endorse Proposal I only if the policy benefits the policymakers themselves; average utilitarianism would be indifferent. For Proposal II, all utilitarian variants unanimously prohibit framing because any action that violates justice cannot be justified on consequentialist grounds. Utilitarianism requires special priority for family and compatriots over strangers and nonhuman animals."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete two‑proposal scenario to probe distinctions: total vs average utility (population math), act vs rule utilitarianism (single‑act exceptions vs rule stability/second‑order evils), and the egalitarian stance on sentience. Options present plausible but distinct theoretical assignments; A matches article descriptions.",
      "concepts_tested": [
        "Utility as the consequentialist standard (maximizing happiness/welfare and aggregation)",
        "Variants and implementation mechanisms (act vs rule utilitarianism; total vs average utilitarianism)",
        "Egalitarian consideration of all sentient beings within utilitarianism"
      ],
      "source_article": "Utilitarianism",
      "x": 1.1843793392181396,
      "y": 0.9994367361068726,
      "level": 2,
      "original_question_hash": "0ab1f84a"
    },
    {
      "question": "You are given a CT scan of a fractured femur represented as a voxel grid of size $512^3$. Your task is to produce a model that meets these requirements: (1) preserve internal bone geometry so you can run accurate finite element analysis (FEM) for stress under load, (2) provide a smooth, manufacturable external surface for CAD/CAM and 3D printing, and (3) keep file size and editing complexity manageable for engineers and surgeons. Which modeling representation and algorithmic pipeline best balances these needs?",
      "options": {
        "A": "Extract a watertight polygonal surface from the voxel grid using Marching Cubes to produce a triangular mesh ($\\\\sim 10^5$ faces), perform topology repair and remeshing/simplification, fit smooth local NURBS or subdivision-surface patches for CAD export, and generate a tetrahedral volume mesh (Delaunay tetrahedralization) for FEM.",
        "B": "Fit a single low-degree implicit algebraic surface directly to the voxel intensities and use that implicit equation as both the CAD model and the domain for FEM, avoiding explicit tessellation to minimize file size.",
        "C": "Encode the femur as a procedural, hierarchical fractal/parametric generator whose parameters reproduce the CT appearance at runtime; render a parametric surface for visualization and attempt to run FEM directly on the procedural description to exploit compact storage and infinite detail.",
        "D": "Keep the model as a downsampled voxel volume (apply Gaussian smoothing and thresholding), then use the voxel representation directly for both 3D printing and FEM to avoid costly surface reconstruction and file conversion."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete engineering/medical scenario requiring tradeoffs between volumetric, polygonal, and parametric/B-rep representations. Option A describes the standard pipeline (voxel → marching cubes → mesh repair → parametric fitting/subdivision + tetrahedralization) that links geometric-modeling algorithms to CAD and FEM needs; other options illustrate common but unsuitable alternatives.",
      "concepts_tested": [
        "Representation paradigms (volumetric, polygonal/mesh, parametric/B-rep, procedural/fractal)",
        "Role of algorithms (marching cubes, mesh repair, remeshing, NURBS/subdivision fitting, tetrahedralization) in converting raw data into usable computer models",
        "How modeling choices affect cross-domain uses (CAD/CAM manufacturability, FEM accuracy, file size and editability in engineering/medical contexts)"
      ],
      "source_article": "Geometric modeling",
      "x": 1.6471272706985474,
      "y": 1.1639413833618164,
      "level": 2,
      "original_question_hash": "c39516c7"
    },
    {
      "question": "BioHeat is a social enterprise that produces improved cookstoves for rural villages. Today its revenue mix is 40% direct sales, 30% grants, and 30% impact loans. The leadership wants to scale to 100,000 households over five years while preserving mission integrity. Which strategic design most coherently implements the \"double bottom line\", adopts a sustainable hybrid funding model, and leverages Internet-based networks and crowdfunding to enable scale?",
      "options": {
        "A": "Formalize as a hybrid non-profit (cross-subsidy model) that legally allows reinvestment of profits from sales to subsidize the poorest customers; adopt a dual measurement system that reports both social metrics (e.g., average reduction in household PM2.5, CO2-equivalent emissions per stove, and incidence of respiratory illness) and financial metrics (e.g., operating margin target of at least $10\\%$ and earned-income covering ≥ $60\\%$ of operating costs); and run targeted crowdfunding pre-sale campaigns and social-media networks to recruit backers, pre-sell units, crowdsource local distribution partners, and disseminate impact data.",
        "B": "Convert to a pure non-profit that stops selling stoves and distributes them free using grants and donations; measure success only by number of beneficiaries reached and qualitative testimonials; use social media primarily for awareness campaigns but avoid commercial crowdfunding to preserve donor confidence.",
        "C": "Switch to a pure for-profit social business venture, focus metrics on ROI and customer lifetime value, finance scale through equity crowdfunding and venture investors, and use online marketing to maximize paid sales—while treating social outcomes as secondary PR metrics rather than core performance indicators.",
        "D": "Remain an informal mix of grants, loans and sales but prioritize maximizing sales volume as the primary metric (units sold per month); rely on local fundraising events and traditional donor networks instead of online platforms; emphasize short-term cost-reduction over systematic social-impact measurement."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete cookstove scaling scenario requiring selection of an integrated strategy that aligns double bottom line metrics, hybrid funding (cross-subsidy / earned income), and use of crowdfunding/social networks; distractors each violate one or more core concepts.",
      "concepts_tested": [
        "double bottom line metrics (social and financial)",
        "hybrid organizational/funding models (cross-subsidy, earned income)",
        "role of networked platforms and crowdfunding for scaling and collaboration"
      ],
      "source_article": "Social entrepreneurship",
      "x": 1.276201844215393,
      "y": 0.9591671824455261,
      "level": 2,
      "original_question_hash": "7de42dfa"
    },
    {
      "question": "GreenHarbor Cooperative is a member-owned organization with 120 voluntary members who contributed a total of $120,000 in member capital. The cooperative also accepted a $30,000 one-time grant from the municipal government for harbor restoration projects, so total capital is $150,000. Last year GreenHarbor generated $50,000 in operating surplus and reinvested $45,000 into community training and low-cost services for disadvantaged fishers. Governance is one-member/one-vote, the board is elected by members, and day-to-day management is autonomous (no government appointments). Using the social enterprise compass (horizontal axis: ownership from public at left to private at right; vertical axis: primary objective from social at top to commercial at bottom) and the social-economy principles from the article, which of the following best classifies GreenHarbor?",
      "options": {
        "A": "A social-economy cooperative located in the upper-right of the compass (private ownership bias, strong social purpose). Private ownership share is $\n\\frac{120{,}000}{150{,}000}=0.8$ and reinvestment rate is $\\frac{45{,}000}{50{,}000}=0.9$, while democratic governance and autonomous management satisfy social-economy criteria despite receiving a public grant.",
        "B": "A social-economy organization located in the upper-left of the compass (public ownership bias, strong social purpose) because it received municipal funding, which makes it primarily publicly owned and thus closer to the public sector.",
        "C": "A private commercial enterprise located in the lower-right of the compass (private ownership, commercial purpose) because it retained any surplus before reinvestment and therefore functions like a profit-seeking firm rather than a social organisation.",
        "D": "A hybrid organisation located in the middle of the compass because receiving any government grant disqualifies it from being primarily private or primarily social; the grant makes its ownership and purpose indeterminate."
      },
      "correct_answer": "A",
      "generation_notes": "Created a numeric scenario with capital and surplus figures to require calculation of private-ownership share and reinvestment rate; asked for compass placement and evaluation of governance/autonomy to test identification as a social-economy cooperative.",
      "concepts_tested": [
        "Value-driven reinvestment and primacy of social objectives over capital",
        "Democratic governance and participatory/autonomous management",
        "Mapping organisations on the social enterprise compass and cross-sector collaboration"
      ],
      "source_article": "Social economy",
      "x": 1.2462927103042603,
      "y": 0.9271208643913269,
      "level": 2,
      "original_question_hash": "6150d013"
    },
    {
      "question": "In the fictional island society of Lirea the following apply: girls typically experience menarche around age $13$ and, by local custom, are considered marriage-ready at first menses; boys enter physical puberty at about $13$ but are not socially deemed able to start families until they have completed economic training and can support a household. The state legally recognizes full adult civil rights at age $18$, but many upper-class youths complete formal schooling and enter bureaucratic posts at age $16$, while lower-class youths typically begin apprenticeships at age $12$ and are expected to assume adult work and family roles by about age $14$. Lirea also has an optional communal rite at age $15$ that some families use to mark transition. Which of the following statements best integrates the three core dynamics discussed in the article — biological maturation, legal/ceremonial markers, and socioeconomic modulation of adulthood timing?",
      "options": {
        "A": "Biological signals such as menarche often serve as culturally recognized triggers for female marriage-readiness, but formal legal majority and ceremonial rites may confer separate civic rights; socioeconomic status mediates timing and roles so upper-class formal education can postpone some adult responsibilities while lower-class apprenticeships accelerate practical entry into adult economic and familial roles.",
        "B": "Legal age of majority is the sole determinant of adulthood in Lirea: once a person reaches $18$ they are an adult in every social and economic sense, irrespective of puberty, rites, or class-based training pathways.",
        "C": "Communal rites like the age-$15$ ceremony universally replace both legal and biological markers, so anyone who participates in the rite is immediately granted all adult rights and obligations regardless of their socioeconomic background or physical maturity.",
        "D": "Socioeconomic status affects only the type of adult occupation obtained (bureaucracy versus apprenticeship) but does not influence when youths are considered adults; biological maturation and the legal age solely determine adult status in Lirea."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete island scenario combining menarche-based female adulthood, male economic-readiness, legal majority, optional rite, and class differences; options contrast integrated explanation (correct) with over-simplified alternatives.",
      "concepts_tested": [
        "Cultural interpretation of biological maturation (menarche/spermarche) as social markers",
        "Distinction and interaction between legal, ceremonial, and biological markers of adulthood",
        "Influence of socioeconomic status on timing and pathways to adult roles (apprenticeship vs formal education)"
      ],
      "source_article": "Coming of age",
      "x": 1.1817396879196167,
      "y": 0.882011353969574,
      "level": 2,
      "original_question_hash": "e5e3fa16"
    },
    {
      "question": "A regional forestry agency manages a 10,000 ha landscape currently dominated by even-aged monoculture pine plantations used for pulp. Problems include low biodiversity, degraded stream water quality, declining local employment, high dependence of nearby villages on fuelwood, and carbon emissions from routine clearcutting and slash burning. The agency must redesign management to follow the multiple-use management principle and apply silvicultural practice informed by ecological and agroecological principles. Which of the following proposed management plans best meets those objectives while balancing timber production, ecosystem services (habitat, water quality, carbon storage), and local livelihoods?",
      "options": {
        "A": "Intensify current monoculture operations: shorten rotation lengths to increase annual timber yield, increase clearcut area to $80\\%$ of the estate, deploy high-yield clonal provenances, and centralize mechanized harvesting to boost short-term employment and revenue; reserve $20\\%$ as small isolated conservation plots.",
        "B": "Convert the entire 10,000 ha to strict protection (no harvesting or fuelwood collection) to maximize carbon sequestration and biodiversity; develop ecotourism infrastructure to create alternative employment, and prohibit community use of forest products.",
        "C": "Adopt a zoned multiple-use silvicultural strategy: restore and maintain $40\\%$ as natural mixed-species, uneven-aged stands managed with selective harvesting and long rotations; establish $40\\%$ as mixed-species plantations using locally adapted provenances to preserve genetic diversity and agroforestry understories for fuelwood and non-timber products; set $10\\%$ as riparian buffers for water-quality and fisheries protection; and allocate $10\\%$ to community-managed fuelwood/recreation areas, using continuous-cover silviculture and promoting local employment.",
        "D": "Replace all forest with intensive tree–crop agroforestry and short-rotation coppice to maximize food and bioenergy production; allow private companies to manage timber rights under intensive inputs (fertilizer, irrigation); remove restrictions on fuelwood collection in non-productive remnants."
      },
      "correct_answer": "C",
      "generation_notes": "Created a scenario requiring application of multiple-use management and silvicultural/ecological principles; crafted four realistic management plans that trade off timber, biodiversity, water quality, carbon, and livelihoods; option C integrates silviculture, genetic diversity/provenance, riparian buffers, and community use.",
      "concepts_tested": [
        "Multiple-use management balancing timber and ecosystem services",
        "Silviculture applying ecological and agroecological principles to plantations and natural stands",
        "Forests as providers of ecosystem services and livelihoods (carbon sequestration, biodiversity, employment, fuelwood dependence)"
      ],
      "source_article": "Forestry",
      "x": 1.684791088104248,
      "y": 0.918592631816864,
      "level": 2,
      "original_question_hash": "7cc258ea"
    },
    {
      "question": "A start-up, AeroBrew, plans to launch a new single‑serve premium coffee machine nationally. Market research divides consumers into three segments with these profiles:\n\n- Segment X (Convenience Seekers): $45\\%$ of market, growth $2\\%/\\text{yr}$, price‑sensitive, prefer e‑commerce distribution, low brand loyalty, contribution margin per unit $\\$10$. \n- Segment Y (Home Baristas): $30\\%$ of market, growth $8\\%/\\text{yr}$, willing to pay a premium for advanced features and quality, prefer specialty retail and boutique online channels, high brand loyalty, contribution margin per unit $\\$30$. \n- Segment Z (Budget Families): $25\\%$ of market, growth $5\\%/\\text{yr}$, buy in high volume through mass retailers, highly price‑sensitive, contribution margin per unit $\\$8$. \n\nAeroBrew has limited marketing and manufacturing resources and must choose an STP strategy (Segmentation → Targeting → Positioning) and a matching marketing mix. Which of the following options best follows the STP framework, reflects the heterogeneity of demand, and allocates resources toward a high‑yield, growing segment?",
      "options": {
        "A": "Select Segment Y as the primary target (concentrated differentiated strategy). Position AeroBrew as a premium, feature‑led brand for Home Baristas; set higher price points, emphasize product quality and advanced features in specialty retail and boutique e‑commerce channels, use influencer and trade‑media promotion, and allocate most R&D and production capacity to premium variants.",
        "B": "Pursue an undifferentiated (mass) strategy targeting all segments with a single low‑price product because Segment X is the largest ($45\\%$); distribute broadly through e‑commerce and mass retailers, run broad price promotions, and focus on maximizing unit volume rather than feature differentiation.",
        "C": "Target both Segment X and Z equally with a value‑oriented dual distribution approach: build a mid‑range product priced moderately, sell via mass retailers for Z and via e‑commerce for X, and run identical price‑led promotions for both segments to capture maximum share while delaying specialized features for future iterations.",
        "D": "Simultaneously target all three segments with three distinct product variants and separate positioning for each (premium, mid‑range, budget). Invest heavily now in multiple distribution channels (specialty stores, e‑commerce, mass retailers) and tailor separate advertising for each segment despite limited resources."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete STP decision scenario with segment sizes, growth, margins, and channel preferences to test choosing a high‑yield target, tailoring the marketing mix (offer, price, promotion, distribution), and following Segmentation→Targeting→Positioning given resource constraints.",
      "concepts_tested": [
        "Heterogeneity in demand necessitates tailored marketing mixes (offers, price, promotion, distribution)",
        "S‑T‑P framework application (Segmentation → Targeting → Positioning)",
        "Identification and prioritization of high‑yield/growing segments for resource allocation"
      ],
      "source_article": "Market segmentation",
      "x": 1.3493484258651733,
      "y": 0.9730769991874695,
      "level": 2,
      "original_question_hash": "4298fbcb"
    },
    {
      "question": "Which of the following summaries best captures the account given in the article of how science changed from the Scientific Revolution through the 20th century?",
      "options": {
        "A": "The Scientific Revolution entailed a methodological and worldview shift toward mechanistic explanations and mathematical formalization (e.g., laws expressed like $F=ma$), together with an empirical, repeatable scientific method; later ‘‘revolutions’’ (for example the chemical revolution, Darwinian evolution, and the rise of genetics) illustrate non‑linear, paradigm‑reframing changes rather than simple cumulative progress; and in the 20th century the increasing technical complexity and wartime/industrial priorities produced ‘‘big science’’—large, often state‑funded laboratories and coordinated projects.",
        "B": "Science developed as a linear extension of Greek thought: the Scientific Revolution simply refined ancient ideas without a fundamental methodological change; subsequent advances were incremental refinements rather than revolutions; and the growth of large research enterprises was driven primarily by private markets and philanthropic patrons rather than by state or military needs.",
        "C": "The Scientific Revolution rejected mathematics and mechanistic explanation in favour of renewed qualitative natural philosophy; the chemical and biological transformations of later centuries were minor technical corrections; and the era of large‑scale, centralized research facilities began in the early 19th century as a direct consequence of industrial corporations, not mid‑20th century state or military agendas.",
        "D": "Major shifts in science are best explained solely by isolated individual geniuses producing new logical proofs; institutional and social factors (scientific societies, journals, state patronage, wartime mobilization) played little role, and the 20th‑century increase in scale of research was unrelated to industrial or military requirements."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a comparative MCQ summarizing the article’s three target themes: methodological/worldview shift in the Scientific Revolution (mechanistic, mathematical, scientific method), non‑linear paradigm changes (chemical revolution, evolution, genetics), and the rise of big science driven by industrial/military/state factors in the 20th century. Distractors preserve plausibility but violate one or more themes.",
      "concepts_tested": [
        "Scientific Revolution: mechanistic worldview and mathematical integration",
        "Non‑linear scientific revolutions and paradigm change",
        "Societal/infrastructural drivers of 20th century 'big science'"
      ],
      "source_article": "History of science",
      "x": 0.8791280388832092,
      "y": 0.2529872953891754,
      "level": 2,
      "original_question_hash": "16c7f1a5"
    },
    {
      "question": "A quantitative trait is measured in two human populations, A and B. The estimated genetic variance is Var(G)=20 in both populations. In population A the combined environmental variance (shared + non-shared + measurement error) is Var(E)=5, while in population B Var(E)=45. Using Var(P)=Var(G)+Var(E) and the broad-sense heritability formula $H^{2}=\\dfrac{\\mathrm{Var}(G)}{\\mathrm{Var}(P)}$, which option gives the correct numerical heritabilities and the best interpretation about what those heritabilities mean for population specificity, environmental influence, and implications for study designs (twin studies, GWAS)?",
      "options": {
        "A": "Correct computations: $H^{2}_{A}=\\dfrac{20}{20+5}=0.80$ and $H^{2}_{B}=\\dfrac{20}{20+45}\\approx0.31$. Interpretation: the higher $H^{2}$ in A reflects lower environmental variance in A, not that 80% of any individual’s trait is ‘‘caused’’ by genes. Heritability is population- and environment-specific; it can change when environmental variance changes. Molecular methods (GWAS) can produce lower ‘‘genomic heritability’’ due to limited variant capture and additive assumptions, and twin/ANOVA methods rely on additivity and can confound shared environment, dominance or G×E if assumptions are violated.",
        "B": "Correct computations but wrong interpretation: $H^{2}_{A}=0.80$ and $H^{2}_{B}\\approx0.31$, therefore genes causally determine 80% of the trait in any individual in A and 31% in any individual in B; the trait in A is largely immutable by environmental change; twin or GWAS results simply confirm these percentages.",
        "C": "Incorrect computation and interpretation: because Var(G) is the same, heritability must be equal in both populations (so $H^{2}=0.5$ in each) — heritability depends only on genetics, not environment; therefore population differences are irrelevant and GWAS estimates should be identical.",
        "D": "Partly correct numerics but misleading conclusion: $H^{2}_{A}=0.80$ implies strong potential for response to artificial selection, so selection will produce a large change regardless of whether the additive variance is large; twin studies and GWAS assumptions are not relevant to this conclusion."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a numerical two-population scenario to force computation of broad-sense heritability and to probe correct conceptual interpretation about population-specificity, environmental contribution, and methodological caveats (twin studies, GWAS, additivity).",
      "concepts_tested": [
        "Heritability is population- and environment-specific",
        "Partitioning phenotypic variance into genetic and environmental components and interpretation"
      ],
      "source_article": "Heritability",
      "x": 1.8136307001113892,
      "y": 1.1265391111373901,
      "level": 2,
      "original_question_hash": "9c580995"
    },
    {
      "question": "A manufacturing firm operates with production function $Q=L^{0.6}K^{0.4}$. At a point in time it employs $L=125$ and its capital stock is $K=64$, composed of $60$ units of machinery (physical capital) and $4$ units of an enduring software license (intangible capital). The firm can either (i) spend 16 units on raw materials and electricity that will be consumed entirely in the current production cycle, or (ii) purchase a new machine that raises the capital stock by 16 units (so $K$ becomes 80); the new machine depreciates at 10% per period thereafter. Which of the following statements is correct?",
      "options": {
        "A": "The raw materials/electricity are intermediate goods (consumed within one cycle) and do not raise $K$; the machine is a capital good (durable) and the software counts as intangible capital. Buying the machine raises $K$ to 80 and increases output from approximately $Q_0\\approx125^{0.6}\\cdot64^{0.4}\\approx97.5$ to $Q_1\\approx125^{0.6}\\cdot80^{0.4}\\approx104.6$, with the machine providing productive services over multiple cycles despite 10% depreciation.",
        "B": "Spending on raw materials and electricity is equivalent to increasing $K$ by 16 because inputs used to produce goods always add to the capital stock; thus $K$ becomes 80 and output rises to $Q\\approx104.6$, while buying the machine would simply be treated as an intermediate expense and not change long-run output.",
        "C": "Only tangible machinery counts toward the capital stock, so the software should be excluded (initially $K=60$). Buying the machine would raise $K$ to 76 and increase current output accordingly, whereas purchases of raw materials never affect measured output in the production function.",
        "D": "Both the electricity/raw materials purchase and the new machine increase capital because any expenditure that improves production capacity is capital; depreciation makes the machine equivalent to an intermediate good after one period, so there is no multi-period service flow from the machine."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete Cobb–Douglas scenario with numeric L and K, included both physical and intangible capital, contrasted an intermediate (consumed) expenditure with a durable capital purchase, and required identification of classifications plus computation of Q before and after the capital increase.",
      "concepts_tested": [
        "Durability and multi-period flow of productive services (capital vs intermediate goods)",
        "Heterogeneous capital stock including tangible and intangible assets",
        "Role of capital K as an input in a production function (e.g., Cobb–Douglas) and effect of changes in K on output Q"
      ],
      "source_article": "Capital (economics)",
      "x": 1.3068037033081055,
      "y": 0.9287669658660889,
      "level": 2,
      "original_question_hash": "42c8a4ee"
    },
    {
      "question": "A tertiary hospital implements a new standardized protocol: all patients with advanced colorectal cancer receive a next-generation sequencing (NGS) tumor panel, and an algorithm trained on genomic, transcriptomic and proteomic features recommends a targeted therapy. The algorithm was validated in peer-reviewed biological research but its training cohort was 85% patients of European ancestry. Some senior clinicians argue for overriding the algorithm based on clinical experience for non-European patients. Which of the following best characterizes the biomedical issues in this scenario, consistent with the article?",
      "options": {
        "A": "Biomedicine emphasizes standardized, evidence-based interventions derived from biological research and seeks to translate multi-level omics (genome, transcriptome, proteome) into diagnostics and therapies; however, its implementation can be shaped by socio-cultural factors (clinical norms, research cohort composition, resource allocation) that may introduce bias and affect equity.",
        "B": "Because biomedicine is strictly objective and free of cultural influence, the hospital should always follow the algorithm for all ancestry groups regardless of training-cohort composition or clinician concerns.",
        "C": "Biomedicine's primary objective is improving overall wellness and social wellbeing rather than curing specific diseases, so the hospital should prioritize patient-reported quality-of-life measures over molecularly targeted therapy.",
        "D": "Biomedicine requires that clinicians always override standardized, evidence-based algorithms when their personal clinical experience suggests a different treatment, because individual judgement supersedes population-derived validation."
      },
      "correct_answer": "A",
      "generation_notes": "Created a clinical vignette with NGS/omics-driven decision-making, an evidence-validated algorithm with ancestral bias, and clinician disagreement to probe understanding of evidence-based standardization, translational multi-omics linkage to therapy, and socio-cultural sources of bias.",
      "concepts_tested": [
        "Evidence-based standardized biomedical practice",
        "Translational linkage of multi-omics (genome, transcriptome, proteome) to diagnosis and targeted therapy",
        "Biomedicine as a socio-cultural system with norms and potential biases influencing implementation"
      ],
      "source_article": "Biomedicine",
      "x": 1.9979579448699951,
      "y": 1.1001492738723755,
      "level": 2,
      "original_question_hash": "9f7b2bdc"
    },
    {
      "question": "In the state of Ruritania, security forces publicly detain and parade opposition leaders on national television (highly visible direct repression). Simultaneously, the intelligence services run a large-scale covert surveillance program (wiretaps, metadata analysis) that is unknown to most citizens. The government also consolidates broadcast media under state ownership and passes a \"Public Order\" law that criminalizes 'destabilizing speech.' Which of the following best explains the expected differences in dissident behavior and regime outcomes, given scholarship on political repression?",
      "options": {
        "A": "The covert surveillance will encourage dissidents to shift toward less detectable, clandestine tactics and reduce observable protest activity, while the televised arrests (direct repression) are likely to increase public awareness of state violence and can catalyze sympathy-based mobilization. State ownership of media and the \"Public Order\" law institutionalize repression, produce preference falsification and self-censorship, and thereby strengthen the leader–citizen power hierarchy, increasing regime durability despite creating latent grievances.",
        "B": "Both the covert surveillance and the televised arrests will equally reduce dissident capacity and visible mobilization; state media consolidation and punitive law simply formalize repression but have no measurable effect on citizens' willingness to mobilize or on regime survival because repression alone fully determines outcomes.",
        "C": "Televised arrests of opposition leaders will primarily demobilize potential challengers because visible punishment demonstrates the futility of dissent, whereas covert surveillance will have little behavioral effect because citizens do not know it exists; therefore the combination will rapidly erode opposition and shorten the time to regime consolidation, with media and law playing a negligible reinforcing role.",
        "D": "Covert surveillance is more likely to provoke immediate mass protests because secrecy breeds rumor and panic, while public arrests calm the population by clarifying who is punished; state-controlled media and the \"Public Order\" statute undermine repression by increasing international scrutiny and legal challenges, thereby reducing regime longevity."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete country scenario comparing visible (direct) vs covert repression, and asked for their differential effects plus institutional reinforcement via media and law; options contrast plausible but incorrect inferences to test nuanced understanding.",
      "concepts_tested": [
        "Direct vs covert repression mechanisms and effects on dissident tactics and visibility",
        "Repression as mechanism for regime stability and creation of power hierarchy",
        "Institutional and informational reinforcement: media ownership, self-censorship, and legal framing"
      ],
      "source_article": "Political repression",
      "x": 1.2079788446426392,
      "y": 0.8986259698867798,
      "level": 2,
      "original_question_hash": "d13ed000"
    },
    {
      "question": "The fictional República de Sombra has undergone institutional changes over five years. Which of the following bundles of government measures most clearly demonstrates an authoritarian consolidation that uses (1) limited political pluralism to constrain opposition, (2) ill-defined or shifting executive powers to extend control, and (3) legitimacy strategies coupled with minimal political mobilization (i.e., emotional appeals, portrayal of the regime as a “necessary evil,” and avoidance of large-scale popular mobilization)?",
      "options": {
        "A": "The ruling party requires new registration rules that disqualify many opposition lists, the president issues broadly worded emergency decrees that bypass parliamentary review and are renewed irregularly, state media runs continual emotive stories about internal threats and economic stability while public political activity is tolerated only in tightly supervised civic groups.",
        "B": "The government lowers barriers to party registration and invites international monitors, the constitution fixes clear executive term limits and judicial review procedures, and the president embarks on frequent mass rallies appealing directly to popular support.",
        "C": "Elections remain legally open but the state deploys large-scale mandatory patriotic rallies, the executive’s powers are precisely codified in a new statute, and opposition leaders are legally prosecuted after selective criminal investigations publicized as anti-corruption efforts.",
        "D": "Independent courts strike down arbitrary detentions, an active opposition legislature blocks many executive proposals, the ministry of information is privatized, and the government commissions academic studies to justify slow economic reforms."
      },
      "correct_answer": "A",
      "generation_notes": "Created a realistic country vignette and presented four plausible policy bundles; option A combines restricted pluralism, vague executive emergency powers, and emotional legitimacy with low mobilization—matching the three targeted concepts.",
      "concepts_tested": [
        "Limited political pluralism and opposition constraint",
        "Ill-defined/shifting executive powers and centralized authority",
        "Legitimacy strategies plus minimal political mobilization (emotive appeals, “necessary evil”, controlled mobilization)"
      ],
      "source_article": "Authoritarianism",
      "x": 1.180998682975769,
      "y": 0.8975183367729187,
      "level": 2,
      "original_question_hash": "ef356c07"
    },
    {
      "question": "On 1 June two cargo vessels collided in the exclusive economic zone (EEZ) 30 nautical miles off the coast of State M (i.e. beyond the 12 nm territorial sea). Vessel Alpha is registered in State R and suffers damage; Vessel Beta is registered in State S. Three crew members injured aboard Alpha are nationals of State T. The collision caused a large oil spill that polluted State M's shoreline. State R commences criminal proceedings in its domestic courts against Beta's captain for negligent navigation. State M announces civil and administrative proceedings for remediation and fines. States R and M are parties to a regional maritime convention creating a compulsory arbitral tribunal; State S is not a party and refuses to participate. According to principles reflected in the Lotus decision and general international law, which statement is most accurate about jurisdiction and the role of the supranational tribunal?",
      "options": {
        "A": "More than one State may lawfully prescribe jurisdiction: State R may prosecute under the nationality (active personality) principle for conduct affecting its-flagged vessel and nationals; State M may assert jurisdiction to regulate and seek civil remedies for pollution affecting its coast (coastal-state environmental jurisdiction in the EEZ) even though the area is not sovereign territory; State S's criminal liability before R depends on custody or consent; and the regional arbitral tribunal is binding only on parties that consented (R and M) and cannot compel S to submit or enforce an award against S without S's cooperation.",
        "B": "Only the flag State S has exclusive criminal jurisdiction over Beta and therefore R's prosecution is unlawful; State M may pursue only limited administrative measures but not civil claims; the regional tribunal can nonetheless bind S because the pollution affected M's shores.",
        "C": "State M's jurisdiction is exclusive because the environmental harm touched its coastline and the EEZ confers full territorial jurisdiction out to 200 nm, so only M may bring criminal or civil proceedings; the regional tribunal's award binds all neighbouring States as a matter of customary international law.",
        "D": "Because the collision occurred beyond the 12 nm territorial sea, no State has territorial criminal jurisdiction and only the supranational tribunal may resolve the dispute; the tribunal may bind State S despite S's non‑consent if the damage affected another State's territory."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete maritime collision in an EEZ to test territorial vs extraterritorial jurisdiction (Lotus), nationality and coastal-state principles, and the limits of supranational arbitration (consent and enforcement).",
      "concepts_tested": [
        "Scope of jurisdiction: territorial and subject‑matter limits, standing and adjudicative power",
        "Territoriality vs extraterritorial jurisdiction (Lotus case, nationality and coastal‑state environmental jurisdiction)",
        "Interaction with supranational bodies: consent to arbitration and limits on enforcement"
      ],
      "source_article": "Jurisdiction",
      "x": 1.2445733547210693,
      "y": 0.8175446391105652,
      "level": 2,
      "original_question_hash": "487468fb"
    },
    {
      "question": "A regional coalition called CareNet is formed by ten autonomous hospitals, three NGOs, and two private home-care firms to coordinate long-term eldercare across three neighboring countries. They do not use formal legally enforceable contracts or price competition; instead members rely on informal, open-ended agreements, share case knowledge across organizations, negotiate treatment protocols in multi-stakeholder meetings, and enforce compliance through reputational sanctions and jointly developed norms. Over two years CareNet shows faster local innovation in care pathways and lower duplication of specialist assessments than the centralized national health agency. Which explanation best captures the governance features that most likely produced CareNet's superior responsiveness and innovation compared with a hierarchical public agency or a competitive market?",
      "options": {
        "A": "CareNet’s informal, socially binding agreements and pluricentric decision-making allow distributed knowledge sharing and negotiation-based coordination; trust and self-constituted norms replace legal enforcement, enabling rapid local adaptation and collective problem-solving.",
        "B": "CareNet outperforms the state because informal agreements eliminate all transaction costs associated with contracting, allowing members to respond instantly to shocks without any negotiation or deliberation.",
        "C": "CareNet’s improvements are primarily due to price competition among the private home-care firms within the coalition, which drives down costs and forces innovation in care delivery faster than any collaborative governance effort could.",
        "D": "CareNet succeeds because it effectively replicates hierarchical control: a single lead hospital imposes standardized protocols and discipline across members through formal sanctions and legally binding directives, producing centralized efficiency."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete multi-actor CareNet scenario to test recognition of network governance features (informal open-ended agreements, pluricentric negotiation, distributed knowledge, trust-based compliance) versus markets or hierarchies; distractors reflect common misconceptions.",
      "concepts_tested": [
        "Network governance as distinct coordinating mechanism (autonomous actors, socially binding open-ended contracts versus markets/hierarchies)",
        "Mechanisms and outcomes of governance networks (pluricentric negotiation rationality, distributed knowledge, trust-based compliance, adaptive efficiency and collective problem-solving)"
      ],
      "source_article": "Network governance",
      "x": 1.2582547664642334,
      "y": 0.9372788667678833,
      "level": 2,
      "original_question_hash": "db098d25"
    },
    {
      "question": "A chemical plant monitors a critical compressor using a predictive maintenance system. Sensors record vibration and temperature every 5 minutes; preprocessing removes baseline drift and computes envelope and spectral features. An anomaly detector raises an alarm when envelope energy exceeds a historical baseline, and a machine‑learning regression model then estimates remaining useful life (RUL) = $12$ days. A maintenance scheduler uses the RUL and resource availability to plan an intervention on day $10$, before the compressor would exceed a performance threshold. Which of the following best characterizes this approach?",
      "options": {
        "A": "This is predictive maintenance: it uses current condition measurements (sensor data) rather than fixed-life statistics, it schedules maintenance at a cost-effective time to convert a potential unplanned stop into a planned one, and it follows the correct pipeline (data collection → preprocessing → early fault detection → time‑to‑failure prediction → maintenance scheduling).",
        "B": "This is time‑based preventive maintenance because maintenance is being scheduled in advance (day $10$) regardless of condition; the RUL estimate is irrelevant and the system effectively mimics a fixed-interval policy.",
        "C": "This is purely an anomaly‑detection system with reactive maintenance: it only detects deviations from baseline and does not predict time to failure or optimize maintenance timing, so it cannot reduce unplanned downtime.",
        "D": "This is a run‑to‑failure (reactive) strategy: sensors and ML are used only for post‑failure diagnostics and do not inform preemptive scheduling or resource optimization."
      },
      "correct_answer": "A",
      "generation_notes": "Built a concrete compressor-monitoring scenario that specifies the PdM pipeline and RUL-based scheduling; distractors reflect preventive, anomaly-only, and reactive interpretations to test differentiation of concepts.",
      "concepts_tested": [
        "Condition‑based (measurement‑driven) maintenance vs time‑based preventive maintenance",
        "Optimization of maintenance timing to reduce unplanned downtime and convert unplanned stops into planned stops",
        "Predictive maintenance implementation workflow: data collection/preprocessing → early fault detection → time‑to‑failure prediction → scheduling/resource optimization"
      ],
      "source_article": "Predictive maintenance",
      "x": 1.637708306312561,
      "y": 1.021973967552185,
      "level": 2,
      "original_question_hash": "a54d450e"
    },
    {
      "question": "A mid-sized city is developing an integrated asset management policy covering (i) a 40-year-old water treatment plant, (ii) the municipal pension fund, and (iii) enterprise software licenses. The city wishes to maximize total value over time while controlling costs, risks, performance, and sustainability, and to document responsibilities and processes in line with international best practice. Which of the following integrated policies best embodies lifecycle-based value optimization, governance consistent with the ISO 55000 series, and an appropriate distinction between active and passive financial management?",
      "options": {
        "A": "Adopt an ISO 55001-compliant asset management system with an EAM registry (GIS-enabled) that defines assets, objectives and roles; implement lifecycle plans for the water plant including planned maintenance, upgrades, and end-of-life disposal with Total Cost of Ownership analysis; manage software licenses in an IT/SAM process that tracks constraints and renewal timing; and allocate the pension fund $80\\%$ to low-cost passive index strategies for core exposure and $20\\%$ to selective active managers for alpha and stewardship.",
        "B": "Implement an asset register without formal ISO alignment; defer major interventions on the water plant until failures occur to save short-term budget; centralize software in a single perpetual-license purchase strategy; and move the pension fund to 100\\% active managers to maximize returns despite higher fees and concentrated ownership.",
        "C": "Forego adopting ISO standards but create lifecycle checklists for physical assets and force all software licenses into perpetual ownership; invest the pension fund entirely in passive index funds to minimize fees, with no allocation to active management or stewardship activities.",
        "D": "Adopt ISO 55001 only for financial assets, treat physical and software assets under ad hoc department control with no coordinated lifecycle planning, and allocate the pension fund 90\\% to active management because active oversight better aligns managers with long-term municipal objectives."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a multipart municipal scenario requiring identification of ISO-based governance, lifecycle planning across asset types, and the active vs passive finance distinction; provided percentage allocation using inline LaTeX to test understanding of trade-offs.",
      "concepts_tested": [
        "Systematic lifecycle management and Total Cost of Ownership",
        "ISO 55000 governance framework for asset management",
        "Distinction between active and passive asset management in finance"
      ],
      "source_article": "Asset management",
      "x": 1.3914297819137573,
      "y": 0.9835549592971802,
      "level": 2,
      "original_question_hash": "8ac3d5ba"
    },
    {
      "question": "Four 17-year-olds are described below. Based on the theory of narrative identity and the research on developmental social mechanisms, which adolescent is most likely to have developed a highly coherent life story that integrates reconstructed past, perceived present, and imagined future (providing unity and purpose) and shows strong autobiographical reasoning linking past selves to the present self?",
      "options": {
        "A": "Elijah: His parents routinely told richly detailed stories of their own lives during his preschool years, used elaborative reminiscing (open-ended questions, emotional detail) when co-constructing past events with him, and were attentive listeners. During adolescence his caregivers continued guided co-constructed reminiscing—prompting discussion of motives, comparing past selves with present goals, and helping him connect events to future plans.",
        "B": "Maya: Her parents emphasized factual timelines and household rules, rarely asked open-ended questions about feelings, and were often distracted while she spoke. In adolescence Maya joined peer storytelling groups and wrote reflective essays on her own, but her caregivers offered little guided reminiscing.",
        "C": "Jorge: His parents used elaborative reminiscing in early childhood, but they were frequently distracted listeners and did not consistently scaffold his narratives. In adolescence Jorge underwent individual therapy that emphasized cognitive reappraisal and future planning, but caregivers did not engage in co-constructed reminiscing.",
        "D": "Priya: Her parents seldom discussed personal past events during childhood. In adolescence, however, her caregivers initiated regular co-constructed reminiscing sessions that focused on meaning and identity, though Priya lacked the early childhood exposure to parental narrative scaffolding."
      },
      "correct_answer": "A",
      "generation_notes": "Presented a comparative vignette requiring application of narrative identity theory: early elaborative parental storytelling + attentive listening + adolescent co-constructed reminiscing best predicts integrated, coherent life story; distractors isolate missing elements (early scaffolding, attentive listening, or adolescent co-construction).",
      "concepts_tested": [
        "Narrative identity as an evolving, integrative life story",
        "Childhood social interaction mechanisms (elaborative reminiscing, parental storytelling, attentive listening) enhancing narrative coherence",
        "Adolescent co-constructed reminiscing by caregivers facilitating autobiographical reasoning and linking past to present identity"
      ],
      "source_article": "Narrative identity",
      "x": 1.2338289022445679,
      "y": 1.0118836164474487,
      "level": 2,
      "original_question_hash": "bfd1ebc2"
    },
    {
      "question": "Archaeologists excavate a stupa complex in northwestern India. Strata dated to the 2nd–1st century BCE show votive friezes and narrative panels that use empty thrones, footprints and wheel motifs to mark the Buddha’s presence but contain no anthropomorphic images. Overlying deposits dated to the 1st–2nd century CE yield sculpted Buddhas with naturalistic human anatomy, wavy hair, drapery covering both shoulders, and an increasing number of bodhisattva figures. Which interpretation best integrates the aniconic-to-iconic transition, the regional artistic influences, and the changing devotional subjects described in the excavation?",
      "options": {
        "A": "Early aniconism reflects a doctrinal or cultural reluctance to anthropomorphize the Buddha, so artists used symbolic markers; the later emergence of human-form Buddhas results from developments in northern production centres (Gandhara and Mathura) where Hellenistic naturalism and local styles produced anthropomorphic images; the rise of bodhisattva images reflects Mahāyāna devotional expansion — a process spread along the northern Silk‑Road cultural network.",
        "B": "The early lack of Buddha statues indicates primitive sculptural skill or perishable wooden images that did not survive; the later anthropomorphic Buddhas are purely a local Mathura innovation independent of foreign influence; bodhisattva prominence is explained by indigenous village cults rather than doctrinal change, and the southern maritime trade routes were the primary vector for artistic change.",
        "C": "The initial aniconic phase was produced under early Islamic influence which forbade images, and the later human-form Buddhas arrived with Tang Chinese imports; bodhisattvas are late Tibetan Tantric additions; therefore the southern coastal routes account for the stylistic changes.",
        "D": "Empty‑throne and footprint motifs indicate later forgeries copying canonical symbols, while the realistic Buddhas reflect private Greco‑Roman merchant worship; bodhisattvas are later Hindu accretions; regional trade routes had little effect on the iconographic evolution."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a scenario-based question that requires integrating (1) doctrinal/cultural aniconism, (2) Greco-Buddhist and Mathura influences producing anthropomorphic images in northern centers, and (3) Mahayana-driven bodhisattva prominence and northern-route diffusion; distractors present plausible but historically inaccurate alternatives.",
      "concepts_tested": [
        "Aniconism versus iconic representation in early Buddhist art",
        "Northern (Silk Road/Greco-Buddhist) diffusion and regional artistic syncretism",
        "Development of Mahāyāna pantheon and rise of bodhisattva devotional imagery"
      ],
      "source_article": "Buddhist art",
      "x": 0.26848581433296204,
      "y": 0.06477552652359009,
      "level": 2,
      "original_question_hash": "ce7dc3df"
    },
    {
      "question": "A bank securitizes a $100$ million pool of loans that pays $6\\%$ interest annually (so the pool's interest cash flow is $6$ million per year). The issuer structures two tranches: a Senior bond of $85$ million and a Junior (subordinated) bond of $10$ million; the remaining $5$ million of pool principal is left as over‑collateralization (OC)/reserve. Shortly after issuance the pool suffers a principal loss of $10$ million (i.e., $10\\%$ of pool principal). According to standard structured finance loss allocation, tranching and credit enhancement principles, which outcome is correct?",
      "options": {
        "A": "The Junior tranche will absorb the $10$ million principal loss (reducing its principal toward $0$), while the Senior tranche's principal and its scheduled interest payments remain intact; this protection reflects credit enhancement via subordination (and OC), allowing the Senior tranche to obtain a higher rating and cheaper funding for the originator.",
        "B": "Losses are allocated pro rata across all bondholders, so the Senior tranche immediately loses $8.5$ million of principal (10\\% of its $85$ million) and the Junior loses $1$ million, meaning Senior investors bear most of the loss despite tranching.",
        "C": "Over‑collateralization requires the Senior tranche to be paid back first from excess principal, so OC increases the coupon paid to Senior investors above the pool's $6\\%$, which means Senior investors receive higher yields than Junior investors.",
        "D": "Tranching creates securities with identical risk/return profiles; therefore both Senior and Junior tranches will trade at the same yield and neither benefits from rating uplift or credit enhancement."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete numeric securitization example ($100$M pool, $85$M senior, $10$M junior, $5$M OC, $10$M loss) to test loss allocation, tranching effects, and credit enhancement implications; only option A correctly applies subordination/OC mechanics and rating/funding consequences.",
      "concepts_tested": [
        "Securitization improving capital efficiency and funding costs",
        "Tranching and allocation of cash flows and losses among investor classes",
        "Credit enhancement via subordination and over‑collateralization enabling higher ratings for senior tranches"
      ],
      "source_article": "Structured finance",
      "x": 1.3562421798706055,
      "y": 0.8805592060089111,
      "level": 2,
      "original_question_hash": "66f5c39f"
    },
    {
      "question": "You run a chemostat experiment with a single limiting nutrient supplied at constant concentration. Two bacterial strains compete: a “fast” strain F with maximal growth rate $r_F = 1.2\\ \\text{day}^{-1}$ and high per-capita nutrient uptake, and a “slow” strain S with $r_S = 0.8\\ \\text{day}^{-1}$ that secretes an extracellular enzyme which increases effective nutrient availability for all cells by a multiplicative factor $(1+\\alpha)$ (where $\\alpha>0$). In parallel, consider two firms in a market that can either undercut each other on price or tacitly coordinate on higher prices. Based on principles of ecology, game theory and economics as described in the article, which of the following integrated conclusions is most accurate?",
      "options": {
        "A": "Because $r_F>r_S$, F will always exclude S by competitive exclusion; analogously, perfect competition in markets drives prices to marginal cost (a zero-sum outcome). Game theory therefore predicts pure competitive equilibria without stable cooperation.",
        "B": "Coexistence is possible: if the enzyme benefit $\\alpha$ or resource trade-offs generate niche differences or change effective carrying capacity, S can persist alongside F despite lower $r$. Likewise, agents (organisms or firms) can both compete and cooperate — game theory (e.g. Nash equilibria, repeated games) can predict stable mixtures of competition and cooperation, and economic competition can produce non-zero-sum gains for consumers and producers.",
        "C": "Production of the public good by S necessarily makes S dominant because communal benefit always favours slower, cooperative types; in markets, competition always reduces welfare and inevitably produces monopolies without exception.",
        "D": "The chemostat will always exhibit sustained oscillations between F and S regardless of $\\alpha$ because intraspecific competition alone drives cyclic frequency-dependent selection; in economics, only heavy regulation (not strategic interaction) determines whether firms compete or cooperate."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete chemostat + market vignette to force students to integrate intraspecific selection and public-good trade-offs with game-theoretic concepts and economic outcomes; option B synthesizes coexistence via trade-offs, mixed cooperation/competition and non-zero-sum possibilities.",
      "concepts_tested": [
        "Intraspecific competition, selection and diversification/coexistence",
        "Coexistence of competition and cooperation and application of game theory to predict mixed strategies",
        "Competition as a cross-domain organizing principle (biology and economics), and zero-sum vs non-zero-sum outcomes"
      ],
      "source_article": "Competition",
      "x": 1.3397964239120483,
      "y": 0.9659521579742432,
      "level": 2,
      "original_question_hash": "71038d64"
    },
    {
      "question": "A research group studies a transient electron-transfer complex formed by proteins X (reductase) and Y (electron carrier). Structural analysis shows two surface Arg residues on X forming salt bridges with two surface Asp residues on Y; several interface water molecules also bridge polar groups. These four residues are strictly conserved across mammals. In mouse, mutating both Asp→Asn in Y causes a 50-fold increase in the dissociation constant Kd for the X–Y complex, a corresponding change in binding free energy given by $\\Delta\\Delta G = -RT\\ln(50)$, a large decrease in electron transfer rate, and failure of the mutant Y to rescue the phenotype when expressed in human cells lacking endogenous Y. Which explanation best accounts for these observations, integrating the physical forces at the interface, the role of complex assembly in signaling/functional networks, and the evolutionary conservation of interface residues?",
      "options": {
        "A": "The Asp→Asn substitutions remove negative charges that formed salt bridges with Arg and disrupt conserved water-mediated hydrogen bonds, substantially reducing electrostatic and hydrogen-bond contributions to binding (making $\\Delta G$ less negative and Kd larger). This weakens assembly of the X–Y complex, impairing electron transfer and downstream network signaling; strict conservation indicates these interface residues are functionally essential, explaining the cross-species complementation failure.",
        "B": "Because Asn can still form hydrogen bonds, the Asp→Asn change should not appreciably alter affinity; the 50-fold Kd change therefore implies the mutation causes global misfolding of Y that prevents complex formation and leads to loss of function and inability to complement in human cells.",
        "C": "The dominant stabilizing force at most protein interfaces is the hydrophobic effect, so replacing Asp with Asn (both polar) would not substantially change hydrophobic packing; the phenotype likely arises from altered subcellular localization of mutant Y rather than loss of electrostatic interactions.",
        "D": "The Asp residues were probably sites for a covalent post-translational modification required for activity; substituting Asn abolishes that modification, leading to rapid degradation of Y and loss of function, while conservation reflects preservation of the modification site across species."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete electron-transfer complex mutation scenario (Asp→Asn) to probe electrostatic/salt-bridge and water-mediated H-bond contributions to affinity, how weakened binding disrupts complex assembly and downstream network function, and why conservation implies functional importance and cross-species noncomplementation.",
      "concepts_tested": [
        "Electrostatic interactions, hydrogen bonds and water-mediated contacts determine PPI specificity and affinity",
        "Assembly of protein complexes enables cellular processes and signaling networks; loss of binding perturbs function",
        "Evolutionary conservation of interface residues indicates functional importance and predicts cross-species interaction consequences"
      ],
      "source_article": "Protein–protein interaction",
      "x": 1.9856445789337158,
      "y": 1.0969563722610474,
      "level": 2,
      "original_question_hash": "a95d8a7e"
    },
    {
      "question": "AeroFab, a mid-sized aerospace components manufacturer, adopted strict standard operating procedures and tightly specified performance metrics after a series of safety incidents. On the shop floor (production lines) workers follow rules precisely but rarely suggest process improvements. When small cross-functional ad hoc teams form outside the formal production schedule, engineers and technicians generate creative process improvements that sometimes get implemented. Senior management attributes the lack of broad innovation to workers' low motivation and proposes mandatory creativity training. Which explanation best integrates (a) micro/meso/macro levels of analysis, (b) role-based differences in behavior within organizations, and (c) OB’s interdisciplinary aim to revitalize organizational theory by combining insights from sociology, economics, and I/O psychology?",
      "options": {
        "A": "A multi-level explanation: at the micro level workers enact role-prescribed behaviors (obeying formal procedures) because organizational role scripts and bounded rationality lead them to satisficing; at the meso level, cross-functional teams provide different norms and psychological safety that enable creative behavior; at the macro level, formal bureaucratic rules and efficiency-driven incentives (an economic logic) constrain discretion. This interpretation draws on I/O psychology (motivation, bounded rationality), sociology (organizational culture and norms), and economics (incentives), suggesting interventions that alter structure and incentives and create sanctioned spaces for experimentation.",
        "B": "A purely macro explanation: the organization’s bureaucratic design and efficiency metrics are the sole cause; therefore the remedy is to overhaul the entire formal structure and replace strict rules with decentralized authority throughout the firm so all workers can innovate without additional team-level changes.",
        "C": "An individual-differences explanation: the problem is primarily workers’ personalities (low openness to experience and low proactive personality). The firm should screen for and hire higher-Openness employees and send existing workers to individual creativity workshops to change traits and boost innovation.",
        "D": "A leadership-only explanation: senior managers should adopt transformational leadership behaviors and charismatic communication to inspire workers to be creative within existing procedures; changing leadership style alone will produce widespread innovation without modifying rules, teams, or incentives."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete company vignette requiring synthesis across micro/meso/macro levels, role-based behavior (role scripts, bounded rationality/satisficing), and interdisciplinary foundations (sociology, economics, I/O psychology). Distractors isolate single-level or single-discipline explanations.",
      "concepts_tested": [
        "Levels of analysis (micro, meso, macro)",
        "Role-based behavior and bounded rationality/satisficing",
        "Interdisciplinary foundations of OB (sociology, economics, I/O psychology) and revitalizing organizational theory"
      ],
      "source_article": "Organizational behavior",
      "x": 1.2975658178329468,
      "y": 1.0052238702774048,
      "level": 2,
      "original_question_hash": "7315a009"
    },
    {
      "question": "A 100 mg oral tablet of Drug X is administered. The tablet requires liberation and absorption from the gut. Measured absolute oral bioavailability of the parent prodrug is $B=0.4$ (i.e. $40\\%$ of the administered dose reaches systemic circulation as parent prodrug plus any pre-systemic metabolite), and the parent is pharmacologically inactive until converted in the liver to an active metabolite M by CYP3A4. Parent drug is primarily renally excreted; metabolite M is primarily hepatically cleared. The patient has chronic renal impairment that reduces renal clearance of the parent to $25\\%$ of normal and is started on a potent CYP3A4 inhibitor that reduces formation of M by $80\\%$. Which of the following is the most likely net outcome for plasma concentrations and the pharmacodynamic (PD) effect compared with a patient with normal renal function and no inhibitor?",
      "options": {
        "A": "Plasma concentration of the inactive parent prodrug will rise (due to reduced renal excretion) but formation and plasma concentration of active metabolite M will fall (due to CYP3A4 inhibition); net PD effect will be reduced despite higher parent levels.",
        "B": "Both parent and active metabolite M concentrations will rise because accumulated parent provides more substrate for conversion, so the PD effect will increase substantially.",
        "C": "Oral bioavailability of the active species will approach 100% because renal impairment prevents loss of drug, therefore PD effect will be unchanged only if dosing is not adjusted.",
        "D": "CYP3A4 inhibition will shift elimination from metabolism to renal excretion so both parent clearance and M clearance will increase, producing no net change in PD effect."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a clinical scenario that requires applying the ADME sequence (liberation, absorption, metabolism, excretion), distinguishing PK (changes in clearance and metabolite formation) from PD (active metabolite drives effect), and reasoning how altered metabolism and renal excretion change concentrations and effect.",
      "concepts_tested": [
        "ADME sequence and how each phase (liberation, absorption, metabolism, excretion) shapes concentration-time and effect",
        "Distinction between pharmacokinetics (PK) and pharmacodynamics (PD) and how PK changes (metabolism inhibition, reduced renal clearance) alter PD via active metabolite levels",
        "Role of metabolism (CYP3A4-mediated activation) and excretion (renal clearance) in determining clearance and duration of action"
      ],
      "source_article": "Pharmacokinetics",
      "x": 1.819639801979065,
      "y": 1.0580092668533325,
      "level": 2,
      "original_question_hash": "8b0f5650"
    },
    {
      "question": "A peptide therapeutic has oral bioavailability $<1\\%$ and a plasma half-life of 1 hour. The development team wants to (i) increase systemic bioavailability to >30\\%, (ii) prolong duration of action to reduce dosing frequency, (iii) deliver antigenic signals to dermal antigen-presenting cells for a vaccine-like immune response, and (iv) enable safe self-administration without conventional hypodermic needles. Which delivery strategy best satisfies these objectives and correctly exemplifies the distinction between \"route of administration\" and \"dosage form\"?",
      "options": {
        "A": "A dissolvable microneedle patch (dosage form) applied via the transdermal route that uses a biodegradable polymer matrix embedding pH-responsive nanoparticles to protect the peptide, release it slowly into dermal tissue, target local antigen-presenting cells, and avoid hypodermic needles.",
        "B": "An oral tablet (dosage form) containing a high-concentration peptide and a permeation enhancer (route: oral) formulated to increase intestinal uptake and include an extended-release coating to prolong systemic exposure.",
        "C": "An intravenous liposomal infusion (dosage form: sterile solution; route: parenteral) with a liver-targeting ligand to concentrate drug in hepatocytes, combined with PEGylation to extend circulation half-life while requiring clinic-based administration.",
        "D": "A topical cream (dosage form) rubbed onto skin (route: topical) that dissolves peptide into the stratum corneum and slowly diffuses systemically, eliminating the need for injections and providing targeting to dermal immune cells."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a clinical scenario comparing realistic delivery strategies; correct answer (microneedle patch) aligns with improving bioavailability/prolonging action/targeting and shows route vs dosage form distinction.",
      "concepts_tested": [
        "Modification of pharmacokinetics and tissue distribution using carriers and devices",
        "Difference and relationship between route of administration and dosage form",
        "Strategies to enhance bioavailability, prolong duration, and improve safety/convenience (e.g., microneedle patches)"
      ],
      "source_article": "Drug delivery",
      "x": 1.8733166456222534,
      "y": 1.0696954727172852,
      "level": 2,
      "original_question_hash": "bb3d1309"
    },
    {
      "question": "A start-up has a high-resolution X-ray structure of a disease-associated enzyme and uses a docking program to identify lead compound L1. Docking predicts a dissociation constant around 100 nM (the thermodynamic relation is $\\Delta G_{bind}=-RT\\ln K_d$) but in vitro ADME profiling indicates poor oral bioavailability: polar surface area (PSA) = 120 Å^2 and the scaffold has $N_{rot}=8$ rotatable bonds. According to structure-based and computer-aided design principles, which of the following optimization strategies is most likely to increase the probability that an orally dosed clinical candidate with retained target affinity and acceptable safety is obtained?",
      "options": {
        "A": "Introduce multiple ionizable groups (e.g., tertiary amines and carboxylates) to create stronger ionic interactions with charged residues in the pocket; rely on docking scores to recover any loss in permeability by increasing predicted affinity.",
        "B": "Use structure-based design to rigidify the scaffold (e.g., intramolecular cyclization or conformational constraints) to reduce $N_{rot}$ to ~4–5 and replace solvent‑exposed polar moieties with less polar isosteres to lower PSA below ~90 Å^2, while ensuring key hydrogen bonds in the binding site are preserved.",
        "C": "Increase lipophilicity by adding bulky hydrophobic substituents to maximize lipophilic contact surface area; expect higher affinity from favorable lipophilic interactions and accept elevated metabolic clearance as a trade-off.",
        "D": "Abandon structure-guided optimization and switch to phenotypic high-throughput screening to find a new lead with better ADME, because computational methods cannot predict ADME properties reliably."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a realistic structure-based lead-optimization scenario linking binding thermodynamics ($\\Delta G_{bind}$, $N_{rot}$) and ADME metrics (PSA). Options map to plausible medicinal-chemistry strategies; B is best because rigidification and reducing PSA improve oral bioavailability and lower entropy penalty while retaining binding via structure-based modeling.",
      "concepts_tested": [
        "Binding complementarity and target modulation",
        "Structure-based and computer-aided optimization of affinity and selectivity",
        "ADME constraints (PSA, rotatable bonds) and their impact on drug development success"
      ],
      "source_article": "Drug design",
      "x": 1.965460181236267,
      "y": 1.0952502489089966,
      "level": 2,
      "original_question_hash": "350dd04b"
    },
    {
      "question": "A biotech company discovers a new chemical entity (NCE) that inhibits a viral protease in vitro. Preclinical program results are: (i) minimal cytotoxicity in cell lines, (ii) reversible liver enzyme elevations in rats at high doses, (iii) dog pharmacokinetics show low oral bioavailability linked to aqueous solubility < 10 µg/mL, (iv) solid form is chemically stable under accelerated conditions, and (v) current synthesis yields grams per batch. Which of the following best describes the required preclinical and CMC activities and how they inform the first‑in‑human (FIH) dose, formulation and regulatory progression before Phase I?",
      "options": {
        "A": "Perform GLP toxicology (including organ‑specific studies for liver, heart, kidney, brain, lungs), and PK/metabolism studies; use those safety/toxicity data to select a conservative FIH dose and schedule (applying appropriate safety margins), develop a formulation or salt form to address poor solubility or choose a parenteral route if necessary, scale up the synthesis and validate CMC processes for kilogram/ton production; compile these data into an IND for regulatory review — only after IND acceptance may Phase I begin.",
        "B": "Because in vitro cytotoxicity is minimal and efficacy in animals is promising, proceed to Phase I using the efficacious animal dose as the human starting dose; CMC scale‑up can be deferred until after Phase II, and an IND is optional if initial trials enroll only a few healthy volunteers.",
        "C": "Ignore the reversible liver enzyme elevations in rats since they occurred only at high doses; file an IND immediately based on in vitro and small‑scale synthesis data and start microdosing human studies to determine both safety and formulation; full GLP toxicology and scale‑up can be completed during Phase I.",
        "D": "Focus regulatory submission only on chemistry (CMC) data showing stability and an existing gram‑scale synthesis; choose an oral capsule formulation despite low aqueous solubility by increasing capsule strength to match animal exposure; first‑in‑human dose should equal the therapeutic dose observed in dogs, and toxicology data are not required before starting Phase I."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete NCE scenario to require integration of preclinical toxicology/PK informing FIH dosing, physicochemical/CMC implications for formulation and scale‑up, and the IND/regulatory gatekeeping role; provided four plausible but distinct regulatory/development pathways.",
      "concepts_tested": [
        "Preclinical toxicology, pharmacokinetics and metabolism informing first‑in‑human dose selection",
        "Physicochemical properties and CMC affecting formulation choice and manufacturing scale‑up",
        "Regulatory framework (IND submission and required animal toxicity studies) as a gatekeeper before clinical trials"
      ],
      "source_article": "Drug development",
      "x": 2.055584669113159,
      "y": 1.087588906288147,
      "level": 2,
      "original_question_hash": "5fdcfc0b"
    },
    {
      "question": "A country is planning reforms for its $100$-seat national legislature. Four parties receive nationwide vote shares A $40\\%$, B $30\\%$, C $20\\%$, and D $10\\%$. Consider three proposed systems: (1) closed-list nationwide pure proportional representation (List PR); (2) $100$ single-member districts using first-past-the-post (FPTP) with geographically uniform vote shares (each district has the same party vote fractions as the national totals); (3) a single nationwide multi-member constituency using party-block (winner-take-all) voting where the party with the most votes takes all seats. Which of the following statements is most accurate about expected seat outcomes and strategic/ theoretical implications?",
      "options": {
        "A": "Under List PR seats will be approximately proportional (A 40, B 30, C 20, D 10). Under FPTP with uniform support and under nationwide party-block voting party A (with 40\\%) would win every single-member contest or every seat respectively, yielding extreme disproportionality. These winner-take-all, single-winner rules create strong incentives for strategic voting; Gibbard’s result implies no deterministic single-winner rule can be both strategy‑proof and non‑dictatorial when there are more than two outcomes, and Arrow’s theorem shows ranked methods cannot satisfy all classical fairness criteria simultaneously.",
        "B": "Because each FPTP district mirrors the national vote, FPTP will produce roughly the same seat proportions as List PR (A 40, B 30, C 20, D 10); therefore strategic voting incentives are minimal and the impossibility results like Gibbard and Arrow are irrelevant to these outcomes.",
        "C": "A compensatory mixed system (MMP) would necessarily give party A a legislative majority because A’s uniform plurality in most constituencies forces top‑up mechanics to award extra compensatory seats to the largest party, so mixed systems always advantage the plurality party.",
        "D": "Single Transferable Vote (STV) in large multi‑member districts would exactly reproduce the national vote shares (A 40, B 30, C 20, D 10) with no strategic incentives; thus STV avoids both disproportionality and the strategic problems demonstrated by Arrow and Gibbard."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete 100-seat scenario comparing List PR, uniform‑vote FPTP, and party‑block voting to test proportionality, winner‑take‑all effects, strategic incentives, and applicability of Arrow/Gibbard.",
      "concepts_tested": [
        "votes-to-seats proportionality (PR vs winner-take-all)",
        "structural categories: single-winner vs multi-winner and mixed systems",
        "theoretical limits on voting systems and strategic voting (Arrow's theorem, Gibbard's theorem)"
      ],
      "source_article": "Electoral system",
      "x": 1.2024259567260742,
      "y": 0.9030275344848633,
      "level": 2,
      "original_question_hash": "0639f4b8"
    },
    {
      "question": "Consider a wetland in the Verde watershed whose natural capital stock of ecological biomass is S = 1000 units. Its net regenerative capacity is g = 0.06 yr^{-1}, so the sustainable provisioning yield is H_s = gS = 60 units/yr. The local economy currently extracts H = 90 units/yr of provisioning services. The wetland also delivers non-market regulating and cultural services the community estimates at $0.2 million/yr, which are not included in national accounts (unpriced). Which of the following statements best describes the ecological and economic implications, and what would a \"natural capital asset check\" most likely reveal?",
      "options": {
        "A": "Because H = 90 > H_s = 60, the wetland's stock will decline over time, causing future reductions in both market yields and non-market services; a natural capital asset check would quantify the trajectory of stock and flow losses, link these losses to future impacts on household well-being and local economic output, and indicate the need to internalize unpriced services (e.g., via regulation or payments) since current GDP measures omit them.",
        "B": "The current extraction (H = 90) exceeds the theoretical sustainable yield slightly, but the difference is trivial so the stock will self-correct and remain effectively stable; a natural capital asset check would therefore show no material long-term impact on services or the local economy.",
        "C": "Although H > H_s, the wetland will quickly reach a new steady-state with lower biomass that sustains H = 90 indefinitely; because markets supply substitutes for non-market services, there will be no meaningful loss in human well-being even though the stock declines.",
        "D": "Since the non-market services are valued at $0.2 million/yr and are unpriced in GDP, the immediate economic consequence of the current extraction is a direct annual reduction in national GDP by $0.2 million; a natural capital asset check would simply convert the unpriced services into a one-time GDP adjustment."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a quantitative wetland scenario using S, g, H to test sustainable yield vs over-extraction, effects on market and non-market ecosystem services, and the role of natural capital asset checks and unpriced capital in economic accounting.",
      "concepts_tested": [
        "Ecosystem services underpin human well-being and the economy",
        "Sustainable yield versus overuse leading to stock depletion",
        "Natural capital asset checks and the problem of unpriced natural capital in GDP/accounting"
      ],
      "source_article": "Natural capital",
      "x": 1.3839561939239502,
      "y": 0.8781353235244751,
      "level": 2,
      "original_question_hash": "80bc5030"
    },
    {
      "question": "A national Professional Engineering Society (PES) maintains a written code of practice requiring engineers to refuse certification of projects that are unsafe. An engineering firm pays a consultant engineer a bribe to certify a deficient bridge design. PES conducts an internal investigation but issues only a confidential reprimand because many senior PES officers have close business ties to the consultant’s firm. News of the reprimand leaks, the public becomes alarmed, clients begin to seek independent, non-PES-certified assessors, and the legislature proposes a statutory regulator with investigatory and sanctioning powers. Which interpretation best reflects the dynamics described, given the concepts of professional codes, self-regulation risks, and public trust?",
      "options": {
        "A": "This scenario shows that codes of practice and disciplinary mechanisms are necessary governance tools to align practitioner behaviour with standards and protect clients; however, when self-regulation is weakened by conflicts of interest and lenient enforcement, public trust is eroded and may prompt statutory regulation or loss of legitimacy and access to services.",
        "B": "Because professions possess specialized knowledge, deference to PES and confidential internal discipline is sufficient; external regulation is unnecessary and the legislature should avoid intervening to respect professional autonomy.",
        "C": "The confidential reprimand preserves the profession’s integrity because public revelations of internal errors always do more harm than good; therefore maintaining secrecy strengthens public trust and prevents loss of business.",
        "D": "Public loss of confidence in engineers results only from technical failures (like a collapsed bridge), not from governance or disciplinary lapses; therefore weak internal sanctions cannot affect legitimacy or trigger statutory oversight."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete engineering ethics scenario illustrating how internal codes and disciplinary bodies function, showing risks of self-regulation (conflicts of interest) and linking enforcement failures to erosion of public trust and possible statutory intervention.",
      "concepts_tested": [
        "Codes of practice and disciplinary mechanisms as governance tools",
        "Limits and risks of self-regulation and conflicts of interest",
        "Relationship between enforcement, public trust, and professional legitimacy/statutory intervention"
      ],
      "source_article": "Professional ethics",
      "x": 1.261926293373108,
      "y": 0.9624737501144409,
      "level": 2,
      "original_question_hash": "16156925"
    },
    {
      "question": "Arcadia (GDP = 1000) enacts two simultaneous measures: (i) a new progressive income tax expected to raise statutory revenue of $50$ and (ii) a 10% VAT expected to raise statutory revenue of $80$. Compliance rates are estimated at $85\\%$ for the income tax and $95\\%$ for the VAT. The government will spend all collected revenue immediately on public infrastructure with an estimated fiscal multiplier of $1.5$. Which of the following best describes the likely macroeconomic and distributional outcomes, and the role of compliance/enforcement?",
      "options": {
        "A": "Statutory receipts ($50+80=130$) will be fully collected; government spending then raises GDP by $(1.5-1)\\times130=65$. The VAT is neutral for equity because it taxes consumption uniformly, and compliance/enforcement are marginal administrative concerns.",
        "B": "Actual revenue equals $0.85\\times50+0.95\\times80 = 118.5$. Spending that amount with multiplier $1.5$ raises GDP by $118.5\\times1.5=177.75$, so the net change in GDP is $177.75-118.5=59.25$. Distributionally, the progressive income tax increases vertical equity while the uniform 10% VAT is regressive, so the combined effect is a trade-off between equity and revenue. Higher compliance improves revenue stability but also means more distortionary tax base is collected (so deadweight losses rise with greater effective collection).",
        "C": "After compliance the government will only collect $118.5$, and because taxes always reduce activity by more than spending raises it, GDP will fall by $59.25$. The VAT makes the overall system more progressive because consumption taxes fall largely on luxury spending.",
        "D": "Most VAT revenue will be borne by foreigners (tourists) so the domestic incidence is negligible; therefore with multiplier $1.5$ the policy is fully expansionary with zero distributive consequences, and compliance is irrelevant to the policy’s success."
      },
      "correct_answer": "B",
      "generation_notes": "Created a concrete numerical scenario combining revenue realization (adjusted for compliance), fiscal multiplier arithmetic, and qualitative incidence (progressive income tax vs. regressive VAT) to test interactions among wealth transfer to government, design choices, and compliance effects.",
      "concepts_tested": [
        "Taxation as transfer of wealth and fiscal multiplier effects",
        "Tax design (progressive vs. flat/indirect) and incidence on equity and incentives",
        "Tax compliance and enforcement effects on revenue stability and economic distortions"
      ],
      "source_article": "Tax",
      "x": 1.2961413860321045,
      "y": 0.8471706509590149,
      "level": 2,
      "original_question_hash": "bbb57b52"
    },
    {
      "question": "Alpha Inc., incorporated in State A of a federal country, operates servers in State C and sells software by online contract to Beta, a consumer domiciled in State B. The online contract contains a clause selecting State A law and State C courts. A data breach occurring while Beta used the software in State B causes economic loss; Beta sues Alpha in State B court for tort and breach of contract. Alpha obtains a favorable judgment in State C court on a preliminary contractual dispute and later moves some assets to Country D, which is party to a bilateral treaty with Alpha's country providing for recognition of foreign judgments that meet certain formal requirements. Which description best explains how jurisdiction, choice of law, and recognition/enforcement will interact to resolve these cross‑jurisdictional disputes, and the role of domestic law versus the treaty?",
      "options": {
        "A": "The State B court will first decide adjudicative jurisdiction under domestic rules (territoriality/minimum contacts). It will apply its lex fori to procedure and then its choice‑of‑law rules: it may respect the parties' contractual choice of State A law for the contract claim (party autonomy) but apply the law most closely connected to the tort (e.g., lex loci actus or proper law). Enforcement of the State C judgment and any subsequent enforcement in Country D depend on each forum's domestic recognition rules; Country D's treaty can oblige recognition only to the extent domestic implementing law makes the treaty enforceable — treaties facilitate but do not automatically displace domestic conflict‑of‑laws rules. In a federal system, disputes between states are resolved by similar domestic conflict‑of‑laws doctrines (comity, minimum contacts) rather than by public international law.",
        "B": "Because the parties selected State A law and State C courts, those choices determine all issues: State C has exclusive jurisdiction over both the tort and contract claims and its judgment is automatically enforceable in State B and Country D by virtue of the contract clause, without regard to State B procedural rules or Country D's domestic implementing law.",
        "C": "Jurisdiction and choice of law are determined solely by the domicile of the defendant: Alpha's State A domicile means only State A law applies to both tort and contract claims, and foreign courts must enforce State A judgments because domestic courts treat domicile as universally decisive under conflict of laws.",
        "D": "The State B court must decline jurisdiction because Alpha obtained a judgment in State C; the treaty between the countries means Country D must enforce State C's judgment immediately, so Beta's only remedy is to seek recognition in Country D under the treaty and domestic courts cannot reassess jurisdiction or choice of law."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete multi‑forum scenario to test (1) adjudicative jurisdiction, choice of law and recognition/enforcement interplay; (2) that applicable law remains domestic and treaties affect enforceability only via domestic implementation; (3) that federal internal conflicts are treated under domestic conflict‑of‑laws principles.",
      "concepts_tested": [
        "Interaction of jurisdiction, choice of law, and recognition/enforcement of foreign judgments",
        "Primacy of domestic law in applicable law and the limited role of international treaties",
        "Application of private international law principles within federal systems (inter‑state conflicts)"
      ],
      "source_article": "Conflict of laws",
      "x": 1.2444827556610107,
      "y": 0.8222604393959045,
      "level": 2,
      "original_question_hash": "1dc3f42b"
    },
    {
      "question": "A university research lab runs a compute cluster of 100 nodes and a dedicated storage array (SAN). The compute nodes access the SAN using the hostname storage.lab.edu. Requirements: (1) one-way access latency from any compute node to the SAN must be < 2 ms, (2) the network must tolerate the failure of any single access switch without disrupting compute-to-storage traffic, and (3) hostname-to-address resolution for storage.lab.edu must continue to work even if the external DNS provider becomes unreachable. All nodes lie within a single building with maximum cable run of 200 m between any node and core equipment. Which network design best satisfies all three requirements, taking into account name resolution, topology, and physical transmission media? (Assume light in fiber propagates at ≈ 2×10^8 m/s, so a 200 m fiber link contributes ≈ \\(\\frac{200}{2\\times10^{8}}=1\\times10^{-6}\\) s propagation delay.)",
      "options": {
        "A": "Deploy an internal authoritative DNS server or local caching resolver that answers storage.lab.edu so name resolution works during external DNS outages; connect nodes and SAN to two redundant access switches so each node is dual‑homed to both switches and those switches connect to two core switches (redundant star/leaf‑spine style) to tolerate a single‑switch failure; use optical fiber trunks (multimode or single‑mode as appropriate) for all 10 Gbit links between switches and to the SAN (fiber easily meets the 2 ms budget given negligible propagation delay).",
        "B": "Populate /etc/hosts on all 100 nodes with the IP address of storage.lab.edu so resolution is independent of DNS; use a single high‑capacity central switch in a star topology to minimize hops and latency; run all 10 Gbit links over Cat6 copper cabling to avoid fiber costs; add Wi‑Fi as a backup for occasional access.",
        "C": "Continue to rely on the external DNS provider but set very short TTLs so clients refresh frequently; implement a ring topology of access switches so traffic can bypass a failed neighbor; use copper for access links and reserve fiber only for a single backbone connection between two distant closets.",
        "D": "Create a VPN overlay between each compute node and the SAN and host a copy of the DNS zone on the VPN server; interconnect all switches in a full mesh using Cat5e copper to simplify cabling and run the SAN traffic through the VPN to isolate it from failures."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a realistic campus building scenario requiring calculation of propagation delay and evaluation of name resolution scalability and resilience; options mix hosts file vs internal DNS, single‑point vs redundant topologies, and copper vs fiber tradeoffs to test all three concepts.",
      "concepts_tested": [
        "Name resolution and addressing (DNS vs hosts file, local caching, availability)",
        "Network topology and resilience (redundant access/core, leaf‑spine vs single central switch)",
        "Physical transmission media effects on latency and reliability (copper length limits, fiber propagation delay and suitability for 10 Gbit)"
      ],
      "source_article": "Computer network",
      "x": 1.4785939455032349,
      "y": 1.0845637321472168,
      "level": 2,
      "original_question_hash": "90166400"
    },
    {
      "question": "A city piloting a youth development program 'YouthLab' is deciding which design best reflects principles of informal education described in the article (i.e., learning as a social, conversation-driven, collaborative process that bridges school and life; learner autonomy with implicit scaffolding via tools and experiences; and the facilitative role of community and youth organizations). Which set of program design elements best embodies those principles?",
      "options": {
        "A": "An open makerspace where adolescents choose projects aligned with their interests; facilitators act as conversational guides (posing problems, suggesting resources) rather than delivering lectures; tangible tools and community mentors (local artisans, elders) are available for ongoing, situated scaffolding; peer collaboration and public showcases connect projects to community life.",
        "B": "A structured after-school curriculum with weekly teacher-led lessons, fixed learning objectives, and periodic standardized assessments to measure competency; students rotate through assigned modules with little choice of topics.",
        "C": "A library of self-paced online modules and toolkits adolescents can access anytime, plus automated quizzes and badges; interaction is limited to asynchronous forum posts with minimal local community involvement.",
        "D": "A volunteer community-service program where youth follow adult-directed task lists to complete neighborhood projects for course credit; adults assign roles and evaluate performance based on task completion speed."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a scenario and four plausible program designs; option A includes conversation-based collaboration, learner choice and implicit scaffolding with community mentors, matching the informal education principles from the article. Distractors isolate or contradict one or more core concepts.",
      "concepts_tested": [
        "Informal education as social, conversation-driven, collaborative process that bridges school and life",
        "Learner autonomy and implicit/ongoing scaffolding via tools and experiences",
        "Role of community and youth organizations in facilitating situated learning"
      ],
      "source_article": "Informal education",
      "x": 1.2418211698532104,
      "y": 0.9782030582427979,
      "level": 2,
      "original_question_hash": "4ef8fc31"
    },
    {
      "question": "Country of Valoria is a parliamentary democracy with a hereditary monarch. Its written constitution gives the monarch formal powers to dissolve parliament, withhold royal assent, and appoint the prime minister; but long‑standing constitutional conventions ordinarily require the monarch to act on the advice of elected ministers. The monarch also meets privately with the prime minister to offer counsel, occasionally uses public statements to signal concern about proposed laws, and retains a reserve power to dismiss a government in an extreme constitutional crisis. Which characterization best reflects the monarch’s role in Valoria under the principles of constitutional monarchy?",
      "options": {
        "A": "A ceremonial yet constitutional head of state who “reigns but does not rule”: formally endowed with powers (dissolution, assent, appointment, reserve powers) that are constrained by legal rules and unwritten conventions, while exercising political influence informally through consultation, encouragement and warning rather than by making policy.",
        "B": "An executive monarch who governs directly: because the constitution vests appointment and assent powers in the monarch, the sovereign is the primary policymaker and may routinely dismiss ministers or set government policy without regard to parliamentary advice.",
        "C": "A purely symbolic figure with no formal authority: the monarch has no legal power to dissolve parliament or withhold assent and cannot influence politics either formally or informally; all state acts are taken exclusively by elected officials.",
        "D": "A quasi‑absolute ruler whose private counsel to the prime minister and occasional public statements equate to de facto control of legislation, since informal influence necessarily replaces formal constitutional limits in practice."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete parliamentary scenario (Valoria) combining formal constitutional powers, unwritten conventions, private counsel, and reserve powers; options contrast ceremonial/constitutional role with executive, purely symbolic, and quasi‑absolute misinterpretations.",
      "concepts_tested": [
        "Separation of roles (monarch as symbolic head, not primary policymaker)",
        "Constraint mechanisms (formal powers limited by constitution and unwritten conventions, reserve powers)",
        "Role and influence within governance (informal influence via consult/encourage/warn without direct policy control)"
      ],
      "source_article": "Constitutional monarchy",
      "x": 0.6893540024757385,
      "y": 0.6385244727134705,
      "level": 2,
      "original_question_hash": "88dfc4d1"
    },
    {
      "question": "A labour economist estimates the model $\\\\ln(\\text{wage})=\\beta_0+\\beta_1\\,(\\text{years of education})+\\varepsilon$ using OLS on a sample of 500 workers and obtains $\\hat{\\beta}_1=0.08$ with standard error $0.02$. Which of the following statements best integrates econometric interpretation of the coefficient, the role of the linear regression model, and the relevant estimator properties?",
      "options": {
        "A": "The estimate $\\hat{\\beta}_1=0.08$ proves that an extra year of education causes wages to increase by exactly 8%; OLS is unbiased, efficient and consistent regardless of whether important covariates (like ability) are omitted.",
        "B": "The estimate $\\hat{\\beta}_1=0.08$ implies roughly an 8% increase in wage per additional year of education and the t-statistic ($4$) indicates statistical significance under the usual assumptions; however, if an omitted variable such as innate ability is correlated with education then OLS will generally be biased and inconsistent, whereas if $E[\\varepsilon\\mid\\text{education}]=0$ and the Gauss–Markov conditions hold, OLS is BLUE (most efficient among linear unbiased estimators).",
        "C": "Because the sample size is 500, OLS estimates are automatically unbiased and consistent even if ability is omitted and correlated with education; efficiency is irrelevant once $n$ is large.",
        "D": "Since the dependent variable is in logs, $\\beta_1$ measures the absolute dollar change in wage per year of education; unbiasedness requires only that the estimator has the smallest possible standard error."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a realistic log-wage OLS scenario to test interpretation of slope in log-linear models, the linear regression structure (intercept, slope, error), and how estimator properties (unbiasedness, consistency, efficiency) depend on assumptions and omitted variables.",
      "concepts_tested": [
        "Integration of economic theory and data via statistical inference",
        "Linear regression model components and interpretation of log-linear slope",
        "Estimator properties: unbiasedness, efficiency (BLUE), and consistency and effects of omitted variables"
      ],
      "source_article": "Econometrics",
      "x": 1.5385104417800903,
      "y": 1.0815685987472534,
      "level": 2,
      "original_question_hash": "a762498d"
    },
    {
      "question": "In the city-state of Lydria, farmers deposit 1,000 bushels of barley in the temple granary and receive matching clay tokens. Each clay token is stamped \"1\" and for accounting purposes the temple ledger records values in barley-units so that $1\\ \\text{token}=1\\ \\text{barley-unit}$. Over time the tokens circulate as a medium in local markets (used to buy pottery and services). Later, the ruler mints bronze coins whose metal content equals the value of $0.8$ barley-units but stamps them with the face value of $1$ barley-unit; the mint promises convertibility of coins into barley at a fixed rate for a time. Finally, the ruler issues paper notes declaring them legal tender without backing by barley or metal. Which explanation best captures (a) how money of account and money of exchange co-evolved in Lydria; (b) the role of the temple's tally/ledger in converting social obligations into monetary units; and (c) what each form of backing (commodity, representative, fiat) implies about value and trust?",
      "options": {
        "A": "The clay tokens initially functioned as money of exchange while the temple's ledger and barley-unit served as money of account: the tally system quantified \"I owe you\" into explicit barley-units that made tokens transferable. Bronze coins stamped with face value $1$ but containing $0.8$ barley-units are representative/commodity-hybrid money whose circulation relied on the mint's promise (convertibility) and on trust; the later paper notes are fiat money whose value depends on legal enforcement and public confidence rather than intrinsic metal or stored barley.",
        "B": "Because the clay tokens circulated, they were the primary money of account from the start; the temple ledger was merely a storage record. Bronze coins containing less metal than their face value remain commodity money because the metal has intrinsic worth, and paper notes are simply a lightweight form of commodity money backed implicitly by the state's future tax receipts.",
        "C": "Money of exchange must precede any money of account, so the tokens created the accounting unit rather than the temple ledger. The minting of coins with $0.8$ barley-units proves seigniorage renders convertibility unnecessary, and fiat notes are effectively identical to representative money as long as people accept them in transactions.",
        "D": "The temple tally system could only record past deposits and could not convert social obligations into standardized units; tokens were therefore only symbolic. Coins stamped at face value above metal content are necessarily fraudulent and cannot circulate unless forcibly accepted; fiat money cannot function without a permanent convertibility guarantee to a commodity like barley."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete historical scenario illustrating temple-ledgers/tokens, coin debasement and transition to paper fiat; options test co-evolution of money of account/exchange, tally as quantification of debt, and distinctions among commodity, representative and fiat money.",
      "concepts_tested": [
        "Money of account versus money of exchange and their co-evolution",
        "Role of tally/ledger/accounting in transforming obligations into monetary units",
        "Differences among commodity money, representative money, and fiat money (value, convertibility, trust)"
      ],
      "source_article": "History of money",
      "x": 0.7966225147247314,
      "y": 0.41241466999053955,
      "level": 2,
      "original_question_hash": "fa6cbec8"
    },
    {
      "question": "In the fictional region of Riverton (c. 1820–1840) policymakers subsidised coal-fired steam engines, financed a canal and a short railway, and offered credit to entrepreneurs who reorganised rural cottage textile workshops into concentrated factory production. Over 20 years: the share of labour in agriculture fell from 80% to 45%, factory wages rose relative to farm incomes, the town population grew from $10{,}000$ to $40{,}000$, household sizes shrank as extended families dispersed, and outbreaks of cholera and typhoid increased in dense neighbourhoods lacking sanitation. Which explanation best integrates the three processes — economic reorganisation for manufacturing, technological/infrastructure catalysts, and urban/social consequences — to account for these changes?",
      "options": {
        "A": "The policy-induced economic reorganisation toward factory manufacturing raised labour productivity and wages, expanding demand for consumer goods and stimulating further industrial investment; contemporaneous technological and infrastructure innovations (steam engines, canals/railways, and later assembly principles) lowered transport and unit production costs and enabled scale economies, which concentrated employment in towns and produced urbanisation; the resulting denser settlements altered family arrangements (extended to nuclear households) and increased disease transmission through overcrowding and inadequate sanitation.",
        "B": "Riverton's growth is primarily explained by agricultural modernisation: improvements in farm yields freed workers who voluntarily preferred urban life for cultural reasons; technological investments like canals and steam engines were marginal, and the rise in disease indicates that urbanisation actually improved public health by making epidemics more visible and therefore manageable.",
        "C": "Technological change alone (the steam engine and rail links) accounts for all observed effects: once transport costs fell, markets automatically expanded and families chose to split into nuclear units for purely economic optimization, while disease increases are coincidental and unrelated to industrial employment patterns or housing density.",
        "D": "The principal driver was expansion of the tertiary sector and services, not manufacturing reorganisation; factories and transport projects only redistributed existing commerce, urban population growth resulted from higher birth rates in cities, and the social changes (smaller households and disease) were temporary demographic fluctuations rather than structural consequences of industrialisation."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete 1820–1840 Riverton scenario with quantitative changes to require integration of manufacturing-led economic reorganisation, role of steam/transport/assembly-line catalysts, and urban social consequences (family shift, disease); distractors isolate or misattribute causes plausibly.",
      "concepts_tested": [
        "Economic reorganisation for manufacturing and market expansion",
        "Role of technological innovations and infrastructure as catalysts (steam engine, canals/railways, assembly line)",
        "Urbanisation and social-structural consequences (family structure shifts, housing density, disease transmission)"
      ],
      "source_article": "Industrialisation",
      "x": 1.2705078125,
      "y": 0.9203110933303833,
      "level": 2,
      "original_question_hash": "c204a46f"
    },
    {
      "question": "Acme Systems is a mid-sized company with 500 employee desktops, 50 servers distributed across four regional offices, and a central IT team of four experienced administrators. They suffer from periodic application slowdowns, intermittent hardware failures, recurring software license overuse, and occasional account compromises. The CIO wants a centralized systems management strategy that (1) maps activities to the FCAPS framework, (2) uses Application Performance Management (APM) techniques — specifically event correlation, system automation, and predictive analytics — to reduce mean time to repair and prevent incidents, and (3) recognizes the trade-offs of centralization given their staff size and expertise. Which of the following prioritized implementation plans best satisfies these requirements?",
      "options": {
        "A": "Keep decentralized, manual administration at branch offices; hire one junior technician per office to perform hands-on inventory and patching; continue ad-hoc monitoring via local logs and spreadsheets; purchase endpoint antivirus only when incidents occur.",
        "B": "Deploy a centralized management platform that aligns to FCAPS: implement correlated event and metric collection and automated alerting (Fault); build a CMDB with automated discovery, provisioning and package management (Configuration); enable software metering and license accounting dashboards (Accounting); deploy APM for end-to-end transaction tracing plus predictive analytics for capacity and failure forecasting (Performance); and implement IAM and SIEM for policy enforcement and security auditing (Security). Pilot in one office, train the central IT team, then roll out gradually.",
        "C": "Outsource security and billing functions to a managed service provider, but retain manual configuration and performance monitoring using simple threshold alerts; track software usage quarterly by sampling users; defer automation until the company grows larger.",
        "D": "Invest heavily in advanced APM and predictive-analytics appliances to detect slow transactions and forecast failures, but retain manual hardware inventories and license management; rely on local administrators to perform configuration changes and on paper-based change logs for audits."
      },
      "correct_answer": "B",
      "generation_notes": "Created a realistic mid-sized company scenario requiring mapping of concrete initiatives to the FCAPS functions, incorporation of event correlation/automation/predictive analytics (APM), and explicit recognition of centralization trade-offs (pilot and training). Distractors emphasize common but insufficient alternatives.",
      "concepts_tested": [
        "FCAPS framework mapping to operational tasks (Fault, Configuration, Accounting, Performance, Security)",
        "Use of event correlation, system automation, and predictive analytics within APM for proactive systems management",
        "Trade-offs of centralized management relative to organizational size and IT staff expertise (scalability, pilot rollout, training)"
      ],
      "source_article": "Systems management",
      "x": 1.4404658079147339,
      "y": 1.0654668807983398,
      "level": 2,
      "original_question_hash": "453c207c"
    },
    {
      "question": "Portville is a mid-sized coastal city projected to experience a $0.5\\,\\text{m}$ sea-level rise by 2050 and more frequent heatwaves. City planners have a limited budget and must prioritize actions that (1) coordinate adaptation with mitigation where feasible, (2) emphasize anticipatory (proactive) measures rather than only reacting after disasters, and (3) deploy a mix of the four adaptation action types (infrastructural, institutional, behavioural, nature-based). Which of the following integrated strategies best meets these criteria?",
      "options": {
        "A": "Build an immediate, high concrete seawall along the entire shoreline and fund emergency shelters and short-term relief after storms. Rely on existing land-use rules and let households choose whether to retrofit buildings. Rationale: prioritize rapid protection and only act when impacts occur.",
        "B": "Adopt a phased plan that raises key coastal infrastructure and installs stormwater pumps (infrastructural), updates zoning and building codes to phase-managed retreat from the most exposed zones and mandates resilient construction standards (institutional), runs city-wide heat-action campaigns and subsidizes household passive cooling retrofits (behavioural), and restores mangrove belts and expands urban tree canopy (nature-based). Simultaneously expand low-emission public transport and rooftop solar to reduce emissions and gain co-benefits for resilience. Implement monitoring, early warning systems and a timeline to implement retreat before critical thresholds are reached (proactive).",
        "C": "Offer generous government-subsidized flood insurance and tax rebates for property repairs after storms, while promoting large-scale assisted migration incentives for vulnerable residents to nearby regions. Delay any new zoning or infrastructure investments until after major events demonstrate the need.",
        "D": "Invest primarily in nature-based measures: plant mangroves and restore wetlands along the coast, and establish urban parks to reduce heat. Leave infrastructural works and institutional reforms to be considered later, and do not link these investments to mitigation policies or planned retreat timelines."
      },
      "correct_answer": "B",
      "generation_notes": "Created a coastal-city scenario requiring evaluation of integrated responses; options contrast reactive vs anticipatory approaches and include combinations of infrastructural, institutional, behavioural, and nature-based measures plus mitigation co-benefits. Option B includes all required elements.",
      "concepts_tested": [
        "Adaptation and mitigation linkage",
        "Proactive (anticipatory) versus reactive adaptation",
        "Four types of adaptation actions (infrastructural, institutional, behavioural, nature-based)"
      ],
      "source_article": "Climate change adaptation",
      "x": 1.3298108577728271,
      "y": 0.8795329332351685,
      "level": 2,
      "original_question_hash": "9ba0b839"
    },
    {
      "question": "A sudden external shock causes a 30% fall in asset prices across a small open economy. Several small, poorly managed banks become insolvent, while a few larger banks are temporarily illiquid but fundamentally solvent. The central bank and treasury must choose a policy that preserves the economy’s ability to finance households and firms, prevents contagion that would disrupt employment and investment, and maintains incentives for efficient risk-taking. Which of the following policy packages best reflects the objective of financial stability as described (system resilience, not prevention of every individual failure, and support for efficient resource allocation and risk management)?",
      "options": {
        "A": "Provide short-term liquidity facilities to solvent but illiquid banks against good collateral, strengthen temporary macroprudential measures (e.g., higher liquidity coverage for systemically important banks), and allow insolvent, poorly managed small banks to be resolved through bankruptcy with limited deposit insurance to protect households — preserving intermediation while enforcing market discipline.",
        "B": "Guarantee all bank liabilities and recapitalize every failing bank regardless of solvency, to prevent any single firm from failing and to avoid any short-run disruption to credit flows.",
        "C": "Adopt a strict non-intervention stance: refuse any liquidity support or guarantees, immediately close all banks that miss payments (including systemically important ones), and let market prices and insolvency procedures sort out winners and losers.",
        "D": "Announce a permanent freeze on asset prices and implement broad-based government asset purchases to backstop bank equity values fully, preventing any decline in observed asset values regardless of underlying risk management failures."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a realistic shock scenario and presented four policy packages contrasting system-wide resilience (liquidity for solvent banks, resolution of insolvent ones) against policies that either prevent individual failures entirely or ignore systemic risk. Correct answer emphasizes self-correction, collective stability, and efficient resource allocation.",
      "concepts_tested": [
        "systemic resilience and shock absorption via self-corrective mechanisms",
        "policy focus on system-wide stability rather than preventing every individual failure",
        "efficient resource allocation and risk management enabling investment, employment, and monetary stability"
      ],
      "source_article": "Financial stability",
      "x": 1.3267103433609009,
      "y": 0.8981788158416748,
      "level": 2,
      "original_question_hash": "7d4736f8"
    },
    {
      "question": "A municipal authority contracts out bridge maintenance to independent construction firms. Firms differ privately in their intrinsic workmanship quality (High or Low) and can exert either High or Low effort after being hired. The authority (principal) cannot observe intrinsic quality before awarding the contract (adverse selection) nor directly observe effort afterwards (moral hazard). You may use regulatory tools (mandatory disclosure, warranties, audits) and contract design (menus, performance pay, retainers). Which combination of measures most directly (i) mitigates adverse selection before contracting, (ii) reduces moral hazard after contracting, and (iii) uses mechanism-design principles to align the contractor (agent) with the authority (principal)?",
      "options": {
        "A": "(i) Require verifiable pre-contract disclosure of past project inspections and offer a menu of contracts (e.g., low-price/short-warranty vs high-price/long-warranty) so firms self-select (screening); (ii) include cost-sharing and performance-linked payments (retainage, completion bonuses) plus random audits to monitor effort; (iii) require performance bonds/warranties and design incentive-compatible contracts (penalties for failure, staged payments) so truthful revelation and effort provision are optimal.",
        "B": "(i) Set a single pooled fixed premium payment to all bidders to equalize expected returns and avoid discriminating between firms; (ii) provide full insurance to contractors for any post-construction defects to ensure participation; (iii) outsource inspection and claims handling entirely to an independent third-party so the principal need not design incentives.",
        "C": "(i) Ban all pre-contract performance disclosures to protect firm privacy so no firm is singled out; (ii) impose large ex-post fines for failures as the main deterrent against shirking; (iii) allow managers of the contracting authority to set their own monitoring intensity without contractual commitment to adapt flexibly to revealed information.",
        "D": "(i) Mandate a government-set community rate (one-size-fits-all) and subsidize lower-quality firms to keep them in the market; (ii) eliminate deductibles and warranties to make contracts simpler and fairer to firms; (iii) demand full transparency of all internal contractor processes to achieve a state of perfect information and thereby avoid designing incentive schemes."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete public procurement scenario requiring students to identify interventions that address adverse selection (pre-contract screening/signaling and mandatory disclosure), moral hazard (post-contract incentives, cost-sharing, audits), and principal–agent/mechanism-design solutions (performance bonds, incentive-compatible contract design). Distractors present plausible but incorrect mixes.",
      "concepts_tested": [
        "Adverse selection and screening/signalling",
        "Moral hazard and post-contract incentives/monitoring",
        "Principal–agent problems and mechanism-design solutions (incentive-compatible contracts, warranties/bonds)"
      ],
      "source_article": "Information asymmetry",
      "x": 1.2774158716201782,
      "y": 0.9363733530044556,
      "level": 2,
      "original_question_hash": "0386e576"
    },
    {
      "question": "A research team collects 10 million 10-minute ECG recordings from wearable devices to develop a clinical guideline for predicting atrial fibrillation risk. The raw recordings include occasional sensor dropouts, baseline wander, and occasional extreme spikes caused by movement artifacts. The dataset is stored across a cluster and totals several petabytes. Which of the following workflows best transforms the raw data into validated clinical knowledge while addressing preprocessing and methodological issues associated with large-scale data?",
      "options": {
        "A": "Apply signal preprocessing (bandpass filtering, remove baseline wander, detect and remove spikes/outliers using a threshold such as values exceeding $3\\sigma$ of a local window, correct or flag sensor dropouts and impute missing segments), extract per-record features (RR intervals, HRV metrics, spectral features), analyze these features using scalable techniques (distributed/parallelized machine learning, dimensionality reduction, cross-validation and calibration), synthesize model outputs into clinically interpretable information, and validate and codify the findings into a guideline with expert review and reproducible (FAIR) data release.",
        "B": "Treat the raw ECG files as ready-to-use data and run classical statistical tests (e.g., t-tests and logistic regression) on a single workstation across the entire dataset; interpret statistically significant p-values as clinical knowledge and publish the guideline immediately to minimize processing time.",
        "C": "Preprocess by taking the mean amplitude of each 10-minute recording (thereby removing short-term variability), remove records with any missing samples, then use conventional regression on the reduced dataset stored on one machine; use the resulting coefficients as the basis for a clinical guideline without further validation.",
        "D": "Randomly sample 0.1% of the recordings, perform complete preprocessing and analysis on this small subset using off-the-shelf machine learning on a single PC, assume the resulting model generalizes to the whole population, and translate the model directly into clinical recommendations without external validation or documentation of data provenance."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a clinical ECG scenario to test mapping from raw data through cleaning (outlier removal, imputation), feature extraction to information, and scalable analysis (distributed ML) to produce validated knowledge; distractors violate preprocessing, lose information, or ignore big-data methodology and reproducibility.",
      "concepts_tested": [
        "Data → information → knowledge transformation",
        "Data collection and preprocessing (filtering, outlier removal, imputation)",
        "Big data methodological implications (scalable ML, distributed processing, reproducibility/FAIR)"
      ],
      "source_article": "Data",
      "x": 1.3723385334014893,
      "y": 1.0772417783737183,
      "level": 2,
      "original_question_hash": "cca3b160"
    },
    {
      "question": "AcmeCorp operates a corporate network where remote users authenticate to a VPN using only passwords, traffic passes through a stateful firewall, endpoints run signature-based anti-virus (AV) and a signature-based intrusion prevention system (IPS) sits at the perimeter. The security team also deploys an anomaly-based Network Detection and Response (NDR) system that uses unsupervised machine learning to profile normal host-to-host flows and a honeypot in a DMZ to observe attacker techniques. An attacker obtains a valid employee password via spear-phishing, logs into the VPN, implants a previously unseen lateral-movement tool on the employee's workstation, and begins scanning internal hosts. Which one of the following statements best describes which controls are likely to (1) have prevented initial access, (2) to what extent the firewall/IPS/AV will detect the implanted tool, and (3) which system is most likely to detect the attacker’s subsequent lateral-scanning behavior?",
      "options": {
        "A": "Requiring two-factor authentication (something the user has in addition to the password) would most likely have prevented initial VPN access; the stateful firewall will enforce allowed services but will not inspect encrypted VPN payloads, and signature-based IPS/AV are likely to miss a novel tool; the anomaly-based NDR (unsupervised ML) is the most likely control to detect unusual lateral scanning and generate alerts for auditing and investigation.",
        "B": "The firewall would have blocked the attacker’s VPN login if rules are strict, so multi-factor authentication is unnecessary; signature-based IPS/AV will detect any implanted tool if it behaves maliciously, and the honeypot is the primary detector of lateral scans because attackers always hit decoys first.",
        "C": "Signature-based AV/IPS would have prevented the initial password-based login by inspecting credentials and blocking malicious sessions; once the attacker is inside, only endpoint AV can stop lateral movement; anomaly-based NDR primarily reduces false positives and is unlikely to detect new lateral-scanning patterns.",
        "D": "Deploying a honeypot in the DMZ would have prevented the compromise by attracting the attacker away from the legitimate user; the firewall will quarantine infected hosts automatically, and anomaly-based NDR is redundant when AV and IPS are present."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete enterprise scenario mapping attack stages (credential theft, VPN access, novel malware, lateral scanning) to specific controls to test understanding of MFA as prevention, limitations of firewall/signature controls, and strengths of anomaly-based NDR and honeypots within a defense-in-depth model.",
      "concepts_tested": [
        "Multi-factor authentication as foundational prevention for access control",
        "Defense-in-depth and relationships/limitations between firewall, AV/IPS, IDS/NDR, and honeypots",
        "Detection and monitoring using anomaly-based systems and unsupervised machine learning for lateral movement"
      ],
      "source_article": "Network security",
      "x": 1.425597071647644,
      "y": 1.0705318450927734,
      "level": 2,
      "original_question_hash": "649fc9ea"
    },
    {
      "question": "A research team sequences a contiguous 1000-base region from 50 individuals sampled across three closely related taxa (populations X, Y, Z). They recover a total of 12 distinct haplotypes: 6 in X, 4 in Y, and 2 in Z. Across the alignment only 30 nucleotide positions are polymorphic. Which of the following interpretations best reflects the principles of molecular phylogenetics and the methodological history described in the article?",
      "options": {
        "A": "This supports the use of sequence data to infer evolutionary relationships and construct a phylogenetic tree; although in principle there are $4^{1000}$ possible sequences, the observed small number of haplotypes is expected because most sites are invariant and many substitutions are correlated; and modern DNA sequencing of defined loci generally provides higher-resolution, more directly evolutionary information than earlier protein/chemotaxonomic methods.",
        "B": "Because only 30 polymorphic sites were found, sequence data from this locus are essentially useless for phylogenetics; historical protein-based methods (chemotaxonomy) are preferable because they sample more of the genotype than sequencing a single locus.",
        "C": "The observation of only 12 haplotypes implies that mutations are not responsible for the differences among taxa; therefore molecular evolution does not explain the relationships and morphological data should be used exclusively to build the tree.",
        "D": "The small number of polymorphic sites indicates the mutation rate in this region is zero, so a molecular clock cannot be applied; hence DNA sequence comparisons cannot be used to estimate divergence times or infer phylogenetic relationships."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete sequencing scenario (1000 bp, 12 haplotypes, 30 polymorphic sites) and provided four interpretations testing three core concepts: use of sequence data for phylogeny, combinatorial space $4^L$ vs observed variation, and the methodological shift from chemotaxonomy to DNA sequencing. Only option A correctly synthesizes those points.",
      "concepts_tested": [
        "DNA sequence data for phylogenetic inference",
        "Combinatorial sequence space and observed haplotype variation (4^L, invariant sites)"
      ],
      "source_article": "Molecular phylogenetics",
      "x": 1.8640931844711304,
      "y": 1.127632737159729,
      "level": 2,
      "original_question_hash": "0d7c7b9b"
    },
    {
      "question": "You study a small forest-dwelling rodent across six habitat fragments and also analyze soil microbial communities from the same sites. For the rodents you genotype 8 microsatellite loci and record allele sizes; pairwise population differentiation yields an average $F_{ST}=0.25$. A Mantel test comparing genetic distance and geographic distance returns $r=0.62$, $p<0.01$. For the soil microbes you PCR-amplify the 16S rRNA gene with universal primers from environmental DNA and perform Illumina sequencing; operational taxonomic unit (OTU) composition is very similar among fragments and shows no significant distance-decay. Which one of the following interpretations is best supported by these molecular-ecological results?",
      "options": {
        "A": "The rodent populations are genetically differentiated with restricted gene flow and show isolation-by-distance (high $F_{ST}$ and significant Mantel result); management should consider fragment connectivity (landscape genetics/management units). The microbial pattern is plausibly due to high dispersal or strong environmental homogenization; PCR + Illumina 16S from environmental DNA is an appropriate way to characterize unculturable soil bacteria without culturing.",
        "B": "The rodent data indicate high contemporary gene flow because microsatellite allele sizes are variable despite $F_{ST}=0.25$, so no conservation action is needed; for microbes, the lack of distance-decay implies sequencing failed and cloning of PCR products would be required to recover true diversity.",
        "C": "Microsatellite allele-size data cannot be used to infer population structure or gene flow, so the rodent results are uninterpretable; environmental 16S sequencing only detects relic DNA and therefore cannot inform about living microbial communities, so microbial similarity is meaningless for ecology.",
        "D": "A significant Mantel $r$ means genetic and geographic distances are independent, so there is no isolation-by-distance; the rodent $F_{ST}=0.25$ therefore indicates large effective population size and high gene flow. Because microbes lack organized populations, OTU similarity across fragments implies that microbes are locally adapted rather than dispersing."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a realistic field study combining microsatellite allele-size analysis with $F_{ST}$ and Mantel test for animals, and PCR+Illumina 16S sequencing for environmental microbes; options test inference about gene flow, isolation-by-distance, suitability of PCR/NGS for unculturable taxa, and conservation/landscape-genetics implications.",
      "concepts_tested": [
        "Use of allele sizes and DNA markers (microsatellites) to quantify genetic diversity and infer population structure and gene flow",
        "PCR-based amplification and next-generation sequencing of marker genes (e.g., 16S) to study unculturable microbes from environmental samples",
        "Integration of molecular results with ecological disciplines (landscape genetics, conservation units, and interpretation of dispersal vs. environmental homogenization)"
      ],
      "source_article": "Molecular ecology",
      "x": 1.805782437324524,
      "y": 1.0934263467788696,
      "level": 2,
      "original_question_hash": "87d24f75"
    },
    {
      "question": "A research group characterizes a novel signalling molecule, Hormone A, produced by gland O. Their observations are: (1) Intravenous injection of purified A raises hepatic glucose output within 30 min and concurrently reduces circulating pituitary hormone P levels. (2) Living tissue slices of gland O show that A released from some cells modifies enzyme expression in neighboring cells; this local effect is abolished when an A-neutralizing antibody is present in the superfusate. (3) Single isolated O cells treated with the secretion inhibitor brefeldin A still show activation of a nuclear receptor and altered gene transcription attributable to A precursors retained inside the cell. (4) Electrical stimulation of a population of neurosecretory neurons that project directly to the bloodstream produces an immediate spike in plasma A. Which interpretation best accounts for these data?",
      "options": {
        "A": "Hormone A is a classical endocrine mediator with pleiotropic systemic effects (hepatic metabolism and suppression of pituitary P) that participates in negative feedback on the pituitary; additionally, it acts locally by paracrine signaling within gland O, can function intracrinally via intracellular precursor activation of nuclear receptors when secretion is blocked, and can be released by neurosecretory neurons (neuroendocrine release).",
        "B": "Hormone A functions exclusively as an endocrine hormone: the organ-slice and single-cell findings are experimental artefacts; neural stimulation merely modulates endocrine cell secretion (not neuroendocrine release), and all physiological effects are mediated solely by circulating A acting at distant targets.",
        "C": "Hormone A is best classified as a steroid-like intracellular hormone whose primary mode is intracrine action on nuclear receptors; the rapid plasma rise and antibody-sensitive local effects are inconsistent and likely reflect contamination or non-specific immune interactions.",
        "D": "Hormone A is primarily an autocrine factor acting on the same O cells that secrete it (single-cell response); the changes seen in neighboring cells and in the pituitary are indirect consequences of altered local cell function, and neurosecretory stimulation is unlikely to reflect bona fide hormone release into the blood."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a multi-part experimental vignette linking systemic pleiotropy and negative feedback with evidence for paracrine, intracrine, and neuroendocrine modes; options mix plausible but incorrect mechanistic interpretations so only A fits all observations.",
      "concepts_tested": [
        "Endocrine signalling and pleiotropy",
        "Feedback regulation and homeostasis",
        "Paracrine, autocrine, intracrine, and neuroendocrine communication modes"
      ],
      "source_article": "Endocrinology",
      "x": 2.0382230281829834,
      "y": 1.1382195949554443,
      "level": 2,
      "original_question_hash": "39777113"
    },
    {
      "question": "A mid-sized SaaS firm currently earns an average annual gross profit of $200 per customer and faces an annual retention rate of 80%. The firm has an existing customer (acquisition cost already sunk) and is deciding between two one-off strategic actions: (1) implement a personalized relationship-marketing program for that customer costing $50 per year that reduces churn so retention rises to 90%; or (2) spend $800 to acquire one new customer who will have the current baseline retention (80%). Using the standard geometric expectation for customer lifetime, where expected lifetime (years) = $1/(1-\\text{retention})$, compute the expected lifetime profit under each option and choose the action that maximizes the firm's lifetime profit contribution. Which is the correct decision?",
      "options": {
        "A": "Implement the personalized relationship-marketing program for the existing customer: expected lifetime becomes $1/(1-0.9)=10$ years and net lifetime profit after the $50/yr program is $(200-50)\\times10=\\$1{,}500$, an increase of $\\$500$ over the no-investment baseline — this yields more value than acquiring a new customer.",
        "B": "Acquire one new customer for $\\$800$: their expected lifetime is $1/(1-0.8)=5$ years so lifetime profit is $200\\times5-800=\\$200$, which is higher than the personalized program's gain, so pursue acquisition.",
        "C": "Split the $\\$800 between partial personalization and partial acquisition: spend $\\$400$ on personalization (covering 8 years at $\\$50/yr) and $\\$400$ on partial acquisition — this hybrid yields higher expected profit than either pure option.",
        "D": "Offer an immediate 10% price discount to boost short-term sales: the reduced margin lowers annual profit below $\\$200$ so this short-term price move is the best way to maximize lifetime profit given current retention rates."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete SaaS scenario with retention rates, lifetime formula $1/(1-r)$, and simple arithmetic to compare incremental lifetime profits from a personalization-driven retention intervention versus new-customer acquisition; options include plausible alternatives to test understanding of long-term CLV, cost efficiency of retention vs acquisition, and personalization/ongoing communication.",
      "concepts_tested": [
        "Long-term orientation and customer lifetime value (CLV)",
        "Cost efficiency of maintaining existing customers versus acquiring new ones",
        "Personalization and two-way communication as mechanisms to increase retention and CLV"
      ],
      "source_article": "Relationship marketing",
      "x": 1.3136171102523804,
      "y": 1.007088303565979,
      "level": 2,
      "original_question_hash": "a25a37bb"
    },
    {
      "question": "A genetic epidemiologist studies a chronic lung disease in a town. Genotype G (risk allele) frequency is 30% and environmental exposure E (airborne pollutant) prevalence is 40%. Incidence rates per 1000 person-years are: G+E+ = 20, G+E- = 5, G-E+ = 8, G-E- = 2. A separate familial study reports that first-degree relatives of affected individuals have a 3-fold higher risk than the general population. Which of the following interpretations best reflects the principles of genetic epidemiology given these data?",
      "options": {
        "A": "The familial relative risk of 3 indicates a substantial inherited contribution; the joint incidence (20) exceeds the additive expectation [baseline 2 + (5-2) + (8-2) = 11], demonstrating positive gene–environment interaction on the additive scale (although the multiplicative expectation also equals 20), so population-level quantitative analysis supports an interacting genetic and environmental model.",
        "B": "There is no genetic contribution: the familial aggregation must be due to shared environmental factors, and because the observed joint incidence (20) equals the multiplicative expectation, there is no gene–environment interaction, implying the disease is entirely environmental.",
        "C": "These data indicate a single-gene Mendelian disorder; the next step is segregation and linkage analysis to find a high-penetrance locus, because population association and family aggregation always imply monogenic inheritance.",
        "D": "Molecular genetics alone can identify the cause here; population incidence patterns and familial aggregation are irrelevant, so association and population-level statistical analyses are unnecessary."
      },
      "correct_answer": "A",
      "generation_notes": "Created a population/family scenario with numeric incidence rates to require calculation of additive vs multiplicative interaction and interpretation of familial aggregation in a population-level genetic epidemiology framework.",
      "concepts_tested": [
        "gene–environment interaction",
        "familial aggregation and inherited contribution to disease",
        "population-level statistical/quantitative analysis in genetic epidemiology"
      ],
      "source_article": "Genetic epidemiology",
      "x": 1.694883108139038,
      "y": 1.1002439260482788,
      "level": 2,
      "original_question_hash": "e7d28d30"
    },
    {
      "question": "A midsize healthcare firm, MediData, is launching a data program after regulators cited potential HIPAA and GDPR gaps following a merger. The CEO mandates an enterprise-wide initiative. The steering committee must (a) establish accountability for decisions about patient data use, retention, and third-party sharing; (b) set policies on classification, retention periods, and approval workflows; and (c) ensure compliance reporting to regulators. The IT and data teams must (i) implement encryption, backups, and secure deletion; (ii) build a metadata catalog, perform data profiling and cleansing, and configure access controls and audit logging; and (iii) run regular data-quality checks and produce operational reports for business units. Which description best characterizes the governance-management relationship, shows governance covering the data lifecycle (creation, storage, sharing, disposal), and attributes the program drivers correctly?",
      "options": {
        "A": "Governance: the steering committee's establishment of policies, accountability structures, and compliance reporting; Management: IT/data teams' technical implementations (encryption, catalogs, quality checks). The governance actions explicitly cover creation (entry/standards), storage (retention & protection), sharing (approval & contractual controls), and disposal (secure deletion). The initiative is driven by C-level mandate motivated by regulatory compliance and operational efficiency.",
        "B": "Governance: IT/data teams implementing encryption, backups, and catalogs, since these operational controls determine how data is governed; Management: the steering committee's role is limited to monitoring technical KPIs. Lifecycle aspects are primarily technical (storage and disposal), so governance need only focus on storage. The program is driven mainly by internal efficiency goals with compliance as a secondary concern.",
        "C": "Governance: appointing data stewards and creating technical procedures for secure deletion; Management: creating retention policies and deciding which third parties can receive data. The governance scope is limited to disposal and sharing, not creation or storage. The program is driven chiefly by customer-service improvement rather than regulatory or executive mandates.",
        "D": "Governance: executing day-to-day data cleansing, profiling, and access log reviews; Management: defining enterprise-wide policies, accountability structures, and regulatory reporting. Governance is therefore operational and does not need to cover the full lifecycle. The primary driver is legacy system consolidation rather than regulatory compliance or executive sponsorship."
      },
      "correct_answer": "A",
      "generation_notes": "Created a clinical-data merger scenario to test separation of governance (policy, accountability, oversight) from management (technical methods), lifecycle coverage, and regulatory/executive drivers; provided plausible distractors that invert or misattribute roles.",
      "concepts_tested": [
        "governance-management relationship",
        "lifecycle-oriented governance",
        "drivers and impact of governance (regulatory and executive drivers)"
      ],
      "source_article": "Data governance",
      "x": 1.3720182180404663,
      "y": 0.9964383840560913,
      "level": 2,
      "original_question_hash": "54f4d2a8"
    },
    {
      "question": "A manufacturer produces a batch of $10{,}000$ aerospace bushings specified with inner diameter $10.00\\ \\text{mm}\\ \\pm\\ 0.05\\ \\text{mm}$. During inspection each part is tested with a calibrated plug gauge and inspectors document that $2\\%$ of the parts ($200$ units) fail the gauge (diameter outside the tolerance). The customer contract requires a minimum acceptance rate of $99\\%$, and rework cost per bushing is high. As the QC lead, which one of the following responses best conforms to established quality control principles — i.e., it preserves the separation of inspection from release decisions, uses measurable tolerance limits and instrumentation correctly, and addresses the three QC dimensions (controls/processes/records; competence; soft elements such as culture and communication)?",
      "options": {
        "A": "Quarantine the batch, record the plug-gauge measurements and nonconformities, notify the contract manager and customer, and initiate a root-cause analysis with a corrective and preventative action (CAPA) plan that includes process control changes, operator retraining, and documented records; the release decision is made only after the contract terms and the CAPA outcome are evaluated.",
        "B": "Accept the entire batch despite the $2\\%$ failures to avoid the high rework cost; record a note that fiscal constraints justified acceptance so production targets are met and no further action is taken.",
        "C": "Immediately reject and scrap all $10{,}000$ parts because any failures indicate systemic process failure; replace the whole batch and hire a new supplier rather than attempt rework or analysis.",
        "D": "Adjust the design tolerance in all records to $\\pm 0.10\\ \\text{mm}$ so that the $200$ failed parts become acceptable on future inspections; update the plug-gauge set accordingly and continue production to avoid delays."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a realistic manufacturing scenario with numeric tolerances and inspection results; options contrast proper QC procedure (separation of inspection/release, use of gauges/tolerance limits, and triadic QC elements) with common but incorrect shortcuts (accept-for-cost, blanket scrap, changing specs).",
      "concepts_tested": [
        "Separation of inspection/testing from release decisions and interaction with fiscal constraints",
        "Use of measurable tolerance limits and calibrated instrumentation (plug gauge) to define acceptability",
        "Triadic QC model: controls/processes/records, competence (training), and soft elements (communication, culture, CAPA)"
      ],
      "source_article": "Quality control",
      "x": 1.4627665281295776,
      "y": 1.0019205808639526,
      "level": 2,
      "original_question_hash": "9f062644"
    },
    {
      "question": "A product team has an initial implementation of a mobile banking \"transfer money\" workflow. Lab usability tests with 20 participants (not representative of the target market) report: task success 85% (target $95\\%$), median completion time 75 s (target $<60$ s), and System Usability Scale (SUS) score 68/100 (target $\\ge 80$). The primary users are older adults (65+) living in rural areas with frequently slow mobile networks. Which single next-step plan best aligns with the ISO definition of usability (effectiveness, efficiency, satisfaction in a specified context of use), user-centered / participatory design practices (e.g., contextual inquiry, personas), and iterative refinement to improve these quantified metrics?",
      "options": {
        "A": "Add an in-app step-by-step tutorial and persistent tooltips to the shipped app, then run an A/B experiment in production to see if time and satisfaction improve.",
        "B": "Have a panel of usability experts perform a heuristic evaluation, implement fixes they recommend, then run a benchmark lab study with 30 convenience-sample college students to measure time-to-complete and report SUS.",
        "C": "Conduct contextual inquiry and observation sessions with representative older rural users in their homes (to capture network and environmental context), synthesize personas, create low-fidelity prototypes, and perform rapid iterative usability tests measuring task success, time, and SUS until the metrics approach the targets.",
        "D": "Add more transfer options (e.g., scheduled transfers, recipient groups) and deploy remote unmoderated surveys to a broad global user base to validate whether the new features increase satisfaction."
      },
      "correct_answer": "C",
      "generation_notes": "Created a concrete mobile-app scenario with quantified effectiveness/efficiency/satisfaction goals and realistic constraints (older rural users, slow networks). Options contrast user-centered contextual inquiry + iterative prototyping (correct) with expert-only, feature-bloat, or production-test shortcuts.",
      "concepts_tested": [
        "Usability defined as effectiveness, efficiency, and satisfaction in a specified context of use",
        "User-centered and participatory design practices (contextual inquiry, personas, involving representative users)",
        "Iterative design and testing to improve usability metrics in-context"
      ],
      "source_article": "Usability",
      "x": 1.415049433708191,
      "y": 1.0655267238616943,
      "level": 2,
      "original_question_hash": "d045a170"
    },
    {
      "question": "An NHS Trust identifies a sustained rise in post‑operative surgical‑site infections at one of its hospitals. The Trust Board wants to respond in a way that conforms to the NHS concept of clinical governance. Which of the following sets of actions best embodies clinical governance as defined by the NHS (recognisable high standards, transparent accountability, continuous improvement), the statutory duties of Trust Boards, and the relationship between clinical and corporate governance?",
      "options": {
        "A": "The Board formally designates a named executive with responsibility for clinical governance, commissions immediate clinical audits and root‑cause analyses of infection control practices, funds targeted refurbishment of sterilisation equipment and CPD for theatre staff, publishes an Annual Review of Clinical Governance that transparently reports findings and remedial plans, and liaises with corporate governance functions (estates, staffing) only where those functions affect delivery of care.",
        "B": "To protect the Trust’s finances and reputation the Board orders a hiring freeze, delegates investigation entirely to clinicians with no Board oversight, restricts external reporting of infection data, and delays capital investment until the next financial year.",
        "C": "The Board replaces its current systems with a single centralised corporate governance regime that standardises all business processes (including catering, procurement and clinical protocols) across the Trust, but does not designate a Board member for clinical quality or produce a separate Annual Review of Clinical Governance.",
        "D": "The Board requires quarterly in‑house education sessions and clinician self‑reporting of outcomes, refuses to prepare an Annual Review of Clinical Governance, limits audits to administrative compliance (not clinical outcomes), and treats the infection rise as an isolated clinical issue without linking to risk management or transparency mechanisms."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a clinical scenario to test (1) the framework attributes (high standards, accountability, continuous improvement), (2) statutory Board duties including designated responsibility and Annual Review, and (3) limits and interface with corporate governance (integrated governance only where it affects care). Option A includes all required elements; others omit or contradict key principles.",
      "concepts_tested": [
        "Clinical governance framework: high standards, accountability, continuous improvement",
        "Statutory duties of NHS Trust Boards: designated responsibility and Annual Review of Clinical Governance",
        "Scope and relationship between clinical and corporate governance (integrated governance; care‑focused scope)"
      ],
      "source_article": "Clinical governance",
      "x": 1.2886823415756226,
      "y": 0.9291515946388245,
      "level": 2,
      "original_question_hash": "231c3ddf"
    },
    {
      "question": "A researcher conducts an observational cohort study comparing two diets, A and B, with $n=200$ patients on each diet. Two outcomes are recorded for each patient over one year: (i) the number of hospital admissions (nonnegative integer counts, possibly 0,1,2,...), and (ii) LDL cholesterol (a continuous measurement in mg/dL). The researcher plans to (a) estimate the difference between diets in average LDL and in admission rates, (b) report $95\\%$ confidence intervals, and (c) base inference on a chosen statistical model. Which of the following statements best describes appropriate modelling, the role of probability distributions, and the interpretation of the reported intervals given this observational design?",
      "options": {
        "A": "Use a count model (e.g. Poisson regression, switching to negative binomial if overdispersion is detected) for hospital admissions because the outcome is discrete counts; use ordinary least squares or a suitable continuous-error model for LDL if residuals are approximately normal but check assumptions. Reported $95\\%$ confidence intervals quantify uncertainty under the chosen models and sampling scheme. Because the study is observational, causal claims require additional assumptions (e.g. no unmeasured confounding) and conclusions depend on the model and those subjective assumptions; descriptive summaries (means, rates) differ from inferential statements that rely on a probability model.",
        "B": "Fit a standard linear regression to the count outcome and to LDL; linear models are preferable because with $n=200$ per group the central limit theorem guarantees normality of estimators, so $95\\%$ confidence intervals are always valid and permit unambiguous causal conclusions even in an observational study.",
        "C": "Treat the number of admissions as binomial (successes/failures) because admissions are events and LDL as a nonparametric ordinal variable; avoid reporting confidence intervals because they only apply to randomized experiments, not to observational data.",
        "D": "Always use nonparametric methods for both outcomes to avoid assumptions: rank-based tests for admissions and LDL give the most powerful inference; model choice is irrelevant for observational studies because large samples eliminate model dependence, so reported intervals can be interpreted as providing objective causal effects."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete observational cohort comparing two diets with a discrete count outcome and a continuous outcome to test model selection (Poisson/negative binomial vs OLS), the centrality of probability distributions, the meaning of $95\\%$ confidence intervals, and that inference from observational studies depends on model/assumptions.",
      "concepts_tested": [
        "distinction between descriptive and inferential statistics and confidence intervals",
        "selection of appropriate probability distributions for discrete (count) vs continuous outcomes (Poisson/negative binomial vs normal/OLS)",
        "dependence of inference on chosen model and subjectivity of causal claims in observational studies"
      ],
      "source_article": "Mathematical statistics",
      "x": 1.6134240627288818,
      "y": 1.1426594257354736,
      "level": 2,
      "original_question_hash": "f1995ffe"
    },
    {
      "question": "Consider the following scenario: the province of Territoria holds a unilateral referendum and proclaims independence from State A. Territoria exercises effective control over a defined land area of 8,000 km^2, has a permanent population of 600,000, an organised government administering public services, and has begun informal contacts with several foreign states and NGOs. A few small states publicly recognise Territoria, but major powers and the UN do not. Immediately thereafter, neighbouring State B sends troops into part of Territoria, occupies territory and declares annexation, justifying its action on historical ties and by claiming it acted to protect an ethnic minority. The UN General Assembly issues a resolution condemning the use of force; some foreign courts treat certain acts of the Territorian authorities as having domestic legal effect within the territory they control. Which of the following best describes the international-legal and diplomatic implications of these facts?",
      "options": {
        "A": "Under the declarative (Montevideo) criteria Territoria appears to satisfy statehood (territory, permanent population, government, capacity to enter relations), so its existence as an international person is not solely dependent on recognition. However, partial recognition by only some states limits Territoria's ability to engage in full diplomatic relations and membership in many international organisations. Westphalian principles of territorial integrity and non‑interference, together with the UN Charter and the prohibition on the use of force (now treated as a jus cogens norm), render State B's forcible annexation unlawful; self‑determination can justify secession politically but does not authorise external military intervention to annex territory.",
        "B": "Because most major powers and the UN refuse to recognise Territoria, the constitutive theory dictates it is not a state; only recognition by leading powers can create statehood in practice. Consequently State B's occupation and annexation are lawful exercises of sovereignty over territory that remains part of State A absent universal recognition.",
        "C": "Territoria's effective control and recognition by several states make it ipso facto both de jure and de facto; therefore State B's intervention is lawful if framed as humanitarian protection of a minority, since humanitarian intervention overrides Westphalian non‑interference and permits forcible annexation when protecting human rights.",
        "D": "Territoria cannot be a state because its territory (8,000 km^2) and population (600,000) fail minimum size thresholds required by international law; recognition by small states cannot create statehood, so the international community should treat Territoria merely as a rebellious subunit and allow either State A or neighbouring powers to reassert sovereignty."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete secession/annexation scenario testing declarative vs constitutive theories, Westphalian non‑interference and territoriality, recognition's practical effects on diplomatic capacity, and the modern jus cogens prohibition on use of force and self‑determination tension.",
      "concepts_tested": [
        "Westphalian sovereignty (territorial integrity and non‑interference)",
        "Role of recognition in statehood and diplomatic relations (declarative vs constitutive)",
        "Evolving international law norms: self‑determination and prohibition on the use of force as jus cogens"
      ],
      "source_article": "Sovereign state",
      "x": 0.649715006351471,
      "y": 0.5065658688545227,
      "level": 2,
      "original_question_hash": "dbfa1d24"
    },
    {
      "question": "A fieldworker conducts an ethnosemantic study of spatial language in two neighboring speech communities. For each language she elicits the full set of directional and locative terms used in everyday navigation and then performs componential analysis on those terms. In Language C (a coastal fishing society) many directional terms are analyzed into features such as [+coastward], [+toward-mouth-of-river], and [+along-coast], while in Language D (an agrarian inland society) many directional terms analyze into features such as [+sunrise-derived], [+sunset-derived], and [+azimuthal]. Which of the following conclusions is best supported by the combination of ethnosemantic data and componential analysis?",
      "options": {
        "A": "The results indicate that each language encodes distinct cultural schemas for orientation—Language C encodes an environment-anchored (landmark/river/coast) schema while Language D encodes a solar/azimuthal schema—so linguistic form reflects group-level cultural cognition and likely shapes habitual spatial framing and related metaphors; ethnosemantic mapping and componential features justify this interpretation without asserting strict cognitive determinism.",
        "B": "Because Language C terms have [+coastward] features, speakers of Language C must be unable to conceptualize directions using sunrise/sunset; componential features therefore demonstrate an innate, mutually exclusive cognitive module for spatial orientation in each community.",
        "C": "The presence of [+sunrise-derived] features in Language D shows that its speakers universally conceptualize time only in solar metaphors and cannot use river- or landmark-based metaphors for temporal or spatial reasoning.",
        "D": "Componential analysis proves that Language C's lexical distinctions cause its speakers to navigate only along rivers and coasts, which demonstrates a deterministic Sapir–Whorf effect where linguistic categories fully determine spatial behavior and cognition."
      },
      "correct_answer": "A",
      "generation_notes": "Created a comparative ethnosemantic/componential analysis scenario contrasting coastal landmark-based vs solar/azimuthal orientation; options test correct inference about language encoding cultural schemas and cautious claims about influence vs determinism.",
      "concepts_tested": [
        "Language encodes cultural schemas and metaphors linking linguistic form to cultural cognition",
        "Use of ethnosemantics and componential analysis to reveal semantic categories and cultural meanings",
        "Cultural variation in spatial orientation semantics demonstrating culture shapes linguistic categorization"
      ],
      "source_article": "Ethnolinguistics",
      "x": 1.2240105867385864,
      "y": 1.0616055727005005,
      "level": 2,
      "original_question_hash": "b4589cc9"
    },
    {
      "question": "A GPS satellite carries a caesium atomic clock in circular orbit 20,200 km above Earth's surface (take Earth's radius = $6.371\\times10^6\\,$m). Its orbital speed is $v=3.874\\times10^3\\,$m/s. Use the approximations for relativistic fractional rate shifts: special relativistic time dilation $\\Delta t/t\\approx -\\tfrac{1}{2}v^2/c^2$ and gravitational (general relativistic) rate difference $\\Delta t/t\\approx +\\dfrac{GM}{c^2}\\left(\\dfrac{1}{r_{\\rm surface}}-\\dfrac{1}{r_{\\rm sat}}\\right)$, where $GM\\approx3.986\\times10^{14}\\,$m^3/s^2$ and $c$ is the speed of light. Which of the following statements is correct about (i) the net daily rate difference between the satellite clock and an identical ground clock and (ii) how GPS timekeeping is synchronized to civil time standards?",
      "options": {
        "A": "The satellite clock ticks faster by about $+38\\,$microseconds per day (GR effect $\\sim+45.6\\,\\mu$s/d minus SR effect $\\sim7.2\\,\\mu$s/d). To keep satellite time readable as a coordinate time on Earth, GPS pre‑offsets the satellite oscillator frequency before launch so the on‑orbit clock rate matches the terrestrial coordinate rate; GPS time itself is a continuous atomic time scale that does not insert leap seconds, so it differs from UTC by an integral second offset that is carried in the navigation messages and used to synchronize to UTC/TAI.",
        "B": "The satellite clock ticks slower by about $-38\\,$microseconds per day, so operators accelerate the on‑orbit clocks to match ground clocks; GPS time is identical to UTC (it includes leap seconds), so no additional integer offset is necessary to synchronize with civil time.",
        "C": "The satellite clock ticks faster by about $+38\\,$microseconds per day, but GPS makes no pre‑launch frequency adjustment — instead every receiver must continuously compute and apply a relativistic correction term from the satellite ephemeris; GPS time is the same as UTC and therefore already accounts for leap seconds.",
        "D": "The satellite clock ticks faster by about $+38\\,$microseconds per day; satellite clocks are left unadjusted and are only corrected intermittently by ground control uploads, while GPS time is maintained equal to UT1 (mean solar time) so receivers need no additional leap‑second corrections."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete GPS satellite scenario using the SR and GR approximations to compute the net +38 μs/day rate; tested operational definition of time (clocks read time), pre‑compensation of satellite oscillators, and distinction between GPS time, TAI, UTC and leap‑second handling.",
      "concepts_tested": [
        "Spacetime and relativistic time dilation (special and gravitational)",
        "Operational definition of time as what clocks read and practical clock adjustments",
        "Global timekeeping standards and synchronization: GPS time vs UTC/TAI and leap second handling"
      ],
      "source_article": "Time",
      "x": 1.587631344795227,
      "y": 1.0417075157165527,
      "level": 2,
      "original_question_hash": "bcf6b284"
    },
    {
      "question": "You lead an interdisciplinary team charged with designing a patient-facing medication-dispensing kiosk for older adults. The product must support multimodal interaction (visual displays, speech-based VUI, and haptic confirmation) and achieve high end-user computing satisfaction. Which combination of decisions and evaluation methods most directly reflects core HCI principles described in the article?",
      "options": {
        "A": "Assemble a multidisciplinary team (UX designers, software engineers, cognitive psychologists, clinicians, and linguists); follow a user-centered, iterative design process that prototypes visual, auditory (TTS/ASR) and haptic feedback; evaluate with representative users (older adults) using empirical usability metrics (time-on-task, error rates), standardized satisfaction questionnaires, and ethnographic observation; iterate until task performance and satisfaction targets are met.",
        "B": "Let engineers implement a high-functionality system with many advanced gestures and features; evaluate primarily using automated server logs, throughput and system latency metrics; pilot only with tech-savvy users and rely on internal heuristics for accessibility compliance before release.",
        "C": "Adopt activity-theory as the sole theoretical framework, build a multimodal prototype emphasizing speech recognition accuracy and BCI sensor integration; measure success by technical recognition rates and hardware sensitivity metrics without structured user satisfaction questionnaires.",
        "D": "Design a conservative GUI-only kiosk with consistent iconography and strict safety certification; deploy widely and infer user satisfaction post-deployment from crash reports, support tickets, and occasional unsolicited user emails."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete design scenario requiring multimodal interface choices, interdisciplinary staffing, iterative user-centered methods, and empirical evaluation tied to end-user satisfaction; distractors violate one or more HCI principles (single modality, narrow metrics, non-representative users).",
      "concepts_tested": [
        "Multimodal dialogic interaction (visual, auditory, haptic) and interface paradigms",
        "Interdisciplinary nature of HCI (design, CS, psychology, linguistics, social sciences)",
        "User satisfaction as a central outcome and empirical usability evaluation methods"
      ],
      "source_article": "Human–computer interaction",
      "x": 1.3642741441726685,
      "y": 1.0619874000549316,
      "level": 2,
      "original_question_hash": "284bd56c"
    },
    {
      "question": "Company Alpha reports cash of $200{,}000$, cash equivalents $50{,}000$, short-term investments $150{,}000$ and notes payable of $300{,}000$. Its annual gross operating surplus is $80{,}000$ and expected annual loan repayments are $70{,}000$. Total debt is $500{,}000$ and equity is $250{,}000$. Based on the Net Liquid Balance formula $NLB = \\text{cash} + \\text{cash equivalents} + \\text{short-term investments} - \\text{notes payable}$ and the other data, which of the following conclusions is most accurate regarding Alpha's short-term solvency, long-term viability, likely borrowing terms, and the applicability of cryptographic proofs of liabilities/assets?",
      "options": {
        "A": "Alpha's $NLB=200{,}000+50{,}000+150{,}000-300{,}000=100{,}000$, so it is short-term solvent. Long-term viability is marginal because gross operating surplus minus repayments is $80{,}000-70{,}000=10{,}000$, which is above zero but not well above it; combined with a leverage ratio of $500{,}000/250{,}000=2$ this raises credit risk, so a lender would likely charge a higher interest rate than for a lower-leverage borrower. Privacy-preserving cryptographic proofs of liabilities/assets can be used to help validate solvency without revealing complete balance-sheet details.",
        "B": "Alpha's $NLB$ is negative ($100{,}000$ in assets minus $300{,}000$ notes payable gives $-200{,}000$), so it is short-term insolvent; therefore long-term viability is moot, and lenders will refuse credit regardless of leverage. Cryptographic proofs are irrelevant because insolvency is obvious.",
        "C": "Alpha's $NLB=100{,}000$ so it is short-term solvent, and because $80{,}000-70{,}000=10{,}000$ is positive the company is comfortably solvent long-term; the leverage ratio of 2 is moderate and will earn Alpha more favourable interest rates than most borrowers. Cryptographic proofs cannot be used to show solvency because they only prove existence of transactions.",
        "D": "Alpha has $NLB=100{,}000$ and therefore is fully solvent both short- and long-term; leverage is immaterial to lenders who focus only on $NLB$. Cryptographic proofs of liabilities/assets are unnecessary and cannot influence lending terms."
      },
      "correct_answer": "A",
      "generation_notes": "Created a numerical firm scenario to require computation of NLB, evaluation of long-run viability via gross operating surplus minus repayments, assessment of leverage impact on credit terms, and inclusion of blockchain proofs of liabilities/assets as a validation tool.",
      "concepts_tested": [
        "Short-term solvency via Net Liquid Balance (NLB)",
        "Long-term solvency assessment using gross operating surplus versus loan repayments and leverage ratio",
        "How solvency assessment affects lending terms and role of cryptographic proofs of liabilities/assets"
      ],
      "source_article": "Solvency",
      "x": 1.360373854637146,
      "y": 0.926463782787323,
      "level": 2,
      "original_question_hash": "726e22eb"
    },
    {
      "question": "The government of the low‑income country Zamora asks your policy advice. Between 2000 and 2015 Zamora's GDP per capita rose from $1,200 to $2,600 (an increase of approximately $\\approx116.7\\%$), life expectancy increased from 54 to 56 years, mean years of schooling remained unchanged, the Gini coefficient rose from 38 to 45, and the share of people in extreme poverty fell from 40% to 20% (a $50\\%$ reduction). Zamora met several MDG targets (safe water access and halving extreme poverty) but faces growing inequality, stagnant education outcomes, and expanding urban slums. Which policy package best exemplifies the post‑GDP, human development approach to international development while appropriately leveraging international institutions and fixing the measurement feedback problems highlighted by the MDG experience?",
      "options": {
        "A": "Double down on market liberalisation: negotiate IMF/World Bank conditional loans that prioritise macroeconomic stabilisation and structural reforms intended to accelerate GDP growth; use GDP per capita and export growth as the primary policy metrics, assuming benefits will eventually diffuse to poorer groups.",
        "B": "Reframe targets around human development: adopt composite and disaggregated indicators (e.g., HDI components, maternal mortality ratio, Gini, school completion rates), implement pro‑poor social programmes and capacity building, engage the SDG framework with participatory monitoring involving NGOs and local communities, and seek concessional finance from multilateral development banks conditioned on social outcomes rather than austerity.",
        "C": "Withdraw from Bretton Woods institutions and global goal processes as neo‑colonial impositions; rely instead on remittances and private foreign direct investment to drive private sector growth, using GDP growth as the sole indicator of success to attract investors.",
        "D": "Prioritise large capital‑intensive infrastructure projects financed by multinational corporations and reported as MDG/SDG achievements; measure success primarily by increased exports and headline GDP growth while delegating social outcomes to future private sector development plans."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete country scenario with numeric changes to illustrate divergence between GDP growth and human development outcomes; options map to historical policy paradigms (IMF/World Bank neoliberalism, rights‑based/SDG approach, withdrawal/FDI reliance, infrastructure‑led growth) to test understanding of the shift from GDP metrics to broader human development, role of international institutions/frameworks, and measurement critique leading from MDGs to SDGs.",
      "concepts_tested": [
        "Shift from GDP‑based economic development to human development/quality‑of‑life metrics",
        "Role of international institutions and global frameworks (Bretton Woods, MDGs, SDGs) in shaping policy",
        "Measurement critique and policy feedback (limits of GDP, need for disaggregated indicators, participatory monitoring)"
      ],
      "source_article": "International development",
      "x": 1.2173296213150024,
      "y": 0.8796822428703308,
      "level": 2,
      "original_question_hash": "7c7ffa0a"
    },
    {
      "question": "A 1500 ha watershed contains a mixed deciduous forest in the headwaters, an intensively managed 300 ha cornfield in the mid-watershed, and a coastal estuary that supports a valuable fishery. Current measurements show: headwater forest NPP ~800 g C m^-2 yr^-1, cornfield nitrate export to the stream ~12 kg N ha^-1 yr^-1, and seasonal hypoxia events in the estuary. Climate models project a $20\\%$ increase in annual precipitation and more intense storm events over the next 30 years. Local managers are considering actions to (1) maintain terrestrial biomass productivity, (2) reduce downstream water-quality degradation, and (3) sustain the estuarine fishery. Which single management package is most consistent with ecosystem-ecology principles (functional processes, cross-ecosystem interdependence, and external drivers) to best achieve all three goals?",
      "options": {
        "A": "Increase nitrogen fertilizer on the cornfield by $30\\%$ to raise mid-watershed NPP, and rely on engineering drainage to speed runoff away from crops so flooded fields diminish, thereby maximizing short-term biomass yields.",
        "B": "Convert 50 ha of the headwater forest to fast-growing pine plantations with higher leaf area index (to increase NPP), and intensify summer harvesting to supply timber, while leaving current fertilizer practices unchanged in the cornfield.",
        "C": "Reduce cornfield fertilizer application rates and adopt precision-fertilization to minimize nitrate export; restore and maintain a 30–50 m riparian forest buffer along streams to intercept nutrient runoff and stabilize stream temperature; and implement staggered harvest rotations in the headwater forest to preserve continuous canopy and litter inputs.",
        "D": "Promote the estuarine fishery by stocking more tolerance-trophic fish and remove large piscivores from the estuary to boost catch; maintain current terrestrial management to maximize production while treating hypoxia episodes with aeration technology when they occur."
      },
      "correct_answer": "C",
      "generation_notes": "Constructed an applied watershed management scenario requiring integration of primary productivity, decomposition/nutrient cycling, trophic interactions, cross-ecosystem flows, and climate-driven runoff; option C reflects reducing external nutrient inputs, restoring riparian buffers, and forest harvest timing to sustain processes and services.",
      "concepts_tested": [
        "Functional processes: primary productivity, decomposition, nutrient cycling",
        "Cross-ecosystem interactions and nebulous boundaries (upstream actions affecting downstream systems)",
        "External drivers: climate-mediated runoff, disturbance and management interventions"
      ],
      "source_article": "Ecosystem ecology",
      "x": 1.6556884050369263,
      "y": 0.9900677800178528,
      "level": 2,
      "original_question_hash": "035ec5d2"
    },
    {
      "question": "A laboratory administers a selective NMDA receptor antagonist locally to the dorsal hippocampus of adult rats for two weeks. In vitro recordings from Schaffer collateral–CA1 synapses show a marked reduction in NMDA-dependent long-term potentiation (LTP) and decreased postsynaptic AMPA receptor insertion after high-frequency stimulation. In behaving animals, in vivo unit recordings reveal less stable CA1 place fields and reduced theta-phase locking, and the rats perform poorly on the Morris water maze. Which interpretation best integrates the multilevel mechanisms (molecular → synaptic → circuit → behavior) and relates these findings to the pathophysiology of a neuropsychiatric disorder?",
      "options": {
        "A": "Chronic NMDA receptor blockade reduces postsynaptic Ca2+ influx (molecular), impairing induction of LTP and AMPA receptor trafficking at individual synapses (synaptic). The weakened synaptic weights destabilize place-cell ensembles and disrupt coordinated theta-phase firing (circuit), producing deficits in spatial memory (behavior). This pattern is consistent with NMDA-hypofunction hypotheses for cognitive symptoms in disorders such as schizophrenia.",
        "B": "Chronic NMDA receptor blockade increases presynaptic glutamate release via homeostatic upregulation (molecular), which preserves LTP but results in aberrant network hyperexcitability (circuit) that paradoxically impairs task performance (behavior); therefore the behavioral deficit reflects network overexcitation rather than reduced synaptic plasticity.",
        "C": "Blocking NMDA receptors primarily disrupts inhibitory interneuron metabolism (molecular), leading to progressive cell death in CA1 (synaptic loss). The observed place-field instability and maze impairment are direct consequences of neurodegeneration rather than altered synaptic plasticity.",
        "D": "NMDA receptor blockade enhances AMPA receptor conductance at remaining synapses (molecular), so LTP magnitude should be unchanged; the behavioral deficits and place-field instability therefore reflect an unrelated stress response to the infusion procedure rather than a change in synaptic mechanisms."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete hippocampal NMDA antagonist scenario to require students to connect molecular (NMDA/Ca2+), synaptic (LTP, AMPA trafficking), circuit (place-cell stability, theta locking), and behavioral (spatial memory) levels and to link to pathophysiology (NMDA hypofunction in schizophrenia). Distractors present plausible but incorrect multilevel accounts (presynaptic compensation, neurodegeneration, non-synaptic stress).",
      "concepts_tested": [
        "Neurotransmission and synaptic plasticity (NMDA-dependent LTP, AMPA trafficking)",
        "Multilevel organization of the nervous system (molecular → synaptic → circuit → behavior)",
        "Pathophysiology of psychiatric disorders as disruptions in synaptic signaling (NMDA hypofunction models)"
      ],
      "source_article": "Neurophysiology",
      "x": 1.8054414987564087,
      "y": 1.1517224311828613,
      "level": 2,
      "original_question_hash": "9e8cffe8"
    },
    {
      "question": "Consider 1,2-dichloroethane which exists in two conformers: anti (dihedral ≈ 180°) and gauche (dihedral ≈ 60°). The gauche conformer lies higher in energy by ΔE = 500 cm^{-1} relative to anti. Taking into account (i) the Boltzmann population at 298 K, (ii) the locality/transferability of bond lengths and bond angles, and (iii) the phase-dependent ensemble of geometries, which of the following statements is MOST accurate?",
      "options": {
        "A": "At 298 K the Boltzmann factor for ΔE = 500 cm^{-1} is approximately 0.089, so roughly 8.9% of molecules occupy the higher-energy (gauche) conformer; the local bond lengths and bond angles (e.g. C–C and C–H distances and the immediate bond angles at each carbon) are largely unchanged between conformers due to their approximately local/transferable character, whereas the dihedral angle differs and alters the molecular dipole moment (so polarity, solubility and some reactivity change); in the solid state one conformer often predominates because of crystal-packing forces, while gas or solution samples show a thermally averaged mixture accessible to IR/microwave spectroscopy, and X-ray crystallography reports the solid-state geometry.",
        "B": "At 298 K about half of the molecules occupy the higher-energy conformer (≈50%); significant changes in C–C and C–H bond lengths occur between anti and gauche because conformational change dramatically redistributes bonding electrons; therefore polarity and reactivity remain essentially unchanged, and all experimental methods (X-ray, IR, microwave) give identical averaged geometries in any phase.",
        "C": "The population of the higher-energy conformer at 298 K is negligible (<0.1%), so only the anti form is relevant; moreover bond angles and bond lengths are not transferable and can differ substantially between the two conformers, so local geometric parameters must be re-measured for each conformer; phase (solid, solution, gas) has negligible effect on which conformer is observed.",
        "D": "The Boltzmann factor for ΔE = 500 cm^{-1} at 298 K is on the order of 10^{-3}, implying essentially zero population of the higher-energy conformer; however, because conformational change only involves rotation the dipole moment and bulk properties are identical for anti and gauche, and diffraction methods in solution will directly yield the full range of conformers present."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete conformational example (1,2-dichloroethane, ΔE = 500 cm^{-1}) to test quantitative Boltzmann population, the transferability/local nature of bond lengths/angles, and phase-dependent conformer populations and appropriate experimental probes.",
      "concepts_tested": [
        "Geometry influences physical properties and reactivity (polarity, solubility, reactivity changes with dihedral)",
        "Locality and transferability of bond lengths and bond angles vs global dihedral changes",
        "Thermal excitation/conformational isomerism and phase dependence of observed geometries (Boltzmann populations, solid vs gas/solution, spectroscopic vs diffraction methods)"
      ],
      "source_article": "Molecular geometry",
      "x": 1.893564224243164,
      "y": 1.072304129600525,
      "level": 2,
      "original_question_hash": "956fc7ad"
    },
    {
      "question": "CaffeNova, a US specialty coffee brand, plans to expand into Australia, Brazil and China by licensing its name, recipes, standard operating procedures and trademarks to local operators. It charges an upfront franchise fee, an ongoing royalty of 6.7% of gross sales and a 2% marketing contribution, requires standardized signage, uniforms and training, and offers exclusive territories for ten years. Which of the following best describes (1) why franchising is attractive to CaffeNova as a growth mechanism, (2) the typical governance dynamic between franchisor and franchisee under such contracts, and (3) the regulatory matters CaffeNova must expect when entering those three countries?",
      "options": {
        "A": "Franchising lets CaffeNova scale rapidly while minimizing its own capital expenditure and liability because local franchisees supply start‑up capital and assume operational risk; the franchisor retains control over brand, know‑how and operating standards via a unilateral franchise agreement (producing a legal/economic imbalance in favour of the franchisor) and earns fees typically calculated on gross sales; moreover CaffeNova must comply with country‑specific disclosure and registration rules (e.g., Australia’s Franchising Code requires a disclosure document and good‑faith obligations with at least a 14‑day disclosure period; Brazil mandates a Franchise Offer Circular and INPI registration; China requires advance disclosure (about 20 days) and historically a minimum track record such as the “two‑shop/one‑year” requirement for foreign franchisors).",
        "B": "Franchising is primarily attractive because the franchisor avoids any operational control and therefore avoids all liability; franchisees become equal partners with full autonomy over brand and pricing; no special disclosure or registration is needed in countries like Australia, Brazil or China because franchising is treated purely as a commercial contract.",
        "C": "Franchising benefits CaffeNova because royalties are normally levied on net profits, so the franchisor shares downside risk with the franchisee; franchise agreements typically leave quality control entirely to franchisees to avoid antitrust problems; entering foreign markets only requires local advertising registration and no pre‑contractual disclosure documents.",
        "D": "Franchising is an inexpensive way to enter foreign markets because it eliminates the need to adapt to local legal regimes; franchisors transfer all employment and regulatory liabilities to franchisees and therefore do not need to provide audited financial disclosures or compliance training; territorial exclusivity is unrestricted in all jurisdictions."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete multinational expansion scenario for a coffee chain to test (1) franchising as capital‑efficient growth via licensing of brand and knowhow, (2) franchisor–franchisee governance and power imbalance enforced by franchise agreements and fee structures (royalties on gross sales), and (3) jurisdictional disclosure/registration obligations (Australia, Brazil, China specific examples).",
      "concepts_tested": [
        "Franchising as a capital‑efficient growth mechanism using licensing of brand, knowhow and IP",
        "Franchisor–franchisee governance: franchise agreements, power imbalance, fees based on gross sales",
        "Legal and regulatory requirements for international franchising (disclosure, registration, country variations)"
      ],
      "source_article": "Franchising",
      "x": 1.456681728363037,
      "y": 0.8153839111328125,
      "level": 2,
      "original_question_hash": "84b7bb11"
    },
    {
      "question": "The country of Novalia is implementing a health system reform. The central Ministry of Health retains stewardship and pools national revenues (general taxation plus a payroll-based social insurance) to buy a defined benefits package. Municipal governments manage and deliver most primary and secondary care, hiring local staff and contracting NGOs and faith-based providers in remote districts. To accelerate results, the Ministry introduced performance-based transfers to municipalities tied to immunization coverage, outpatient wait times, and per-capita primary-care cost. After three years, immunization coverage rose markedly and out-of-pocket (OOP) payments fell overall, but rural hospital bed-to-population ratios declined and patient satisfaction shows large inter-municipal variation. Which explanation best interprets these outcomes in terms of the four health-system functions (provision, resource generation, financing, stewardship), system goals (good health, responsiveness, fair financing), and governance (centralized stewardship with decentralized provision)?",
      "options": {
        "A": "Central stewardship and pooled financing improved fairness of financing and allowed targeted purchases (hence lower OOP and higher immunization). Decentralized provision and locally driven resource generation produced heterogeneity in infrastructure and staff allocation (falling beds, variable satisfaction). Performance-based transfers improved the measured outcomes but likely redirected municipal effort toward indicators at the expense of unmeasured services. The pattern shows that coordinated functioning of provision, resource generation, financing and stewardship is required to meet health, responsiveness and equity goals.",
        "B": "The mixed financing model (general taxation plus social insurance) is inherently unstable and directly caused the reduction in hospital beds and uneven satisfaction; decentralization played no material role. Because mixed financing always reduces investment in infrastructure, the Ministry should abandon social insurance to restore bed capacity.",
        "C": "Decentralization of delivery inevitably increases responsiveness and efficiency everywhere; the observed variation and bed decline reflect temporary staffing cycles unrelated to system design. Therefore, the reform is already optimal: rising immunization and lower OOP prove the system meets its goals despite local differences, which will self-correct.",
        "D": "Improvements in immunization and lower OOP show the reform succeeded in all four system functions; declines in bed ratios and variable satisfaction are minor and do not reflect problems with governance or resource generation. Performance-based payments eliminate the need to assess equity or quality beyond the selected indicators."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete country scenario that requires integrating the four functions, system goals/dimensions, and governance structure to explain mixed outcomes; options contrast coordinated-function explanation with plausible but incorrect attributions to financing mix, decentralization inevitability, or overreliance on indicators.",
      "concepts_tested": [
        "Four essential health-system functions (provision, resource generation, financing, stewardship) and their interactions",
        "Relationship between system goals (good health, responsiveness, fair financing), performance dimensions (quality, efficiency, equity), and governance structures (centralized stewardship vs decentralized provision)"
      ],
      "source_article": "Health system",
      "x": 1.2821866273880005,
      "y": 0.8851454257965088,
      "level": 2,
      "original_question_hash": "880b41f8"
    },
    {
      "question": "Four nations respond differently to an existential multi-year war. Which nation's wartime and postwar measures best exemplify Förster's dimensions of total mobilisation and total control and Peccia's concept of total change?",
      "options": {
        "A": "Nation A institutes universal male conscription, rationing of food and fuel, and government-funded battlefield hospitals; propaganda remains decentralized and private firms continue to choose production lines; after the war the economy and social roles largely revert to prewar norms.",
        "B": "Nation B pursues a strategy of strategic bombing and scorched-earth tactics, centralises the military command but leaves education, media and most civilian industries under private or local control; after the conflict the state dismantles emergency controls and there are few long-term social reforms.",
        "C": "Nation C enacts compulsory labour and service for men, women and teenagers across agriculture and munitions (including forced relocation of factories); a small wartime War Council takes direct control of education curricula, national media, economic planning, price controls and rationing; after the war women remain in industrial roles, the education system retains a militarized civic curriculum, and wartime technologies are converted into new civilian industries, permanently reshaping politics, culture and the economy.",
        "D": "Nation D finances the war primarily through voluntary bonds and incentives for private companies to retool; media outlets remain legally independent though encouraged to support the war, and postwar there is a rapid demobilisation with veterans receiving limited support while civilian institutions keep their prewar autonomy."
      },
      "correct_answer": "C",
      "generation_notes": "Created four concrete country vignettes contrasting presence/absence of (1) inclusion of non-traditional participants, (2) multisectoral centralisation by a small ruling body, and (3) enduring societal/technological transformations—option C contains all three per Förster and Peccia.",
      "concepts_tested": [
        "Total mobilisation (inclusion of non-traditional participants)",
        "Total control (multisectoral centralisation and coordination)",
        "Total change (long-term social, political, economic, and technological transformation)"
      ],
      "source_article": "Total war",
      "x": 0.8154816031455994,
      "y": 0.5639582872390747,
      "level": 2,
      "original_question_hash": "5825f5f8"
    },
    {
      "question": "Consider the sentence: \"The ex-painter repainted the blue mural with the old paintbrush.\" The lexicon contains the following entries: paint (root verb), painter (agent noun formed by root + -er), repaint (re- + paint), ex- (former, bound prefix), paintbrush (compound: paint + brush), brush (a kind of tool), blue (a color). Which of the following analyses best integrates (i) how lexical meaning and syntactic structure compose to yield the sentence meaning, (ii) the correct taxonomic relation between paintbrush and brush, and (iii) the morphological status of ex- and -er?",
      "options": {
        "A": "The sentence meaning is computed compositionally: 'repainted' combines the bound prefix re- ('again') with the verb paint and the VP structure assigns the theme role to 'the blue mural'; 'paintbrush' is a hyponym (specific kind) of brush (the hypernym/tool); and ex- and -er are bound morphemes that modify lexical items but cannot occur as independent free morphemes.",
        "B": "'Repainted' must be stored as an idiosyncratic lexical entry in the lexicon rather than composed from re- + paint; 'paintbrush' is a hypernym of brush; and ex- is a free morpheme that can appear independently to indicate former status.",
        "C": "'Paintbrush' is derived compositionally but brush is a hyponym of paintbrush (i.e., brush is a specific kind of paintbrush); the sentence meaning is determined solely by syntactic structure with morphology irrelevant; and -er is a free morpheme that projects its own syntactic arguments.",
        "D": "'Blue' and 'mural' are taxonomically equivalent (synonyms) so the color contributes no thematic information; 'paint' as verb and 'paint' as noun are homonyms with unrelated meanings which blocks compositional interpretation; morphology plays no role in assigning thematic roles."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete sentence containing affixation and compounding and specified lexical entries; options contrast compositional syntax-semantics interface, taxonomic hyponymy/hypernymy, and free vs bound morpheme status to test integrated understanding.",
      "concepts_tested": [
        "syntax-semantics interface and compositionality",
        "lexical relations and taxonomy (hyponymy/hypernymy)",
        "morphology: free vs bound morphemes, compounding"
      ],
      "source_article": "Lexical semantics",
      "x": 1.2662675380706787,
      "y": 1.1167865991592407,
      "level": 2,
      "original_question_hash": "ce57d34c"
    },
    {
      "question": "Country X adopts an official STEM definition that explicitly includes social sciences (e.g., economics, political science, psychology), while Country Y defines STEM narrowly to exclude social sciences and groups them with humanities (e.g., as HASS/SHAPE). Which of the following policy outcomes most accurately illustrates how these differing definitions affect real-world education, workforce, and immigration outcomes?",
      "options": {
        "A": "Country X will be more likely to extend STEM-designated education funding and visa/OPT-style immigration privileges to social science graduates, increasing the pipeline of domestically trained analysts available for workforce development and national-security policymaking; Country Y will concentrate STEM funding and immigration advantages on physical sciences, engineering, mathematics, and computing, reducing comparable institutional support and visa pathways for social scientists.",
        "B": "Country Y's decision to exclude social sciences from STEM will automatically redirect social scientists into engineering and natural sciences degree programs, eliminating any workforce or immigration consequences because graduates will simply retrain into traditional STEM fields.",
        "C": "Because labor-market demand alone determines policy, both countries will show identical patterns of STEM employment and immigration outcomes regardless of their official STEM definitions, so the definitional difference has no practical implication for education or national-security staffing.",
        "D": "Country X's inclusion of social sciences in STEM will primarily result in a short-term drop in mathematics and engineering enrollments due to students preferring social science majors, while Country Y will face immediate national-security gaps because excluding social sciences prevents hiring of any policy analysts."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a comparative policy scenario contrasting an inclusive (NSF-like) vs exclusive (UK-like) STEM definition; options probe consequences for education funding, workforce pipelines, and immigration policy, testing links between definition and real-world outcomes.",
      "concepts_tested": [
        "STEM as an umbrella grouping based on shared analytical and problem‑solving emphasis",
        "Variability in which disciplines are included in STEM across countries and organizations",
        "How definitional choices influence education policy, workforce development, national security, and immigration outcomes"
      ],
      "source_article": "Science, technology, engineering, and mathematics",
      "x": 1.183795690536499,
      "y": 0.8955315351486206,
      "level": 2,
      "original_question_hash": "747dc667"
    },
    {
      "question": "An experimentalist measures the vortex‑shedding frequency $f$ (s$^{-1}$) behind a circular cylinder of diameter $D$ (m) in an incompressible fluid with free‑stream speed $U$ (m s$^{-1}$), density $\\rho$ (kg m$^{-3}$) and dynamic viscosity $\\mu$ (kg m$^{-1}$ s$^{-1}$). Using the Buckingham $\\pi$ theorem, determine the number $p$ of independent dimensionless groups and select the correct pair of independent $\\pi$‑groups. Also state which of these dimensionless numbers primarily governs the flow regime (laminar vs turbulent / vortex‑shedding behaviour) and which represents the normalized shedding frequency.",
      "options": {
        "A": "$p=2$. Independent groups: $\\pi_1=\\dfrac{fD}{U}$ (Strouhal number) and $\\pi_2=\\dfrac{\\rho U D}{\\mu}$ (Reynolds number). The Reynolds number $\\pi_2$ controls the flow regime; $\\pi_1$ is the dimensionless shedding frequency.",
        "B": "$p=3$. Independent groups: $\\pi_1=\\dfrac{fD}{U}$, $\\pi_2=\\dfrac{\\rho f D^2}{\\mu}$, $\\pi_3=\\dfrac{\\rho U D}{\\mu}$. The Reynolds number $\\pi_3$ controls the regime; the other groups further refine scaling.",
        "C": "$p=2$. Independent groups: $\\pi_1=\\dfrac{fD}{\\nu}$ where $\\nu=\\dfrac{\\mu}{\\rho}$, and $\\pi_2=\\dfrac{\\rho U D}{\\mu}$. Here $\\pi_1$ is stated as the normalized frequency and $\\pi_2$ as the Reynolds number.",
        "D": "$p=1$. Single group $\\pi=\\dfrac{fD}{\\rho U/\\mu}$; Buckingham gives $p=n-k=1$, so all experiments must match this single combination to be similar."
      },
      "correct_answer": "A",
      "generation_notes": "Used a concrete vortex‑shedding experiment with variables f,U,D,ρ,μ; applied Buckingham π theorem (n=5, k=3 so p=2) to derive Strouhal $\\pi_1=fD/U$ and Reynolds $\\pi_2=ρUD/μ$ and noted their physical roles.",
      "concepts_tested": [
        "Dimensional analysis and Buckingham π theorem (n−k independent groups)",
        "Construction of dimensionless groups from physical variables (Strouhal and Reynolds)",
        "Physical interpretation: Reynolds controls flow regime; Strouhal is normalized shedding frequency"
      ],
      "source_article": "Dimensionless quantity",
      "x": 1.7262656688690186,
      "y": 1.0755797624588013,
      "level": 2,
      "original_question_hash": "09ad55e4"
    },
    {
      "question": "In the coastal region of Marina the sustainable annual fish catch has fallen by 40% over a decade to a new limit $Q$. The workforce is aging, capital is scarce, peak consumer demand occurs in summer months, and institutional enforcement (monitoring, courts) is weak. Policymakers must choose an economic arrangement that resolves the four fundamental economic problems (what to produce and in what quantities; how to produce; for whom to produce/distribute; and when to produce) while improving information flows and aligning incentives. Which of the following policy packages most coherently uses property rights, institutions, and a mix of market and state mechanisms to address these problems?",
      "options": {
        "A": "Pure market (laissez-faire): leave allocation to existing private fishing firms and market prices; allow open access with no quotas, rely on seasonal price signals to shift supply; encourage private investment without public enforcement or subsidies.",
        "B": "Central planning (command): the state sets fixed yearly catch quotas per vessel and assigns production methods centrally; all catches and distribution are routed through state purchasing agencies and consumers receive equal per-capita allocations during peak months.",
        "C": "Mixed approach: create secure, enforceable individual transferable quotas (ITQs) so each licensed fisher initially receives $q_i = Q\\cdot\\frac{s_i}{\\sum_j s_j}$ based on historical catch $s_i$, allow trading of quotas to reveal valuations; the state enforces closed seasons and monitors stocks with publicly funded electronic reporting; provide matching grants for capital investment in selective gear (reduces bycatch, shifts method toward sustainable capital intensity) and establish a cooperative-run safety-net for retired fishers to address distributional concerns.",
        "D": "Communal non-monetary regime: abolish market sales within Marina, transfer common ownership of boats and gear to local councils who allocate catch in-kind according to need; production decisions are made by rotating community labor teams using labor-intensive methods, with informal norms governing seasonality."
      },
      "correct_answer": "C",
      "generation_notes": "Created an applied coastal fisheries scenario requiring evaluation of policy packages corresponding to market, planned, mixed, and communal systems; correct choice (C) uses property rights (ITQs), state enforcement, information systems, and redistribution mechanisms to address the four problems.",
      "concepts_tested": [
        "The four fundamental economic problems and how pricing, production methods, distribution, and timing affect resource allocation",
        "Role of institutions, information flows, and property rights in shaping decisions and outcomes",
        "How different economic systems (market vs planned vs mixed) determine feasible solutions to allocation problems"
      ],
      "source_article": "Economic system",
      "x": 1.2585214376449585,
      "y": 0.9542575478553772,
      "level": 2,
      "original_question_hash": "f006b1f3"
    },
    {
      "question": "The state of Novara historically applied a \"Bloody Code\" approach that imposed identical severe sanctions for any theft. A reform commission proposes three sentencing models for non‑violent property offenses. Which model most closely aligns with contemporary penology—emphasizing proportionality, rehabilitation and reintegration (including community corrections and probation) and therefore is most likely to reduce recidivism?",
      "options": {
        "A": "Impose a single mandatory custodial sentence of 8 years for all thefts above $50, regardless of the value stolen or offender circumstances, to demonstrate society's denunciation and incapacitate offenders.",
        "B": "Adopt a tiered, proportional regime: petty thefts (< $500) receive fines or community service; mid‑level thefts receive short custodial terms coupled with mandatory vocational and cognitive‑behavioral programs; repeat or high‑value offenders receive longer sentences but enter supervised probation/aftercare on release.",
        "C": "Prioritize certainty and swiftness: every reported theft leads to immediate short-term detention (48–72 hours) and a fine, with no routine rehabilitation programs or community supervision, to maximize deterrence through rapid sanctioning.",
        "D": "Eliminate formal criminal sanctions for most thefts and rely on voluntary restorative circles requiring offender apologies and optional community work, with no compulsory supervision or graduated sanctions."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a realistic policy scenario comparing proportional/rehabilitative/community corrections model to retributive/incapacitation, deterrence-focused, and purely restorative alternatives; correct answer maps to penology principles and recidivism outcomes.",
      "concepts_tested": [
        "purposes of punishment: deterrence, retribution, incapacitation, rehabilitation",
        "proportionality and historical shift from uniform harsh penalties to nuanced sentencing",
        "rehabilitation, probation, community corrections, and impact on recidivism"
      ],
      "source_article": "Penology",
      "x": 1.2099487781524658,
      "y": 0.7583690285682678,
      "level": 2,
      "original_question_hash": "303d2aa7"
    },
    {
      "question": "A team studies a bird species with light plumage on the mainland and three recently colonized islands (A, B, C). On island A most birds are dark and sequencing reveals a unique point mutation in the coding region of the tyrosinase gene: a single base substitution that changes one amino acid in the enzyme (allele D). Allele D is absent on the mainland. On island B many birds are dark but sequencing shows no D allele; instead dark birds carry a particular combination of alleles at two pigment loci that is rare on the mainland. Island C was founded by a small number of mainland individuals and shows large random fluctuations in allele frequencies across generations; occasional storms bring a few migrants from the mainland into islands B and C. Which of the following best explains (1) the origin of allele D on island A, (2) the proximate cause of dark plumage on island A and on island B, and (3) the primary processes responsible for the different allele-frequency patterns observed among islands A, B, and C?",
      "options": {
        "A": "Allele D arose on island A by a de novo point mutation (the ultimate source of new alleles). The dark plumage on A is caused by the nonsynonymous substitution that changed tyrosinase amino-acid sequence and altered enzyme function (genotype→phenotype linkage). Dark plumage on B results from recombination producing a novel combination of preexisting alleles at two pigment loci (new phenotype without a new base change). Different allele-frequency patterns reflect strong selection favoring D on A, recombination-driven assembly of dark genotypes on B, and founder effects plus genetic drift in small C; intermittent gene flow from migrants can introduce mainland alleles and partially homogenize populations.",
        "B": "Allele D was generated by recombination during meiosis on island A (recombination creates new base changes). Dark plumage on both A and B is caused by environmental induction rather than changes in protein sequence. Differences among islands are primarily produced by ongoing natural selection alone; genetic drift and founder effects are negligible.",
        "C": "Allele D existed on the mainland at low frequency but was only expressed on island A because island conditions activate that allele; dark plumage on B must therefore be caused by the same allele expressed differently. The differences among islands are best explained by irreversible effects of selection that prevent gene flow from altering frequencies.",
        "D": "Allele D arose from a point mutation on island A (new allele), and the dark phenotype on A is due to the amino-acid change affecting enzyme function. However, recombination cannot change phenotypes by assembling alleles, so the dark birds on B must carry a different undetected mutation. The allele-frequency differences among islands are due solely to migration (gene flow) rather than drift or founder events."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed an island-colonization scenario to test: (1) mutation as origin of novel alleles, (2) nonsynonymous changes linking genotype to altered protein function and phenotype, (3) roles of recombination in generating novel multilocus genotypes (not new base changes), and (4) effects of selection, founder events/genetic drift, and gene flow on population allele frequencies.",
      "concepts_tested": [
        "Mutation as ultimate source of novel alleles",
        "Genotype–phenotype linkage via nonsynonymous substitution altering protein function",
        "Recombination creating new combinations of existing alleles (multilocus genotypes)",
        "Genetic drift/founder effects changing allele frequencies in small populations",
        "Gene flow homogenizing allele frequencies and natural selection altering frequencies"
      ],
      "source_article": "Genetic variation",
      "x": 1.8745017051696777,
      "y": 1.1215393543243408,
      "level": 2,
      "original_question_hash": "9ab0a373"
    },
    {
      "question": "Producer P obtains an exclusive license from novelist A to adapt A's copyrighted novel \"Nocturne\" into a feature film. In adapting the book, P writes substantial new scenes, invents two supporting characters (Mira and Jon), composes an original score, and devises distinctive cinematographic sequences that clearly reflect P's personal authorship. P registers copyright in the finished film. Two years later Studio S, without contacting A, proposes a spin-off that centers exclusively on Mira and Jon and reuses several of P's new scenes and shots, but does not reproduce any verbatim passages or named characters that originated in A's novel. Which statement best describes the copyright situation under U.S. law?",
      "options": {
        "A": "P owns copyright protection in the film only to the extent of P's original contributions (the new scenes, Mira and Jon, the score, and the cinematography) and therefore can lawfully authorize S to make a spin-off using only those contributions; however, P cannot grant rights in expressive elements that are still copyrighted in A's novel, so if the spin-off would rely on any protected expressive elements of A's work, A's permission would also be required.",
        "B": "P's copyright in the derivative film vests P with exclusive rights over all expressive elements present in the film, including the underlying novel's protected material incorporated into it, so P alone may license S to make the spin-off without A's consent.",
        "C": "Because the film is a derivative work, P's copyright is limited to reproduction and distribution of copies and does not include the exclusive right to prepare further derivative works; therefore P cannot block S from preparing a spin-off even if it copies P's original scenes and characters.",
        "D": "A derivative work based on a novel never qualifies for independent protection, so P has no copyright in the new scenes or characters; consequently neither A nor P can prevent S from using those elements."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete adaptation-and-spin-off scenario to test that derivative copyright covers only the author's original contributions, requires substantial originality, and includes the right to prepare and authorize further derivative works but does not enlarge the underlying author's rights.",
      "concepts_tested": [
        "Independence of derivative work and limitation of protection to new contributed material",
        "Requirement of substantial transformation and originality (author's personality)",
        "Scope of exclusive rights for derivative works (reproduction, preparing derivative works, distribution) and interaction with underlying work rights"
      ],
      "source_article": "Derivative work",
      "x": 1.2740669250488281,
      "y": 0.8946549296379089,
      "level": 2,
      "original_question_hash": "643b6e03"
    },
    {
      "question": "An online retailer deploys its web front end to a public cloud. During a flash sale traffic spikes and an autoscaling policy drives the deployed web server fleet from $5$ instances to $50$ instances within five minutes. The autoscaling action was initiated by the retailer's monitoring service via the cloud provider's REST API and completed without any human interaction with the provider. The new instances were allocated from the provider's shared pool of virtual machines that hosts multiple tenants, and the provider's console displays detailed per-instance CPU-hour and network-bandwidth metrics and bills the retailer on a per-minute basis. Which set of NIST cloud characteristics best describes the behaviours in this scenario?",
      "options": {
        "A": "On-demand self-service; Resource pooling and rapid elasticity; Measured service and transparency",
        "B": "Broad network access; Resource pooling; Limited elasticity (manual scaling required)",
        "C": "On-demand self-service; Dedicated single-tenant resources; No metering (flat-rate subscription)",
        "D": "Measured service only; No automated provisioning; Physical colocation of dedicated servers"
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete flash-sale autoscaling scenario ($5\\to50$ instances) that maps automated API-driven provisioning to on-demand self-service, shared VM allocation and rapid scaling to resource pooling/rapid elasticity, and per-instance metrics + per-minute billing to measured service/transparency.",
      "concepts_tested": [
        "On-demand self-service and automated provisioning",
        "Resource pooling and rapid elasticity",
        "Measured service (metering) and transparency"
      ],
      "source_article": "Cloud computing",
      "x": 1.4398466348648071,
      "y": 1.0853849649429321,
      "level": 2,
      "original_question_hash": "6b0c85c2"
    },
    {
      "question": "A coastal biotech company, AquaCure, plans a 6-week bioprospecting expedition to a coral-reef lagoon after local fishers tell them a particular sponge expedites wound healing. AquaCure's objectives are (1) to maximize the probability of discovering a novel antibacterial compound, (2) to ensure any commercial development is legally and ethically defensible, and (3) to avoid harming the reef ecosystem or local livelihoods. Which of the following operational plans best satisfies the scientific screening, legal/ethical, and conservation requirements described in contemporary bioprospecting practice?",
      "options": {
        "A": "Use an information-guided screening approach: prioritise sponge specimens selected using local ethnomedical knowledge and ecological indicators; obtain prior informed consent from the indigenous/local community and a formal access permit from the state; negotiate a benefit-sharing agreement consistent with the CBD/Nagoya Protocol before sampling; collect minimal, well-documented voucher specimens and deposit them in a recognised repository; perform dereplication and standardized bioassays to avoid rediscovering known compounds; attempt microbial isolation or laboratory cultivation (or metagenomic mining followed by heterologous expression) to enable ex situ scale-up rather than large-scale wild harvesting; and include a monitoring plan to assess and mitigate environmental impacts.",
        "B": "Adopt a high-volume random-sampling strategy across the reef to maximise chemical diversity; delay community consultation until promising leads emerge to avoid biasing tests; rely on patent law to secure exclusive rights to any isolated compound (patenting the isolated chemical, not the organism); and collect large quantities of sponge initially so there is ample material for fractionation and assays, handling conservation concerns only if harvests visibly damage the reef.",
        "C": "Forego ethnobiological information and instead sequence environmental DNA (eDNA) from many reef sites, then use in silico screens to identify biosynthetic gene clusters; consider eDNA sampling outside the scope of the CBD so prior informed consent is unnecessary; if a promising biosynthetic pathway is found, synthesize the molecule chemically and file patents on the synthetic route, while avoiding physical collection and thus not needing permits or benefit-sharing agreements.",
        "D": "Rely solely on prior publications about sponge chemistry to pick specimens; obtain only a general research permit from the national museum (no community agreement required); collect abundant material for immediate bioactivity-guided fractionation; if a hit is found, secure patent protection on the sponge variety or on the organism itself to maximise commercial return, and negotiate royalties with the community only after regulatory approval and revenue generation."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a realistic reef-based scenario to require decisions about screening strategy (random vs information-guided), obligations under CBD/Nagoya (prior informed consent, benefit-sharing, permits), and conservation measures (minimal sampling, cultivation/scale-up, voucher deposition, monitoring). Options B–D present common but flawed alternatives reflecting ethical, legal, or ecological pitfalls.",
      "concepts_tested": [
        "Screening strategies in bioprospecting: random versus rational/informed selection and dereplication",
        "Ethical and legal frameworks: biopiracy, prior informed consent, CBD and Nagoya Protocol, benefit-sharing",
        "Conservation and governance: overharvesting risks, permits, voucher specimens, ex situ scale-up and monitoring"
      ],
      "source_article": "Bioprospecting",
      "x": 1.9299602508544922,
      "y": 1.0467175245285034,
      "level": 2,
      "original_question_hash": "09daa162"
    },
    {
      "question": "In the fictional state of Aurora, the municipal government enacts a statute sharply restricting the voting rights of a religious minority. A coalition of citizens responds as follows: they occupy the council chamber in broad daylight, announce publicly that they believe the statute is unjust, refuse to disperse when ordered, allow themselves to be arrested without resisting, and at their trial plead guilty while delivering statements that appeal to the community’s conscience and urge legal reform. Which description best characterizes this coalition’s action in light of theories of civil disobedience (Thoreau, King, Rawls, Dworkin) and the strategic role such acts play in contesting legitimacy and seeking reform?",
      "options": {
        "A": "This is paradigmatic civil disobedience: a public, nonviolent, conscientious political act contrary to law intended to prompt legal change and moral dialogue; accepting punishment functions as a communicative strategy that underscores respect for the rule of law while contesting a law’s legitimacy.",
        "B": "This is not civil disobedience because the activists deliberately broke the law; by accepting arrest they implicitly concede the law’s legitimacy, so their action is better described as criminal protest without normative force.",
        "C": "This is primarily a legal strategy rather than civil disobedience: occupying the chamber and then pleading guilty is an attempt to manufacture a test case in court to obtain a judicial ruling, which disqualifies it as civil disobedience under standard accounts.",
        "D": "This action counts as civil disobedience only if it is violent or coercive enough to force the government to repeal the statute; nonviolent occupation and acceptance of punishment are symbolic and therefore ineffective and not genuine civil disobedience."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete municipal scenario testing public/nonviolent/conscientious criteria, acceptance of penalty as communicative strategy, and the link to legitimacy and reform; distractors misapply theory (concession to law, legal test-case, necessity of violence).",
      "concepts_tested": [
        "tension between law and conscience",
        "nonviolence and penalty mechanism",
        "civil disobedience as strategic contestation of legitimacy and pathway to reform"
      ],
      "source_article": "Civil disobedience",
      "x": 0.8738549947738647,
      "y": 0.6168928742408752,
      "level": 2,
      "original_question_hash": "e7d3c63d"
    },
    {
      "question": "Consider an industry with an incumbent firm I and a potential entrant E. Consumers face switching transaction costs $t>0$. The incumbent can choose to vertically integrate and foreclose entrant access to a key input (raising E's marginal cost), or remain unintegrated and compete on price. The entrant observes only the incumbent's posted price but not whether I has integrated (imperfect information). A regulator must choose between two policies before firms move: prohibit vertical integration (policy P) or allow integration (policy A). Using standard game-theoretic reasoning about entry deterrence, signaling, and market contestability, which regulatory policy is most likely to maximize long-run consumer welfare and why?",
      "options": {
        "A": "Prohibit vertical integration (policy P). With $t>0$ and imperfect information, allowing integration strengthens incumbent foreclosure and entry barriers; the incumbent can more easily sustain a high-price equilibrium by deterring entry through pricing and foreclosure strategies. Prohibiting integration preserves contestability and therefore tends to lower long-run prices and increase consumer welfare.",
        "B": "Allow vertical integration (policy A). Integration creates production and distribution efficiencies that reduce marginal cost; even with switching costs $t$ and imperfect information, cost savings will be passed through to consumers and entry is not meaningfully deterred, so consumer welfare rises under A.",
        "C": "Either policy yields the same long-run consumer welfare because switching costs $t$ alone determine whether entry occurs; vertical integration has negligible additional impact on strategic behavior or prices when $t>0$.",
        "D": "The regulator should be indifferent between P and A because imposing an ex ante price cap always dominates structural policy: price caps eliminate any exercise of market power regardless of integration, making the choice about vertical integration irrelevant for consumer welfare."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete two-firm sequential-entry scenario with switching costs $t$ and imperfect information about vertical integration; tested how foreclosure and entry deterrence interact with regulatory choices using game-theoretic logic.",
      "concepts_tested": [
        "Relationship between market structure, firm behavior, and regulation/policy",
        "Role of transaction costs, imperfect information, and barriers to entry in strategic behavior",
        "Use of game theory to model strategic interactions and inform regulatory outcomes"
      ],
      "source_article": "Industrial organization",
      "x": 1.3158904314041138,
      "y": 0.9425077438354492,
      "level": 2,
      "original_question_hash": "7542b8e6"
    },
    {
      "question": "You are given a case study of the Kora River Valley: c.7000 BCE mobile foragers begin settling in the floodplain during the Neolithic, clearing riparian forests and domesticating cereals; by c.1500 CE regional trade intensifies, prompting intensified irrigation and monoculture; by the 19th–20th centuries an industrial canal and textile mills expand cotton cultivation, producing heavy water pollution and progressive salinization that collapses yields and triggers out-migration. During the sequence local ritual observances dedicated to a river deity wane and are replaced by engineering manuals and hydrological surveys that justify further canal expansion. Which one of the following interpretations best applies the tripartite environmental‑history framework (nature itself, human use of nature, and human thought about nature) while also reflecting the reciprocal influences and the role of major historical shifts (Neolithic and Industrial Revolutions) in driving long‑term change?",
      "options": {
        "A": "The narrative exemplifies reciprocal interaction: the river’s ecology shaped settlement (nature itself); human uses—deforestation, irrigation, monoculture, industrialization—transformed the landscape (human use of nature); and changing beliefs and technical knowledge—ritual to engineering—both enabled and rationalized those uses. The Neolithic and Industrial transitions acted as punctuated shifts that integrated all three components and produced deferred environmental consequences (e.g. salinization, pollution).",
        "B": "This sequence is primarily cultural: changing ideas about the river (from deity to engineering) were the essential cause; ecological impacts such as salinization and pollution are incidental outcomes of belief changes rather than reciprocal processes.",
        "C": "The case is best explained by environmental determinism: natural processes (the river’s hydrology and eventual salinization) alone determined settlement, agriculture and demographic decline; human agency and intellectual change played negligible roles.",
        "D": "Only demographic pressure matters: population growth and market demand drove technology and land use changes, so the tripartite framework is unnecessary—technology and numbers fully explain the environmental degradation described."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a multi‑period river valley vignette linking Neolithic and Industrial revolutions to illustrate the tripartite framework and reciprocal causality; distractors reflect cultural determinism, environmental determinism, and demographic/technological reductionism.",
      "concepts_tested": [
        "reciprocal human–environment relationships",
        "tripartite analytical framework (nature; human use; human thought)"
      ],
      "source_article": "Environmental history",
      "x": 1.296654462814331,
      "y": 0.8983712196350098,
      "level": 2,
      "original_question_hash": "3d079ca3"
    },
    {
      "question": "A technology startup has a corpus of $10^7$ unlabeled natural images gathered cheaply from the web. Labeling an image costs \\$1, and the startup's budget allows labeling at most $10^4$ images. The final product requires a 100‑class image classifier. Which training workflow is most likely to produce the highest classification accuracy on held‑out test images, and why?",
      "options": {
        "A": "Pretrain a convolutional encoder on the $10^7$ unlabeled images with a reconstruction/masked modeling objective (e.g., autoencoder or masked image modeling) to learn representations, then fine‑tune the pretrained encoder together with a task head on the $10^4$ labeled images.",
        "B": "Ignore the unlabeled corpus; train a deep supervised classifier from random initialization using only the $10^4$ labeled images, since supervised labels directly target the classification task.",
        "C": "Train an autoencoder on the $10^7$ images, then freeze the encoder and train only a linear classifier on top using the $10^4$ labeled images (no further fine‑tuning of encoder weights).",
        "D": "Cluster the $10^7$ images (e.g., k‑means) into 100 clusters, treat cluster indices as pseudo‑labels, train a classifier on the pseudo‑labeled data, and then deploy that classifier without any supervised fine‑tuning on the $10^4$ true labels."
      },
      "correct_answer": "A",
      "generation_notes": "Created a practical dataset/label-cost scenario contrasting supervised training on limited labels versus unsupervised pretraining with reconstruction/masked objectives and downstream fine‑tuning; options include plausible alternatives (frozen features, pure clustering) to test comparative reasoning.",
      "concepts_tested": [
        "Value and cost advantages of large unlabeled datasets vs expensive labeled data",
        "Representation learning via reconstruction/masked modeling objectives (autoencoders, masked modeling)",
        "Transfer learning workflow: unsupervised pretraining followed by supervised fine‑tuning for downstream tasks"
      ],
      "source_article": "Unsupervised learning",
      "x": 1.656825304031372,
      "y": 1.132175326347351,
      "level": 2,
      "original_question_hash": "7c48a625"
    },
    {
      "question": "You study the decomposition of hydrogen peroxide in water at 25°C (initial [H2O2] = 0.10 M). The uncatalyzed reaction is first order with $k_0 = 1.0\\times10^{-5}\\ \\mathrm{s^{-1}}$. Three experiments give the following initial rates: (i) add 0.010 g MnO2 powder with particle surface area $5\\ \\mathrm{m^2/g}$, no stirring: $v_1 = 5.0\\times10^{-5}\\ \\mathrm{M\\ s^{-1}}$; (ii) use the same mass of MnO2 but crush it to increase surface area to $50\\ \\mathrm{m^2/g}$ and vigorously stir: $v_2 = 5.0\\times10^{-4}\\ \\mathrm{M\\ s^{-1}}$; (iii) instead add 1.0 µM catalase enzyme (soluble): $v_3 = 1.0\\times10^{-2}\\ \\mathrm{M\\ s^{-1}}$. Which of the following explanations best accounts for these observations?",
      "options": {
        "A": "MnO2 functions as a heterogeneous (solid) catalyst that provides an alternative, lower–activation–energy surface pathway; increasing particle surface area and vigorous stirring increases the number of accessible active sites and reduces mass‑transfer limitations, so the heterogeneous catalytic rate rises ($v_1\\to v_2$). Catalase is a homogeneous biocatalyst that operates by a much faster enzymatic pathway (giving $v_3$). Both catalysts are regenerated (not consumed), so the observed initial rate is the sum of the catalyzed and uncatalyzed contributions; none of these catalysts change the reaction equilibrium.",
        "B": "MnO2 is acting stoichiometrically (consumed) so crushing it exposes more reactant and increases the rate; catalase shifts the chemical equilibrium toward products, which explains the very large $v_3$. Therefore the catalysts are being consumed and change equilibrium positions.",
        "C": "Catalase is behaving as a heterogeneous catalyst by adsorbing to vessel surfaces, and reducing MnO2 particle size should decrease activity because smaller particles sinter and lose active facets; vigorous stirring therefore reduces the observed rate, so the given data imply measurement artefacts rather than catalytic pathways.",
        "D": "All observed rate increases are due to the uncatalyzed pathway being accelerated by improved collision frequency: increasing surface area and stirring simply raise the effective encounter rate for the uncatalyzed mechanism, and catalase acts by heating the local solution to accelerate the uncatalyzed reaction, so catalysts do not provide distinct lower‑energy pathways."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete H2O2 decomposition scenario with numeric rates to test understanding that catalysts (heterogeneous vs homogeneous/enzymes) provide alternative lower‑energy pathways, are regenerated (not consumed), and that mixing and surface area affect observed heterogeneous catalytic rates and the combined catalyzed + uncatalyzed rate.",
      "concepts_tested": [
        "Catalysts provide alternative faster reaction pathways with lower activation energy and are regenerated",
        "Classification by phase: heterogeneous (solid MnO2) vs homogeneous/enzymatic (catalase) and implications for mechanism",
        "Observed rate enhancement arises from both catalyzed and uncatalyzed pathways and is influenced by mixing, surface area, and transport limitations"
      ],
      "source_article": "Catalysis",
      "x": 1.9931741952896118,
      "y": 1.0469502210617065,
      "level": 2,
      "original_question_hash": "93a9701b"
    },
    {
      "question": "Aurora Wearables is a mid‑sized firm that designs and sells wearable health trackers. Its leadership is preparing a three‑year strategic plan and needs to classify environmental factors correctly to decide actions. Which of the following options best describes accurate classifications of internal, micro, and macro environmental factors and an appropriate managerial response?",
      "options": {
        "A": "Correct: Internal factors (R&D choices about sensor features, finance approval of marketing budgets, production scheduling) are within managerial influence and directly shape marketing decisions; micro factors include customers (consumer and business buyers), partners (resellers, distribution warehouses, advertising agencies, banks) and competitors — the firm should build strategic advantage via customer segmentation and partner management; macro factors are broad societal forces (e.g., new EU health‑data regulation = political/legal, an ageing population = demographic, lithium battery shortages = natural/environmental, rapid AI platforms = technological) that the firm must monitor and adapt to through product design, compliance and long‑term planning.",
        "B": "Internal factors such as R&D, accounting and operations are essentially uncontrollable and therefore should be treated the same as macro forces; marketing strategy should focus mainly on predicting macro trends like demographics and technology rather than changing internal processes.",
        "C": "Micro environment is limited to immediate partners and competitors only; demographic shifts, legal changes and technological breakthroughs are all micro factors because they directly affect individual buyers and should be handled by sales teams rather than strategic planning.",
        "D": "Macro factors are primarily market intermediaries like resellers and warehouses, while competitors and customers are internal to the firm; therefore the firm should prioritize controlling the macro environment by vertically integrating distribution and ignore external legal or demographic analyses."
      },
      "correct_answer": "A",
      "generation_notes": "Created a firm scenario (wearable health trackers) and presented four options that test correct classification and managerial implications for internal (controllable), micro (customers/partners/competitors) and macro (PESTEL) environments; distractors misclassify controllability and level membership.",
      "concepts_tested": [
        "Internal environment controllability and influence on marketing decisions",
        "Macro environment as PESTEL broad societal forces requiring analysis and adaptation",
        "Micro environment consisting of customers, partners and competitors and its strategic implications"
      ],
      "source_article": "Market environment",
      "x": 1.3363685607910156,
      "y": 0.9819366931915283,
      "level": 2,
      "original_question_hash": "066114bf"
    },
    {
      "question": "A star hosts a newly discovered planetary system with three planets at semimajor axes $a=0.5,\\,1.2,\\,$and $5\\ \\mathrm{AU}$. Observations indicate: the inner world is a silicate-dominated rocky planet (transit spectroscopy shows strong silicate absorption and a resolved crustal albedo pattern), the middle planet has a dense, banded atmosphere with strong zonal winds (Doppler-resolved cloud motions and variable transit depths), and the outer body exhibits infrared plume emissions and radar returns consistent with cryovolcanic activity and surface hydrocarbon lakes. A flyby spacecraft has provided high-resolution imaging, radar altimetry, and gravity anomalies for the inner and middle planets; additionally a sample-return delivered a 0.5 kg unaltered rock from the inner planet. Your team’s goals are to determine (1) the formation chronology and compositional differentiation of the bodies, (2) the long-term dynamical stability and migration history of the system, and (3) present-day exchanges of volatiles and atmosphere–surface coupling. Which combination of disciplinary expertise and observational/theoretical methods is most appropriate to achieve all three goals?",
      "options": {
        "A": "Integrate cosmochemistry (laboratory isotopic and petrologic analysis of the returned rock), planetary geology/geophysics (interpret gravity anomalies, interior/tectonic models), atmospheric science (general circulation models constrained by spectroscopy and Doppler winds), planetary oceanography/glaciology for the outer body's cryovolcanism and lakes, and exoplanetary astronomy for comparative context. Observational methods: remote sensing (multiwavelength spectroscopy, radar altimetry, imaging), spacecraft gravity mapping, and laboratory sample analyses. Theoretical methods: N-body orbital and migration simulations for stability/history, thermal-evolution and differentiation models for formation chronology, GCMs and coupled surface–atmosphere volatile-transport models for present-day exchanges, plus Earth-analogue comparative studies.",
        "B": "Rely on extensive remote sensing and spacecraft datasets (spectroscopy, imaging, radar, gravity) combined with atmospheric GCMs and thermal-evolution models, and perform N-body orbital simulations for migration. Do not perform laboratory cosmochemical analysis of samples; instead infer composition solely from telescopic spectra and remote measurements.",
        "C": "Concentrate on planetary geology/geochemistry and laboratory petrology of the returned sample to reconstruct formation and differentiation, and use interior thermal models to infer evolution. Use only simple Keplerian fits (no N-body migration simulations) to characterize orbits, and minimal atmospheric modelling (qualitative comparisons to Earth), omitting specialized oceanography/glaciology for the outer body.",
        "D": "Prioritize additional observational missions: more flybys, higher-resolution imaging, and expanded gravity surveys, and compile a comprehensive observational database. Avoid complex theoretical modelling beyond fitting orbital elements and empirical correlations; plan to defer detailed laboratory cosmochemistry and numerical simulations until later."
      },
      "correct_answer": "A",
      "generation_notes": "Built a concrete multi-planet scenario requiring integration of cosmochemistry, geology, atmospheric science, oceanography/glaciology, remote sensing and sample analysis, and theoretical modelling (N-body, thermal evolution, GCMs). Options differ by omitted disciplines/methods to test recognition of necessary interdisciplinary and methodological mix.",
      "concepts_tested": [
        "Interdisciplinary integration across planetary geology, cosmochemistry, atmospheric science, oceanography/glaciology, and exoplanetology",
        "Observational versus theoretical methodology (remote sensing, sample analysis, gravity mapping vs. N-body simulations, GCMs, thermal-evolution models)",
        "Reconstruction of formation, dynamical evolution, and present-day interrelations (volatile exchange, atmosphere–surface coupling)"
      ],
      "source_article": "Planetary science",
      "x": 2.0279242992401123,
      "y": 1.3755347728729248,
      "level": 2,
      "original_question_hash": "1a4bb907"
    },
    {
      "question": "A research group subjects bovine articular cartilage explants to cyclic compressive loading ($1\\ \\text{Hz}$, $20\\%$ compressive strain) for 2 hours daily and observes a sustained increase in chondrocyte matrix metalloproteinase (MMP) expression and progressive loss of proteoglycan over 2 weeks. The group wants to determine whether the MMP upregulation is driven primarily by chondrocyte mechanotransduction (cellular sensing via integrins, stretch-activated channels, etc.) or by passive biphasic mechanics (fluid exudation, changes in hydrostatic/ionic pressure due to the solid–fluid ECM interaction). Which single experimental manipulation would most specifically test the requirement for cell-mediated mechanotransduction in producing the observed MMP response?",
      "options": {
        "A": "Pre-treat explants with function-blocking antibodies against β1 integrins (to inhibit integrin-mediated cell–ECM signaling) during cyclic loading and then assay MMP expression.",
        "B": "Enzymatically deplete aggrecan (e.g., with aggrecanase) to reduce fixed negative charge density and repeat the same loading protocol to assess whether reduced swelling pressure alters MMP induction.",
        "C": "Perform the identical cyclic loading in a hyperosmotic bath (increased extracellular NaCl) to minimize interstitial fluid exudation and hydrostatic pressure changes while measuring MMP expression.",
        "D": "Apply the same cyclic compression to a decellularized cartilage matrix scaffold and assay for MMP activity after loading to determine whether the matrix alone accounts for the response."
      },
      "correct_answer": "A",
      "generation_notes": "Designed a concrete experimental scenario combining cartilage biphasic mechanics and mechanotransduction; options propose specific perturbations that isolate integrin-based signaling (correct) versus alterations of biphasic fluid/charge or removal of cells/matrix to test passive effects.",
      "concepts_tested": [
        "Mechanotransduction (cellular receptors and signaling pathways)",
        "Biphasic mechanics of articular cartilage (solid ECM, interstitial fluid, fixed charge effects)",
        "Mechanical environment driving tissue remodeling and disease (loading-induced MMP expression and proteoglycan loss)"
      ],
      "source_article": "Mechanobiology",
      "x": 1.827982783317566,
      "y": 1.1007121801376343,
      "level": 2,
      "original_question_hash": "9264e93c"
    },
    {
      "question": "A university ethics committee must evaluate claims about a new cognitive-enhancement supplement. Available information: a preliminary double-blind randomized trial with $n=30$ reports an effect size $d=0.4$ and $p=0.06$; several journalists publish persuasive anecdotes of dramatic improvements; a religious leader asserts the supplement enhances the soul; and independent reviewers flag potential measurement unreliability in the cognitive tests. Which committee response best exemplifies a combination of moderate philosophical skepticism and scientific skepticism as described in the article?",
      "options": {
        "A": "Conclude the supplement is effective because the trial shows a moderate effect ($d=0.4$) and anecdotal reports align with participant experiences, so recommend immediate adoption while planning to monitor outcomes.",
        "B": "Reject the supplement outright and declare it false, since the trial did not achieve conventional statistical significance ($p=0.06$) and anecdotes are unreliable; refuse any further investigation.",
        "C": "Suspend belief pending stronger evidence: acknowledge the preliminary trial as suggestive but insufficient, demand larger, well-powered preregistered replications with validated outcome measures and independent verification, and treat the religious claim as a non-empirical assertion outside the remit of empirical confirmation.",
        "D": "Adopt radical skepticism: assert that no reliable knowledge about the supplement (or any empirical claim) is possible, therefore indefinitely refuse to accept, reject, or test the claim and make no policy recommendation."
      },
      "correct_answer": "C",
      "generation_notes": "Constructed an applied evaluation scenario using trial statistics ($n$, $d$, $p$) and varied claim types to test (1) suspension vs disbelief under insufficient evidence, (2) distinctions among moderate/radical, religious, and scientific skepticism, and (3) scientific-skeptical procedures (replication, preregistration, validated measures).",
      "concepts_tested": [
        "Suspension of belief versus disbelief motivated by insufficient evidence",
        "Distinctions among forms of skepticism: moderate vs radical; philosophical, religious, and scientific skepticism",
        "Scientific skepticism as testing beliefs via well-designed empirical methods (replication, preregistration, validated measures)"
      ],
      "source_article": "Skepticism",
      "x": 1.1231956481933594,
      "y": 1.072453498840332,
      "level": 2,
      "original_question_hash": "5af84f5d"
    },
    {
      "question": "A multinational biotech company has developed a somatic gene therapy, NeuroLuxe, that increases adult cognitive processing speed by approximately 30% and reduces several biomarkers associated with aging. Independent modelling suggests that broad adoption could advance general AI research by a factor of 1.4 (due to prolonged productive lifespans and increased R&D throughput), modestly increasing the probability of an unaligned AGI emerging in the next 30 years. Adoption under pure market conditions would initially be concentrated among wealthy populations, producing substantial inequality. Which of the following policy responses best aligns with the central aims and internal debates of transhumanism as described in the article — i.e., promoting technological enhancement (longevity, cognition, well‑being), addressing ethical concerns about fair access and bodily autonomy, and mitigating existential risks from accelerating dangerous technologies?",
      "options": {
        "A": "Allow immediate unrestricted commercial release worldwide to maximize individual cognitive liberty and morphological freedom; rely on market forces and innovation to address downstream harms.",
        "B": "Impose an international moratorium banning NeuroLuxe permanently on the grounds that any human enhancement that risks societal disruption or accelerates AGI is morally unacceptable (precautionary principle).",
        "C": "Fund universal, free public distribution of NeuroLuxe as a matter of social justice without additional constraints, to eliminate the initial inequality and fast‑track the human enhancement transition.",
        "D": "Authorize a phased, regulated rollout: require extensive clinical and societal safety trials; establish an international governance framework to ensure equitable access and to prioritize investment in protective technologies (e.g., AI alignment, robust surveillance of risky R&D); and apply policies to slow development of particularly hazard‑amplifying technologies while accelerating defensive capabilities (i.e., control differential technological development)."
      },
      "correct_answer": "D",
      "generation_notes": "Constructed a realistic policy scenario (NeuroLuxe) requiring trade‑off analysis among enhancement aims, equity/ethical governance, and existential risk mitigation. Options reflect libertarian, precautionary, egalitarian, and Bostromian differential‑development responses; the latter (D) best integrates transhumanist values and risk management.",
      "concepts_tested": [
        "Technological human enhancement and transition toward posthuman capabilities",
        "Ethics and governance of emerging enhancement technologies (access, bodily autonomy, regulatory frameworks)",
        "Existential risk mitigation and control of differential technological development"
      ],
      "source_article": "Transhumanism",
      "x": 1.1474926471710205,
      "y": 1.0181901454925537,
      "level": 2,
      "original_question_hash": "e91d35c9"
    },
    {
      "question": "A linguist proposes a study comparing a corpus of naturally recorded physician–patient consultation transcripts with a corpus of pharmaceutical company press releases to investigate how medical authority is enacted in talk and text. Which research design best exemplifies discourse analysis as described in the article?",
      "options": {
        "A": "Use the naturally occurring consultation transcripts and press releases, code speaker turns and intersentential relations to infer socio‑psychological stances (e.g., deference, persuasion), and apply sublanguage analysis techniques — including transforming multi‑sentence stretches into a canonical form to make informational structure explicit — to compare how authority is constructed across the two corpora.",
        "B": "Elicit invented example sentences from participants in a lab, analyze their sentence‑level syntactic correctness and semantic acceptability, and generalize findings about medical discourse from these controlled items.",
        "C": "Treat each press release as an isolated text unit, compute cohesion and coherence metrics within sentences to describe text structure, and draw conclusions about corporate communication strategies without analyzing interactional turns or speaker psychology.",
        "D": "Survey physicians and corporate communicators about their preferred words and then perform frequency counts of keywords per sentence to infer attitudes toward patients and consumers, without using naturally occurring conversational data or examining relations across turns."
      },
      "correct_answer": "A",
      "generation_notes": "Created a corpus-comparison scenario requiring selection of the design that (1) uses naturally occurring data, (2) analyzes beyond sentence boundaries to infer socio‑psychological characteristics, and (3) employs sublanguage/canonical transformation methods to reveal informational structure.",
      "concepts_tested": [
        "discourse analysis focuses on socio-psychological characteristics and analysis beyond the sentence",
        "preference for naturally occurring language over invented examples",
        "use of sublanguage analysis and canonical transformations to reveal informational structure"
      ],
      "source_article": "Discourse analysis",
      "x": 1.2399982213974,
      "y": 1.0826404094696045,
      "level": 2,
      "original_question_hash": "dfdbd38d"
    },
    {
      "question": "Astronomers report two newly discovered Sun-orbiting bodies. Object X has mean radius $650\\,\\mathrm{km}$, bulk density $1.6\\,\\mathrm{g\\,cm^{-3}}$, an approximately ellipsoidal shape in images, and a nearly circular orbit at $42\\,$AU within the classical Kuiper belt. Object Y has mean radius $400\\,\\mathrm{km}$, bulk density $3.2\\,\\mathrm{g\\,cm^{-3}}$, an irregular, non-equilibrium shape, and an orbit with semimajor axis $1.02\\,$AU and perihelion $0.80\\,$AU that crosses Earth's orbit. According to the IAU criteria and the taxonomy of small bodies, which classification best fits these objects and why?",
      "options": {
        "A": "Object X should be classed as a dwarf planet (it meets hydrostatic equilibrium and orbits in the Kuiper belt, so it does not clear its neighbourhood) while Object Y is a small Solar System body / near-Earth asteroid (it is not in hydrostatic equilibrium and is an Earth‑crosser that does not clear its orbit).",
        "B": "Both Object X and Object Y should be classed as dwarf planets because each is large enough that self‑gravity dominates material strength and hence both will relax to ellipsoidal shapes over time regardless of current observed shape.",
        "C": "Object X is a small Solar System body because Kuiper‑belt objects cannot be dwarf planets by definition, and Object Y is a planet because its orbit intersects Earth's and it is dynamically active in the inner Solar System.",
        "D": "Object X should be classed as a planet because its radius exceeds $600\\,$km so it must clear a neighbourhood eventually, and Object Y is most properly classified as a comet because Earth‑crossing orbits imply volatile activity even if none has been observed."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a two‑object scenario contrasting hydrostatic-equilibrium (size, density, observed ellipsoid) vs non-equilibrium shape, and orbital context (Kuiper belt vs Earth‑crossing) to test dwarf‑planet criterion, orbital‑clearing requirement, and population taxonomy (KBO vs near‑Earth asteroid).",
      "concepts_tested": [
        "Hydrostatic equilibrium as dwarf planet criterion",
        "Orbital neighbourhood clearing distinguishing planets from minor bodies",
        "Minor‑planet population taxonomy (Kuiper belt objects, near‑Earth asteroids) and historical/IAU classification"
      ],
      "source_article": "Minor planet",
      "x": 2.011462926864624,
      "y": 1.3877993822097778,
      "level": 2,
      "original_question_hash": "1e38f109"
    },
    {
      "question": "Astronomers discover object 2025 QX9 orbiting the Sun with semi-major axis $a=39.4\\,$AU and a stable 3:2 mean-motion resonance with Neptune. Observations give diameter $1200\\,$km and mean density $1.8\\,$g cm$^{-3}$. Using $M=\\frac{4}{3}\\pi r^3\\rho$ with $r=600\\,$km (converted to SI units) yields $M\\approx 1.6\\times10^{21}\\,$kg, implying sufficient self-gravity to attain hydrostatic equilibrium. Under the 2006 IAU definitions, which classification best fits 2025 QX9, and which major planet is primarily responsible for establishing its resonance and orbital confinement?",
      "options": {
        "A": "Dwarf planet; Neptune",
        "B": "Small Solar System Body; Neptune",
        "C": "Planet; Neptune",
        "D": "Dwarf planet; Jupiter"
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete Kuiper-belt scenario with numerical radius, density and mass (using $M=\\frac{4}{3}\\pi r^3\\rho$) to test: (1) hydrostatic-equilibrium criterion for dwarf-planet status under the 2006 IAU definition, and (2) the role of Neptune in producing 3:2 resonances in the trans-Neptunian population.",
      "concepts_tested": [
        "IAU classification framework (planet vs dwarf planet vs SSSB)",
        "Hydrostatic equilibrium as criterion for dwarf-planet status",
        "Orbital resonances and perturbations by giant planets (Neptune shaping Kuiper belt structure)"
      ],
      "source_article": "Small Solar System body",
      "x": 2.017225503921509,
      "y": 1.3925853967666626,
      "level": 2,
      "original_question_hash": "6e55914c"
    },
    {
      "question": "A community health center serving a linguistically diverse, low–health-literacy population piloted two interventions to increase adoption of daily glucose self-monitoring among patients with type 2 diabetes. Intervention X sent a generic, one-page pamphlet in English to all patients and posted clinic posters. Intervention Y replaced the pamphlet with a 10–minute clinic visit in which clinicians used tailored messages in the patient’s preferred language, applied motivational interviewing techniques, practiced shared decision–making about monitoring schedules, and elicited brief personal illness narratives to contextualize recommendations. After 6 months, Intervention X clinics showed a 9% absolute increase in daily monitoring, while Intervention Y clinics showed a 28% absolute increase; knowledge test scores and patient satisfaction were also higher in Y. Which explanation best accounts for why Intervention Y outperformed Intervention X?",
      "options": {
        "A": "Intervention Y combined audience-tailored messaging with interpersonal strategies (motivational interviewing, shared decision–making, and narrative elicitation), which increased health literacy, reduced psychological reactance, and improved patient–provider dialogue—mechanisms that plausibly produced greater sustained behavior change.",
        "B": "Intervention Y succeeded primarily because narrative medicine (eliciting personal stories) alone always drives behavior change by creating emotional responses, so the other elements (tailoring, MI, SDM) were unnecessary.",
        "C": "Intervention X failed because printed materials in English are ineffective only because they were distributed too infrequently; if the pamphlet had been mailed weekly the outcome would match Intervention Y regardless of tailoring or clinician interaction.",
        "D": "Intervention Y’s effect is best explained by clinicians giving more explicit orders during the visit; adherence improved because patients were instructed what to do, not because of tailoring, motivational interviewing, shared decision–making, or increased health literacy."
      },
      "correct_answer": "A",
      "generation_notes": "Created a clinic-level experimental scenario with comparative outcomes; options contrast mechanisms (tailoring + MI + SDM + narratives vs single-factor explanations) to test understanding of tailored communication, interpersonal strategies, and their links to health literacy and behavior change.",
      "concepts_tested": [
        "Tailoring health communication to audience and situation",
        "Use of shared decision-making, motivational interviewing, and narrative medicine to engage patients",
        "Relationship between improved health literacy/patient-provider dialogue and behavior change"
      ],
      "source_article": "Health communication",
      "x": 1.2737679481506348,
      "y": 0.9930544495582581,
      "level": 2,
      "original_question_hash": "d038135a"
    },
    {
      "question": "You are a researcher asked to compare the offensive value of two historical hitters: Player X (1898) and Player Y (2015). For Player X you have only traditional box-score totals (H, AB, R, HR) and spotty error and RBI recording; for Player Y you have modern play-by-play, pitch-tracking, and fully standardized official-scoring logs. Which research strategy best uses advances in computing and sabermetrics while respecting the anchoring role of official scoring definitions to produce a defensible cross-era comparison?",
      "options": {
        "A": "Directly compare simple counting and rate stats available for both players (batting average H/AB, HR totals, and runs) without further adjustment; assume these raw numbers are sufficient because they are defined by the official scorer and therefore directly comparable across eras.",
        "B": "Use modern computing to reconstruct as many missing events for Player X as possible from historical box scores and contemporaneous accounts, apply retroactive standardization consistent with official-scoring definitions where feasibly justifiable, then compute era-adjusted sabermetric measures (e.g., wOBA or OPS+ / wRC+) that normalize for park and league run environment to compare the two players.",
        "C": "Estimate both players' offensive value using pitch-tracking and swing-miss metrics (e.g., whiff rate, QOP) by extrapolating those rates from Player X's box-score line; because sophisticated pitch metrics capture true skill, they are preferable even if they must be inferred for the older player.",
        "D": "Limit the comparison to traditional counting stats that are explicitly listed in the Official Baseball Rules (RBI, SB, HR) and use those unadjusted because only officially defined categories should be used in historical comparisons to avoid introducing bias from modern metrics or computing techniques."
      },
      "correct_answer": "B",
      "generation_notes": "Created a concrete historical vs modern comparison scenario requiring students to integrate three ideas: computing advances (reconstruction/PC-era computation), sabermetric era-/park-adjustment (wOBA, OPS+, wRC+), and necessity of applying or respecting official-scoring definitions when retroactively standardizing historical data.",
      "concepts_tested": [
        "Technology-driven data reconstruction and standardization (PC revolution and computing)",
        "Sabermetrics enabling broader, era-adjusted performance metrics for cross-era comparison",
        "Role of official scoring definitions as the anchor for measurement and retroactive standardization"
      ],
      "source_article": "Baseball statistics",
      "x": -0.28629979491233826,
      "y": 2.4673471450805664,
      "level": 2,
      "original_question_hash": "4691d540"
    },
    {
      "question": "The city of Midvale plans to deploy 200 air-quality sensors and a public exposure app. They need expertise to: (1) quantify sensor measurement error and propagate that uncertainty through population exposure maps; (2) design interactive visualizations that communicate uncertainty to non-expert residents; (3) integrate mobility patterns to produce spatial models of exposure risk; and (4) evaluate how different app designs may alter residents' behaviour and equity outcomes. Which description best captures the role of GIScience (as distinct from routine GIS work) in this project?",
      "options": {
        "A": "Conducting research to develop methods for representing and propagating uncertainty in spatial models, designing visualizations that convey uncertainty to lay users, formulating and validating spatial-analytic models that incorporate human mobility, and assessing societal impacts of the technology.",
        "B": "Installing and calibrating sensors, maintaining the geospatial database, creating map tiles and projecting layers into coordinate systems, and producing static operational maps for city staff.",
        "C": "Writing server-side code to stream sensor data, optimizing query performance for mobile apps, and deploying scalable cloud infrastructure to serve map tiles rapidly to users.",
        "D": "Applying off-the-shelf machine-learning models to predict pollutant concentrations from sensor data without addressing spatial semantics, visualization strategies, or the explicit representation of measurement uncertainty."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a municipal smart-city scenario to distinguish GIScience (research on uncertainty, visualization, spatial analysis, societal impacts) from GIS operational, software-engineering, and generic data-science roles.",
      "concepts_tested": [
        "Distinction between GIScience and GIS (research/analysis vs tool/data operations)",
        "Core GIScience emphases: spatial analysis, visualization, and representation/handling of uncertainty",
        "Interdisciplinary and evolving nature of GIScience including human dynamics and societal impacts"
      ],
      "source_article": "Geographic information science",
      "x": 1.3919496536254883,
      "y": 0.9818063974380493,
      "level": 2,
      "original_question_hash": "29bce06f"
    },
    {
      "question": "In the agronomy community studying soil nutrient transport, the prevailing 'Fluidic Soil' paradigm P defines canonical experimental techniques, problem formulations, and acceptable explanations; it appears in textbooks and shapes grant priorities. Over a decade, researchers document a systematic set of anomalies: crops on highly aerated soils uptake nutrients in ways P cannot predict. A group conducts exploratory studies, develops a new explanatory framework Q (the 'Biogenic Transport' model) that predicts previously anomalous isotope fractionation signatures, and publishes an influential methodological monograph Q* that students and labs adopt as a training example. According to Kuhn's account of scientific change, which interpretation best describes this sequence?",
      "options": {
        "A": "This sequence exemplifies Kuhn's model: P functioned as the community-wide paradigm that governed normal science, accumulating anomalies led to crisis and extraordinary research, Q gained acceptance because it better explained and predicted anomalies, and Q* serves as an exemplar that helps institutionalize the new paradigm.",
        "B": "This sequence shows that a single decisive experiment falsified P and immediately replaced it; paradigms are just individual exemplar texts (like Q*) and not broader sets of rules or practices shared by a community.",
        "C": "Kuhn would argue that because paradigms are incommensurable no rational comparison between P and Q is possible, so Q's adoption is purely a sociological shift unrelated to explanatory or predictive success.",
        "D": "The correct Kuhnian reading is that paradigms do not guide normal science; scientists constantly compare rival theories, and anomalies are normally absorbed into P without producing crises—so Q's proposal is just one of many incremental improvements rather than a paradigm shift."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete agronomy scenario mapping Kuhn's stages: normal science guided by a community paradigm, anomaly accumulation, extraordinary research producing a new paradigm, and role of exemplars; distractors reflect common misunderstandings (immediate falsification, radical incommensurability/relativism, and continuous incrementalism).",
      "concepts_tested": [
        "Paradigm as shared framework guiding normal science (rules, practices, standards)",
        "Mechanism of change: anomalies → crisis → extraordinary research → adoption of new paradigm",
        "Dual meanings of paradigm: community-wide framework versus single exemplary model and their role in institutionalization"
      ],
      "source_article": "Paradigm shift",
      "x": 1.19086754322052,
      "y": 1.06862211227417,
      "level": 2,
      "original_question_hash": "4c9ee9f1"
    },
    {
      "question": "Two countries, Novaterra and Minoria, have different research funding profiles. Novaterra is a high‑GDP economy with GERD equal to $17.5 billion, corresponding to GERD/GDP = $3.5\\%$, and its GERD is funded 65\\% by industry, 20\\% by universities, and 15\\% by government. Minoria is a lower‑GDP economy with GERD equal to $0.4 billion, corresponding to GERD/GDP = $0.8\\%$, and its GERD is funded 25\\% by industry, 20\\% by universities, and 55\\% by government. Both countries use competitive grant processes for a large share of public funding. A research team in each country submits two proposals to their national competitive agencies: (1) a blue‑sky fundamental nuclear fusion physics program with low immediate appropriability and high social return potential; (2) a near‑term advanced battery commercialization project with clear private appropriability and short expected time to market. Based on the funding compositions and the role of competitive selection in shaping research portfolios, which of the following outcomes is most likely?",
      "options": {
        "A": "Both proposals have roughly equal probability of receiving public competitive grants in both countries because competitive peer review is neutral to funding source and selects only on scientific merit.",
        "B": "The battery commercialization project is far more likely to be funded in Novaterra (industry‑dominated GERD), whereas the fusion blue‑sky program is more likely to secure public support in Minoria (government‑dominated GERD), because industry funding biases portfolios toward near‑term appropriation while government funding is more willing to underwrite low‑appropriability, high social‑return research.",
        "C": "The fusion program will be preferentially funded in Novaterra because its larger absolute GERD ($17.5 billion) means more capacity for high‑risk basic research, while Minoria cannot afford big science and will fund only incremental applied projects.",
        "D": "Neither country will fund the fusion program because competitive funding always disfavors projects with low commercial prospects; both will instead fund the battery project exclusively."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a comparative country scenario with GERD/GDP and funding‑source shares to probe how competitive funding and funder composition bias selection toward commercialization or blue‑sky research; created distractors that reflect plausible misunderstandings (neutrality of peer review, absolute GERD size, universal bias against basic research).",
      "concepts_tested": [
        "Competitive funding shapes research portfolios by selecting economically viable projects",
        "Funding source (industry vs government) biases research toward near‑term commercialization or foundational research",
        "Macro‑level funding distribution and national context (GERD/GDP, funding shares) influence which research areas receive support"
      ],
      "source_article": "Funding of science",
      "x": 1.2876145839691162,
      "y": 0.95167076587677,
      "level": 2,
      "original_question_hash": "12c89055"
    },
    {
      "question": "Attorney Rivera is a privately retained defense lawyer licensed in State Q, which has adopted a version of the ABA Model Rules. During plea negotiations in a fraud prosecution, the prosecutor offers to drop a related charge if Rivera reveals privileged communications between the defendant and Rivera. Rivera also wants to include a brochure guarantee of \"acquittal or your money back\" to attract clients. Which one of the following best describes Rivera’s ethical obligations and the likely enforcement mechanism if she violates them?",
      "options": {
        "A": "Rivera must refuse to disclose client confidences absent client consent or a narrow statutory exception; she owes candor to the tribunal and truthfulness to others, and advertising a guaranteed acquittal would likely be misleading under professional conduct rules. Enforcement would be by the state bar (with oversight often vested in the state’s highest court) which can impose sanctions up to disbarment; the MPRE is an admission exam and does not substitute for disciplinary enforcement.",
        "B": "Rivera may disclose privileged communications to obtain a favorable plea because vigorous advocacy permits disclosure of client secrets for strategic advantage; guarantees in advertising are permissible commercial speech so long as some clients benefit; disciplinary enforcement lies exclusively with federal courts when litigation is underway.",
        "C": "Because candor to the tribunal is paramount, Rivera must disclose client confidences if nondisclosure might mislead the court; advertising guarantees are outside the scope of ethics rules; the ABA Model Rules are binding federal law and therefore state bars have no role in enforcement.",
        "D": "Rivera can include a guaranteed acquittal in her brochure if she adds a short disclaimer; a high MPRE score prevents later disciplinary action; only client complaints (not bar authorities or courts) can trigger sanctions for ethical violations."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete trial-and-advertising scenario to test core duties (confidentiality, candor, truthfulness, professional independence), the role of Model Rules/MPRE in standardization, and enforcement by state bars and courts.",
      "concepts_tested": [
        "Core ethical duties (confidentiality, candor, truthfulness, professional independence)",
        "Regulatory standardization and enforcement mechanisms (Model Rules, MPRE, state bar/highest court oversight)"
      ],
      "source_article": "Legal ethics",
      "x": 1.2481390237808228,
      "y": 0.8290007710456848,
      "level": 2,
      "original_question_hash": "70dc740a"
    },
    {
      "question": "Country A is a middle-income economy whose garment firms currently perform labour-intensive assembly for a European multinational enterprise (MNE) that retains design, marketing, and distribution. The government wants firms to achieve functional upgrading (move into design and branding) within five years while anticipating that open-source digital fabrication (e.g., low-cost RepRap 3D printers) could enable near‑shoring of some activities to high-demand markets. Which policy package is most likely to produce sustained functional upgrading and reduce the risk that localization from digital fabrication will eliminate the country’s position in global value chains (GVCs)?",
      "options": {
        "A": "Require and incentivize joint ventures and supplier-development agreements with MNEs that include mandated technology, skills and knowledge transfer; invest in tertiary and vocational programs in design, QMS and marketing; strengthen IP, certification and digital infrastructure; subsidize local prototyping labs using 3D printers to accelerate product development; and promote supplier diversification so firms serve multiple value chains.",
        "B": "Offer tax holidays and relaxed labor regulations to attract more assembly FDI; build large export processing zones focused on reducing transport times; and prioritize immediate job creation over investment in education, IP protection or local R&D.",
        "C": "Prohibit or heavily restrict the import and use of desktop 3D printers and open-source hardware to prevent any domestic replication of design and small‑batch manufacturing; rely on MNEs to transfer design capabilities voluntarily in the medium term.",
        "D": "Subsidize capital‑intensive automation (industrial robots) and logistics infrastructure to reduce unit costs of large‑scale manufacture; keep firm skills and education funding at current levels and retain dependence on single dominant foreign buyer relationships."
      },
      "correct_answer": "A",
      "generation_notes": "Created a policy-choice vignette that requires reasoning about mechanisms of upgrading via MNE-driven technology/skill transfer, the cross-border nature of GVC activities (design vs assembly vs marketing), and the disruptive potential of open-source 3D printing; options present plausible but distinct development strategies.",
      "concepts_tested": [
        "Upgrading through MNE-led transfer of technology, skills and knowledge (functional upgrading)",
        "Global value chains as cross-border networks of design, production and postproduction activities (difference from simple supply chains)",
        "Impact of open-source hardware and 3D printing on localization and reconfiguration of GVCs"
      ],
      "source_article": "Global value chain",
      "x": 1.2714611291885376,
      "y": 0.9273715615272522,
      "level": 2,
      "original_question_hash": "07de413b"
    },
    {
      "question": "An engineering team must choose a shaft material for a seawater-exposed, load-bearing shaft. Two candidates are: Steel S1 — plain carbon steel with 0.6 wt% C, austenitized at 900°C, water-quenched and then tempered at 200°C; Alloy S2 — an 18 wt% Cr, 8 wt% Ni, 0.08 wt% C austenitic stainless steel that is hot-rolled and annealed. Which single statement best explains the expected differences in mechanical and corrosion performance, correctly assigns the primary metallurgical subfield(s) concerned with each phenomenon, and accurately describes the role of a metallurgist in this project?",
      "options": {
        "A": "S1 will develop a largely martensitic microstructure after quenching, giving high hardness and strength but lower toughness and poor corrosion resistance; tempering at 200°C lowers brittleness while reducing some hardness — these phase-transformation and heat‑treatment effects fall under physical metallurgy. S2 will retain an FCC austenitic structure after anneal, providing good ductility and excellent corrosion resistance due to a Cr‑rich passive oxide layer — corrosion/passivation and electrochemical behavior are primarily concerns of chemical metallurgy. A metallurgist integrates both chemical and physical metallurgy to select alloy composition, recommend processing and surface or heat‑treatment strategies, and perform failure analysis, distinct from the hands‑on craft of metalworking.",
        "B": "S2 will be mechanically stronger than S1 because hot‑rolling work‑hardens the stainless, and S1 will be more corrosion resistant than S2 because the higher carbon raises resistance to seawater; work‑hardening and surface corrosion are both mainly physical metallurgy problems. The metallurgist's role is therefore mostly to recommend forming schedules and shop practices.",
        "C": "Quenching S1 creates martensite but tempering at 200°C will fully restore ductility and eliminate martensite so S1 will match S2 in toughness and corrosion resistance; corrosion is unrelated to alloy chemistry and is dealt with mostly by changing the geometry of parts. Thus metallurgists focus primarily on mechanical testing rather than chemical or electrochemical issues.",
        "D": "Because S2 contains nickel, it will be strongly magnetic and thus unsuitable for rotating shafts; chemical metallurgy only concerns ore extraction and never surface chemistry or corrosion. Therefore, metallurgists are essentially miners and are not involved in component design or heat‑treatment recommendations."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a realistic design scenario contrasting a quenched‑and‑tempered 0.6 wt% C steel with an 18Cr‑8Ni austenitic stainless; options test knowledge of martensite formation, tempering effects, passivation by Cr, assignment of topics to chemical vs physical metallurgy, and the metallurgist's integrative role.",
      "concepts_tested": [
        "Distinction between chemical metallurgy and physical metallurgy (corrosion/electrochemistry/phase transformations)",
        "Interplay of alloy composition, processing (heat treatment/rolling), microstructure (martensite/austenite) and resultant mechanical/corrosion properties",
        "Role and scope of metallurgists bridging science and technology versus metalworking"
      ],
      "source_article": "Metallurgy",
      "x": 1.7952420711517334,
      "y": 0.9764789342880249,
      "level": 2,
      "original_question_hash": "5143c312"
    },
    {
      "question": "A multinational retailer, VerdeMart, enters Country X. Within two years $70\\%$ of domestic retailers display a locally issued \"TrustBlue\" certification badge at store entrances. The national government has introduced partial product-safety regulations that apply to some categories, the industry association has issued best-practice guidelines recommending TrustBlue, and consumer surveys show that shoppers in Country X interpret the badge as a credible sign of product safety. VerdeMart adopts the badge despite higher short-term costs. Which institutional-theory explanation best accounts for VerdeMart's decision and the rapid diffusion of TrustBlue across retailers?",
      "options": {
        "A": "VerdeMart sought legitimacy through institutional isomorphism: mimetic and normative pressures (industry guidance and peer adoption) plus some coercive/regulative pressure led it to conform. The adoption reflects the three institutional components—cultural-cognitive (public belief that the badge signals safety), normative (professional/industry standards), and regulative (government safety rules)—and diffusion occurred via carriers such as the symbolic system (the badge), new organizational routines (certification procedures), and factual signals (sales and survey data) that propagated the practice.",
        "B": "VerdeMart calculated that adopting TrustBlue would immediately minimize production costs and maximize short-term profit; diffusion occurred because firms that adopted gained operational efficiencies. This explanation relies solely on rational economic optimization and need not invoke legitimacy, institutional components, or social carriers.",
        "C": "The spread of TrustBlue is best explained by a purely cultural-cognitive process: firms internalized a shared belief about what constitutes a morally correct business practice and voluntarily adopted the badge. Normative standards and regulative pressures played no material role, and diffusion was achieved mainly through advertising campaigns.",
        "D": "The adoption resulted entirely from coercive pressure: the government legally required all firms to display TrustBlue, so VerdeMart complied. Diffusion was transmitted exclusively through formal legal instruments and contracts rather than through symbolic systems, routines, or public beliefs."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete multinational-entry scenario that requires integrating institutional isomorphism/legitimacy, the three institutional components, and carriers of diffusion; distractors isolate single mechanisms (economic rationality, cultural-cognitive only, coercive only).",
      "concepts_tested": [
        "Institutional isomorphism and legitimacy as drivers of conformity",
        "Cultural-cognitive, normative, and regulative components of institutions",
        "Diffusion, adoption, and transmission via carriers (symbolic systems, routines, facts)"
      ],
      "source_article": "Institutional theory",
      "x": 1.229720115661621,
      "y": 0.9700599312782288,
      "level": 2,
      "original_question_hash": "43f6c462"
    },
    {
      "question": "You perform an experiment exposing two human cell lines to identical DNA-damaging conditions (UV plus increased intracellular ROS and a low dose of ionizing radiation). Line R is a rapidly proliferating colorectal epithelial line that is partially deficient in mismatch repair (MMR) but has intact p53-mediated checkpoints. Line N is a population of terminally differentiated cortical neurons (post-mitotic), with high mitochondrial ROS production and reduced nucleotide excision repair (NER) capacity. After exposure and a period allowing Line R to undergo several rounds of replication while Line N remains non-dividing, which of the following outcomes best follows from known DNA damage response (DDR) principles and repair-pathway biology?",
      "options": {
        "A": "Line R will preferentially convert many lesions into fixed base-sequence mutations (due to replication across unrepaired lesions and error-prone translesion synthesis), increasing the chance of clonal expansion and tumorigenic transformation; Line N will accumulate unrepaired transcription-blocking lesions that perturb gene expression and drive senescence or functional decline rather than high rates of heritable mutation.",
        "B": "Line R will mainly enter irreversible senescence because proliferating epithelial cells are more sensitive to oxidative DNA lesions, whereas Line N will avoid dysfunction because non-dividing neurons do not replicate damaged DNA and therefore do not accumulate lesions.",
        "C": "Both lines will predominantly repair the induced damage by homologous recombination (HR), so neither will show significant increases in mutation rate or functional decline; HR is the dominant repair pathway for UV, ROS and low-dose ionizing radiation lesions in all human cells.",
        "D": "Line R will rapidly undergo apoptosis in most cells because mismatch repair deficiency triggers immediate cell death upon any DNA damage, while Line N will acquire many oncogenic mutations over time because reduced NER in non-dividing cells promotes mutation fixation even without replication."
      },
      "correct_answer": "A",
      "generation_notes": "Created a comparative experimental scenario contrasting a dividing, MMR-deficient epithelial line and non-dividing neurons with reduced NER and high ROS; options test DDR outcomes (mutation fixation, senescence, apoptosis) and dependence on cell type, replication status, and repair pathways.",
      "concepts_tested": [
        "DNA damage response coordinates detection and repair and failure can cause senescence/apoptosis/uncontrolled division",
        "Repair efficiency depends on cell type, replication status, age and environment influencing aging and cancer risk",
        "Different lesions disrupt transcription and unrepaired lesions in critical genes can lead to tumor formation"
      ],
      "source_article": "DNA repair",
      "x": 2.077514171600342,
      "y": 1.1501049995422363,
      "level": 2,
      "original_question_hash": "5d2488b4"
    },
    {
      "question": "Two neighboring communities occupy the same arid plateau and have experienced a multi‑decadal decline in rainfall. Community X relies on a multi‑century system of communal canal irrigation, centralized water management, and religious ceremonies centered on the annual canal opening. Community Y practices seasonal rain‑fed cultivation, has experienced out‑migration, and developed drought myths but no centralized irrigation institutions. An anthropologist using Julian Steward’s cultural ecology approach seeks to explain the divergent trajectories. Which interpretation best fits Stewardian cultural ecology while distinguishing it from strict environmental determinism and from a solely political‑economy explanation?",
      "options": {
        "A": "The divergence is best explained by historically inherited technologies and practices: communal canals and their management enabled Community X to maintain population and develop water‑centered institutions and rituals. Stewardian analysis would document the exploitation technologies, map associated behavioral patterns, and assess how those patterns influenced other cultural domains; where necessary, the analyst can combine this with political‑economy data (producing political ecology) to incorporate external market or state effects. This acknowledges environmental influence without claiming it deterministically produces culture.",
        "B": "The rainfall decline alone deterministically produced the observed cultural differences: the environment is the sole causal factor, so any differences between X and Y are accidental or superficial; the anthropologist need only measure rainfall and expect identical adaptive responses in all communities occupying the plateau.",
        "C": "The divergent cultural outcomes are best explained entirely by ideational factors and symbolic choice: rituals and institutions emerged purely from cognitive or religious creativity unrelated to subsistence technologies, so environmental and technological variables can be ignored in analysis.",
        "D": "Local environmental conditions are negligible; the differences between X and Y can only be explained by external political‑economic forces (markets, state policies); Stewardian cultural ecology would be irrelevant because only large‑scale political economy shapes their trajectories."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a comparative vignette requiring students to identify Steward’s key claim that environment influences but does not determine culture, mediated by technologies/practices/knowledge, and his three‑step methodological procedure; included link to political ecology as integration with political economy.",
      "concepts_tested": [
        "Non‑deterministic influence of environment on culture",
        "Steward’s methodological steps: document technologies, analyze behavior patterns, assess cultural impacts",
        "Relation of cultural ecology to political ecology (integration with political economy)"
      ],
      "source_article": "Cultural ecology",
      "x": 1.2745734453201294,
      "y": 0.9835293889045715,
      "level": 2,
      "original_question_hash": "b5341a2e"
    },
    {
      "question": "In 1739 a group of London merchants and philanthropically minded citizens establish the 'Harbour Children's Home' to care for abandoned children. They raise operating funds by annual public subscriptions from middle‑class donors, run the institution as an incorporated voluntary association with a board of trustees, publish pamphlets to recruit supporters, and secure a royal charter but receive no regular state funding. Which explanation best justifies historians classifying the Home as an example of modern philanthropy rather than as a government provision, a commercial enterprise, or simply medieval charity?",
      "options": {
        "A": "It was a private initiative focused on public welfare — financed by voluntary subscriptions and managed as an incorporated association — reflecting the 18th‑century emergence of organized, often Protestant‑led philanthropic engagement that institutionalized charity beyond medieval parish and guild relief.",
        "B": "Because it obtained a royal charter the Home should be seen as a government agency providing public services: the charter makes it equivalent to state provision despite the lack of ongoing government funding.",
        "C": "Its funding by merchant donors and regular subscriptions indicates a commercial model: the Home functioned as a business providing private benefits to patrons and was motivated primarily by economic return rather than public good.",
        "D": "The Home is best described as medieval charity, since donations to care institutions were largely motivated by religious concerns for salvation and traditionally administered through parishes and guilds rather than voluntary civic associations."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete 1739 London institutional scenario (modeled on the Foundling Hospital) and offered answer choices that contrast private-public-good definition, funding/organisational mechanisms (subscriptions, voluntary associations), and historical shift from medieval religious charity to modern organized philanthropy.",
      "concepts_tested": [
        "Philanthropy as private initiative for the public good contrasted with business and government",
        "Mechanisms of philanthropic activity: subscriptions, voluntary associations, charitable institutions",
        "Historical relationship between medieval charity, religious motivations, and emergence of organized philanthropy"
      ],
      "source_article": "Philanthropy",
      "x": 1.2219548225402832,
      "y": 0.8890497088432312,
      "level": 2,
      "original_question_hash": "4bebdb1f"
    },
    {
      "question": "A chemist, Dr. Imani, invents an enzyme that degrades a class of hospital biofilms. She notices hospitals struggle with biofilm-related infections (an unmet need), partners with a medical-device firm to incorporate the enzyme into a sterilization cartridge, secures seed funding, obtains regulatory advice to redesign the delivery mechanism, and pilots the cartridge in two hospitals—bearing personal financial risk and uncertainty about clinical approval. Which description best captures the entrepreneurial dynamics at work in Dr. Imani's project?",
      "options": {
        "A": "Dr. Imani identified an opportunity and recombined resources (scientific IP, a manufacturing partner, funding, regulatory expertise) while adapting actions to the regulatory and clinical context; her agency interacts with processual constraints to translate an invention into a risky but potentially value-creating marketable product.",
        "B": "This is primarily a demonstration of individual entrepreneurial traits—Dr. Imani's risk tolerance and creativity are sufficient explanation; the external context and resource recombination are secondary to her personal qualities.",
        "C": "The case shows entrepreneurship as mere resource recombination: assembling partners and capital is the defining activity, regardless of how the regulatory or clinical context shapes decisions or how the inventor engages with stakeholders.",
        "D": "Dr. Imani's activity is best described as small-business management: improving an existing sterilization process with modest risk and without significant innovation or translation from invention to market."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete biomedical scenario testing (1) opportunity identification and resource recombination, (2) agency–process–context interaction (processual vs functionalistic), and (3) translating an invention into marketable value under risk; distractors isolate each concept incorrectly.",
      "concepts_tested": [
        "Opportunity identification and resource recombination",
        "Agency–process–context relationship (functionalistic vs processual)",
        "Value creation under risk via innovation and commercialization"
      ],
      "source_article": "Entrepreneurship",
      "x": 1.3352209329605103,
      "y": 0.9764782190322876,
      "level": 2,
      "original_question_hash": "c376ff9b"
    },
    {
      "question": "A mid-sized electronics assembly plant produces printed circuit boards (PCBs). Management wants to increase throughput and reduce defects without relying on artisanal skills of a few experienced assemblers. They propose several interventions. Which single intervention most closely follows the core principles of scientific management — systematic analysis and synthesis of workflows, elimination of waste through standardization, and transferring worker knowledge into documented tools and procedures — while also addressing worker incentives?",
      "options": {
        "A": "Commission a time-and-motion study to identify the optimal sequence of assembly steps and eliminate unnecessary motions; redesign the workstation layout and tools to standardize conditions; develop step-by-step standard operating procedures (SOPs) and training programs so less-experienced workers can perform to the standard; and implement a bonus pay scheme for meeting the standardized output and quality targets.",
        "B": "Shift to a craft-based model where each assembler is allowed to choose their own sequence of tasks and tooling based on personal technique, encouraging job rotation so workers build broad skills, with pay determined by tenure and peer reputation rather than output.",
        "C": "Install high-resolution sensors and software to monitor individual workers’ hand movements and cycle times in real time; use the monitoring data to discipline underperformers and to produce performance dashboards, but make no changes to station layouts, tooling, or written procedures.",
        "D": "Invest heavily in robotic automation to replace manual assembly tasks performed by the most skilled workers; purchase turnkey machines and let vendor engineers tune them, but do not produce internal documentation of the manual methods or train remaining staff in standardized procedures."
      },
      "correct_answer": "A",
      "generation_notes": "Framed a concrete factory scenario and offered four plausible management interventions. Option A incorporates time-and-motion analysis, elimination of waste via layout/tool standardization, codification into SOPs and training, and incentive alignment — matching Taylorist principles. Other options are plausible but omit one or more core elements.",
      "concepts_tested": [
        "Systematic analysis and synthesis of workflows (time-and-motion studies)",
        "Standardization and elimination of waste (muda, muri, mura)",
        "Knowledge transfer and codification into documentation and training"
      ],
      "source_article": "Scientific management",
      "x": 1.1911665201187134,
      "y": 0.9200366735458374,
      "level": 2,
      "original_question_hash": "7172abfc"
    },
    {
      "question": "Country Aurelia consumes 1000 TWh/year. Current electricity mix is 40% coal, 30% natural gas, 25% renewables, 5% nuclear. Emission intensities are 1 MtCO2/TWh for coal, 0.5 MtCO2/TWh for gas, and 0 for renewables/nuclear. Current annual CO2 emissions from the power sector are therefore $1000(0.4\\times1+0.3\\times0.5)=550$ MtCO2. Aurelia has committed under international agreements to reduce power‑sector CO2 by 50% by 2035 (target 275 MtCO2). Which of the following policy packages is most likely to both ensure Aurelia meets that 50% reduction by 2035 and accelerate longer‑term decarbonization by steering private investment into low‑carbon technologies?",
      "options": {
        "A": "A national Renewable Portfolio Standard (RPS) mandating 60% renewables by 2030, with no carbon price and only modest removal of fossil fuel subsidies. Net effect: fossil share falls to 35% (coal 20%, gas 15%), giving emissions $1000(0.2\\times1+0.15\\times0.5)=275$ MtCO2 (meets the 50% target).",
        "B": "A single instrument: a uniform carbon price of $50/ton CO2 without complementary mandates or targeted support. Market response reduces fossil share to 40% (coal 20%, gas 20%), giving emissions $1000(0.2\\times1+0.2\\times0.5)=300$ MtCO2 (does not meet the 50% target).",
        "C": "Large subsidies for new natural gas generation as a ‘bridge’ fuel (60% subsidy on new gas plants) while keeping other policies unchanged. This raises gas to 45%, coal to 25%, renewables 25%, nuclear 5%, giving emissions $1000(0.25\\times1+0.45\\times0.5)=475$ MtCO2 (worsens emissions).",
        "D": "A combined package: an explicit coal phase‑out timetable (ban on new coal + scheduled retirements), a moderate carbon price, an RPS to 70% plus targeted subsidies for storage and grid upgrades. Market and investment shifts reduce coal to 5%, gas to 20%, renewables to 70%, nuclear 5%, giving emissions $1000(0.05\\times1+0.2\\times0.5)=150$ MtCO2 (well below the 50% target) and mobilizing long‑lived low‑carbon investments consistent with international commitments."
      },
      "correct_answer": "D",
      "generation_notes": "Created a numerical scenario (1000 TWh) with emission factors and resulting emissions to compare policy instruments (legislation, carbon pricing, subsidies, combined package). Options present plausible market responses and emissions calculations; correct answer is the combined policy which both meets target and accelerates decarbonization.",
      "concepts_tested": [
        "Policy instruments shaping investment and emissions (legislation, subsidies, carbon pricing)",
        "Impact of energy infrastructure mix (coal vs renewables/nuclear) on carbon footprint and decarbonization rate",
        "Interaction of national energy policy with climate commitments and how combined measures align with international targets"
      ],
      "source_article": "Energy policy",
      "x": 1.404709815979004,
      "y": 0.8308010697364807,
      "level": 2,
      "original_question_hash": "c2eef90e"
    },
    {
      "question": "Country X emits 1,000 MtCO2-eq per year: 700 Mt (70%) from the energy sector (largely coal and gas), 200 Mt (20%) from agriculture, forestry and land use (AFOLU) including substantial deforestation, 60 Mt (6%) from methane sources (livestock and fossil‑fuel fugitive emissions), and 40 Mt (4%) from other industry. The government has committed to cut national greenhouse gas emissions by $43\\%$ by 2030. Which one of the following integrated policy and technology packages is the most plausible route to achieve that target rapidly while addressing intermittency, land‑use carbon dynamics, and practical implementation barriers?",
      "options": {
        "A": "Deploy large-scale wind and solar to replace coal quickly, couple with rapid electrification of transport, heating and industry; invest immediately in grid upgrades (long‑distance transmission, smart grids), storage (pumped hydro where geography allows, batteries for short duration) and demand management/sector coupling; implement a rising carbon price while phasing out fossil‑fuel subsidies and redirecting revenues to clean‑energy incentives and community benefit schemes; and simultaneously implement AFOLU measures (end deforestation, restore degraded soils and wetlands, reduce food waste and promote lower‑meat diets, plus targeted methane mitigation in livestock).",
        "B": "Prioritise a national program to build many new nuclear reactors and scale up BECCS as the principal emissions reduction and negative‑emissions strategy, while leaving the transmission grid largely unchanged; retain most fossil‑fuel subsidies during plant construction and delay carbon pricing until new plants are online to avoid political backlash.",
        "C": "Launch an ambitious afforestation campaign aiming to convert very large areas (millions of hectares) to monoculture plantations and expand bioenergy crop production to offset ongoing fossil‑fuel use; rely on land sinks to meet the $43\\%$ cut without major changes to the power system or carbon pricing, accepting some loss of agricultural land.",
        "D": "Focus mainly on demand‑side lifestyle policies (national campaigns to reduce meat consumption, less flying and car use, more recycling) combined with modest increases in rooftop solar; keep existing subsidies and no nation‑wide carbon price; rely on voluntary behaviour change to deliver most of the reductions by 2030."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete national emissions breakdown and a $43\\%$ by‑2030 target; designed four plausible policy/technology packages that test understanding of energy system transition (renewables, electrification, grid upgrades, storage, demand management), land‑use and carbon cycle levers (AFOLU actions, methane reductions, fast vs slow carbon cycles), and policy instruments/barriers (carbon pricing, subsidy shifts, environmental/community objections, grid constraints). Option A integrates the necessary technical, land‑use and policy measures and addresses implementation barriers, so it is correct.",
      "concepts_tested": [
        "Energy system transition: renewables, electrification, grid upgrades, storage, demand management",
        "Land‑use and carbon cycle dynamics: AFOLU mitigation, methane sources, soil/forest sinks and diet/food‑waste interventions",
        "Policy and economic instruments and barriers: carbon pricing, subsidy reform, incentives, environmental objections and grid constraints"
      ],
      "source_article": "Climate change mitigation",
      "x": 1.3344577550888062,
      "y": 0.8467176556587219,
      "level": 2,
      "original_question_hash": "90a7dafb"
    },
    {
      "question": "Sam is a novice lifter with a 1RM bench press of 100 kg. He performs 3 sets of 8 reps at 70 kg (training volume per session = $3\\times8\\times70=1680$ kg) three times per week. His coach prescribes increasing the load by 2.5 kg each week (while keeping sets and reps constant), instructs Sam to use strict technique for all but the last set, and allows 1–2 controlled \"cheat\" reps on the final set when form begins to fail. After 8 weeks, which pattern of physiological adaptation and training rationale is most consistent with the principles described in the article?",
      "options": {
        "A": "Early strength improvements (especially in weeks 1–4) will be dominated by neural adaptations (improved motor unit recruitment, rate coding, and coordination) rather than by large increases in muscle cross-sectional area; the weekly 2.5 kg increments implement progressive overload (increasing load and therefore sets×reps×load over time); strict form is necessary to ensure the target muscles receive the overload and to minimise injury risk, while the occasional, controlled cheat rep on the last set can be used strategically by trainees approaching a plateau but should not replace strict technique for novices.",
        "B": "Most of Sam’s strength gains over the first 8 weeks will be due to muscle hypertrophy, so the coach should prioritise rapidly increasing total repetitions rather than load; allowing cheat reps from the start will accelerate hypertrophy by recruiting additional fibres, and strict technique is not critical for novices.",
        "C": "Progressive overload is best achieved by keeping load constant and increasing inter-set rest periods; early neural adaptations are negligible in novices, and any form of cheating should be eliminated entirely because it always reduces activation of the intended muscle and therefore reduces training effectiveness.",
        "D": "Sam’s programme will primarily improve his aerobic capacity and lactate threshold rather than strength; early gains are driven mainly by systemic hormonal changes, and cheating every set is necessary to maximally recruit fast-twitch fibres and produce the largest strength adaptations."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete 1RM-based bench-press scenario with numeric training volume and a weekly load increment to test understanding of progressive overload, emphasize neural vs hypertrophic early adaptations, and evaluate the role of strict form versus strategic cheating.",
      "concepts_tested": [
        "progressive overload (sets×reps×load, incremental load increases)",
        "neural adaptations in early strength gains and role of proper form/strategic cheating"
      ],
      "source_article": "Strength training",
      "x": 1.8305386304855347,
      "y": 1.135414958000183,
      "level": 2,
      "original_question_hash": "a4c7851c"
    },
    {
      "question": "Anna is a thirty-year-old research scientist with a secure position in a prestigious lab. Over several years she has felt increasing dissatisfaction: she finds laboratory work technically interesting but feels it does not answer her need to teach, create, and relate to students. Her colleagues and family point to objective reasons to stay (salary, pension, prestige) and present a detailed cost–benefit projection showing that leaving would be financially risky. Anna considers four responses. Which response best exemplifies an existentialist commitment to authenticity, personal freedom and responsibility, and the critique of pure rationalism in favor of lived subjective meaning in the face of life's absurdity?",
      "options": {
        "A": "Anna resigns from the lab to become a teacher-artist. Before acting she reflects on her past obligations and realistic constraints, accepts responsibility for the consequences, and deliberately chooses a life that aligns with her values—even though the choice may seem irrational by objective measures and may expose her to uncertainty.",
        "B": "Anna remains in the lab because the cost–benefit projection demonstrates objectively superior outcomes; she regards meaning as something that follows from prudent, rational life-planning, and she suppresses her teaching impulses as mere distractions.",
        "C": "Anna quits impulsively one night and tells everyone she left 'on a whim' to avoid having to justify or take responsibility for the change; she insists her choice was random and therefore not her responsibility.",
        "D": "Anna keeps her job and explains that her dissatisfaction is entirely the result of her socioeconomic background and upbringing—conditions she could not have chosen—and therefore she has no real freedom to change her path."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete career decision scenario to contrast authentic choice (accepting freedom/responsibility and acknowledging facticity) with rationalist, inauthentic (bad faith), and deterministic responses; selects the option that aligns with existentialist virtues and critique of pure rationalism.",
      "concepts_tested": [
        "Authenticity and authentic choice",
        "Personal freedom, responsibility, and deliberate choice",
        "Rejection of rationalism in favor of subjective lived experience and response to absurdity"
      ],
      "source_article": "Existentialism",
      "x": 0.8687182068824768,
      "y": 1.0594171285629272,
      "level": 2,
      "original_question_hash": "aff72375"
    },
    {
      "question": "A company operates a collaborative editing service ‘DocFlow’. Browser clients send edit requests to a front-end web server, which forwards them to an application server that enforces business logic and exposes a REST API. The application server then calls a synchronization server to propagate the update; multiple synchronization servers communicate among themselves to keep replicas consistent. Some power users run a local desktop client that also hosts a temporary local server for offline edits; when reconnecting, that local server behaves as a client to the cloud synchronization servers to reconcile changes. Which of the following statements best characterizes how this deployment illustrates core client–server principles?",
      "options": {
        "A": "This deployment exemplifies decoupling: servers (web, application, synchronization) provide reusable services to clients, clients initiate request–response interactions governed by an application-layer protocol and the REST API hides server internals; synchronization servers perform inter-server communication to replicate state—so servers and clients can coexist on the same host without changing roles.",
        "B": "Because some users run a local server, the system has become a pure peer-to-peer network where no central servers exist; thus request–response interactions and application-layer protocols are unnecessary and replication is achieved by peers broadcasting edits.",
        "C": "The presence of a REST API forces clients to understand the application server's internal algorithms and state representations, increasing coupling; therefore APIs are primarily for performance rather than abstraction or cross-platform exchange.",
        "D": "Load balancing and scaling are irrelevant in this architecture because the client always initiates requests; server resources need not be provisioned beyond a single synchronization server since clients handle most processing when offline."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete multi-component scenario (web, application, sync servers and local server) to test decoupling, request–response and application-layer protocols/APIs, and flexible deployment including inter-server communication and co-located client/server roles.",
      "concepts_tested": [
        "Decoupling of clients and servers; modular service provisioning",
        "Request–response pattern governed by application-layer protocols and APIs",
        "Flexible deployment and inter-server communication; co-located client and server components"
      ],
      "source_article": "Client–server model",
      "x": 1.4411027431488037,
      "y": 1.0782699584960938,
      "level": 2,
      "original_question_hash": "b211239e"
    },
    {
      "question": "Dr. Lee formulates the following statement: \"If human fibroblasts are exposed to 10 μM compound X for 24 hours (P), then their proliferation rate will decline by $\\ge 30\\%$ relative to untreated controls measured by cell count at 48 hours (Q).\" Which of the following characterizations of Dr. Lee's statement is most accurate in light of scientific and logical norms?",
      "options": {
        "A": "All of the following are correct: (i) the statement is a properly framed scientific hypothesis because it defines operational measures and yields reproducible, falsifiable predictions; (ii) if independent experiments repeatedly confirm the prediction, the hypothesis can become part of or lead to a scientific theory; (iii) provisionally accepting the statement to plan experiments makes it a working hypothesis; (iv) in the conditional \"If P, then Q\", P functions as the antecedent (hypothesis) and Q as the consequent.",
        "B": "Only (i) is correct: the statement is a properly framed scientific hypothesis, but it cannot become part of a scientific theory even if repeatedly confirmed, and accepting it provisionally does not make it a working hypothesis.",
        "C": "Statements (i)–(iii) are correct but (iv) is incorrect because in scientific practice the antecedent is the consequence and the consequent is the hypothesis.",
        "D": "None are correct: the statement is not a scientific hypothesis because the predicted decline could be due to random variation (requiring statistical testing), so it is unfalsifiable and cannot be used as a working hypothesis or relate to theory formation."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed an experimental cellular scenario with explicit operational definitions and logical form to test testability/falsifiability, working-hypothesis provisional use, and the antecedent/consequent role; distractors reflect common misconceptions about theory formation and logic.",
      "concepts_tested": [
        "Testability and falsifiability of scientific hypotheses",
        "Relationship between hypotheses, working hypotheses, and scientific theories",
        "Logical structure of hypotheses as antecedent (If P then Q) and need for operational definitions"
      ],
      "source_article": "Hypothesis",
      "x": 1.382477045059204,
      "y": 1.0940642356872559,
      "level": 2,
      "original_question_hash": "7b5d2d73"
    },
    {
      "question": "Consider sintering of a compact made of spherical ceramic powder particles. Early-stage sintering produces necks between particles while later stages eliminate pores and densify the compact. Which one of the following options correctly identifies (i) the dominant atom-transport mechanisms responsible for initial neck growth vs final pore elimination, (ii) the thermodynamic driving force for densification, and (iii) how particle size and temperature influence the densification rate?",
      "options": {
        "A": "Initial neck growth is dominated by surface diffusion and vapor transport (non‑densifying); final pore elimination is dominated by grain‑boundary and lattice diffusion (densifying). The thermodynamic driving force is the reduction of total surface area and surface free energy (creation of lower‑energy solid–solid interfaces), producing curvature‑driven chemical potential gradients and capillary pressure (∼2γ/R). Smaller particle radius (higher curvature) increases the driving force and capillary pressure, and higher temperature exponentially increases diffusion rates, so fine powders at elevated temperature promote faster densification (though at low T surface diffusion may enlarge necks without densifying).",
        "B": "Initial neck growth is driven mainly by lattice diffusion through particle volumes (densifying) while final pore elimination proceeds by surface diffusion (non‑densifying). The driving force is primarily stored elastic strain in the green compact rather than surface energy. Larger particles provide a larger driving force because they have more volume to diffuse from, and increasing temperature mainly reduces capillary pressure so densification slows at high temperature.",
        "C": "Both initial neck growth and final densification are controlled exclusively by vapor transport between particles; the driving force is vapor pressure differences unrelated to surface area. Particle size has negligible effect on driving force, but higher temperature always reduces densification because it increases vapor pressure and causes pore growth.",
        "D": "Initial neck growth is dominated by plastic deformation and dislocation motion (densifying), while final pore elimination is controlled only by surface diffusion (non‑densifying). The driving force is conversion of grain‑boundary energy into elastic energy. Small particles slow down densification because their high surface area stabilizes pores; temperature has only a linear effect on diffusion rates."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a single-choice scenario that requires distinguishing non‑densifying (surface/vapor) vs densifying (grain‑boundary/lattice) transport, stating the thermodynamic surface‑energy driving force with capillary pressure scaling ∼2γ/R, and explaining how smaller particle size and higher temperature accelerate densification while noting surface diffusion can be non‑densifying at low T.",
      "concepts_tested": [
        "Diffusion mechanisms: surface/vapor vs grain‑boundary/lattice diffusion and their densifying character",
        "Thermodynamic driving force: reduction of surface area and surface free energy, curvature‑driven chemical potential",
        "Influence of particle size (curvature, capillary pressure) and temperature on diffusion rates and densification"
      ],
      "source_article": "Sintering",
      "x": 1.7754490375518799,
      "y": 0.9956833720207214,
      "level": 2,
      "original_question_hash": "4906d1c5"
    },
    {
      "question": "AgriTech Solutions is a mid-sized firm combining commercial agriculture and research. Senior leadership has set two strategic objectives for the next three years: (1) increase research commercialization by 40% and (2) improve cross-functional collaboration between R&D and field operations. Current practices include individual quarterly sales targets, stack-ranking of employees, annual performance appraisals, and large individual commissions. After a year, collaboration has deteriorated, and innovation throughput declined. Which redesign of their performance-management system best aligns activities, processes, and metrics with the stated strategic objectives while using feedback and incentives to promote collaboration and trust?",
      "options": {
        "A": "Maintain individual quarterly targets and large commissions but add a public leaderboard displaying each employee's sales figures to increase transparency and motivate top performers; keep annual appraisals.",
        "B": "Implement a Balanced Scorecard with organizational, team, and individual KPIs that include research commercialization milestones and cross-functional collaboration metrics; introduce quarterly coaching and 360-degree feedback, and allocate a portion of variable pay to team- and company-level rewards.",
        "C": "Switch to a strictly top-down approach: set more aggressive individual targets tied to annual pay-for-performance ratings, eliminate stack-ranking but keep individual bonuses, and reduce frequency of formal feedback to avoid micromanagement.",
        "D": "Adopt a real-time analytics dashboard tracking R&D outputs and sales pipeline, and use the data to enforce compliance with prescribed processes; tie disciplinary consequences to missed process checkpoints to ensure adherence."
      },
      "correct_answer": "B",
      "generation_notes": "Created a concrete organizational scenario (AgriTech Solutions) and presented four plausible PMS redesigns; option B aligns metrics to strategy, uses coaching and 360-degree feedback, and balances individual/team incentives to foster collaboration and trust.",
      "concepts_tested": [
        "Strategic alignment of activities, processes, and metrics",
        "Use of feedback, coaching, and incentives to influence behavior",
        "Importance of collaboration and trust versus internal competition"
      ],
      "source_article": "Business performance management",
      "x": 1.3485000133514404,
      "y": 0.9952040910720825,
      "level": 2,
      "original_question_hash": "38e9151a"
    },
    {
      "question": "After a multinational firm reduces its workforce by 30% and reallocates year-end bonuses, Maria’s unit reports a sharp decline in productivity and rising turnover intentions. Employee survey responses indicate (a) many employees received substantially smaller bonuses than peers, (b) decision procedures were perceived as inconsistent and employees had little opportunity to voice concerns, and (c) managers delivered terse explanations and showed little respect. Which of the following best explains the decline in productivity and increased turnover intentions in terms of organizational justice theory?",
      "options": {
        "A": "The decline is best explained by a combination of low distributive justice (unfavorable bonus outcomes) together with low procedural, interpersonal, and informational justice (inconsistent procedures, lack of voice, disrespectful treatment, and inadequate explanations). Equity-theory comparisons of inputs and outcomes generate negative affect, and affect mediates a causal chain from perceived injustice to reduced motivation, lower job satisfaction and trust, decreased performance, and higher turnover intentions. Note that outcome favorability (getting a larger bonus) influences distributive justice, whereas outcome justice refers to the moral propriety of decisions and can be defended by fair procedures and explanations even when outcomes are unfavorable.",
        "B": "Only distributive injustice (the smaller bonuses) explains the productivity drop: employees compare inputs and outcomes, so outcome favorability alone determines effort. Procedural and interactional factors are secondary and have negligible causal influence on motivation or turnover.",
        "C": "Outcome favorability and outcome justice are the same concept, so if employees ultimately receive adequate pay the process does not matter; the productivity decline must therefore be due to external economic factors rather than perceptions of fairness within the organization.",
        "D": "Procedural justice affects only person-level outcomes such as pay satisfaction, while distributive justice affects only organization-level outcomes such as commitment; interpersonal and informational justice have no independent behavioral effects—thus the observed decline must arise solely from procedural lapses rather than the bonus distributions or manager behavior."
      },
      "correct_answer": "A",
      "generation_notes": "Created a realistic downsizing/bonus-reallocation scenario requiring integration of the four justice components, the affect-mediated causal chain to outcomes (motivation, satisfaction, productivity, turnover), and the distinction between outcome favorability and outcome justice.",
      "concepts_tested": [
        "four components of organizational justice (distributive, procedural, interpersonal, informational)",
        "causal relationship from perceived justice to organizational outcomes via affect and equity theory",
        "distinction between outcome favorability and outcome justice (moral propriety)"
      ],
      "source_article": "Organizational justice",
      "x": 1.2877681255340576,
      "y": 0.9958865642547607,
      "level": 2,
      "original_question_hash": "f6a8a14b"
    },
    {
      "question": "A multinational research team is mapping soils across a coastal watershed that spans a rainfall gradient and supports mixed agriculture. Their tasks are: (1) produce an international-compatible soil classification map that emphasizes the soils' developmental history and observable profile morphology, (2) assess how those soils constrain crop productivity and potential for long-term carbon sequestration, and (3) design monitoring that links soil carbon dynamics to vegetation, hydrology, and atmospheric CO2 fluxes. Which allocation of responsibilities, classification system choice, and research emphasis best aligns with established soil‑science concepts (pedology vs. edaphology), the World Reference Base (WRB) classification principles, and pedosphere integrative approaches?",
      "options": {
        "A": "Assign pedologists to lead the classification map using WRB (emphasizing morphology and pedogenesis, with climate only inferred via profile expressions); assign edaphologists to study crop constraints and carbon sequestration; design monitoring to integrate soil biology, hydrology and atmospheric flux measurements across the pedosphere.",
        "B": "Have edaphologists produce the classification map using USDA taxonomy (explicitly incorporating climatic zones into the classification), then have pedologists evaluate crop productivity; focus carbon measurements exclusively on soil physical properties (bulk density and texture) without biotic or atmospheric measurements.",
        "C": "Let pedologists map soils with WRB for their genesis and morphology, but confine the carbon sequestration assessment to laboratory measures of soil organic carbon stocks only, omitting field hydrological and CO2 flux monitoring because those are outside soil science.",
        "D": "Use edaphology to both classify soils and assess management; adopt WRB but explicitly add climate zone categories as primary classification criteria; monitor only vegetation biomass as a proxy for soil carbon changes, ignoring soil microbial and hydrological processes."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a landscape scenario requiring choices about who does classification vs. use-oriented study, WRB's morphology/pedogenesis basis (excluding climate as an explicit criterion), and the need for interdisciplinary pedosphere monitoring linking biology, hydrology, and atmosphere.",
      "concepts_tested": [
        "Distinction between pedology and edaphology",
        "WRB classification principles and treatment of climate",
        "Pedosphere as an integrated system requiring interdisciplinary approaches for carbon sequestration studies"
      ],
      "source_article": "Soil science",
      "x": 1.7981724739074707,
      "y": 0.9788057804107666,
      "level": 2,
      "original_question_hash": "8be3317b"
    },
    {
      "question": "A medical school curriculum committee is redesigning a 4-year MD program to satisfy LCME-style accreditation, implement AAMC Entrustable Professional Activities (EPAs), and adopt Health Systems Science (HSS) as the \"third pillar\" alongside preclinical and clinical studies. Which of the following curriculum designs best aligns with evidence-based medical education principles described in the article?",
      "options": {
        "A": "Integrate HSS longitudinally across all years; assess clinical skills with standardized OSCE stations and use reliable checklist-based ratings for professionalism; map each of the AAMC EPAs to entrustment levels that require decreasing direct supervision before independent practice; avoid relying on learning-styles matching or Edgar Dales' Cone of Learning as instructional justification.",
        "B": "Keep HSS as an optional elective during residency; separate preclinical and clinical content with no explicit HSS thread; assess students primarily with end-of-course multiple-choice exams and supervisor impressions; base teaching methods on matching instruction to students' preferred learning styles.",
        "C": "Introduce HSS only in a single third-year course; use OSCEs for summative assessment but rely mainly on written knowledge tests to certify EPA completion; allow students to attest they meet EPAs without formal entrustment levels or decreased supervision benchmarks.",
        "D": "Adopt EPAs but evaluate them exclusively through unstructured narrative evaluations by clinical faculty; emphasize Silberman-style group activities and Edgar Dales' Cone of Learning to prioritize audiovisual materials; postpone implementing checklist-based professionalism assessments until fellowship training."
      },
      "correct_answer": "A",
      "generation_notes": "Created a scenario requiring application of evidence-based techniques (OSCE, checklists), competency-based EPAs with entrustment progression, and HSS integration; distractors include common but ineffective practices (learning-styles, Dales' Cone) and misaligned assessment strategies.",
      "concepts_tested": [
        "Evidence-based medical education methods (OSCE, checklist-based assessments)",
        "Entrustable Professional Activities and competency-based entrustment progression",
        "Health Systems Science as a curricular pillar integrated with preclinical and clinical studies"
      ],
      "source_article": "Medical education",
      "x": 1.222764492034912,
      "y": 0.9137728214263916,
      "level": 2,
      "original_question_hash": "fee50b41"
    },
    {
      "question": "A multidisciplinary team discovers a promising compound in cell culture and validates efficacy in animal models with acceptable preclinical safety. They are ready to design a first‑in‑human Phase I study, but face delays getting the candidate into clinical testing. Later, after successful Phase II/III trials and guideline endorsement, uptake into routine clinical practice is slow across healthcare systems. According to translational medicine frameworks, which option correctly identifies (1) the translational block responsible for the delay before first‑in‑human trials, (2) the block responsible for the slow uptake after proven efficacy, and (3) the pillar or type of organization best positioned to drive implementation of proven interventions into community practice?",
      "options": {
        "A": "(1) T1 translational block; (2) T2 translational block; (3) the community pillar (public health implementation and stakeholder organizations such as professional societies and public‑health agencies, e.g., European Society for Translational Medicine or local health authorities).",
        "B": "(1) T2 translational block; (2) T1 translational block; (3) the bedside pillar (hospital clinicians and NCATS‑style translational research centers).",
        "C": "(1) T1 translational block; (2) T2 translational block; (3) infrastructure and funding hubs (e.g., Clinical and Translational Science Awards and NCATS) rather than community/public‑health organizations.",
        "D": "(1) a regulatory/policy block unrelated to T1/T2; (2) lack of research funding rather than a translational block; (3) benchside academic laboratories to spearhead community implementation."
      },
      "correct_answer": "A",
      "generation_notes": "Created a clinical scenario mapping lab discovery → first‑in‑human → guideline adoption to test identification of T1 vs T2 translational blocks and the role of the community pillar/organizations in implementation; distractors swap blocks or misassign organizational roles.",
      "concepts_tested": [
        "bench‑to‑bedside translation and T1 vs T2 blocks",
        "roles of benchside/bedside/community pillars",
        "organizations and infrastructures that facilitate implementation (e.g., professional societies, CTSA, NCATS, public health)"
      ],
      "source_article": "Translational medicine",
      "x": 1.132129430770874,
      "y": 0.8878079652786255,
      "level": 2,
      "original_question_hash": "ca41746c"
    },
    {
      "question": "Evoke VR currently charges a flat fee per rock‑climbing session (a service). Management wants to reposition the company according to the experience economy and ultimately charge for transformations. Which strategic package of actions best embodies Pine & Gilmore's prescription that memory and transformation are the product and that businesses must orchestrate memorable events to create economic value?",
      "options": {
        "A": "Invest heavily in lower‑cost VR hardware and sell sessions at volume; emphasize technical performance specs in marketing so customers choose the best tangible product.",
        "B": "Develop multi‑session, story‑driven climbing narratives with curated sensory elements (sound, lighting, guided scripts) to create memorable events; sell outcome‑oriented packages priced for the promised transformation (e.g., confidence after 12 sessions); measure psychological outcomes (self‑efficacy) and offer complimentary basic trials to draw people into the higher‑value experience.",
        "C": "Standardize and automate sessions to maximize throughput, outsource content creation, and adopt an efficiency‑focused pricing model that charges per hour while minimizing bespoke orchestration.",
        "D": "Adopt dynamic per‑seat pricing like airlines, prioritize seat occupancy over session design, and market the sessions primarily on short‑term discounts and loyalty points rather than on the lasting memories they produce."
      },
      "correct_answer": "B",
      "generation_notes": "Constructed a concrete firm-level scenario requiring students to identify which combination of orchestration, memory-as-product, transformation pricing, and psychological measurement aligns with the experience economy thesis.",
      "concepts_tested": [
        "Experience as product and charging for transformation value",
        "Orchestration of memorable events to create customer value",
        "Economic value of experience as a psychological process underpinning customer experience management"
      ],
      "source_article": "Experience economy",
      "x": 1.3029735088348389,
      "y": 0.9850576519966125,
      "level": 2,
      "original_question_hash": "8fb28f2d"
    },
    {
      "question": "A tech company creates an AI-generated persona called \"Nova\": a fully fabricated biography, staged photoshoots, livestreams with scripted interactions, and branded products tied to Nova's supposed lifestyle. Nova has never been a real person, but millions follow and treat Nova as a credible celebrity — journalists quote Nova, fans buy Nova-branded goods to signal status, and political actors court Nova's endorsement. Which scenario best exemplifies Baudrillard's notion of hyperreality — where simulacra replace any original referent and media compress representation and reality so that the medium reshapes or devours the content?",
      "options": {
        "A": "A luxury clothing brand runs cinematic advertisements that present wearing its jacket as the marker of an \"authentic\" self; consumers purchase the jacket to perform that identity, but the product itself is a real, manufactured garment.",
        "B": "The AI persona Nova, which has no human origin, becomes socially consequential: its fabricated image and narrative are accepted as real, drive consumer behavior, and are circulated and amplified by media until Nova's simulations function as a social reality.",
        "C": "A news platform reforms its interface so stories are delivered as 15-second clips optimized for shares; audiences learn headlines and emotional summaries rather than in-depth facts, and the platform's format increasingly determines what is considered newsworthy.",
        "D": "A regional museum reconstructs a historical village using accurate artifacts and guided tours to educate visitors about a documented past, explicitly distinguishing reproduction from original objects."
      },
      "correct_answer": "B",
      "generation_notes": "Presented a concrete vignette of an AI-generated influencer to test (1) simulacrum with no original referent, (2) blending of representation and consensus reality, and (3) media's role in reshaping and amplifying simulation per Baudrillard; distractors isolate individual themes but do not combine all three.",
      "concepts_tested": [
        "Simulacra and signs without an original referent",
        "Blending/compression of reality and representation by media and culture",
        "Media's influence on meaning (the medium devouring or reshaping content)"
      ],
      "source_article": "Hyperreality",
      "x": 1.0724867582321167,
      "y": 1.0653376579284668,
      "level": 2,
      "original_question_hash": "dfd46fd7"
    },
    {
      "question": "A regional telephone operator currently transports PSTN voice using copper-based E1 trunks (each E1 carries $2.048\\ \\text{Mb/s}$) with standard PCM voice channels at $64\\ \\text{kb/s}$ per call. The operator plans two upgrades: (1) replace the long‑haul copper links with a single single‑mode optical fibre using WDM that supports 32 wavelengths each at $10\\ \\text{Gb/s}$, and (2) migrate subscriber voice onto VoIP running over an IP backbone shared with data. Which of the following statements best describes the technical and conceptual consequences of these choices?",
      "options": {
        "A": "The fibre with $32\\times10\\ \\text{Gb/s}$ WDM capacity can carry orders of magnitude more simultaneous PCM calls than a set of E1 trunks (one E1 only supports about $2.048/0.064\\approx32$ 64 kb/s channels); WDM is a form of frequency/wavelength multiplexing, and migrating voice to VoIP lets voice and data share the same medium (the Internet/IP layer is largely independent of the underlying physical medium). Digital voice over packets is also more resistant to additive noise on long links though it introduces quantization errors during digitization.",
        "B": "Because E1 trunks are analogue, they inherently provide better long‑distance fidelity for voice than optical fibre; replacing copper with fibre will therefore reduce call quality despite WDM increasing raw capacity, and VoIP cannot match E1 quality because packets cannot carry time‑sensitive voice reliably.",
        "C": "WDM on the optical fibre functions by allocating recurring time slots to each sender (like TDM), so moving to WDM simply replicates the E1/TDM model at a higher speed; therefore there is no practical advantage in converting voice to packets since multiplexing is unchanged.",
        "D": "Migrating to VoIP over the IP backbone removes all noise and fidelity issues because digitization eliminates quantization and analogue noise; consequently, once on IP the choice of physical medium (copper vs fibre) is irrelevant to capacity and quality."
      },
      "correct_answer": "A",
      "generation_notes": "Created a concrete operator upgrade scenario comparing E1 copper trunks to WDM optical fibre and VoIP; options test understanding of multiplexing (WDM vs TDM), digital advantages/limitations (noise resistance vs quantization), and the Internetʼs medium‑agnostic nature.",
      "concepts_tested": [
        "transmission mechanisms and multiplexing (WDM/TDM/FDM)",
        "digital evolution and voice-data convergence (VoIP, digitization, noise vs quantization)"
      ],
      "source_article": "Telecommunications",
      "x": 1.38199782371521,
      "y": 1.0126653909683228,
      "level": 2,
      "original_question_hash": "cbc65983"
    },
    {
      "question": "Four real-world scenarios are described. Using Zeder's and Purugganan's definitions (domestication = a long-term, multi‑generational mutualism in which the domesticator actively manages another species' survival and reproduction to secure predictable resources), which set correctly identifies which scenarios qualify as domestication (D) and which do not (N)? Scenarios: (1) A foraging group collects and stores wild barley but does not select or replant seed. (2) A Neolithic community repeatedly harvests wheat carrying a non‑shattering mutation and preferentially replants those seeds across generations. (3) Leafcutter ants cut leaves, cultivate a fungal cultivar, transmit it to new nests and control its propagation for food across ant generations. (4) A hunter captures and tames a lone wolf as a companion but never controls its breeding or transmits its lineage under human management.",
      "options": {
        "A": "1: N, 2: D, 3: D, 4: N",
        "B": "1: D, 2: D, 3: D, 4: N",
        "C": "1: N, 2: D, 3: N, 4: D",
        "D": "1: N, 2: N, 3: D, 4: N"
      },
      "correct_answer": "A",
      "generation_notes": "Presented four concrete scenarios and applied Zeder/Purugganan criteria (active multi‑generational management of survival and reproduction) to classify domestication vs taming/collection; distractors reflect common misinterpretations (storage = domestication; taming = domestication; cultivated fungi not domestication).",
      "concepts_tested": [
        "Definition of domestication as a multi‑generational mutualistic relationship with active management",
        "Mechanisms of domestication: gradual selection across generations producing heritable behavioral and morphological changes",
        "Distinction between domestication and mere human practices (storage, taming) and relation to agriculture"
      ],
      "source_article": "Domestication",
      "x": 1.7317819595336914,
      "y": 0.891860842704773,
      "level": 2,
      "original_question_hash": "0f228059"
    },
    {
      "question": "At a university orientation, domestic students repeatedly describe international students' communal dining rituals as \"unsanitary\" and insist campus catering should prioritize local dishes. They express pride in their own culinary traditions and refer to visiting students as \"backward,\" despite having little contact with them. Which interpretation best characterizes these reactions in terms of ethnocentrism, its psychological mechanism, and its relation to racism and cultural relativism?",
      "options": {
        "A": "This pattern exemplifies ethnocentrism: strong social identification with the domestic group produces in-group favoritism (positive valuation of local cuisine) and out-group derogation (negative valuation of international dining), driven by in-group/out-group differentiation; it is related to racism and stereotyping but not identical to them, and contrasts with cultural relativism, which would seek to understand the rituals on their own terms.",
        "B": "These reactions are best labeled as racism because the domestic students believe international students are biologically inferior; cultural relativism would therefore support the domestic students' judgments as valid local standards.",
        "C": "The behavior is simply a product of unfamiliarity or limited contact; ethnocentrism does not involve a positive valuation of the in-group and would disappear entirely with more exposure, so the main issue is lack of interaction rather than social identification.",
        "D": "The domestic students are demonstrating cultural relativism by using their own cultural norms to evaluate others and attempting to assimilate international practices into the campus mainstream; this shows tolerance expressed through comparison."
      },
      "correct_answer": "A",
      "generation_notes": "Constructed a concrete campus scenario to test recognition of ethnocentrism as driven by social identification and dual in-group/out-group attitudes, and to contrast it with racism and cultural relativism; distractors reflect common confusions.",
      "concepts_tested": [
        "In-group/out-group differentiation and social identification",
        "Dual attitude: positive valuation of in-group and negative valuation of out-group",
        "Relation to racism/stereotyping/xenophobia and contrast with cultural relativism"
      ],
      "source_article": "Ethnocentrism",
      "x": 1.2151553630828857,
      "y": 0.9885339736938477,
      "level": 2,
      "original_question_hash": "a32290ef"
    },
    {
      "question": "Consider the mid-sized city of Ashford. Between 1970 and 1990 it lost $45\\%$ of its manufacturing employment after a major plant closed. In 1965 an interstate was routed along Ashford's east edge, diverting through-traffic away from the historic commercial corridor, and local banks routinely refused mortgages in three inner neighborhoods until 1977 (redlining). By 2000 vacancy rates in those neighborhoods reached $18\\%$, violent crime doubled, and many young adults relocated to suburbs. Which single package of policy interventions would most directly address the upstream economic drivers, the planning-induced spatial distortions, and the social-capital feedback loops that are sustaining Ashford's urban decay?",
      "options": {
        "A": "Offer large tax abatements to out-of-state developers to build a downtown sports stadium and luxury condominiums while keeping current zoning and vehicle infrastructure unchanged.",
        "B": "Clear derelict blocks and expand arterial road capacity through the inner neighborhoods to attract suburban car-borne customers back into the city.",
        "C": "Implement a combined program of targeted job-retraining and small-business credit in the affected neighborhoods, enforce fair-lending to reverse redlining, restore a transit and pedestrian link across the interstate to the commercial corridor, and fund community centers and housing rehabilitation to rebuild social networks.",
        "D": "Increase police funding for aggressive zero-tolerance enforcement, speed up eviction processes to remove criminal elements, and privatize remaining public housing to encourage market turnover."
      },
      "correct_answer": "C",
      "generation_notes": "Constructed a concrete case linking job loss, freeway routing, and redlining to vacancy, crime, and outmigration; produced four plausible policy mixes testing which addresses economic roots, planning fixes, and social-capital rebuilding. Option C addresses all three.",
      "concepts_tested": [
        "Deindustrialization and economic restructuring as upstream drivers",
        "Urban planning and policy decisions (freeways, redlining, suburbanization) as causal mechanisms",
        "Social capital and community cohesion feedbacks on persistence of urban decay"
      ],
      "source_article": "Urban decay",
      "x": 1.2094228267669678,
      "y": 0.8721502423286438,
      "level": 2,
      "original_question_hash": "232de535"
    },
    {
      "question": "The coastal city of Portsville hosts a busy deep-water port handling bulk commodities, an airport with increasing business travel and express cargo demand, an aging freight rail corridor that runs inland, and congested arterial roads used by commuters and trucks. The municipal government wants to (1) restructure urban land use to reflect the city's role in regional exchange, (2) improve efficiency of production and distribution across modes, and (3) assign transport tasks according to mode-specific characteristics (cost, speed, friction, origin/destination). Which package of interventions is most consistent with transport geography principles to achieve these goals?",
      "options": {
        "A": "Build a new outer ring highway and offer tax incentives for logistics firms to locate anywhere in the suburbs; widen arterial roads in the city and expand urban parking to reduce congestion. Keep port, rail, and airport operations independent.",
        "B": "Prioritize airport expansion for both passenger and cargo, subsidize air freight for perishable and time-sensitive goods, maintain separate port and rail facilities with no rail–ship transfers, and concentrate retail distribution in the central business district.",
        "C": "Develop an intermodal logistics park adjacent to the port with direct rail–ship transfer terminals and highway connectors; revise zoning so warehousing and light manufacturing cluster near that hub; invest in the airport for business travel and express/high-value cargo; and expand urban rapid transit and park-and-ride for commuters while scheduling freight movements off-peak. This leverages ships for low-cost bulk export, rail for inland heavy freight, and road/air for speed and passenger convenience.",
        "D": "Ban heavy trucks from the central city and mandate that all freight use the rail corridor without building new terminals; convert the waterfront to predominantly mixed-use residential and boost ferry passenger services for commuters."
      },
      "correct_answer": "C",
      "generation_notes": "Created a scenario requiring students to evaluate land-use change, intermodal integration, and mode-specific allocation; options present plausible but distinct policy mixes emphasizing different transport-geography principles.",
      "concepts_tested": [
        "Transport systems shape urban form and land-use through exchange and clustering",
        "Intermodality and integration of modes improve production and distribution efficiency",
        "Mode-specific characteristics (cost, speed, friction, origin/destination) determine appropriate roles for air, road, water, and rail"
      ],
      "source_article": "Transport geography",
      "x": 1.397367238998413,
      "y": 0.9480920433998108,
      "level": 2,
      "original_question_hash": "fa9cc7b6"
    }
  ],
  "metadata": {
    "level": 2,
    "target_audience": "undergraduate students",
    "total_original": 1201,
    "total_simplified": 1000,
    "pass_1_success": 636,
    "pass_2_success": 364,
    "excluded": 201,
    "exclusion_reasons": {
      "article_not_found": 0,
      "content_loss_both_passes": 201
    },
    "simplification_date": "2025-11-21T08:18:29.499495",
    "api_model": "gpt-5-mini",
    "pilot_mode": false,
    "pilot_count": null
  }
}