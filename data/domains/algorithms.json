{"domain":{"id":"algorithms","name":"Algorithms","parent_id":"computer-science","level":"sub","region":{"x_min":0.601212,"x_max":0.653115,"y_min":0.621121,"y_max":0.656572},"grid_size":70},"questions":[{"id":"5a250d9ce6f49b0c","question_text":"According to the decision tree model, what is the asymptotic lower bound on the number of comparisons any comparison-based sorting algorithm must make in the worst case?","options":{"A":"$\\Omega(n \\sqrt{n})$ comparisons","B":"$\\Omega(n)$ comparisons","C":"$\\Omega(n \\log n)$ comparisons","D":"$\\Omega(n^2)$ comparisons"},"correct_answer":"C","difficulty":1,"source_article":"Sorting algorithm","domain_ids":["algorithms"],"concepts_tested":["sorting"],"x":0.632086,"y":0.647583},{"id":"9694d1ae3c204353","question_text":"In the worst case, how many elements must a linear search examine to find a target value in an unsorted list of $n$ elements?","options":{"A":"At most $\\sqrt{n}$ elements in the list","B":"At most $n/2$ elements in the list","C":"At most $\\log_2 n$ elements in the list","D":"All $n$ elements in the list"},"correct_answer":"D","difficulty":1,"source_article":"Search algorithm","domain_ids":["algorithms"],"concepts_tested":["searching"],"x":0.629398,"y":0.655505},{"id":"65523ca5068e55c7","question_text":"What is the maximum number of comparisons binary search requires to find or rule out a target in a sorted array of $n$ elements?","options":{"A":"$2 \\log_2 n$ comparisons","B":"$\\lfloor \\log_2 n \\rfloor + 1$ comparisons","C":"$n / 2$ comparisons","D":"$\\lceil \\log_2 n \\rceil - 1$ comparisons"},"correct_answer":"B","difficulty":1,"source_article":"Binary search algorithm","domain_ids":["algorithms"],"concepts_tested":["binary search"],"x":0.633744,"y":0.647553},{"id":"b6b816d8bf0f05ae","question_text":"In recursion, what compiler optimization allows a function whose very last action is a recursive call to reuse the current stack frame instead of allocating a new one?","options":{"A":"Inline expansion","B":"Tail-call elimination","C":"Loop unrolling","D":"Constant folding"},"correct_answer":"B","difficulty":1,"source_article":"Recursion (computer science)","domain_ids":["algorithms"],"concepts_tested":["recursion"],"x":0.635995,"y":0.641373},{"id":"bf5604bebc02332d","question_text":"In graph theory, the degree sum formula (handshaking lemma) states that the sum of the degrees of all vertices in an undirected graph equals what value?","options":{"A":"Three times the number of vertices","B":"Twice the number of edges","C":"The number of edges plus the number of vertices","D":"The square of the number of edges"},"correct_answer":"B","difficulty":1,"source_article":"Graph (discrete mathematics)","domain_ids":["algorithms"],"concepts_tested":["graph"],"x":0.634189,"y":0.655873},{"id":"bde746ba9bab88cf","question_text":"A tree with $n$ vertices is a connected acyclic graph. How many edges does such a tree always have?","options":{"A":"Exactly $2n - 2$ edges","B":"Exactly $n - 1$ edges","C":"Exactly $n + 1$ edges","D":"Exactly $n$ edges"},"correct_answer":"B","difficulty":1,"source_article":"Tree (data structure)","domain_ids":["algorithms"],"concepts_tested":["tree"],"x":0.634187,"y":0.651224},{"id":"d77798ce29cddf92","question_text":"Edsger Dijkstra's shunting-yard algorithm converts infix expressions to postfix notation. Which data structure does the algorithm rely on to hold operators during parsing?","options":{"A":"A priority queue","B":"A queue","C":"A stack","D":"A hash table"},"correct_answer":"C","difficulty":1,"source_article":"Stack (abstract data type)","domain_ids":["algorithms"],"concepts_tested":["stack"],"x":0.653463,"y":0.658436},{"id":"71a103eed120792b","question_text":"Breadth-first search (BFS) explores a graph level by level, visiting all neighbors of a node before moving deeper. Which data structure does BFS use to track the next node to visit?","options":{"A":"A binary search tree","B":"A queue","C":"A hash set","D":"A stack"},"correct_answer":"B","difficulty":1,"source_article":"Queue (abstract data type)","domain_ids":["algorithms"],"concepts_tested":["queue"],"x":0.639897,"y":0.640235},{"id":"0563485eb86143da","question_text":"Linked lists were originally developed in 1955-1956 as the primary data structure for the Information Processing Language (IPL). Who created them?","options":{"A":"Allen Newell, Cliff Shaw, and Herbert A. Simon","B":"Donald Knuth, Edsger Dijkstra, and Tony Hoare","C":"John von Neumann, Alan Turing, and Claude Shannon","D":"John McCarthy, Marvin Minsky, and John Backus"},"correct_answer":"A","difficulty":1,"source_article":"Linked list","domain_ids":["algorithms"],"concepts_tested":["linked list"],"x":0.63006,"y":0.645512},{"id":"8a2fc4a857c4f87f","question_text":"When multiple keys map to the same index in a hash table, the collision can be resolved by storing all colliding entries in a linked list at that index. What is this technique called?","options":{"A":"Double hashing","B":"Cuckoo hashing","C":"Separate chaining","D":"Linear probing"},"correct_answer":"C","difficulty":1,"source_article":"Hash table","domain_ids":["algorithms"],"concepts_tested":["hash table"],"x":0.63006,"y":0.645512},{"id":"3bf2fdfb39285c10","question_text":"Big O notation was first introduced in 1894 by the German mathematician Paul Bachmann. The letter O stands for the German word 'Ordnung.' What does 'Ordnung' mean in this context?","options":{"A":"The optimization of growth","B":"The origin of computation","C":"The order of approximation","D":"The output of the function"},"correct_answer":"C","difficulty":1,"source_article":"Big O notation","domain_ids":["algorithms"],"concepts_tested":["Big O notation"],"x":0.641371,"y":0.648842},{"id":"0f2eb656b735880b","question_text":"Richard Bellman coined the term 'dynamic programming' in the 1950s while working at which United States research organization?","options":{"A":"Los Alamos National Laboratory","B":"MIT Lincoln Laboratory","C":"Bell Laboratories","D":"The RAND Corporation"},"correct_answer":"D","difficulty":1,"source_article":"Dynamic programming","domain_ids":["algorithms"],"concepts_tested":["dynamic programming"],"x":0.764368,"y":0.781291},{"id":"86ba147a115b686c","question_text":"A greedy algorithm builds a solution by making the locally optimal choice at each step without reconsidering past decisions. Which two properties must a problem exhibit for a greedy approach to yield an optimal solution?","options":{"A":"Greedy choice property and overlapping subproblems","B":"Overlapping subproblems and optimal substructure","C":"Divide-and-conquer property and memoization","D":"Greedy choice property and optimal substructure"},"correct_answer":"D","difficulty":1,"source_article":"Greedy algorithm","domain_ids":["algorithms"],"concepts_tested":["greedy algorithm"],"x":0.642923,"y":0.637799},{"id":"a2bf7d05e62ec6e3","question_text":"Tony Hoare developed quicksort in 1959 while working on machine translation at Moscow State University. What is quicksort's worst-case time complexity?","options":{"A":"$O(n^2 \\log n)$, occurring when duplicate elements dominate the input array","B":"$O(n)$, occurring when the input array is already sorted in ascending order","C":"$O(n^2)$, occurring when the pivot selection consistently produces the most unbalanced partitions","D":"$O(n \\log n)$, occurring when the pivot always lands near the median element"},"correct_answer":"C","difficulty":2,"source_article":"Quicksort","domain_ids":["algorithms"],"concepts_tested":["quicksort"],"x":0.626115,"y":0.651501},{"id":"75e0b6ec5a95f564","question_text":"Merge sort, invented by John von Neumann in 1945, guarantees $O(n \\log n)$ time in all cases. What property makes it preferable for sorting linked lists and records with satellite data?","options":{"A":"It is a stable sort, preserving the relative order of equal elements","B":"It is an in-place sort, requiring only $O(1)$ additional memory","C":"It is a comparison-free sort, using hashing instead of element comparisons","D":"It is an adaptive sort, running in $O(n)$ time on nearly sorted input"},"correct_answer":"A","difficulty":2,"source_article":"Merge sort","domain_ids":["algorithms"],"concepts_tested":["merge sort"],"x":0.624854,"y":0.654138},{"id":"fb86db0cb7cf19c6","question_text":"Dijkstra's algorithm, conceived in 1956, solves the single-source shortest path problem for graphs with non-negative edge weights. Why does it fail on graphs containing negative edge weights?","options":{"A":"Its priority queue cannot store negative values, so negative-weight edges cause a runtime overflow error","B":"It only counts the number of edges on each path, so negative weights have no meaningful interpretation","C":"It requires all edges to form a directed acyclic graph, and negative weights create forbidden cycles","D":"It greedily finalizes vertex distances, so a later negative edge can invalidate an already-committed shortest path"},"correct_answer":"D","difficulty":2,"source_article":"Dijkstra's algorithm","domain_ids":["algorithms"],"concepts_tested":["Dijkstra's algorithm"],"x":0.632086,"y":0.647583},{"id":"dba2f500312c9275","question_text":"Breadth-first search (BFS) explores a graph level by level, visiting all neighbors at distance $k$ before those at distance $k+1$. What useful property does this guarantee for unweighted graphs?","options":{"A":"It identifies the minimum spanning tree of the graph","B":"It finds the shortest path from the source to every reachable vertex","C":"It produces a valid topological ordering of the vertices","D":"It detects all strongly connected components in a single pass"},"correct_answer":"B","difficulty":2,"source_article":"Breadth-first search","domain_ids":["algorithms"],"concepts_tested":["breadth-first search"],"x":0.643137,"y":0.639425},{"id":"ad3cb6122ee65ee9","question_text":"Depth-first search (DFS) explores a graph by going as deep as possible along each branch before backtracking. Which data structure does DFS use to track the exploration frontier?","options":{"A":"A priority queue, selecting the vertex with the smallest edge weight","B":"A stack, either explicitly or implicitly via the call stack through recursion","C":"A hash table, mapping each vertex to its current traversal depth","D":"A queue, processing vertices in the order they were first discovered"},"correct_answer":"B","difficulty":2,"source_article":"Depth-first search","domain_ids":["algorithms"],"concepts_tested":["depth-first search"],"x":0.638408,"y":0.640788},{"id":"88a175225f392ed7","question_text":"A binary search tree (BST) maintains the invariant that every node's key is greater than all keys in its left subtree and less than all keys in its right subtree. When does a BST's search operation degrade to $O(n)$?","options":{"A":"When the tree contains exactly $2^k - 1$ nodes for some integer $k$","B":"When all inserted keys are distinct and randomly distributed","C":"When the tree is perfectly balanced with equal-height subtrees at every node","D":"When the tree becomes skewed (degenerate), effectively forming a linked list"},"correct_answer":"D","difficulty":2,"source_article":"Binary search tree","domain_ids":["algorithms"],"concepts_tested":["binary search tree"],"x":0.629398,"y":0.655505},{"id":"6d3b716c6fcdd3e0","question_text":"A binary heap is a complete binary tree satisfying the heap property and is commonly implemented using an array. What is the time complexity of accessing the minimum element in a min-heap?","options":{"A":"$O(1)$, because the minimum is always stored at the root of the heap","B":"$O(\\log n)$, because a sift-down operation is required to locate it","C":"$O(n)$, because a linear scan of the leaf nodes is needed","D":"$O(n \\log n)$, because the heap must be fully sorted first"},"correct_answer":"A","difficulty":2,"source_article":"Heap (data structure)","domain_ids":["algorithms"],"concepts_tested":["heap"],"x":0.629398,"y":0.655505},{"id":"708185a710b7fdc3","question_text":"Why is a binary heap always stored as an array rather than using explicit pointers?","options":{"A":"Because a heap is a complete binary tree, parent-child relationships can be computed from array indices without pointers.","B":"Because arrays allow the heap to store duplicate keys, which linked structures with pointers cannot support.","C":"Because a heap requires random access to any node in constant time, which only hash-based arrays can provide.","D":"Because pointer-based trees cannot maintain the heap ordering property during insertion and deletion operations."},"correct_answer":"A","difficulty":2,"source_article":"Heap (data structure)","domain_ids":["algorithms"],"concepts_tested":["heap"],"x":0.629398,"y":0.655505},{"id":"db9252b0f274988b","question_text":"Why is heapsort not a stable sorting algorithm?","options":{"A":"The heap operations can change the relative order of elements with equal keys during sift-down and extraction.","B":"The algorithm discards the original index information when elements are first inserted into the auxiliary heap.","C":"The merge step combines two sorted halves without tracking the original positions of equal-key elements.","D":"The pivot selection during partitioning may swap equal-key elements across the partition boundary unpredictably."},"correct_answer":"A","difficulty":2,"source_article":"Heapsort","domain_ids":["algorithms"],"concepts_tested":["heap sort"],"x":0.629663,"y":0.654302},{"id":"e33a8dfff9291c0b","question_text":"What is the worst-case input for insertion sort, and how many comparisons does it require?","options":{"A":"An already sorted array, requiring $2n - 1$ comparisons since each element is compared with both its neighbors.","B":"A reverse-sorted array, requiring $n(n-1)/2$ comparisons since every element must shift past all previously sorted elements.","C":"A randomly shuffled array, requiring exactly $n^2$ comparisons since each element is compared with every other element.","D":"An array of identical elements, requiring $n \\log n$ comparisons since the algorithm must verify equality at every level."},"correct_answer":"B","difficulty":2,"source_article":"Insertion sort","domain_ids":["algorithms"],"concepts_tested":["insertion sort"],"x":0.631104,"y":0.653443},{"id":"e7f0d0706c161a52","question_text":"How does the Bellman-Ford algorithm detect negative-weight cycles in a graph?","options":{"A":"During each relaxation pass, it counts the number of updated vertices; if this count exceeds $|V|$, a negative cycle exists.","B":"Before beginning relaxation, it searches for back edges using depth-first search and reports any that have negative weight.","C":"After $|V|-1$ relaxation passes over all edges, it performs one additional pass; if any distance decreases, a negative cycle exists.","D":"After building the shortest-path tree, it checks whether any tree edge has a negative weight and reports a cycle if so."},"correct_answer":"C","difficulty":2,"source_article":"Bellman\u2013Ford algorithm","domain_ids":["algorithms"],"concepts_tested":["Bellman-Ford algorithm"],"x":0.630907,"y":0.654245},{"id":"61068a8d84a65710","question_text":"What data structure does Kruskal's algorithm use to efficiently determine whether adding an edge would create a cycle?","options":{"A":"A min-priority queue, which stores edges sorted by weight and removes those forming cycles in logarithmic time.","B":"An adjacency matrix, which checks for existing paths between two vertices in constant time per lookup.","C":"A union-find (disjoint-set) data structure, which tracks connected components and detects cycles in nearly constant time.","D":"A depth-first search stack, which traces paths between endpoints and detects back edges in linear time per query."},"correct_answer":"C","difficulty":2,"source_article":"Kruskal's algorithm","domain_ids":["algorithms"],"concepts_tested":["Kruskal's algorithm"],"x":0.633744,"y":0.647553},{"id":"d759817f4d7b9140","question_text":"How does memoization differ from the bottom-up tabulation approach to dynamic programming?","options":{"A":"Memoization stores only the final optimal solution in memory, while tabulation stores every intermediate variable used during computation.","B":"Memoization solves subproblems top-down via recursion and caches results lazily, while tabulation fills a table iteratively from base cases up.","C":"Memoization applies only to problems with non-overlapping subproblems, while tabulation requires overlapping subproblems to be efficient.","D":"Memoization solves subproblems in parallel across multiple threads, while tabulation processes them sequentially on a single thread."},"correct_answer":"B","difficulty":2,"source_article":"Memoization","domain_ids":["algorithms"],"concepts_tested":["memoization"],"x":0.64607,"y":0.622966},{"id":"6681af7cf6297bf1","question_text":"Why does the amortized cost of appending to a dynamic array remain $O(1)$ despite occasional $O(n)$ resizing?","options":{"A":"The array uses lazy copying that defers element transfers to idle CPU cycles, making each append appear to take constant time.","B":"The operating system provides free memory pages during resizing, so the copy operation incurs no actual computational cost to the running program.","C":"The array increases by a fixed constant number of slots on each resize, spreading the cost equally across all future appends.","D":"The array doubles in size on each resize, so the total cost of $n$ appends is at most $O(n)$, giving $O(1)$ per operation on average."},"correct_answer":"D","difficulty":3,"source_article":"Amortized analysis","domain_ids":["algorithms"],"concepts_tested":["amortized analysis"],"x":0.629398,"y":0.655505},{"id":"6a95445727a7cc19","question_text":"Who originally invented the data structure later named the red-black tree, and who gave it its current name?","options":{"A":"Adelson-Velsky and Landis invented it in 1962 as the height-balanced tree; Cormen and Rivest renamed it the red-black tree in 1990.","B":"Rudolf Bayer invented it in 1972 as the symmetric binary B-tree; Guibas and Sedgewick renamed it the red-black tree in 1978.","C":"John Hopcroft invented it in 1970 as the chromatic search tree; Aho and Ullman renamed it the red-black tree in 1974.","D":"Donald Knuth invented it in 1968 as the colored binary tree; Tarjan and Sleator renamed it the red-black tree in 1985."},"correct_answer":"B","difficulty":3,"source_article":"Red\u2013black tree","domain_ids":["algorithms"],"concepts_tested":["red-black tree"],"x":0.626766,"y":0.648338},{"id":"658b3cf6d58f561d","question_text":"Why is the AVL tree considered historically significant in computer science?","options":{"A":"Invented by Dijkstra and Floyd in 1960, it was the first data structure to guarantee constant-time insertion and deletion.","B":"Invented by Knuth and Morris in 1965, it was the first tree structure to support logarithmic-time string matching.","C":"Invented by Bayer and McCreight in 1970, it was the first tree optimized for disk-based storage and database indexing.","D":"Invented by Adelson-Velsky and Landis in 1962, it was the first self-balancing binary search tree ever devised."},"correct_answer":"D","difficulty":3,"source_article":"AVL tree","domain_ids":["algorithms"],"concepts_tested":["AVL tree"],"x":0.624854,"y":0.654138},{"id":"a155b1db8364668b","question_text":"What algorithm does A* search reduce to when its heuristic function $h(n)$ is set to zero for all nodes?","options":{"A":"Dijkstra's algorithm, because with $h(n) = 0$ the evaluation function $f(n)$ equals $g(n)$, the actual cost from the start.","B":"Breadth-first search, because with $h(n) = 0$ every edge is treated as having unit weight regardless of actual cost.","C":"Depth-first search, because with $h(n) = 0$ the algorithm always expands the most recently discovered node first.","D":"Bellman-Ford algorithm, because with $h(n) = 0$ the algorithm relaxes all edges repeatedly instead of using a priority queue."},"correct_answer":"A","difficulty":3,"source_article":"A* search algorithm","domain_ids":["algorithms"],"concepts_tested":["A* search algorithm"],"x":0.638167,"y":0.642358},{"id":"ca1fb90b386814d9","question_text":"How does Kahn's algorithm for topological sorting detect whether a directed graph contains a cycle?","options":{"A":"If the priority queue overflows during vertex processing, the algorithm concludes that circular dependencies among nodes forced repeated vertex insertions.","B":"If the sum of all edge weights becomes negative after full processing, the algorithm reports that a negative-weight cycle exists in the graph.","C":"If the final ordering contains fewer vertices than the graph has, some vertices were never enqueued because a cycle kept their in-degrees above zero.","D":"If any vertex is visited more than once during breadth-first traversal, the algorithm marks it as part of a strongly connected component."},"correct_answer":"C","difficulty":3,"source_article":"Topological sorting","domain_ids":["algorithms"],"concepts_tested":["topological sort"],"x":0.631104,"y":0.653443},{"id":"4db4ae4dc45ff980","question_text":"What amortized time complexity do union-find operations achieve when both path compression and union by rank are applied, and why is it considered practically constant?","options":{"A":"Amortized $O(\\alpha(n))$ per operation, where $\\alpha$ is the inverse Ackermann function, which is at most 4 for any physically representable value of $n$.","B":"Amortized $O(\\log^* n)$ per operation, where the iterated logarithm equals the number of times $n$ must be halved to reach 1.","C":"Amortized $O(1/n)$ per operation, because each successive operation on the same set becomes cheaper as the tree flattens toward a single root node.","D":"Amortized $O(\\log \\log n)$ per operation, where the double logarithm grows slowly enough that it stays below 5 for all inputs up to $2^{64}$."},"correct_answer":"A","difficulty":3,"source_article":"Disjoint-set data structure","domain_ids":["algorithms"],"concepts_tested":["union-find"],"x":0.629398,"y":0.655505},{"id":"e1d9223cedc54d45","question_text":"Prim's algorithm is often attributed to Robert C. Prim (1957), but who originally discovered the algorithm and when?","options":{"A":"Czech mathematician Vojt\u011bch Jarn\u00edk originally discovered the algorithm in 1930; Edsger Dijkstra also independently rediscovered it in 1959.","B":"Hungarian mathematician John von Neumann originally discovered the algorithm in 1928; Alan Turing also independently rediscovered it in 1948.","C":"German mathematician David Hilbert originally discovered the algorithm in 1925; Claude Shannon also independently rediscovered it in 1950.","D":"Polish mathematician Kazimierz Kuratowski originally discovered the algorithm in 1935; Richard Bellman also independently rediscovered it in 1956."},"correct_answer":"A","difficulty":3,"source_article":"Prim's algorithm","domain_ids":["algorithms"],"concepts_tested":["Prim's algorithm"],"x":0.626115,"y":0.651501},{"id":"0eed1cc86d559056","question_text":"What classic Unix utility relies on solving the longest common subsequence problem, and what does the LCS correspond to in its output?","options":{"A":"The sort utility uses LCS to merge files; the longest common subsequence corresponds to the already-ordered runs that do not need re-sorting.","B":"The diff utility uses LCS to compare files; the longest common subsequence corresponds to the unchanged lines between two file versions.","C":"The awk utility uses LCS to parse files; the longest common subsequence corresponds to the shared field delimiters across consecutive records.","D":"The grep utility uses LCS to search files; the longest common subsequence corresponds to the matching pattern occurrences across multiple lines."},"correct_answer":"B","difficulty":3,"source_article":"Longest common subsequence problem","domain_ids":["algorithms"],"concepts_tested":["longest common subsequence"],"x":0.757962,"y":0.767929},{"id":"376ff628ec55c9aa","question_text":"Why is the 0/1 knapsack problem classified as weakly NP-hard rather than strongly NP-hard?","options":{"A":"It has a subexponential $O(2^{\\sqrt{n}})$ branch-and-bound solution, faster than the $O(2^n)$ threshold defining strong NP-hardness.","B":"It has a pseudo-polynomial $O(nW)$ dynamic programming solution, polynomial in the capacity value $W$ but exponential in its bit length.","C":"It has a polynomial $O(n \\log n)$ greedy solution that selects items by value-to-weight ratio and always finds the optimal answer.","D":"It reduces to linear programming in polynomial time, and interior-point methods solve the resulting relaxation to find exact integer solutions."},"correct_answer":"B","difficulty":3,"source_article":"Knapsack problem","domain_ids":["algorithms"],"concepts_tested":["knapsack problem"],"x":0.631104,"y":0.653443},{"id":"4d22ce7d18b8a7de","question_text":"What happens when a new key is inserted into a full node of a B-tree of order $m$?","options":{"A":"The oldest key in the node is evicted to a separate overflow page on disk, making room for the new key without splitting.","B":"The node splits into two nodes, each with roughly half the keys, and the median key is promoted into the parent node.","C":"The node doubles its maximum capacity to $2m$ keys and rebalances by redistributing entries across all sibling nodes at the same level.","D":"The tree increases its order from $m$ to $m+1$ so every node gains one additional key slot without requiring any structural changes."},"correct_answer":"B","difficulty":3,"source_article":"B-tree","domain_ids":["algorithms"],"concepts_tested":["B-tree"],"x":0.629398,"y":0.655505},{"id":"9a3c13a2e3664acc","question_text":"What distinguishes a Las Vegas randomized algorithm from a Monte Carlo randomized algorithm?","options":{"A":"Las Vegas uses uniformly distributed random numbers for decisions; Monte Carlo uses normally distributed random numbers for statistical sampling.","B":"Las Vegas always gives the correct result but has random runtime; Monte Carlo has bounded runtime but may produce an incorrect answer.","C":"Las Vegas applies only to discrete optimization over finite sets; Monte Carlo applies only to continuous numerical integration over real domains.","D":"Las Vegas guarantees polynomial worst-case running time with certainty; Monte Carlo guarantees exponential worst-case time but achieves higher accuracy."},"correct_answer":"B","difficulty":3,"source_article":"Randomized algorithm","domain_ids":["algorithms"],"concepts_tested":["randomized algorithm"],"x":0.690774,"y":0.710093},{"id":"1def55879f23714a","question_text":"How does the KMP algorithm's failure function (partial match table) enable linear-time string matching?","options":{"A":"It precomputes the longest prefix that is also a suffix at each pattern position, so mismatches shift the pattern forward without re-examining text characters.","B":"It precomputes each character's frequency within the pattern string, allowing mismatches to skip ahead by the rarest character's next occurrence distance.","C":"It precomputes hash values for every possible pattern substring, allowing mismatches to jump directly to the next position with a matching hash.","D":"It precomputes a suffix tree over the entire text during initialization, allowing mismatches to look up the next occurrence in constant time."},"correct_answer":"A","difficulty":3,"source_article":"Knuth\u2013Morris\u2013Pratt algorithm","domain_ids":["algorithms"],"concepts_tested":["KMP algorithm"],"x":0.626659,"y":0.651656},{"id":"3da6acd960d577b0","question_text":"Why does the Fibonacci heap achieve $O(1)$ amortized time for decrease-key, and what structural mechanism enforces the degree bound that makes this possible?","options":{"A":"Decrease-key bubbles the node up through its ancestors in $O(1)$; lazy deletion caps tree height at $O(\\sqrt{n})$, keeping extract-min logarithmic.","B":"Decrease-key cuts the node to a new root in $O(1)$; cascading cuts limit each non-root to losing one child, keeping degrees $O(\\log n)$.","C":"Decrease-key recolors the node and its ancestors in $O(1)$; red-black invariants bound all root-to-leaf paths to at most $2 \\log n$ nodes.","D":"Decrease-key rehashes the node into a new heap bucket in $O(1)$; Fibonacci hashing bounds collision chain length to expected $O(\\log n)$."},"correct_answer":"B","difficulty":4,"source_article":"Fibonacci heap","domain_ids":["algorithms"],"concepts_tested":["Fibonacci heap"],"x":0.629398,"y":0.655505},{"id":"3341a5406ce0ed5c","question_text":"Why were suffix arrays introduced as an alternative to suffix trees, and what time complexity can be achieved for their construction?","options":{"A":"Ukkonen and Weiner introduced suffix arrays in 1995 because they allow online construction unlike suffix trees. Optimal $O(n \\sqrt{n})$ construction algorithms exist.","B":"Knuth and Pratt introduced suffix arrays in 1977 because they support constant-time pattern matching unlike suffix trees. Optimal $O(n \\log n)$ construction algorithms exist.","C":"Aho and Corasick introduced suffix arrays in 1985 because they handle multi-pattern search unlike suffix trees. Optimal $O(n \\log \\log n)$ construction algorithms exist.","D":"Manber and Myers introduced suffix arrays in 1990 because they use three to five times less space than suffix trees. Optimal $O(n)$ construction algorithms exist."},"correct_answer":"D","difficulty":4,"source_article":"Suffix array","domain_ids":["algorithms"],"concepts_tested":["suffix array"],"x":0.624854,"y":0.654138},{"id":"5ec2b397032fe350","question_text":"The max-flow min-cut theorem is a fundamental result in network flow theory. What does it state about the relationship between maximum flow and minimum cut in a flow network?","options":{"A":"The maximum flow from source to sink equals the weight of the shortest path from source to sink.","B":"The maximum flow from source to sink equals the sum of all edge capacities in the network.","C":"The maximum flow from source to sink equals the number of edge-disjoint paths from source to sink.","D":"The maximum flow from source to sink equals the capacity of the minimum cut separating source and sink."},"correct_answer":"D","difficulty":4,"source_article":"Maximum flow problem","domain_ids":["algorithms"],"concepts_tested":["network flow","max-flow min-cut theorem","minimum cut"],"x":0.646336,"y":0.640735},{"id":"0d67bb92d9eb6af9","question_text":"The Ford-Fulkerson method computes maximum flow by repeatedly finding augmenting paths in the residual graph. What pathological behavior can occur with irrational edge capacities, and which refinement resolves it?","options":{"A":"It may cycle through the same augmenting path indefinitely; the Dinic refinement uses BFS to guarantee $O(V^2 E)$ termination.","B":"It may produce negative flow values on some edges; the Edmonds-Karp refinement uses DFS to guarantee $O(VE^2)$ termination.","C":"It may fail to terminate or converge to the maximum flow; the Edmonds-Karp refinement uses BFS to guarantee $O(VE^2)$ termination.","D":"It may terminate prematurely at a local optimum; the Edmonds-Karp refinement uses BFS to guarantee $O(V^2 E^2)$ termination."},"correct_answer":"C","difficulty":4,"source_article":"Ford\u2013Fulkerson algorithm","domain_ids":["algorithms"],"concepts_tested":["Ford-Fulkerson algorithm","augmenting path","Edmonds-Karp algorithm","residual graph"],"x":0.633744,"y":0.647553},{"id":"550ad4cf5a264c05","question_text":"The van Emde Boas tree supports predecessor, successor, insert, and delete operations in $O(\\log \\log U)$ time, where $U$ is the universe size. What recursive structural technique achieves this bound?","options":{"A":"It partitions the universe into $\\sqrt{U}$ clusters of size $\\sqrt{U}$, recursing on both the cluster and the summary structure simultaneously at each level.","B":"It partitions the universe into $U / 2$ clusters of size $2$, recursing on each half independently, reducing $U$ by half at each level.","C":"It partitions the universe into $\\log U$ clusters of size $U / \\log U$, recursing on the largest non-empty cluster at each level.","D":"It partitions the universe into $\\sqrt{U}$ clusters of size $\\sqrt{U}$, recursing on a cluster or a summary structure, reducing $U$ to $\\sqrt{U}$ each level."},"correct_answer":"D","difficulty":4,"source_article":"Van Emde Boas tree","domain_ids":["algorithms"],"concepts_tested":["van Emde Boas tree","recursive universe splitting","predecessor query"],"x":0.629398,"y":0.655505},{"id":"9715172a191784d1","question_text":"A suffix automaton is the smallest partial deterministic finite automaton that accepts all suffixes of a given string. For a string of length $n \\geq 2$, what are the tight upper bounds on the number of states and transitions?","options":{"A":"At most $n + 1$ states and at most $2n$ transitions, independent of alphabet size.","B":"At most $n^2$ states and at most $3n - 4$ transitions, independent of alphabet size.","C":"At most $2n - 1$ states and at most $2n - 2$ transitions, scaling linearly with alphabet size.","D":"At most $2n - 1$ states and at most $3n - 4$ transitions, independent of alphabet size."},"correct_answer":"D","difficulty":4,"source_article":"Suffix automaton","domain_ids":["algorithms"],"concepts_tested":["suffix automaton","DAWG","state complexity","string algorithms"],"x":0.63006,"y":0.645512},{"id":"74d583848a6b7538","question_text":"In linear programming, the feasible region defined by a set of linear inequality constraints has a specific geometric structure. What is it, and why does the simplex method exploit this?","options":{"A":"The feasible region is a convex ellipsoid, and the simplex method exploits this by cutting the ellipsoid in half at each step to converge on a solution.","B":"The feasible region is a convex polytope, and the simplex method exploits this by moving along edges between vertices, where an optimal solution must exist.","C":"The feasible region is a convex cone, and the simplex method exploits this by projecting onto the cone boundary where optimal solutions must lie.","D":"The feasible region is a convex polytope, and the simplex method exploits this by searching interior points that minimize the gradient of the objective function."},"correct_answer":"B","difficulty":4,"source_article":"Linear programming","domain_ids":["algorithms"],"concepts_tested":["linear programming","convex polytope","simplex method","feasible region"],"x":0.645663,"y":0.654605},{"id":"23a9fa475d1a1afe","question_text":"For the NP-hard minimum vertex cover problem, a classical approximation algorithm repeatedly selects an arbitrary uncovered edge and adds both endpoints to the cover. What approximation ratio does this achieve?","options":{"A":"A 1.5-approximation: it produces a vertex cover at most 1.5 times the size of the optimal minimum vertex cover.","B":"A 3-approximation: it produces a vertex cover at most three times the size of the optimal minimum vertex cover.","C":"A 2-approximation: it produces a vertex cover at most twice the size of the optimal minimum vertex cover.","D":"An $O(\\log n)$-approximation: it produces a vertex cover at most $O(\\log n)$ times the size of the optimal minimum vertex cover."},"correct_answer":"C","difficulty":4,"source_article":"Approximation algorithm","domain_ids":["algorithms"],"concepts_tested":["approximation algorithm","approximation ratio","vertex cover"],"x":0.635284,"y":0.648454},{"id":"2569e51960f0c47e","question_text":"The Cook-Levin theorem, proved independently by Stephen Cook (1971) and Leonid Levin, established the concept of NP-completeness. What does the theorem prove, and what consequence does it have for $P$ versus $NP$?","options":{"A":"It proves 3-SAT is NP-complete, implying that a polynomial-time SAT algorithm would yield $P = NP$.","B":"It proves the Halting Problem is NP-complete, implying that a polynomial-time algorithm for it would yield $P = NP$.","C":"It proves Boolean satisfiability (SAT) is NP-hard but not in NP, implying that no polynomial-time algorithm can verify SAT solutions.","D":"It proves Boolean satisfiability (SAT) is NP-complete, implying that a polynomial-time SAT algorithm would yield $P = NP$."},"correct_answer":"D","difficulty":4,"source_article":"NP-completeness","domain_ids":["algorithms"],"concepts_tested":["NP-completeness","Cook-Levin theorem","Boolean satisfiability","P versus NP"],"x":0.638946,"y":0.646641},{"id":"6131bbe28a873788","question_text":"A skip list, invented by William Pugh in 1989, is a probabilistic data structure that provides expected $O(\\log n)$ search, insertion, and deletion. How are the levels of each node determined?","options":{"A":"Each node's level is assigned by a round-robin scheme cycling through all possible levels sequentially during insertion.","B":"Each node's level is chosen randomly via a geometric distribution, typically by repeated coin flips with probability $p = 1/2$ for promotion.","C":"Each node's level is chosen deterministically from its key value modulo the maximum level, ensuring uniform distribution across levels.","D":"Each node's level is chosen randomly from a uniform distribution over all possible levels, giving each level equal probability."},"correct_answer":"B","difficulty":4,"source_article":"Skip list","domain_ids":["algorithms"],"concepts_tested":["skip list","probabilistic data structure","randomized level assignment"],"x":0.624854,"y":0.654138},{"id":"f55aa4b78628a93b","question_text":"The Cooley-Tukey fast Fourier transform (FFT) algorithm computes the discrete Fourier transform in $O(n \\log n)$ time instead of the naive $O(n^2)$. What is the core divide-and-conquer strategy of the radix-2 variant?","options":{"A":"It splits the input into the first half and second half, recursively computes their DFTs, and combines results using twiddle factors (roots of unity).","B":"It splits the input into even-indexed and odd-indexed elements, recursively computes their inverse DFTs, and combines results using convolution.","C":"It splits the input into real and imaginary components, recursively computes their DFTs, and combines results using polynomial interpolation.","D":"It splits the input into even-indexed and odd-indexed elements, recursively computes their DFTs, and combines results using twiddle factors (roots of unity)."},"correct_answer":"D","difficulty":4,"source_article":"Fast Fourier transform","domain_ids":["algorithms"],"concepts_tested":["fast Fourier transform","Cooley-Tukey algorithm","divide and conquer","roots of unity"],"x":0.655705,"y":0.685778},{"id":"1ce8a3beae81f38a","question_text":"The Aho-Corasick algorithm (1975) performs multi-pattern string matching in $O(n + m + z)$ time, where $n$ is text length, $m$ is total pattern length, and $z$ is the number of matches. What additional structure beyond the trie enables this linear-time scanning?","options":{"A":"Output links that connect each node directly to the root, restarting the search from the beginning on every mismatch.","B":"Failure links (analogous to KMP failure functions) that redirect the automaton on mismatches without backtracking in the text.","C":"Suffix links that connect each node to the longest proper suffix node, requiring occasional backtracking in the text.","D":"Hash-based shortcut pointers that map character pairs to trie nodes, enabling constant-time two-character lookahead."},"correct_answer":"B","difficulty":4,"source_article":"Aho\u2013Corasick algorithm","domain_ids":["algorithms"],"concepts_tested":["Aho-Corasick algorithm","failure links","trie","multi-pattern matching"],"x":0.626339,"y":0.65276}],"labels":[],"articles":[]}