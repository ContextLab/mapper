In the last few videos, we saw that if we had n points, each of them have x and y coordinates. So let me draw n of those points. So let's call this point 1. It has a coordinates x1, y1. You have the second point over here that has a coordinates x2, y2. And then we keep putting points up here, and eventually we get to the nth point over here, the nth point that has a coordinates xn, yn. What we saw is that there is a line that we can find. We can find a line that minimizes the squared distance. So this line right here, I'll call it y is equal to mx plus b, that there is some line that minimizes the squared distance to the points. And let me just review what those squared distances are. So sometimes it's called the squared error. So this is the error between the line and point 1. So I'll call that error 1. This is the error between the line and point 2. We'll call this error 2. This is the error between the line and point n. So if you wanted the total error, if you want the total squared error, and this is actually how we started off this whole discussion, the total squared error between the points and the line, you literally just take. You literally take the y value at each point. So for example, you take y1, that's this value right over here, you take y1 minus the y value at this point in the line. Well, that point in the line is essentially the y value you get when you substitute x1 into this equation. So I'll just substitute x1 into this equation. So minus mx1 plus b. This right here, that is this y value right over here. That is mx1 plus b. I don't want to get my graph too cluttered, so I'll just delete that there. That is error 1 right over there. That is error 1. And we want the squared errors between each of the points in the line, so that's the first one. Then you do the same thing for the second point. And we started our discussion this way. y2 minus mx2 plus b squared all the way, all the way. I'll do dot, dot, dot to show that there are a bunch of d's that we have to do until we get to the nth point, all the way to yn minus mxn plus b squared. And now that we actually know how to find these m's and b's, I showed you the formula. In fact, we've proved the formula of how to find these m's and b's. We can find this line. And if we wanted to say, well, how good is it, how much error is there, we can then calculate it. Because we now know the m's and the b's. So we can calculate it for a certain set of data. Now what I want to do is kind of come up with a more meaningful estimate of how good this line is fitting the data points that we have. And to do that, we're going to ask ourselves the question, how much, or we could even say, what percentage of the variation in y is described by the variation in x? And so let's think about this. How much of the total variation in y, there's obviously change variation in y. This y value is over here. This point's y value is over here. There's clearly a bunch of variation in the y. But how much of that is essentially described by the variation in x or described by the line? So let's think about that. First, let's think about what the total variation is. How much of the, we can even say, total variation. How much of the total variation in y? So let's just figure out what the total variation in y is. The total variation, and it's really just a tool for measuring, total variation in y. Well, when we think about variation, and this is even true when we talk about variance, which was the mean variation in y, is we think about the square distance from some central tendency. And the best central measure we can have of y is the arithmetic mean. So we could just say the total variation in y is just going to be the sum of the distances of each of the y's. So you get y1. Let me do this in another color. You get y1. This y1 over here. This is y1 over here. You get y1 minus the mean of all the y's, minus the mean of all the y's squared, plus y2, plus y2, minus the mean of all of the y squared, plus, and you just keep going all the way to the nth y value, to yn minus the mean of all the y's squared. This gives you the total variation in y. You can just take out all the y values, find their mean. It'll be some value. Maybe it's right over here someplace. Maybe that is the mean value of all the y's. And so you can even visualize it the same way we visualized the squared error from the line. So if you visualize it, you can imagine a line that's y is equal to the mean of y, which would look just like that. And what we're measuring over here, this error right over here, is the square of this distance right over here, between this point vertically and this line. The second one is going to be this distance, just right up to the line. The nth one is going to be the distance from there, all the way to the line right over there. And then there are these other points in between. This is the total variation y. Makes sense. If you divide this by n, you actually will get the, I should say this is the total variation in y. If you divide this by n, you're going to get what we typically associate as the variance of y, which is kind of the average square distance. Now we have the total square distance. So what we want to do is how much of this, how much of the total variation y is described by the variation in x. So maybe we can think of it this way. So our denominator, we want what percentage of the total variation in y. So let me write it this way. Let me call this as the squared error from the average. Let me call this, this is equal to the squared error. Maybe I'll call this the squared error from the mean of y. And this is really the total variation in y. So let's put that as the denominator. Let's put that as the denominator. The total variation y, which is the squared error from the mean of the y's. Now we want to know what percentage of this is described by the variation in x. Now what is not described by the variation in x? We want how much is described by the variation in x. But what if we want how much of the total error, how much of the total variation is not described by the line over here, is not described by the regression line. How much of the total data is not? Well, we already have a measure for that. We have the squared error of the line. This tells us the square of the distances from each point to our line. So it is exactly this measure. It tells us how much of the total variation is not described by the regression line. So if you want to know what percentage of the total variation is not described by the regression line, you would just say, it would just be the squared error of the line. Because this is the total variation not described by the regression line, divided by the total variation. So let me make it clear. This right over here tells us what percentage of the total variation is not described by the variation in x. Or by the line. So to answer our question, what percentage is described by the variation, well, the rest of it has to be described by the variation in x. Because our question is, what percentage of the total variation is described by the variation in x? This is the percentage that is not described. So if this number right here, if this number is, I don't know, 30%, if 30% of the variation in y is not described by the line, then the remainder will be described by the line. So we could essentially just subtract this from 1. So if we take 1 minus the squared error between our data points and the line, over the squared error between the data points, between the y's and the mean y, we now have a percentage, this actually tells us what percentage of total variation is described by the line. Is described by the line or by the variation in x. Is described by the variation in x. And this number right here, this is called the coefficient of determination. This is called the coefficient of determination. It's just what statisticians have decided to name it. Coefficient of determination. And it's also called r squared. You might have even heard that term when people talk about regression. Now, let's think about it. If the squared error of the line, if the squared error is really small, what does that mean? It means that these errors right over here are really small, which means that the line is a really good fit. So if the squared error of the line is small, it tells us that the line is a good fit. Now, what would happen over here? Well, if this number is really small, this is going to be a very small fraction over here. 1 minus a very small fraction is going to be a number close to 1. So then we're going to have our r squared will be close to 1, which tells us that a lot of the variation in y is described by the variation in x, which makes sense, because the line is a good fit. You take the opposite case. If the squared error of the line is huge, if this number over here is huge, then that means there's a lot of error between the data points and the line. And so if this number is huge, then this number over here is going to be huge. Or it's going to be a percentage close to 1. And 1 minus that is going to be close to 0. And so if the squared error of the line is large, if this is large, this whole thing is going to be close to 1. And if this whole thing is close to 1, the whole coefficient of determination, the whole r squared is going to be close to 0, which makes sense. r squared will be close to 0, which makes sense. That tells us that very little of the total variation in y is described by the variation in x, or described by the line. Well, anyway, everything I've been dealing with so far has been a little bit in the abstract. In the next video, I'll actually put this, we'll actually look at some data samples and calculate their regression line, and also calculate the r squared and see how good of a fit it really is.