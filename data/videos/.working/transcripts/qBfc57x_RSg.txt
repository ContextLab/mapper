I've got this matrix A here. It's a 2 by 3 matrix. And just as a bit of review, let's figure out its null space and its column space. So the null space of A is the set of all vectors x that are a member of, let's see, we have three columns here. So a member of R3, such that A times the vector are going to be equal to the 0 vector. So we could just set this up. So we just need to figure out all of the x's that satisfy this in R3. So we take our matrix A, 2, minus 1, minus 3, minus 4, 2, 6, multiply them times some arbitrary vector in R3 here. So you get x1, x2, x3. And you set them equal to the 0 vector. It's going to be the 0 vector in R2, because we have two rows here. You multiply a 2 by 3 matrix times a vector in R3. You're going to get a 2 by 1 vector, or a 2 by 1 matrix. So you're going to get the 0 vector in R3. And to solve what is essentially a system of equations, you get 2x1 minus x2 minus 3x3 is equal to 0, and so on and so forth. We can just set up an augmented matrix. So we can just set up this augmented matrix right here. 2 minus 1 minus 3, minus 4, 2, 6. And then augment it with what we're trying to set it equal to to solve the system. And you know, we're going to perform a bunch of row operations here to put this in reduced row echelon form. And they're not going to change the right-hand side of this augmented matrix. And that's essentially the argument as to why the null space of the reduced row echelon form of A is the same thing as the null space of A. But anyway, that's just a bit of review. So let's perform some row operations to solve this a little bit better. So the first thing I might want to do is divide. Let's divide the first row by 2. So if I divide the first row by 2, I get a 1 minus 1 half minus 3 halves. And then of course, 0 divided by 2 is 0. And let's just divide this row right here. Let's just divide it by, I don't know, just to simplify things, let's divide it by 4. So I'm doing two row operations in one step. You can do that. I could have done it in two separate steps. So if we divide it by 4, this becomes minus 1 1 half. And then you get 3 halves. And then you get 0. And now let's keep my first row the same. I'm going to keep my first row the same. It's 1 minus 1 half minus 3 halves. And of course, the 0 is the right-hand side. And let's replace my second row with my second row plus my first row. So these are just linear operations on these guys. So negative 1 plus 1 is 0. 1 half plus minus 1 half is 0. 3 halves plus minus 3 halves is 0. And of course, 0 plus 0 is 0. So what are we left with? We're left with this right here, or this is another way of saying that x1, let me write it this way. x1, I guess the easiest way to think about it is, you're multiplying the reduced row echelon form of a now. 1 minus 1 half minus 3 halves, you have a bunch of 0s here, times x1, x2, x3 is equal to the r2, 0 vector. This is another interpretation of this augmented matrix. So this is just saying, this is useless. This is saying 0 times that plus 0 times that plus 0 times that is equal to 0. So it's giving us no information. But this first row tells us that, let me switch colors, 1 times x1, x1 minus 1 half times x2 minus 1 half times x2, minus 3 halves times x3 is equal to 0. All of the vectors whose components satisfy this are in my null space. So I want to write it a little bit differently. I could write it as x1 is equal to 1 half x2 plus 3 halves x3. Or if I wanted to write my solution set in vector form, I could write that my null space is going to be the set of all the vectors, x1, x2, x3, that satisfy these conditions that are equal to what? Well, x2 and x3 are free variables. They're associated with the non-pivot entries, or the non-pivot columns in our reduced row echelon form. That is a pivot column. I could set, so let me write it this way. It's going to be x2 times something plus x3 times something. Those are my two free variables. And we have here x1 is 1 half x2, 1 half times x2, plus 3 halves times x3. x2 is just going to be x2 times 1 plus 0 times x3. x3 is going to be 0 times x2 plus 1 times x3. So our null space, these can be any real numbers right here. They're free variables. So our null space is essentially all of the linear combinations of this guy and that guy. Or another way to write it, the null space of A is equal to the span, which is the same thing as all the linear combinations of, the span of 1 half, 1, 0. Notice these are vectors in R3, and that makes sense because the null space is going to be a set of vectors in R3. So it's a span of that and that right there. So 3 halves, 0, and 1, just like that. And what is the column space of our original matrix A? So the column space of A is equal to just all of the subspace created by all of the linear combinations of these guys, or essentially the span of the column vectors, is equal to the span of 2 minus 4, minus 1, 2, minus 3, 6. These are all each separate vectors. So it's the span of these three vectors. Now, these guys might not be linearly independent. And actually, when you put this guy in reduced row echelon form, you know that the basis vectors for this are the vectors that are associated with our pivot column. So we have one pivot column here. It's our first column. So we could say that we could use this as a basis vector. And it makes sense because this guy right here is minus 2 times this guy. This guy right here is what? Minus 3 halves times that guy. So these two guys can definitely be represented as linear combinations of that guy. So it's equal to the span of just the vector 2 minus 4. So if you were to ask me, and this is the basis for our column space, so if you want to know the rank, and this is all a bit of review, the rank of A is equal to the number of vectors in our basis for our column space. So it's going to be equal to 1. Now, everything I just did is a bit of review. But with the last couple of videos, we've been dealing with transposes. So let's actually figure out the same ideas for the transpose of A. So A transpose looks like this. A transpose is equal to the vector 2 minus 1, or the matrix, 2 minus 1 minus 3 is the first column right there. And then the second column is going to be minus 4, 2, and 6. That is our transpose. So let's figure out the null space and the column space of our transpose. Well, let me put this in reduced row echelon form so we can get the null space of this guy. So we could do the exact same exercise. Let me write it this way. The null space of A transpose. A transpose is a 3 by 2 matrix. So it's equal to all of the vectors x that are members of R2, that are members of R2. Not R3 anymore, because now we're taking the transposes null space, such that A transpose times R vectors are equal to the 0 vector in R3. And we can do that the same exact way we did before. We set up an augmented matrix. We could just put it in reduced row echelon form and set them all equal to 0. So let's just do that. Let me just put it in reduced row echelon form. So let me divide my first row by 2. The first row divided by 2 is 1 minus 2. And then the second row, let me divide it. I'll just keep it the same. So minus 1, 2. And then this last row, let me divide it by 3. So it becomes minus 1 and 2. And now, let me keep my first row the same, 1 minus 2. And now let me replace my second row with my second row plus my first row. So minus 1 plus 1 is 0. 2 plus minus 2 is 0. So you get some zeros. I'm going to do the same thing with the third row. Replace it with it plus the first row. Once again, you're going to get some zeros. So this is the reduced row echelon form of A transpose. So this is the reduced row echelon form of A transpose. And its null space is the same as A transpose's null space. So we could set, we could say, to find this null space, we can find all of the solutions to this equation times the vectors x1 and x2 is equal to 0, 0, and 0. These aren't vectors. These are just entries right here. 0, 0, 0. So these two lines give us no information, but this first one does. So we get 1 times x1. And notice, this is the pivot column right here. It's associated. So x1 is going to be a pivot variable. This x2 will be a free variable. And just to be clear, the first column is our pivot column. So if we go back to A transpose, it's this first column here that is associated with the pivot column. So when we talk about its column space, this by itself will span the column space. This is all a review of what we did before. We're just applying it to the transpose. Let's go back to our null space. So this tells us that 1 times x1, so x1 minus 2 times x2, minus 2 times x2, is equal to 0. Or we could say that x1 is equal to 2 times x2. So all of the vectors in R2 that satisfy these conditions with this entries will be in the null space of A transpose. Let me write it this way. So the null space of A is going to be the set of all the vectors, let me write it here, x1, x2, that are a member of R2 clearly, such that x1, x2 is going to be equal to, well, our free variable is x2. So it's x2 times the vector. So x1 has to be 2 times x2. And obviously x2, that's a 2, x2 is going to be 1 times x2. So what is this going to be? Well, this is all of the linear combinations of this vector right here. So we could say it's equal to the span of our vector to 1. Now, that's the null space. Sorry, this was the null space of A transpose. I have to be very careful there. Now, what is the column space of A transpose? Well, the column space of A transpose is the set of all vectors spanned by the columns of A. So you could just say the span of this column vector and this column vector. But we know when we put it into reduced row echelon form, only this column vector was associated with a pivot column. So this by itself, this guy is a linear combination of this guy. If you multiply him by minus 2, you get that guy right there. So it's consistent with everything we've learned. So it equals the span of just this guy right here, of just the vector 2, minus 1, and minus 3. Now, that's just a nice neat exercise that we did. Notice that your span here, it's in R3, but it's just going to be a line in R3. But maybe in the next video, I'll do a more graphical representation of it. But I did this whole exercise to introduce you to the ideas of the null space of your transpose and the column space of your transpose. Think about what the column space of your transpose is. It's the subspace spanned by this vector and that vector. And it turns out that this guy is a multiple of that guy, so we could say just by that guy. But these were the rows of our original matrix A. So we could also view this as the span of the row vectors of our original guy. This is that column that is the basis for the column span of the R transpose matrix. And of course, this guy was a linear combination of that. So we can also view the column span of our transpose matrix. It's equivalent to the subspace spanned by these rows. Or we could call that the row space of A. So let me write that down. So the column space of A transpose, and this is just general, let me write this generally. It doesn't just apply to this example. So the column space of the transpose of any matrix, this is called the row space of A. And it's a very natural name, because if A's got a bunch of rows, we could call them the transpose of some vectors. So that's the first row. You got the second row all the way to maybe the mth row. The mth row, just like that. These are vector transposes. They're really just rows. If you imagine the space that's spanned by these vectors, by the different rows, that's essentially the column space of the transpose. Because when you transpose it, each of these guys become columns. So that's what the row space is. Now, the null space of our transpose, let's write it like this. It was all of the vectors x that satisfied this equation. Now, what happens if we take the transpose of both sides of this equation? So if we take the transpose of both sides of that equation. Well, we've learned from our transpose properties, this is equal to the reverse product of each of those transposes. So this is going to be equal to, this is a vector, the vector x transpose. So now it's going to become, if this is a column vector before, now it's going to become a row vector. And then times A transpose transpose, and that's going to be equal to the transpose of the zero vector. Or we could just write this as, let's write it like this. We could write this as some matrix, well, let me just write it like this, some column vector x. What's the transpose of A transpose? Well, that's just equal to A. So you take the transpose of this column vector, you now get a row vector. You could view it as a matrix if you want. If this was a member of Rn, this is now going to be an n by or 1 by n matrix, if this was a member of Rn. And now it's essentially, we kind of switch the orders and we multiply it times the transpose of the transpose. We just get the matrix A. And we set that equal to the transpose of the zero vector. Now this is interesting. We now have it in terms of our original matrix A. Now what did the null space of our matrix A look like? The null space where all of the vectors x that satisfy this equation is equal to zero. So the x was on the right. So the null space is all the x's that satisfy this. The null space of our transpose is all of the x's that satisfy this equation. And this is also called, so let me say the set of all of the x's such that A transpose times x is equal to zero. That is the null space of A transpose. Or we could also write this as the set of all of the x's such that the transpose of our x times A is equal to the transpose of the zero vector. And we have another name for this. This is called the left null space. The left null space of A. Why is it called the left null space? Because now we have x on our left. And just a regular null space, you have x on the right. But now if you take the null space of the transpose, using just our transpose properties, that's equivalent to this transpose vector right here. Actually, let me write this transpose right there. This transpose vector multiplying A from the left hand side. So all of the x's that satisfy this is the left null space. And it's going to be different than your null space, because it was equal to, notice, your null space of A transpose was a span of this right here. This is also the left null space of A. Now what was just the regular null space of A? The regular null space of A was essentially a plane in R3. That's the null space of A. The left null space of A is just a line in R2. Very different things. Very different things. And if you go to the row space, what is the row space of A? The row space of A is a line in R3. Well, what is the column space of A? The column space of A right here. Where did I have it? It was a, well, this is the only literally independent vector. It was essentially a line in R2. So they're all very different things. And we'll study a little bit more how they're all related. Now there's one thing I want to relate to you. We figured out that the rank of this vector right here is 1. Because when you put it in reduced row echelon form, there was one pivot column. And the basis vectors are those associated with that pivot column. And if you count your basis vectors, that's your dimension of your space. So the dimension of your column space is 1. And that's the same thing as your rank. Now what is the rank of A transpose? The rank of A transpose in the example, when you put it in reduced row echelon form, you got one linearly independent column vector. So the basis for our column space was also equal to 1. And in general, that's always going to be the case. That the rank of A, which is the dimension of its column space, is equal to the rank of A transpose. And if you think about it, it makes a lot of sense. To figure out the rank of A, to figure out the rank of A, you essentially just have to figure out how many pivot columns they have, or another way to say it is, how many pivot entries they have. Now, when you figure out the row space, when you figure out the column space of, when you want to find the rank of your transpose vector, you're essentially just saying, and I know this is maybe getting a little bit confusing, but when you want the rank of your transpose vector, you're saying how many of these columns are linearly independent, or which of these are linearly independent. And that's the same question as saying, how many of your rows up here are linearly independent? If you want to know how many columns in your transpose are linearly independent, that's equivalent to asking how many rows in your original matrix are linearly independent. And when you put this matrix in reduced row echelon form, everything in reduced row echelon form are just row operations. So there's linear combinations of these things up here. Or you could go vice versa. Everything up here is just linear combinations of your matrix in reduced row echelon form. So if you only have one pivot entry, then this guy right here, by himself, or one pivot row, that guy by himself can represent a basis for your row space. Or all of your rows can be represented by a linear combination of your pivot rows. And because of that, you just count that. You say, OK, there's one in this case. So the dimension of my row space is one. And that's the same thing as the dimension of my column space is transposed. I know it's all of my transpose is column space. I know it's getting all confusing and it's late in the day for me as well. So hopefully, it will convince you that the rank of our transpose is the same as the rank of our original matrix.