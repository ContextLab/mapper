So we are interested in studying the relationship between the amount that folks study for a test and their score on a test, where the score is between zero and six. And so what we're going to do is go look at the people who took the tests, we're going to plot for each person the amount that they studied and their score. So for example, this data point is someone who studied an hour and they got a one on the test, and then we're going to fit a regression line. And this blue regression line is the actual regression line for these four data points. And here is the equation for that regression line. Now there's a couple of things to keep in mind. Normally when you're doing this type of analysis, you would do it with far more than four data points. The reason why I kept this to four is because we're actually going to calculate how good a fit this regression line is by hand. And typically you would not do it by hand, we have computers for that. Now the way that we're going to measure how good a fit this regression line is to the data has several names. One name is the standard deviation of the residuals. Another name is the root mean square deviation, sometimes abbreviated RMSD, sometimes it's called root mean square error. So what we're going to do is, is for every point, we're going to calculate the residual, and then we're going to square it, and then we're going to add up the sum of those squared residuals. So we're going to take the sum of the residuals, residuals squared, and then we're going to divide that by the number of data points we have minus two. And we can talk in future videos or a more advanced statistics class of why you divide by two. But it's related to the idea that what we're calculating here is a statistic, and we're trying to estimate a true parameter as best as possible, and n minus two actually does the trick for us. But to calculate the root mean square deviation, we would then take a square root of this. And some of you might recognize strong parallels between this and how we calculated sample standard deviation early in our statistics career, and I encourage you to think about it. But let's actually calculate it by hand, as I mentioned earlier in this video, to see how things actually play out. So to do that, I'm going to give ourselves a little table here. So let's say that is our x value in that column. Let's make this our y value. Let's make this y hat, which is going to be equal to 2.5x minus two. And then let's make this the residual squared, which is going to be our y value minus our y hat value, our actual minus our estimate for that given x squared. And then we're going to sum them all up, divide by n minus two, and take the square root. So first let's do this data point. So that's the point one comma one, one comma one. Now what is the estimate from our regression line? Well for that x value, when x is equal to one, it's gonna be 2.5 times one minus two. So it's gonna be 2.5 times one minus two, which is equal to 0.5. And so our residual squared is going to be one minus 0.5, one minus 0.5 squared, which is equal to, that's gonna be 0.5 squared, which is going to be 0.25. All right, let's do the next data point. We have this one right over here. It is two comma two. Now our estimate from the regression line when x equals two is going to be equal to 2.5 times our x value times two minus two, which is going to be equal to three. And so our residual squared is going to be two minus three, two minus three squared, which is negative one squared, which is going to be equal to one. Then we can go to this point. So that's the point two comma three, two comma three. Now our estimate from our regression line is going to be 2.5 times our x value times two minus two, which is going to be equal to three. And so our residual here is going to be zero. And you can see that that point sits on the regression line. So it's going to be three minus three, three minus three squared, which is equal to zero. And then last but not least, we have this point right over here. When x is three, our y value, this person studied three hours and they got a six on the test. So y is equal to six. And so our estimate from the regression line, you could say what you would have expected to get based on that regression line is 2.5 times our x value times three minus two is equal to 5.5. And so our residual squared is six minus 5.5 squared, minus 5.5 squared. So it's .5 squared, which is 0.25. So now the next step, let me take the sum of all of these squared residuals. So this is, let me just write it this way. Actually let me just do it like this. So the sum of the residuals, residuals squared is equal to, if I just sum all of this up, it's going to be 1.5. 1.5. And then if I divide that by n minus two, so if I divide by n minus two, that's going to be equal to, I have four data points. So I'm going to divide by four minus two, so I'm going to divide by two. And then I'm going to want to take the square root of that. Then we want to take the square root of that. And so this is going to get us, 1.5 over two is the same thing as 3 fourths. So it's the square root of 3 fourths or the square root of three over two. And you could use a calculator to figure out what that is as a decimal, but this gives us a sense of how good a fit this regression line is. The closer this is to zero, the better the fit of the regression line. The further away from zero, the worse fit. And what would be the units for the root mean square deviation? Well, it would be in terms of whatever your units are for your y-axis. In this case, it would be the score on the test. And that's one of the other values of this calculation of taking the square root of the sum of the squares of the residuals dividing by n minus two. So big picture, this square root of three over two can be viewed as the approximate size of a typical or average prediction error between these points and what the regression line would have predicted. Or you could view it as the approximate size of a typical or average residual. So this is the square root of three over two.