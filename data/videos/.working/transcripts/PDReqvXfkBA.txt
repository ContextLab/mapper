In the last video, we saw why when we take any non-zero number divided by zero, why mathematicians have left that as being undefined. But it might have raised a question in your brain. What about zero divided by zero? Isn't there an argument why that could be defined? So we're going to think about zero divided by zero. Well, there's a couple of lines of reasoning here. One, you could start taking numbers closer and closer to zero and dividing them by themselves. So for example, you take 0.1 divided by 0.1. Well, that's going to be 1. Let's get even closer to zero. 0.001 divided by 0.001. Well, that also equals 1. Let's get super close to 0. 0.00001 divided by 0.00001. Well, once again, that also equals 1. It didn't even matter whether these are positive or negative. I could make these negative, and I'd still get the same result. Negative this thing divided by negative this thing still gets me to 1. So based on this logic, you might say, hey, well, this seems like a pretty reasonable argument for zero divided by zero to be defined as being equal to 1. But someone could come along and say, well, no, what happens if we divide zero by numbers closer and closer to zero? Not a number by itself, but zero by smaller and smaller numbers, or numbers closer and closer to zero. And so they say, for example, 0 divided by 0.1, well, that's just going to be 0. 0 divided by 0.001, well, that's also going to be 0. 0 divided by 0.00001, also going to be equal to 0. And it didn't matter whether we were dividing by a positive or a negative number. Make all of these negatives, you still get the same answer. So this line of reasoning tells you that it's completely legitimate, or to think at least, then maybe 0 divided by 0 could be equal to 0. And these are equally valid arguments. And because they're equally valid, and frankly neither of them is consistent with the rest of mathematics, once again, mathematicians have left 0 divided by 0 as undefined.