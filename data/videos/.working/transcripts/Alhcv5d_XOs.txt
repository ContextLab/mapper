I think by now we have a reasonable sense of what linear dependence means. So let's just do a slightly more formal definition of linear dependence. So we're going to say that a set of vectors, let me just define my set of vectors. So let me call my set S of vectors v1, v2, all the way to vn. So I'm going to say that they are linearly dependent. So sometimes it's written if with a lot of f's in there. So sometimes it's written if and only if. Sometimes it's shown like an arrow in two directions. If and only if I can satisfy this equation, I can find a set of constants. So I can take a linear combination of my vectors all the way to cn, vn, that satisfy the equation that I can create this into the zero vector. So sometimes it's just written as bold zero, and sometimes you could just write it. We don't know the dimensionality of this vector. It would be a bunch of zeros. We don't know how many actual elements are in each of these vectors, but you get the idea. So my set of vectors is linearly dependent. Remember, I'm saying dependent, not independent. Is linearly dependent if and only if I can satisfy this equation for some ci's where not all of them are equal to zero. For some ci's where not all, and this is key, not all are zero. Or you could say it the other way. You could say at least one is non-zero. So how does this gel with what we were talking about in the previous video where I said look, a set is linearly dependent if one of the vectors can be represented by a combination of the other vectors. So let me write that down. So in the last video I said look, one vector can be, well let me write it this way. One vector being represented by the sum of the other vectors I can just write it like this. I could write it a little bit more math-y. In the last video I said that linear dependence means that let me just pick an arbitrary vector. V1 could be represented by some combination of the other vectors. Let me call them a1 times V, let me be careful, a2 times V2 plus a3 times V3 plus all the way up to an times Vn. This is what we said in the previous video. Look, this is linearly dependent if any one of these guys can be represented as some combination of the other ones. So how does this imply that? And in order to show this if and only if, I have to show that this implies that. And I have to show that that implies this. So this is almost a trivially easy proof. Because if I subtract V1 from both sides of this equation, I get 0 is equal to minus 1 V1 plus a2 V2 plus a3 V3 all the way to an Vn. And clearly I've just said, well, this is linear dependent, that means that I can represent this vector as a sum of the other vectors. Which means that minus 1 times V1 plus some combination of the other vectors is equal to 0. Which means that I've been able to satisfy this equation, and at least one of my constants is non-zero. So I've shown you that if I can represent one of the vectors by a sum of the other ones, then this condition is definitely going to be true. Now let me go the other way. Let me show you if I have this situation, that I can definitely represent one of the vectors as the sum of the others. So let's say that this is true. And one of these constants, remember it's not just this, at least one is non-zero. So let me just assume, just for the sake of simplicity, I mean these are all arbitrary, let me just assume, I'll do it in a new color, let me do it in the magenta, let me assume that C1 is not equal to 0. If C1 is not equal to 0, then I can divide both sides of this equation by C1. And what do I get? I get V1 plus C2 over C1 V2 plus all the way up to CN over C1 is equal to 0. And then I can multiply both sides of this by, or I could add negative V1 to both sides of this equation or subtract V1 from both sides. And I get C2 over C1 V2 plus all the way up to CN over C1 VN, there's a VN here, is equal to minus V1. Now if I just multiply both sides of this by negative 1, I get a minus, and all these become minuses, and this becomes a plus. And I just showed you that if at least one of these constants is non-zero, that I can represent my vector V1 as some combination of the other vectors. So we're able to go this way too. If this condition is true, then I can represent one of the vectors as a combination of the others. If I can represent one of the vectors as a combination of the others, then this condition is true. So hopefully that kind of proves that these two definitions are equivalent. Maybe it's a little bit of overkill. But let's apply that definition now to actually test, you might say, hey Sal, why did you go through all of this effort? And I went through all of this effort because this is actually a really useful way to test whether things are linearly independent or dependent. Let's try it out. Let's use our newly found tool. So let's say I have the set of vectors. Let me do it up here. I want to be efficient with my space usage. So let's say I have the set of vectors 2, 1, 2, 1, and 3, 2. And my question to you is, are these linearly independent or are they linearly dependent? Well, in order for them to be linearly dependent, that means that there's some constant times 2, 1 plus some other constant times this second vector, 3, 2, where this should be equal to 0, where these both aren't necessarily 0. Before I go forth this problem, let's remember what we're going to find out. If either of these are non-zero, so if c1 or c2 non-zero, then this implies that we are dealing with a linearly dependent set. Now, if c1 and c2 are both 0, if the only way to satisfy this equation, I mean, you can always satisfy it by setting everything equal to 0. But if the only way to satisfy it is by making both of these guys 0, then we're dealing with a linearly independent set. So let's try to do some math. And this will just take us back to our algebra 1 days. So in order for this to be true, that means 2 times c1 plus 3 times c2 is equal to, when I say this is equal to 0, it's really the 0 vector. So I can rewrite this as 0, 0. So 2 times c1 plus 3 times c2 would be equal to that 0 there. And then we'd have 1 times c1 plus 2 times c2 is equal to that 0 right there. And now this is just a system, two equations, two unknowns, of a couple of things we could do. Let's just multiply this top equation by 1 half. And then so if you multiply it by 1 half, you get c1 plus 3 halves c2 is equal to 0. And then if we subtract the green equation from the red equation, this becomes 0. 2 minus 1 and 1 half, 3 halves is 1 and 1 half. So this is just 1 half c2 is equal to 0. And this is easy to solve. c2 is equal to 0. So what's c1? Let's just substitute this back in. c2 is equal to 0. So this is equal to 0. So c1 plus 0 is equal to 0. So c1 is also equal to 0. We could have substituted back into that top equation as well. So the only solution to this equation involves both c1 and c2 being equal to 0. So they both have to be 0. So this is a linearly independent set of vectors. Which means that neither of them are redundant of the other one. You can't represent one as a combination of the other. And since we have two vectors here, and they're linearly independent, we can actually know that this will span r2. The span of my r vectors is equal to r2. If one of these vectors was just some multiple of the other, then the span would have been some line within r2. Not all of them. Now I can represent any vector in r2 as some combination of those. Let's do another example. Let's say, let me scroll to the right. Because sometimes this thing, when I go too far down, I haven't figured out why. When I go too far down, it starts messing up. So my next example is the set of vectors. So I have the vector 2, 1. I have the vector 3, 2. And I have the vector 1, 2. And I want to know, are these linearly dependent or linearly independent? So I do go through the same drill. I use that little theorem that I proved at the beginning of this video. So in order for these to be linearly dependent, there must be some set of weights that I can multiply these guys. So c1 times this vector plus c2 times this vector plus c3 times that vector. That will equal the 0 vector. And if one of these is non-zero, then we're dealing with a linearly dependent set of vectors. And if all of them are 0, then it's independent. So let's just do our linear algebra. So this means that 2 times c1 plus 3 times c2 plus c3 is equal to that 0 up there. And then if we do the bottom rows, remember when you multiply scalar times vector, you multiply it by each of these terms. So c1 times 1. So 1c1 plus 2c2 plus 2c3 is equal to 0. Now, there's a couple of giveaways on this problem. If you have three two-dimensional vectors, one of them is going to be redundant. Because in the very best case, even if you assume that, let's say, that that vector and that vector are linearly independent, then these would span r2, which means that any vector in your two-dimensional space can be represented by some combination of those two, in which case, this is going to be one of them. Because this is just a vector in two-dimensional space. So it'd be linearly dependent. And then if you say, well, these aren't linearly independent, well, then if these aren't linearly independent, then there's multiples of each other, in which case, this would definitely be a linearly dependent set. So when you see three vectors that are each only vectors in r2, that are each two-dimensional vectors, it's a complete giveaway that this is linearly dependent. But I'm going to show it to you using our little theorem here. So I'm going to show you that I can get non-zeros c3, c2's, and c1's such that I can get 0 here. If all of these had to be 0, I mean, you can always set them equal to 0. But if they had to be equal 0, then it would be linearly independent. But let me just show you. I can just pick some random c3. Let me pick c3 to be equal to negative 1. So what would these two equations reduce to? I mean, if you have three unknowns and two equations, it means you don't have enough constraints on your system. So if I just set c3, I just picked that out of a hat. I could have picked c3 to be anything. But if I set c3 to be equal to negative 1, what do these equations become? You get 2 c1 plus 3 c2 minus 1 is equal to 0. And you get c1 plus 2 c2 minus 2 is equal to 0, right? 2 times minus 1. And then I can, what can I do here? If I multiply this second equation by 2, what do I get? I get 2 plus 4 c2 minus 4 is equal to 0. And now, let's subtract this equation from that equation. So the c1s cancel out. 3 c2 minus 4 c2 is minus c2. And then minus 1 minus minus 4. So that's minus 1 plus 4. That's plus 3 is equal to 0. And so we get our, let me make sure I got that right. We have a minus 1 minus a minus 4. So plus 4. So we have a plus 3. So that is a minus 2. So we have our c2 is equal to minus 3. Or c2 is equal to 3. So we get c2 is equal to 3. And if c2 is equal to 3 and c3 is equal to minus 1, let's just substitute here. So we get c1 plus 2 times c2. So plus 6 plus 2 times c3. So minus 2 is equal to 0. c1 plus 4 is equal to 0. c1 is equal to minus 4. So I'm giving you a combination of c's that will give us a 0 vector. If I multiply minus 4 times our first vector, 2, 1, that's c1, plus 3 times our second vector, 3, 2, minus 1 times our third vector, 1, 2. This should be equal to 0. Let's verify it just for fun. Minus 4 times 2 is minus 8 plus 9 minus 1. That's 0. Minus 4 plus 6 minus 2. So we've just shown a linear combination of these vectors where actually none of the constants are 0. But all we had to show is that at least one of the constants had to be non-zero. And we actually showed all three of them were. But at least one of these had to be non-zero. And I was able to satisfy this equation. I was able to make them into the 0 vector. So this shows, this proves, that this is a linearly dependent set of vectors, linearly dependent, which means one of the vectors is redundant. And you can never just say, oh, this is the redundant vector because I can represent this as a combination of those two. You could just as easily pick this guy as the redundant vector and say, hey, I can represent this guy as the sum of those two. There's not kind of one bad apple in the bunch. Any of them can be represented by the combination of some other by all of the rest of them. So hopefully you have a better intuition of linear dependence and independence. Maybe I'll do a few more examples in the next video.