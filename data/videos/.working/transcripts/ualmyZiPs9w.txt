What I want to do in this video is introduce you to the idea of the covariance between two random variables. And it's defined as the expected value of the distance, or I guess the product of the distances, of each random variable from their mean, or from their expected value. So let me just write that down. So if I take expect, I'll have x first, I'll do this in another color. So it's the expected value of random variable x minus the expected value of x. You could view this as the population mean of x times, and then this is random variable y, so times the distance from y to its expected value, or the population mean of y. And if it doesn't make a lot of intuitive sense yet, well, one, you can just always think about what it's doing, play around with some numbers here. But the reality is, it's saying how much they vary together. So you always take an x and a y for each of the data points. Let's say you have the whole population. So every x and y that go together with each other, that are a coordinate, you put into this. And what happens is, let's say that x is above its mean when y is below its mean. So let's say in the population you had the point. So one instantiation of the random variables, you sample once, I guess, from the universe, and you get x is equal to 1, and that y is equal to, let's say, y is equal to 3. And let's say that you knew ahead of time that the expected value of x is 0. And let's say that the expected value of y is equal to 4. So in this situation, what just happened? Now we don't know the entire covariance. We only have one sample here of this random variable. But what just happened here? We have 1 minus, so we're not going to calculate the entire expected value. I just want to calculate what happens when we do what's inside the expected value. We'll have 1 minus 0, so you'll have a 1, times a 3 minus 4, times a negative 1. So you're going to have 1 times negative 1, which is negative 1. And what is that telling us? Well, it's telling us at least for this sample, this one time that we sampled the random variables x and y, x was above its expected value when y was below its expected value. And if we kept doing this, let's say for the entire population this happened, then it would make sense that they have a negative covariance. When one goes up, the other one goes down. When one goes down, the other one goes up. If they both go up together, they would have a positive variance, or if they both go down together, and the degree to which they do it together will tell you the magnitude of the covariance. Hopefully that gives you a little bit of intuition about what the covariance is trying to tell us. But the more important thing that I want to do in this video is to connect this formula, is I want to connect this definition of covariance to everything we've been doing with least-quared regression. And really, it's just kind of a fun math thing to do to show you all of these connections. And where really the definition of covariance really becomes useful. And I really do think it's motivated to a large degree by where it shows up in regressions. And this is all stuff that we've kind of seen before. You're just going to see it in a different way. So this whole video, I'm just going to rewrite this definition of covariance right over here. So this is going to be the same thing as the expected value of, and I'm just going to multiply these two binomials in here. So the expected value of our random variable x times our random variable y minus, well I'll just do the x first. So plus x times the negative expected value of y. So I'll just say minus x times the expected value of y. The expected value of y. And that negative sign comes from this negative sign right over here. And then we have minus expected value of x times y minus the expected value of x times this y. This is doing the distributed property twice. And then finally, you have the negative expected value of x times the negative expected value of y. And the negatives cancel out. And so you're just going to have plus the expected value of x times the expected value of y. And of course, it's the expected value of this entire thing. It's the expected value of this entire thing. Now let's see if we can rewrite this. Well the expected value of the sum of a bunch of random variables, or the sum and difference of a bunch of random variables, is just the sum or difference of their expected values. So this is going to be the same thing. And remember, expected value, in a lot of contexts, you can view it as just the arithmetic mean. Or in a continuous distribution, you could view it as a probability weighted sum or probability weighted integral. Either way, it's nothing. We've seen it before, I think. So let's rewrite this. So this is equal to the expected value of the random variables x and y. x times y. Trying to keep them color coded for you. And then we have minus x times the expected value of y. So then we're going to have minus the expected value of y. times the expected value of y. Then you're going to have minus the expected value of this thing. minus the expected value of, I'll close the parentheses of this thing right over here, expected value of x times y. And this might look really confusing with all of the embedded expected values. But one way to think about it is the things that already have the expected value, you can kind of view these as numbers, you already view them as known. So we're actually going to take them out of the expected value, because the expected value of an expected value is the same thing as the expected value. Actually, let me write this over here, just to remind ourselves, the expected value of x is just going to be the expected value of x. Think of it this way, you could view this as the population mean for the random variable. So that's just going to be a known, it's out there, it's in the universe, so the expected value of that is just going to be itself. If the population mean, or the expected value of x, is 5, this is like saying the expected value of 5. Well, the expected value of 5 is going to be 5, which is the same thing as the expected value of x. Hopefully that makes sense, we're going to use that in a second. So we're almost done, we did the expected value of this and we have one term left. And then the final term, the expected value of this guy. And here, we can actually use the property right from the get go, I'll write it down. So the expected value of, put some big brackets up, of this thing right over here. Expected value of x times the expected value of y. Let's see if we can simplify it right here. So this is just going to be the expected value of the product of these two random variables. I'll just leave that the way it is. So let me just, the stuff that I'm going to leave the way it is, I'm just going to kind of freeze them. So the expected value of x, y. Now what do we have over here? We have the expected value of x times, once again, you can kind of view it if you go back to what we just said, is this is just going to be a number, expected value of y. So we can just bring this out. If this was the expected value of 3x, it would be the same thing as 3 times the expected value of x. So we could rewrite this as negative expected value of y times the expected value of x. So you can kind of view this as we took it out of the expected value. We factored it out. So just like that. And then you have minus, same thing over here. You can factor out this expected value of x, minus the expected value of x times the expected value of y. Let me write it, times the expected value of y. This is getting confusing with all the e's laying around. And then finally, you have the expected value of this thing, of two expected values. Well, that's just going to be the product of those two expected values. So that's just going to be plus, I'll freeze this, expected value of x times the expected value of y. Now what do we have here? We have expected value of y times the expected value of x. And then we are subtracting the expected value of x times the expected value of y. These two things are the exact same thing. And actually, look at this. We're subtracting it twice. And then we have one more. These are all the same thing. This is the expected value of y times the expected value of x. This is the expected value of y times the expected value of x, just written in a different order. And this is the expected value of y times the expected value of x. We're subtracting it twice. And then we're adding it. Or one way to think about it is that this guy and that guy will cancel out. You could have also picked that guy and that guy. But what do we have left? We have the covariance of these two random variables, x and y, are equal to the expected value of, I'll switch back to my colors just because this is the final result, the expected value of x times the expected value of the product of xy, the expected value of the product minus the expected value of y times the expected value of x. Now, you can calculate these expected values if you know everything about the probability distribution or density functions for each of these random variables, or if you had the entire population that you're sampling from whenever you take an instantiation of these random variables. But let's say you just had a sample of these random variables. How could you estimate them? Well, if you were estimating it, the expected value, let's say you just have a bunch of data points, a bunch of coordinates. And I think you'll start to see how this relates to what we did with regression. The expected value of x times y, it can be approximated by the sample mean of the products of x and y. This is going to be the sample mean of x and y. You take each of your xy associations, take the product, and then take the mean of all of them. So that's going to be the product of x and y. And then this thing right over here, the expected value of y that can be approximated by the sample mean of y. And the expected value of x can be approximated by the sample mean of x. So what can the covariance of two random variables be approximated by? Well, this right here is the mean of their product. This right here is the mean of their product from your sample minus the mean of your sample y's times the mean of your sample x's times the mean of your sample x's. And this should start looking familiar. This should look a little bit familiar, because what is this? This was the numerator. This right here is the numerator when we were trying to figure out the slope of the regression line. So we tried to figure out the slope of the regression line. Let me just rewrite the formula here just to remind you. It was literally the mean of the products of each of our data points minus, or the xy's, minus the mean of y's times the mean of the x's. All of that over the mean of the x squareds, and you can even view it as this, over the mean of the x times the x's. But I could just write the x squareds over here minus the mean of x squared. This is how we figured out the slope of our regression line. Or maybe a better way to think about it, if we assumed in our regression line that the points that we have were sample from an entire universe of possible points, then you could say that we are approximating the slope of our regression line. And you might see this little hat notation in a lot of books. Don't want you to be confused. You're saying that you're approximating the population's regression line from a sample of it. Now, this right here, so everything we've learned right now, this right here is the covariance, or this is an estimate of the covariance of x and y. Now what is this over here? Well, I just said you could rewrite this very easily as, so this bottom part right here, you could write as the mean of x times x, that's the same thing as x squared, minus the mean of x times the mean of x. That's what the mean of x squared is. What's this? Well, you could view this as the covariance of x with x. But we've actually already seen this. And I've actually shown you, many, many videos ago when we first learned about it, what this is. The covariance of a random variable with itself is really just the variance of that random variable. And you could verify it for yourself. If you change this y to an x, this becomes x minus the expected value of x times x minus the expected value of x. Or that's the expected value of x minus the expected value of x squared. That's your definition of variance. So another way of thinking about the slope of our regression line, it can be literally viewed as the covariance of our two random variables over the variance of x, so you can kind of view it as the independent random variable. That right there is the slope of our regression line. Anyway, I thought that was interesting. And I wanted to make connections between things you see in different parts of statistics and show you that they really are connected.