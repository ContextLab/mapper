We saw that we could take a system of two equations with two unknowns and represent it as a matrix equation, where the matrix A is the coefficients here on the left-hand side. The column vector x has our two unknown variables, s and t. And then the column vector b is essentially representing the right-hand side over here. And what was interesting about it, then that would be the equation A, the matrix A times the column vector x being equal to the column vector b. And what was interesting about that is we saw, well, look, if A is invertible, we can multiply both the left and the right-hand sides of the equation. And we have to multiply them on the left-hand sides of their respective sides by A inverse. Because remember, when matrix multiplication order matters, we're multiplying the left-hand side of both sides of the equation. If we do that, then we can get to essentially solving for the unknown column vector. If we know what column vector x is, then we know what s and t are, and then we've essentially solved the system of equations. So now let's actually do that. Let's actually figure out what A inverse is and multiply that times the column vector b to figure out what the column vector x is and what s and t are. So A inverse is equal to 1 over the determinant of A. The determinant of A for a 2 by 2 here is going to be 2 times 4 minus negative 2 times negative 5. So it's going to be 8 minus positive 10, which would be negative 2. So this would become negative 2 right over here. Once again, 2 times 4 is 8 minus negative 2 times negative 5. So minus positive 10, which gets us negative 2. And you multiply 1 over the determinant times what is sometimes called the adjoint of A, which is essentially swapping the top left and bottom right, or at least for a 2 by 2 matrix. So this would be a 4. This would be a 2. Notice I just swapped these. And making these two negative, the negative of what they already are. So this is from a negative 2. This is going to become a positive 2. And this right over here is going to become a positive 5. If all of this looks completely unfamiliar to you, you might want to review the tutorial on inverting matrices, because that's all I'm doing here. And so A inverse is going to be equal to negative 1 half times 4 is negative 2. Negative 1 half times 5 is negative 2.5. And negative 1 half times 2 is negative 1. Negative 1 half times 2 is negative 1. So that's A inverse right over here. So now let's multiply A inverse times our column vector 7, negative 6. So let's do that. So this is A inverse. I'll rewrite it. Negative 2, negative 2.5, negative 1, negative 1, times 7 and negative 6. Times, I'll just write them all in white here now. 7, negative 6, we've had a lot of practice multiplying matrices. So what is this going to be equal to? So the first entry is going to be negative 2 times 7, which is negative 14, plus negative 2.5 times negative 6. So let's see. That's going to be positive. That's going to be 12 plus another 3. So it's going to be plus 15. Negative 2.5 times negative 6 is positive 15. And then we're going to have negative 1 times 7, which is negative 7, plus negative 1 times negative 6. Well, that is positive 6. And so the product A inverse B, which is the same thing as the column vector x, is equal to, we deserve a little bit of a drumroll now, the column vector 1, negative 1. So we have just shown that this is equal to 1, negative 1. Or that x is equal to 1, negative 1. Or we could even say that the column vector ST, S, column vector with the entries S and T, is equal to 1, negative 1, which is another way of saying that S is equal to 1 and T is equal to negative 1. And I know what you're saying. I said this in the last video, and I'll say it again in this video. You're like, well, it was so much easier to just solve the system directly, just with using elimination or using substitution. And I agree with you. But this is a useful technique, because when you are doing problems in computation, there may be situations where you have the left-hand side of the system stays the same. But there's many, many, many different values for the right-hand side of the system. And so it might be easier to just compute the inverse once and just keep multiplying this inverse times what we have on the right-hand side. And you probably are familiar with some types of graphics processors and graphics cards on computers, and they talk about special graphics processors. What these are really all about are the hardware that is special purpose for really fast matrix multiplication, because when you're doing graphics processing, when you're thinking about modeling things in three dimensions and you're doing all these transformations, you're really just doing a lot of matrix multiplications really, really, really fast in real time so that to the user playing the game or whatever they're doing, it feels like they're in some type of a 3D real-time reality. So anyway, I just want to point that out. This wouldn't be, if I saw this just randomly, my instincts would be to solve this with elimination. But this ability to think of this as a matrix equation is a very, very useful concept. One, actually not just in computation, but also as you go into higher level sciences, especially physics, you will see a lot of matrix vector equations like this that kind of speak in generalities. And it's really important to think about what these actually represent and how they can actually be solved.