In the last video, we saw a method of figuring out what the basis for column space is. And we used these exact examples where if this was matrix A, I just took it and I put it in reduced row echelon form. And I figured out which of these columns in my reduced row echelon form of A are pivot columns. And it turned out to be the first one, the second one, and the fourth one. And then the method is you say, look, the corresponding columns in A, so the first one, the second one, the fourth one, formed my basis for my column space. And since they formed the basis, and if you want to know the dimension of your basis of your column space, which is also called the rank, you just say, well, there's three in there. So it has a rank of one, two, three. In this video, I want to discuss a little bit about why this worked. Why were we able to just take the corresponding columns? Why did linear independence of these three guys imply linear independence of these three guys? Why was the fact that I can represent these guys, this guy right here, as a linear combination of these three, or this guy as a linear combination of these three? Why does that imply that I can construct this guy as a linear combination of my basis vectors? So the first thing I think that wasn't too much of a stretch of the imagination in the last video was the idea that these pivot vectors are linearly independent. So R1, R2, and R4. And everything I'm doing, I'm kind of applying to the special case just so that it's easier to understand. But it should be generalizable. In fact, it definitely is generalizable. That all of the pivot columns in reduced row echelon form are linearly independent. And that's because the very nature of reduced row echelon form is that you are the only pivot column that has a one in that respective row. So the only way to construct it is with that vector. You can't construct it with the other pivot columns, because they're all going to have 0 in that row. And when I say it's linearly independent, I'm just saying the set of pivot columns. So let me say this in general. The set of pivot columns for any reduced row echelon form matrix is linearly independent. And it's just a very straightforward argument, because each column is going to have one in a very unique place. All of the other pivot columns are going to have a 0 in that same place. And so you can't take any linear combination to get to that 1, because 0 times anything minus or plus 0 times anything can never be equal to 1. So I think you can accept that. Now that means that the solution to C1 times R1 plus C2 times R2 plus, let me say C4 times R4, the solution to this equation, because these guys are linearly independent, we know that this only has one solution. And that's C1, C2, and C4 is equal to 0. That's the only solution to that. So another way we could say it is, if we write R times some vector x, some vector, well, I'll just write it times this particular x, where I write it as C1, C2, 0, C4, and 0 is equal to 0. So this will be some special member of your null space. It's a particular solution to the equation. This is equal to 1, 2, 3, 4, 0, because we have four rows here. Now, if we just expand this out, if we just multiply 1 times C1 plus 0 times C2 minus 1 times 0, you'll get, or actually, let me do a better way to explain it. This multiplication right here can be written as, and we've seen this multiple times, C1 times R1 plus C2 times R2 plus 0 times R3, so we could just ignore that term, plus C4 times R4 plus 0 times R5, that's R5 right there. All of that equal to 0. So the only solution to this, because we know that these three columns are linearly independent, or the set of just those three pivot columns are linearly independent, the only solution here is all of these equal to 0. That's exactly what I said right up here. So the only solution here, where if these two are 0, is that these guys also all have to equal 0, if I already constrained these two. Now, the one thing that we've done over and over again, we know that the solution set of this equation, the solution set of Rx is equal to 0, is the same as the solution set of Ax is equal to 0. Now, how do we know that? Or what do I mean? Well, the solution set of this is just the null space. The solution set is just the null space of R. It's all of the x that satisfy this equation, and we know that that is equal to the null space of A, because R is just A in reduced row echelon form. So the null space of A is all of the x's that satisfy this equation. Now, the only version of this that satisfied this equation was when C1, C2, and C4 are equal to 0. So that tells us that the only version of this C1, C2, 0, C4, 0 that satisfies this equation, or this equation, is when C1, C2, and C4 is equal to 0. Or another way of saying that, if this is vector A1, A2, A4 right here, if you multiply this out, you get C1, let me do it over here, you get C1 times A1 plus C2 times A2, and then 0 times A3 plus C4 times A4 is equal to 0. Now, these guys are going to be linearly independent if and only if the only solution to this equation is they all equal to 0. Well, we know that the only solution to this is that they all equal to 0, because anything that's a solution to this is a solution to this. And the only solution to this was, if I go ahead and I constrain these two terms to being equal to 0, the only solution to this is all of these C's have to be 0. So likewise, if I constrain these to be 0, the only solution to this is that C1, C2, and C4 have to be 0. So those guys have to be 0, which imply that these three vectors, A1, A2, and A4, so that implies that the set A1, A2, and A4 are linearly independent. So we're halfway there. We've shown that, look, because the pivot columns here are linearly independent, they have the same solution set. The null space of the reduced row echelon form is the same as the null space of our original matrix. We were able to show that the only solution to C1 times this plus C2 times this plus C4 times this is when all the constants are 0, which shows that these three vectors, or set of those three vectors, are definitely linearly independent. Now, the next thing to prove that they are a basis is to show that, look, all of the other column vectors can be represented as multiples of these three guys. And I realize just for the sake of clarity, or maybe not boring you too much, I'll do that in the next video. So in this one, we saw that, look, if the pivot columns are linearly independent, they always are, all pivot columns by definition are linearly independent, or the set of pivot columns are always linearly independent when you take away the non-pivot columns, then the corresponding pivot columns in your original vector are also linearly independent. And the next one will show that these three guys also span your column space.