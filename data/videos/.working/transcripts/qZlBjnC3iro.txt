So in the last couple videos, I talked about the multivariable chain rule, which I have written up here, and if you haven't seen those, go take a look. And here I want to write it out in vector notation, and this helps us generalize it a little bit when the intermediary space is a little bit higher dimensional. So instead of writing x of t and y of t as separate functions and just trying to emphasize, oh, they have the same input space and whatever x takes in, that's the same number y takes in, it's better and a little bit cleaner. If we say there's a vector-valued function, it takes in a single number t, and then it outputs some kind of vector. In this case, you could say the components of v are x of t and y of t, and that's fine. But I want to talk about what this looks like if we start writing everything in vector notation. And just since we see dx, dt, and dy, dt here, you might start thinking, oh, well, which should take the derivative of that vector-valued function, the derivative of v with respect to t. And when we compute this, it's nothing more than just taking the derivatives of each component. So in this case, the derivative of x, so you'd write dx, dt, and the derivative of y, dy, dt. This is the vector-valued derivative. And now you might start to notice something here. Okay, so we've got one of those components multiplied by a certain value, and another component multiplied by a certain value. You might recognize this as a dot product. This would be the dot product between the vector that contains the derivatives, the partial derivatives, partial of f with respect to y, partial of f with respect to x. Whoops, don't know why I wrote it that way. So up here, that's with respect to x, and then here to y. This whole thing, we're taking the dot product with the vector that contains ordinary derivative, dx, dt, and ordinary derivative, dy, dt. And of course, both of these are special vectors. They're not just random. The left one, that's the gradient of f, and the right vector here, that's what we just wrote. That's the derivative of v with respect to t. Just for being quick, I'm going to write that as v' of t. That's saying completely the same thing as dv dt. And this right here is another way to write the multivariable chain rule. And maybe if you were being a little bit more exact, you would emphasize that when you take the gradient of f, the thing that you input into it is the output of that vector-valued function. You're throwing in x of t and y of t. So you might emphasize that you take in that as an input, and then you multiply it by the derivative, the vector-valued derivative of v of t. And when I say multiply, I mean dot product. These are vectors, and you're taking the dot product. And this should seem very familiar to the single-variable chain rule. And just to remind us, I'll throw it up here. If you take the derivative of a composition of two single-variable functions, f and g, you take the derivative of the outside, f' and throw in g, throw in what was the interior function, and you multiply it by the derivative of that interior function, g' of t. And this is super helpful in single-variable calculus for computing a lot of derivatives. And over here, it has a very similar form, right? The gradient, which really serves the function of the true extension of the derivative for multivariable functions, for scalar-valued multivariable functions at least, you take that derivative and throw in the inner function, which just happens to be a vector-valued function, but you throw it in there, and then you multiply it by the derivative of that. But multiplying vectors in this context means taking the dot product of the two. And this could mean if you have a function with a whole bunch of different variables, so let's say you have some f of x, or not f of x, but f of x1 and x2, and it takes in a whole bunch of different variables, and it goes out to x100. And then what you throw into it is a vector-valued function that's something that's vector-valued, takes in a single variable, and in order to be able to compose them, it's going to have a whole bunch of intermediary functions. You can write it as x1, x2, x3, all the way up to x100, and these are all functions at this point. These are component functions of your vector-valued v. This expression still makes sense, right? You can still take the gradient of f, it's going to have 100 components. You can plug in any vector, any set of 100 different numbers, and in particular, the output of a vector-valued function with 100 different components is going to work, and then you take the dot product with the derivative of this. And that's the more general version of the multivariable chain rule. And another cool way about writing it like this is you can interpret it in terms of the directional derivative. And I think I'll do that in the next video, I won't do that here, so a certain way to interpret this with the directional derivative.