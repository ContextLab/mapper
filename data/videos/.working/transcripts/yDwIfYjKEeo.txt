In the last couple of videos, we've seen that if we have some matrix C that is n by n, it's a square matrix, and its columns form an orthonormal set, which just means that the columns are each, they've each been normalized, so they each have length of 1, if you view them as column vectors. And they're all mutually orthogonal to each other. So if you dot it with yourself, you get 1. If you dot it with any of the other columns, you get 0. We've seen this multiple times. It's orthogonal to everything else. If you have a matrix like this, and I actually forgot to tell you the name of this. This is called an orthogonal matrix. We've already seen that the transpose of this matrix is the same thing as the inverse of this matrix, which makes it super, super, duper useful to deal with. The transpose of this matrix is equal to the inverse. Now this statement leads to some other interesting things about this. So so far we've been dealing this mainly with the change of basis. I can kind of draw the diagram that you're probably tired of by now. If I have some, let's say that's the standard basis. Let's say that I have x in coordinates with another basis. We've seen I can multiply this guy times C to get that up there, I could multiply that guy by C inverse to get this guy right here. In that world we viewed C as just a change of basis. We're representing the same vector. We're just changing the coordinates of how we represented. But we also know that any matrix product, any matrix vector product, is also a linear transformation. This change of basis is really just a linear transformation. What I want to show you in this video, and you can view it either as a change of basis or as a linear transformation, is that when you multiply this orthogonal matrix times some vector, it preserves. Let me write this down. It preserves. Lengths and angles. So let's have a little touchy-feely discussion of what that means. So let's view it as a transformation. Let's say I have some set of vectors in my domain. Let's say they look like this. Let me do it like that one like that guy, and then this guy like that. There's some angle between them. Angles are easy to visualize in R2, R3, maybe a little harder once we get to higher dimensions. But that's the angle between them. Now, if we're saying that we're preserving the angles and the lengths, that means if I were to multiply these vectors times c, then we can view it as a transformation. Maybe I rotate them or do something like that. So maybe that pink vector will now look like this. But it's going to have the same length. This length is going to be the same thing as that length. And even more, when I said it preserves lengths and angles, this yellow vector is going to look something like this, where the angle is going to be the same, where this theta is going to be that theta. That's what I mean by preserves angles. If we didn't have this case, we could imagine a transformation that doesn't preserve angles. Let me draw one that doesn't. If this got transformed to, I don't know, let's say this guy got a lot longer, and then let's say this guy also got longer, and actually I want to show that the angle also doesn't get preserved. Not only did it get longer, but it got distorted a little bit. So the angle also changed. This transformation right there is not preserving angles. So when you have a change of basis matrix that's orthogonal, or when you have a transformation matrix that's orthogonal, all it's essentially doing to your vectors is that it can kind of rotate them around, but it's not going to really distort them. And I'll write that in quotes, because it's not a mathematically rigorous term. So no distortion of vectors. So I've kind of shown you the intuition of what that means. Let's actually prove it to ourselves that this is the case. So I'm saying that if I have, let's say this pink vector here is x, and that this pink vector here is c times x, I'm claiming that the length of x is equal to the length of c times x. Let's see if that's actually the case. So the length of x squared, or let me do the length of, let me write it this way, the length of c x squared is the same thing as c x dot c x. And here it's always useful for me to kind of remind myself that if I take two vectors, let me do it over here. Let's say I have y dot y. This is the same thing as y transpose. If you view them as matrices, y transpose times y. y transpose y is just y1, y2, all the way to yn times y1, y2, all the way to yn. And if you were to do this 1 by n times n by 1 matrix product, you're going to get a 1 by 1 matrix, or just a number, that's going to be y1 times y1 plus y2 times y2 all the way to yn times yn. This is the same thing as y dot y. So let's, I think I did this 10 or 20 videos ago, but it's always good refresher. So let's use this property right here. So these two dotted with each other, this is the same thing as taking one of their transpose times the other one, so turn this from a vector dot product to a matrix product. So this is the same thing as Cx transpose, so you can view this as a 1 by n matrix now, times the n by 1 matrix, which is just the column vector Cx. These are the same thing. Now we also know that a times b transpose is the same thing as b transpose a transpose. We saw that a long time ago. So this thing right here is going to be equal to x transpose c transpose. I just switched the order and take the transpose of each. x transpose times c transpose. And then you have that times cx. And now we know that c transpose is the same thing as c inverse. This is where we need the orthogonality of the matrix C. This is where we need it to be a square matrix where all of its columns are mutually orthogonal and they're all normal. So this thing is just going to become the identity matrix. So I could write the identity matrix there, but that's just going to disappear. So this is going to be equal to x transpose x. x transpose x is the same thing as x dot x, which is the same thing as the length of x squared. So the length of Cx squared is the same thing as the length of x squared. Well, that tells us that the length of x, or the length of Cx is the length of x. Because both of these are going to be positive quantities. So I've shown you that orthogonal matrices definitely preserve length. Let's see if they preserve angles. So we actually had to define angles. Because throughout our mathematical careers, we understood what angles mean in R2 or R3. But in linear algebra, we like to be general. And we defined an angle using the dot product. So we use a lot of cosines. And we took an analogy to kind of a triangle in R2. But we defined an angle, or we said that the dot product v. Let me write it this way. Oh, I'll just write it this way. v dot w. It's equal to the lengths of those two vectors, the products of the lengths of those two vectors, times the cosine of the angle between them. Or you could say that the cosine of the angle between two vectors, we defined as the dot product of those two vectors divided by the lengths of those two vectors. This was a definition so that we could extend the idea of an angle to an arbitrarily high dimension, to our Google if we had to. So let's see if it preserves. So let's see what the angle is if we multiply these guys by c. So if we want to say our new angle, so cosine of theta c. Once we perform our transformation. So our new angle, well, we're going to perform the transformation on all of these characters. It's going to be cv dot cw over the lengths of cv times the length of cw. Now, we already know that lengths are preserved. We already know that the length of cw and cv are just going to be w and v. We just proved that. So let me write that. So the code sine of theta c is equal to cv dot cw over the lengths of v times w. Because we know we already shown that it preserved length. So let's see what this top part equals. So we could just use the general property, the dot product is equal to the transpose of one guy as kind of a matrix times the second guy. So this is equal to cw transpose times cv. And all of that over these lengths, the length of w. And then this is going to be equal to, I'm going to write it down here to have some space. We can switch these guys and take their transpose. So it's w transpose times c transpose times cv. All of that over their lengths, the product of their lengths, v and w. And this is the identity matrix. So this is going to be equal to w transpose times v over the products of their lengths. And this is the same thing as v dot w. This is v dot w over their lengths, which is cosine of theta, which is equal to cosine of theta. So notice by our definition of an angle is the dot product divided by the vector lengths. When you perform a transformation, or you could imagine a change of basis, either way with an orthogonal matrix c, the angle between the transformed vectors does not change. It's the same as the angle between the vectors before they were transformed. Which is a really neat thing to know, that change of basis or transformations with orthogonal matrices don't distort the vectors. They might just kind of rotate them around or shift them a little bit, but it doesn't change the angles between them.