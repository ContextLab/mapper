Let's say I have some subspace V that is a subset of Rn. Let's say we also have its orthogonal complement. So we write that as Vperp. That'll also be a subset of Rn. Now a couple of videos ago, it might have even been the last video if I remember properly, we learned that the dimension of V plus the dimension of our orthogonal complement of V, which is also another subspace, is going to be equal to n. And remember, dimension is just the number of vectors you need to have the number of linearly independent vectors you need to have a basis for V. And the dimension here is the number of linearly independent vectors you need to have a basis for the orthogonal complement of V. Now given this, let's see if we can come up with some other interesting ways in which these two subspaces relate to each other, or how they might relate to all of the vectors in Rn. So the first question is, do these two subspaces have anything in common? Are there any vectors that are in common with the two? And to test whether there is, let's just assume there is, and see what the properties of that vector would have to be. Let's assume right here that I have some vector x. That I have some vector x that is a member of my subspace V. Let's also assume that x is a member of the orthogonal complement of V. So x is a member of the orthogonal complement of V. Now what does the second statement mean? Membership in the orthogonal complement means, well let me write it this way. This means that x dot V for any V that is a member of our subspace is going to be equal to 0. Let me write it this way actually. x dot V is equal to 0 for any V that is a member of our subspace. That's what it means to be a member of V's orthogonal complement. Now we assume that x is also a member of V. So that means that we can stick x here as well for any member of V. x is also a member of V. So that implies that x dot x is equal to 0. Or another way to write that is that the length of x squared is equal to 0. Or the length of x is equal to 0. And that's only true for one vector. You can even try it out with the different constituents of x. The only vector that that's true for is the 0 vector. So x has to be equal to 0 vector. That's the only vector in Rn that when you dot it with itself you get 0. Or whose the square of its length is equal to 0. And we've shown that many, many, many videos ago. So what this tells us is that the intersection between V and V complement, this kind of upside down U, just means intersection. It just means where do these two sets overlap? The only place that these overlap is with the subset of the 0 vector. So if I were to draw all of Rn like this, let's say that this is Rn. Let's say that's Rn. And let's say I draw the subspace V. Let's say it looks like this. And let's say I draw this is V, my subspace V. And let's say I draw the orthogonal complement to V. Let's say it's all of these vectors right here. Let's say this is the orthogonal complement to V right there. So this is V perp. These are all of the vectors for which if I take any vector, these are all of the vectors that when I dot it with any vector here I'm going to get equal to 0. So this is V perp. And the intersection, their overlap, the only vector that is a member of both, the only vector that is a member of both right there is the 0 vector. That's their only intersection. So that's fair enough. The only vector that's a member of a subspace in its orthogonal complement is the 0 vector. Nothing too profound there. Let's see if we can come up with some other interesting relations between the subspace and its orthogonal complement and maybe some arbitrary vectors in Rn. So let's just write down. Well, we know that the dimension of our subspace V is equal to k. If it's equal to k, we know that its dimension plus its orthogonal complement has to be equal to n, because we're dealing in Rn. Let me write this. V is a subset of Rn. And we also know that the orthogonal complement of V is a subset of Rn. I drew it right here. The dimension of V is equal to k. That's a k right there. Then what's the dimension of the orthogonal complement of V going to be? Well, when you add them together, I wrote that up here, they have to equal n. So this guy's going to have to be n minus k. If you have k here, this guy's dimension is k. This guy's dimension right here. If it's n minus k, then when you add these two up, k plus n minus k is going to be equal to n. So this guy will have a dimension of n minus k. Now what does dimension mean? It means that that's the number of linearly independent vectors you need to form a basis. So let's say that I have k vectors as a basis for V. So let's say I have V1, V2, all the way to Vk. And this is a basis for V, which just means they're all linearly independent. And they span V. Any member of V right here can be represented as a linear combination of these vectors. Fair enough. Now the dimension of the orthogonal complement of V is n minus k. So we could have n minus k vectors. Let's call them w1, w2, all the way to wn minus k. We have n minus k of these characters. And this set is a basis for the orthogonal complement of V. So any vector in here can be represented as a linear combination of these guys right here. And all of these guys are linearly independent. So you don't have any redundant vectors there, if you will. Now let's explore. And I'll tell you where I'm trying to go. I'm trying to see if I combine these two sets, whether I get a basis for all of Rn. That's what I'm trying to understand. So let's just say that for some constants, C1 times V1 plus C2 times V2 plus all the way to Ck times Vk plus, for the constants on these guys I'll use d, plus d1 times w1 plus d2 times w2 all the way to plus dn minus k times the basis vector wn minus k. Let's say that I'm curious about setting this sum equal to 0, equaling the 0 vector. For some scalars, the scalars are the Cs and these ds. For some scalars. And we know that there's at least one solution set of scalars, for which this is true. We could multiply all of these constants, C1, C2, Ck, d1, d2, all the way to dn minus k. They could all be 0. Or there might be more than one solution. In fact, if the only solution is that all of these constants have to be equal to 0, then we know that all of these vectors are linearly independent with respect to each other. And if they're all linearly independent with respect to each other, then we know, well, I'll touch on that at the end of the video, we know that they could be a basis for Rn. But we don't know that yet. We don't know that the only solution to this is all of the constants being equal to 0. So let's see if we can experiment with this a little bit. If we take this equation, which I just wrote down, we know that one solution is all of the constants, the Cs and ds equaling to 0. But we don't know that that's the only one. Let's just subtract all of the w vectors from both sides of this equation. So what are we going to get? We're going to get C1, V1 plus C2, V2, all the way to plus Ck, Vk. And we're going to subtract this from both sides of the equation. It's going to be equal to the 0 vector, which is really just 0, I don't even have to write it down. But maybe I'll write it down there just so you understand. I'm just taking this equation, I'm subtracting these guys from both sides. So 0 vector minus d1, w1 plus d2, w2 plus all the way to dn minus k, wn minus k. And all I did is I subtracted these terms right here from both sides of this equation. And I don't even have to write this as 0 here, that's a bit redundant. So what I have here is I have some combination of the basis vectors of V. So if I look at this, this is some linear combination of the basis vectors in V. So if I call this a vector, let me call this some vector x, let's say x is equal to C1, V1 plus C2, V2, all the way to Ck, Vk. We know that it's a linear combination of our basis vectors of V. So x is a member of V. By definition, any linear combination of the basis vectors for subspace is going to be a member of that subspace. Well, similarly, what do we have on the right hand side of this equation? On the right hand side of this equation, I have some linear combination of the basis vectors of V complement. So this right here, and you could just put a minus all along that, but that won't change the fact that this is some linear combination. This is some linear combination of V complements basis vectors. So this vector over here is going to be a member of, let me call this, this could be some other vector, we're going to be equal to each other, so we could also call this x. So x is equal to this, but it's also going to be equal to this, and since it can be represented as a linear combination of the orthogonal complement of these basis vectors, or V perp's basis vectors, we know that this also has to be a member of V perp. Let me just review this, because it can be a little bit confusing. I just set up this equation right here. We know that there's at least one solution, all of the constants equaling to 0. Anyone could do this. Now I subtracted all of the yellow terms from both sides, and I got this equality. The left-hand side of this equality is linear combinations of the basis vectors of V. So any linear combinations of the basis vectors of V is going to be a member of V. That's the definition of basis vectors. So if I set x equaling to this left-hand side, so if I say x is equal to this left-hand side, I can say that x is a member of V. Well, if x is equal to the left-hand side, it's also equal to the right-hand side. But the right-hand side is some linear combination of V perps, or the orthogonal complement of these basis vectors, which tells us that x is also a member of V perp. But what does that mean? That means that x must be equal to 0. I just showed you at the beginning of the video, the only vector that's a member of a subspace and its complement is the 0 vector. So we know that because these are orthogonal complements, or at least V perp is the orthogonal complement of V, we know that x must be equal to 0. So if x is equal to 0, let's write this down here. x is equal to 0. So we know zeros has to equal these both of these sides of the equation, and these are the same constants that we had to begin with. But what do we know about these two sets? Let me write this a little bit neater. Let me erase this so that I can just write the 0 vector right there. So we know that the 0 vector has to be equal to this. That's the only vector in Rn that's a member of both of V and of the orthogonal complement of V. Now, this is a 0 vector, and we have this linear combination of V's being set equaling to the 0 vector. Now, what do we know about these constants? What does C1, C2, all the way to Ck have to be? We know that V1 through Vk is a basis for V. That tells us that they span V and that they are linearly independent. Linear independence by definition means that the only solution to this equation right here is that all of the constants have to be 0. So the linear independence tells us that C1, C2, all the way through Ck must be 0. Must be 0. So all of these guys right here are 0, which is the same as all of these guys. All of these guys must be 0. Now, let's look at the right hand side of this equation. We could put the minus all the way, but the same argument holds. This linear combination of V perp's basis vectors is equal to 0. The only solution to this, because each of these w1's, w2's, and wn minus k's are linearly independent, the only solution to this being equal to 0 is all of the constants have to be equal to 0. That falls out of linear independence. You could just multiply this. If this negative is confusing a bit, if it makes it look different than that, you could just multiply this negative out and say, oh, well, those would be minus d1 would have to be equal to 0, minus d2 would have to be 0, minus dn minus k would have to be 0. But it's the exact same argument. Linear independence, which falls out of the fact that this is a basis set, implies that the only solution to this being equal to 0 is each of the constants being equal to 0. Well, that means that d1, d2, all the way to dn minus k must be 0. Now let's go back to what I wrote up here. This was the original equation that we were experimenting with. Just manipulating this equation a bit and understanding that the only intersection between v and v-purpose is a 0 vector, and that linear independence means that you only have linear independence if the only solution to these vectors equal to 0 is all of their constants equal to 0. Then we know that all of these terms, c1 through ck, d1 through dn minus k, they all have to be equal to 0. That's the only solution to this larger equation that I wrote up here. Well, the only solution to this large equation that I wrote up here is that all of the constants are equal to 0. That implies, all of this implies, that if I were to take the set right here of v1, v2, all the way to vk, and I were to augment that with the basis vectors of v-purpe, which are w1, I want to do that in a different color, w1, w2, all the way to wn minus k, that this is a linearly independent set. And I know that because the only solution to this equation is each of these constants having to be equal to 0. That's what linear independence means. This implies this, linear independence, linear independence implies that. We used the fact that linear independence implies that all of these equal 0 to get the fact that c1 was all the way to ck was equal to 0, and we've got this right here. And then we used it again when we set this thing also being equal to the 0 vector. We knew that all of the d's had to be equal to 0. I don't know if you remember, the 0 vector came out from the fact that that was the only vector that is a member of both sets. I know I'm being a little bit repetitive, but I really want you all to understand that this proof isn't some type of circular proof. We just wrote this equation. We wondered about what the solution set is to it. We rearranged it. We said, hey, both sides of this equation are members of both v and v-purpe. The only vector that's a member of both is the 0 vector. So both of these sides of the equation have to be equal to 0. The only solution to that is all of these constants being equal to 0, because each of these are linearly independent sets. So therefore, all of these constants have to be equal to 0, and then this augmented set, where if you combined all of the basis vectors, that that is going to be linearly independent. Now, many, many, many, many, many videos ago, we learned that if we have some subspace with dimension, with dimension n, and we have n vectors that are, and let me write it this way, n linearly independent vectors that are members of your subspace, that are members of your subspace, then those n linearly independent vectors, then those n vectors, or the set of your n vectors, let me write that a little bit, the set of those n vectors, is a basis for the subspace. Now, Rn is a subspace of itself. Rn is a n dimensional subspace. Rn is an n dimensional subspace. We could write the dimension of Rn is equal to n. Now, we have n linearly independent vectors in Rn. So that tells us that these guys right here are a basis. They are a basis for Rn. We have n linearly independent vectors. We have n minus k that are coming from v perp, and we have n that are coming from v, from the basis for those subspaces. So now we have a total of n vectors. They're linearly independent. They're all members of Rn. So they are a basis for Rn, which tells us that any vector in Rn can be represented by linear combinations of these guys, which is fascinating. So this tells us, so this is a basis for Rn. So that tells us that we can take any vector. Let's say a is a member of Rn, some vector. That means, since this is a basis for Rn, that a can be represented as some linear combination of all of these guys. So it can be represented as c1 times v1 plus c2 times v2 plus all the way to plus ck times vk. And then let me use a different letter just to make sure that you understand that this is a different equation than I'm writing, than I wrote earlier in the video. So I could write this, and then I could have some other constants. Let's say plus, I don't know, e1 times our v perp basis vector 1 plus e2 times this guy plus all the way to en minus k times the n minus k basis vector for v perp. I can represent any vector in Rn this way. Or another way to say it, what is this? What is this right here? This is some vector that is a member of our subspace v. And then this is some vector over here. Let me write it as x. That is a member of the orthogonal complement of v. This is just a linear combination of v perp's basis vectors. This is just a linear combination of these basis vectors. So given that all of these characters are our basis for Rn, tells us that any member of Rn can be represented as a linear combination of them. But that means that any member of Rn can be represented as a sum of a member of our subspace v plus a member of your subspace v perp. This is a member of v, and this is a member of v perp. And that's a really, really interesting idea. You give me a subspace, you give me a subspace, and then we can figure out its orthogonal complement. Any other vector in Rn can be represented as a combination or a sum of some vector in our subspace and some vector in its orthogonal complement. Now the next question you might be asking, is this representation unique? So is this unique? Let's test it out. Let's test it out by assuming it's not unique. So that means that I have, so for some vector a that is a member of Rn, I could represent it two ways. I could represent it as equaling some member of my subspace v plus some member of the orthogonal complement of v, I could represent it that way, or I could represent it as some other member of my subspace v plus some other member of my orthogonal complement. The x's, so x1, x2 are members of v perp, and then v1 and v2 are members of v. So if we assume it's not unique, there's two ways that I could do this, and I'm representing it as these two vectors. Now, clearly, this side of this equation is equal to that. These are both representations of a. So we can rearrange this a little bit. We could say that v1 minus v2, if I subtract v2 from both sides, I get v1 minus v2 is equal to, if that's subtracting v2 from both sides, and if I subtract x1 from both sides, it's equal to x2 minus x1. Now, if I subtract, these are both members of the subspace v. And any subspace is closed under addition, which is, and subtraction, which is really just almost a special case of addition. So if this is a member, so v1, this vector right here, the vector v1, let me write it this way. Let me call some vector z being equal to both of these guys, which are equal to each other. z is the vector v1 minus v2. Any subspace is closed under addition, so if you take the difference of two vectors in the subspace, if you take two vectors, you find their difference in the subspace, then the resulting difference is also going to be in the subspace. So z is going to be a member of our subspace v. And then this vector right here, which is also the same thing, we just set that to be equal to our vector z, this thing right here is going to be a member of our vperp. Why? Because both x1 and x2 are members of the subspace v's orthogonal complement. And that is a subspace as well. This is a subspace, so it is closed under addition and subtraction. So this is also going to be a member of your subspace. So we could also say that z is a member of vperp, or the orthogonal complement of v. Well, we've done this multiple times. This was the first thing we showed in the video. The only vector that's a member of a subspace and its orthogonal complement is the zero vector. So z has to be equal to the zero vector. So this is equal to the zero vector. Well, both of these are equal to the zero vector. We know that v1 minus v2 is equal to the zero vector, which implies that v1 must be equal to v2. And we also know that x2 minus x1 is equal to the zero vector, or x2 is equal to x1. So we tried to say that, hey, there's two ways to construct some arbitrary vector a that's an rn, and we set that equal, and we wrote that down. But then we found out that, no, v1 must be equal to v2, and x1 must be equal to x2. So there's only a unique way to write any member of rn as a sum of a vector that's in our subspace v, and a vector that is in the orthogonal complement of v.