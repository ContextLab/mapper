{
  "domain": "artificial-intelligence-ml",
  "parent_domain": "computer-science",
  "total_concepts": 50,
  "distribution": { "L1": 13, "L2": 13, "L3": 12, "L4": 12 },
  "concepts": {
    "L1": [
      {"concept": "artificial intelligence", "wikipedia_article": "Artificial intelligence", "brief_rationale": "The broad field of creating machines that perform tasks requiring human-like intelligence; the umbrella term universally recognized in popular culture and every introductory course."},
      {"concept": "machine learning", "wikipedia_article": "Machine learning", "brief_rationale": "The subfield of AI in which systems learn from data rather than being explicitly programmed; the dominant paradigm driving modern AI and widely known outside academia."},
      {"concept": "neural network", "wikipedia_article": "Neural network (machine learning)", "brief_rationale": "A computational model loosely inspired by biological neurons, organized in interconnected layers; the central architectural concept behind deep learning and universally recognized."},
      {"concept": "deep learning", "wikipedia_article": "Deep learning", "brief_rationale": "Machine learning using neural networks with many layers; the technology behind image recognition, language models, and speech systems, widely known in mainstream discourse."},
      {"concept": "algorithm", "wikipedia_article": "Algorithm", "brief_rationale": "A step-by-step procedure for solving a problem or performing a computation; the foundational concept underlying all of computer science and AI."},
      {"concept": "training data", "wikipedia_article": "Training, validation, and test data sets", "brief_rationale": "The dataset used to fit a machine learning model; universally encountered at the first introduction to ML and critical to understanding model behavior."},
      {"concept": "supervised learning", "wikipedia_article": "Supervised learning", "brief_rationale": "Learning paradigm where a model is trained on labeled input-output pairs; the most common ML setting and the first category taught in any ML course."},
      {"concept": "unsupervised learning", "wikipedia_article": "Unsupervised learning", "brief_rationale": "Learning paradigm where a model finds structure in unlabeled data; universally contrasted with supervised learning as one of the two primary ML paradigms."},
      {"concept": "classification", "wikipedia_article": "Statistical classification", "brief_rationale": "The task of assigning inputs to discrete categories; one of the two most fundamental ML tasks alongside regression and encountered in every introductory treatment."},
      {"concept": "regression", "wikipedia_article": "Regression analysis", "brief_rationale": "The task of predicting a continuous output from inputs; the other most fundamental ML task and universally taught as the entry point into predictive modeling."},
      {"concept": "robot", "wikipedia_article": "Robot", "brief_rationale": "A machine capable of carrying out complex actions automatically; universally associated with AI in popular culture and foundational to robotics as an applied AI domain."},
      {"concept": "natural language processing", "wikipedia_article": "Natural language processing", "brief_rationale": "The subfield of AI concerned with enabling computers to understand and generate human language; widely known due to chatbots, search engines, and voice assistants."},
      {"concept": "computer vision", "wikipedia_article": "Computer vision", "brief_rationale": "The subfield of AI focused on enabling machines to interpret and understand visual information; universally recognized through applications like facial recognition and self-driving cars."}
    ],
    "L2": [
      {"concept": "gradient descent", "wikipedia_article": "Gradient descent", "brief_rationale": "The iterative optimization algorithm that updates model parameters in the direction of steepest loss decrease; the named workhorse optimization method every ML student learns first."},
      {"concept": "overfitting", "wikipedia_article": "Overfitting", "brief_rationale": "When a model learns the training data too well, including noise, and generalizes poorly to new data; the central named failure mode taught in every introductory ML course."},
      {"concept": "convolutional neural network", "wikipedia_article": "Convolutional neural network", "brief_rationale": "A neural network architecture using convolutional filters designed for grid-structured data like images; the named architecture that revolutionized computer vision and is widely recognized by students."},
      {"concept": "recurrent neural network", "wikipedia_article": "Recurrent neural network", "brief_rationale": "A neural network architecture with cyclic connections suited for sequential data; the named architecture foundational to sequence modeling before transformers."},
      {"concept": "backpropagation", "wikipedia_article": "Backpropagation", "brief_rationale": "The algorithm for computing gradients in neural networks by applying the chain rule backwards through the computation graph; the named algorithm every deep learning student encounters early."},
      {"concept": "reinforcement learning", "wikipedia_article": "Reinforcement learning", "brief_rationale": "A learning paradigm in which an agent learns by receiving rewards and penalties from an environment; the named third ML paradigm widely known through game-playing AI like AlphaGo."},
      {"concept": "decision tree", "wikipedia_article": "Decision tree learning", "brief_rationale": "A tree-structured model that partitions input space through a sequence of binary splits; the named interpretable model universally taught as a foundational ML algorithm."},
      {"concept": "support vector machine", "wikipedia_article": "Support vector machine", "brief_rationale": "A classifier that finds the maximum-margin hyperplane separating classes in feature space; a named classic ML method studied in every undergraduate ML course."},
      {"concept": "k-means clustering", "wikipedia_article": "K-means clustering", "brief_rationale": "An unsupervised algorithm partitioning data into k clusters by iteratively assigning points to nearest centroids; the named canonical clustering algorithm introduced in every ML course."},
      {"concept": "transfer learning", "wikipedia_article": "Transfer learning", "brief_rationale": "Reusing a model trained on one task as a starting point for a related task; a named technique widely discussed due to its practical importance in fine-tuning large pretrained models."},
      {"concept": "generative adversarial network", "wikipedia_article": "Generative adversarial network", "brief_rationale": "A framework pairing a generator and discriminator trained adversarially to produce realistic synthetic data; a named architecture widely recognized for image generation applications."},
      {"concept": "transformer", "wikipedia_article": "Transformer (deep learning architecture)", "brief_rationale": "The attention-based neural network architecture underlying modern large language models; a named architecture familiar to any current ML student as the dominant paradigm in NLP."},
      {"concept": "large language model", "wikipedia_article": "Large language model", "brief_rationale": "A neural language model trained on massive text corpora with billions of parameters; the named class of systems behind ChatGPT and similar widely-discussed AI tools."}
    ],
    "L3": [
      {"concept": "bias-variance tradeoff", "wikipedia_article": "Bias–variance tradeoff", "brief_rationale": "The tension between a model's ability to minimize bias (systematic error) and variance (sensitivity to training data); requires working knowledge of how model complexity governs generalization and how to diagnose underfitting vs. overfitting."},
      {"concept": "regularization", "wikipedia_article": "Regularization (mathematics)", "brief_rationale": "Techniques that add a penalty on model complexity to the loss function to reduce overfitting; requires working knowledge of L1 vs. L2 penalties, their geometric interpretations, and their effect on learned weights."},
      {"concept": "cross-validation", "wikipedia_article": "Cross-validation (statistics)", "brief_rationale": "Model evaluation procedure that partitions data into multiple train-test folds to estimate generalization error; requires working knowledge of k-fold strategy, its bias-variance properties, and proper use for model selection."},
      {"concept": "batch normalization", "wikipedia_article": "Batch normalization", "brief_rationale": "A technique normalizing layer activations over each training mini-batch to stabilize and accelerate deep network training; requires working knowledge of how it reduces internal covariate shift, its learnable parameters, and behavior differences at train vs. test time."},
      {"concept": "attention mechanism", "wikipedia_article": "Attention (machine learning)", "brief_rationale": "A mechanism allowing models to dynamically weight different parts of an input sequence when producing each output; requires working knowledge of query-key-value computation, softmax weighting, and how self-attention enables transformers to capture long-range dependencies."},
      {"concept": "word embedding", "wikipedia_article": "Word embedding", "brief_rationale": "Dense vector representations of words learned so that semantic similarity corresponds to geometric proximity; requires working knowledge of how models like Word2Vec and GloVe are trained and how embeddings encode relational structure."},
      {"concept": "Markov decision process", "wikipedia_article": "Markov decision process", "brief_rationale": "The formal framework for sequential decision-making comprising states, actions, transition probabilities, and rewards; requires working knowledge of the Bellman equations and how MDPs underpin reinforcement learning algorithms."},
      {"concept": "random forest", "wikipedia_article": "Random forest", "brief_rationale": "An ensemble method combining many decision trees trained on bootstrap samples with random feature subsets; requires working knowledge of bagging, feature randomness, how out-of-bag error is computed, and why ensembling reduces variance."},
      {"concept": "principal component analysis", "wikipedia_article": "Principal component analysis", "brief_rationale": "Dimensionality reduction technique projecting data onto orthogonal directions of maximum variance via eigendecomposition of the covariance matrix; requires working knowledge of how principal components are derived and how variance explained guides dimensionality choice."},
      {"concept": "vanishing gradient problem", "wikipedia_article": "Vanishing gradient problem", "brief_rationale": "The phenomenon where gradients shrink exponentially as they are backpropagated through deep or recurrent networks, making early layers learn slowly; requires working knowledge of why it occurs with saturating activations and how remedies like ReLU, residual connections, and gradient clipping address it."},
      {"concept": "dropout", "wikipedia_article": "Dropout (neural networks)", "brief_rationale": "Regularization technique randomly zeroing a fraction of neurons during each training forward pass to prevent co-adaptation; requires working knowledge of how it approximates model averaging, its effect on effective network capacity, and its disable at inference time."},
      {"concept": "BERT", "wikipedia_article": "BERT (language model)", "brief_rationale": "Bidirectional Encoder Representations from Transformers; a masked language model pretrained on large text corpora that set new benchmarks across NLP tasks; requires working knowledge of masked language modeling, next-sentence prediction, and how fine-tuning adapts it to downstream tasks."}
    ],
    "L4": [
      {"concept": "PAC learning", "wikipedia_article": "Probably approximately correct learning", "brief_rationale": "A formal framework for analyzing the sample complexity needed to learn a concept class to arbitrary accuracy with high probability; requires deep knowledge of the VC dimension, sample complexity bounds, and the relationship between combinatorial complexity and learnability."},
      {"concept": "VC dimension", "wikipedia_article": "VC dimension", "brief_rationale": "A measure of the capacity of a hypothesis class defined as the largest set of points that can be shattered by classifiers in the class; requires deep knowledge of how it bounds generalization error via uniform convergence and determines PAC learnability."},
      {"concept": "variational autoencoder", "wikipedia_article": "Variational autoencoder", "brief_rationale": "A generative model combining an encoder mapping inputs to a latent distribution with a decoder trained by maximizing the evidence lower bound (ELBO); requires deep knowledge of the reparameterization trick, KL divergence regularization, and the relationship between variational inference and generative modeling."},
      {"concept": "expectation-maximization algorithm", "wikipedia_article": "Expectation–maximization algorithm", "brief_rationale": "An iterative algorithm for maximum likelihood estimation in the presence of latent variables, alternating between computing expected sufficient statistics (E-step) and maximizing the likelihood (M-step); requires deep knowledge of its convergence guarantees, the lower bound it optimizes, and applications to mixture models."},
      {"concept": "Q-learning", "wikipedia_article": "Q-learning", "brief_rationale": "A model-free reinforcement learning algorithm that learns the optimal action-value function by updating Q-values via temporal difference errors; requires deep knowledge of the Bellman optimality equation, off-policy learning, convergence conditions, and how it underlies deep Q-networks."},
      {"concept": "kernel method", "wikipedia_article": "Kernel method", "brief_rationale": "A family of algorithms using positive-definite kernel functions to implicitly map data into high-dimensional feature spaces enabling nonlinear learning; requires deep knowledge of Mercer's theorem, the representer theorem, and the kernel trick's role in SVMs and Gaussian processes."},
      {"concept": "information bottleneck", "wikipedia_article": "Information bottleneck method", "brief_rationale": "A framework characterizing optimal representations by trading off compression of the input with preservation of information about the target, formalized via mutual information; requires deep knowledge of the rate-distortion tradeoff, the IB Lagrangian, and its proposed connection to deep network generalization."},
      {"concept": "neural architecture search", "wikipedia_article": "Neural architecture search", "brief_rationale": "Automated methods for discovering optimal neural network architectures using reinforcement learning, evolutionary algorithms, or differentiable relaxations; requires deep knowledge of the search space definition, proxy tasks, weight sharing strategies, and the efficiency challenges of evaluating candidate architectures."},
      {"concept": "normalizing flow", "wikipedia_article": "Flow-based generative model", "brief_rationale": "A class of generative models constructing complex distributions by composing a sequence of invertible differentiable transformations applied to a simple base distribution, with exact likelihood computation via the change-of-variables formula; requires deep knowledge of the log-determinant Jacobian computation, invertibility constraints, and expressiveness tradeoffs."},
      {"concept": "proximal policy optimization", "wikipedia_article": "Proximal Policy Optimization", "brief_rationale": "A policy gradient reinforcement learning algorithm that clips the probability ratio between old and new policies to constrain update size, stabilizing training; requires deep knowledge of the surrogate objective, clipping mechanism, advantage estimation via generalized advantage estimation, and how it improves on TRPO."},
      {"concept": "graph neural network", "wikipedia_article": "Graph neural network", "brief_rationale": "A class of neural networks operating on graph-structured data by iteratively aggregating and transforming features from local neighborhoods; requires deep knowledge of message-passing frameworks, spectral vs. spatial formulations, over-smoothing, and expressiveness limits characterized by the Weisfeiler-Leman isomorphism test."},
      {"concept": "diffusion model", "wikipedia_article": "Diffusion model", "brief_rationale": "A generative model that learns to reverse a gradual noising process, typically formulated as a Markov chain adding Gaussian noise; requires deep knowledge of the forward diffusion schedule, the denoising score matching objective, DDPM parameterization, and the connection to score-based generative modeling via Langevin dynamics."}
    ]
  }
}
