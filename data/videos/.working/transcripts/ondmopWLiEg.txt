I think you're pretty familiar with the idea of matrix vector products. What I want to do in this video is show you that taking a product of a vector with a matrix is equivalent to a transformation. It's actually a linear transformation. So let's say we have some matrix A, and let's say that its columns are v1, their column vector is v2, all the way to vn. So this guy has n columns, and let's see, he has m rows. So it's an m by n matrix. And let's say I define some transformation. Let's say my transformation goes from Rn to Rm. This is the domain. I can take any vector in Rn, and it will map it to some vector in Rm. And I define my transformation. So T of x, where this is some vector in Rn, is equal to A. This is this A. Let me write it in this color right here. It should be bolded. I kind of get careless sometimes with the bolding. That big bold A times the vector x. So the first thing you might say, Sal, gee, this transformation looks very odd relative to how we've been defining transformations or functions so far. So the first thing we have to just feel comfortable with is the idea that this is a transformation. So what are we doing? We're taking something from Rn, and then what does Ax produce? If we write Ax like this, if this is x, where it's x1, x2, it's going to have n terms, because it's in Rn. This can be rewritten as x1 times v1 plus x2 times v2, all the way to xn times vn. So it's going to be a sum of a bunch of these column vectors. And each of these column vectors, v1, v2, all the way to vn, what set are they members of? This is an m by n matrix. The matrix has m rows, or each of these column vectors will have m entries. So all of these guys are members of Rm. So if I just take a linear combination of all of these guys, I'm going to get another member of Rm. So this guy right here is going to be a member of Rm, another vector. So clearly, by multiplying my vector x times a, I'm creating a mapping from Rn. Let me pick another color. 2 Rm. And I'm saying it in very general terms. Maybe n is 3, maybe m is 5. Who knows? But I'm saying it in very general terms. And so if this is a particular instance, a particular member of set Rn, so this is that vector, our transformation or our function is going to map it to this guy right here. And this guy will be a member of Rm, and we could call him ax. Or maybe if we said ax equal b, we could call him the vector b. Whatever. But this is our transformation mapping. So this does fit our kind of definition or our terminology for a function or a transformation as a mapping from one set to another. But it still might not be satisfying because everything we saw before looked kind of like this. If we had a transformation, I would write it like the transformation of I would write x1 and x2 and xn is equal to, and I'd write m terms here and commas. How does this relate to that? And to do that, I'll do a specific example. So let's say that I have the matrix. Let me do a different letter. Let's say I have my matrix B, and it is a fairly simple matrix. Let's say 2, minus 1, 3, and 4. And I define some transformation. So I define some transformation T, and it goes from R2 to R2. And I define T, T of some vector x, is equal to this matrix B times that vector x. Now what would that equal? Well, the matrix is right there. Let me write it in purple. 2, minus 1, 3, and 4 times x, x1, x2. And so what does this equal? Well, this equals another vector. It equals a vector in the codomain R2, where the first term is 2 times x1. I'm just doing the definition of matrix vector multiplication. 2 times x1, 2x1, plus minus 1 times x2, or minus x2. That's that row times our vector. And then the second row times that vector, we get 3 times x1, 3 times x1, plus 4 times x2, plus 4 times x2. So this is what we might be more familiar with. And I could rewrite this transformation as T of x1, x2, is equal to 2x1 minus x2, comma, 3x1 plus 4x2. So hopefully you're satisfied now that a matrix multiplication, it isn't some new exotic form of transformation, that they really are just another way that this statement right here is just another way of writing this exact transformation right here. Now the next question you might ask, and I already told you the answer to this at the beginning of the video, is multiplication by a matrix always going to be a linear transformation? And what are the two constraints for being a linear transformation? We know that the transformation of two vectors, a plus b, the sum of two vectors, should be equal to the sum of their transformations. Transformation of a plus the transformation of b. And then the other requirement is that the transformation of a scaled version of a vector should be equal to a scaled version of the transformation. These are our two requirements for being a linear transformation. So let's see if matrix multiplication applies there. And I've touched on this in the past, and I've even told you that you should prove it. And I've already assumed you know it, but I'll prove it to you here, because I'm tired of telling you that you should prove it. I should do it at least once. So let's see, matrix multiplication. If I multiply a matrix A times some vector x, let's say, let me write it this way. We know that this is equivalent to, I said our matrix, we could just write it, let's say this is an m by n matrix. We can write any matrix as just a series of column vectors. So this guy could have n column vectors. So let's say it's v1, v2, all the way to vn column vectors. And each of these guys are going to have m components. Times x1, x2, all the way down to xn. And we've seen this multiple, multiple times before. This, by the definition of matrix vector multiplication, is equal to x1 times v1, that times that, this scalar times that vector, plus x2 times v2, all the way to plus xn times vn. This was by definition of a matrix vector multiplication. And of course, this is going to, and I did this at the top of the video, this is going to have, right here, this vector is going to be a member of Rm. It's going to have m components. So what happens if I take some matrix A, some m by n matrix A, and I multiply it times the sum of two vectors, A plus B? Where I could rewrite this as this thing right here. So my matrix A times, the sum of A plus B, the first term will just be a1 plus v1. Second term is a2 plus b2, all the way down to an plus bn. This is the same thing as this. I'm not saying A of A plus B. I'm saying A times. Maybe I should put a dot right there. I'm multiplying the matrix. It's not, maybe I want to be careful with my notation. This is the matrix vector multiplication. It's not some type of new matrix dot product. But this is the same thing as this multiplication right here. And based on what I just told you up here, which we've seen multiple, multiple times, this is the same thing as a1 plus v1 times the first column in A, which is that vector right there. This A is the same as this A. So times v1 plus a2 plus b2 times v2 all the way to plus an plus bn times vn. All I did, each xi term here is just being replaced by an ai plus bi term. So each x1 here is replaced by an a1 plus b1 here. So this is equivalent to this. And then from the fact that we know that vector products times scalars exhibit the distributive property, we can say that this is equal to a1 times v1. Let me actually write all of the a1 terms. Well, let me write this. a1 times v1 plus b1 times v1 plus a2 times v2 plus b2 times v2 all the way to plus an times vn plus bn times vn. And then if we just re-associate this, if we just group all of the a's together, all of the a terms together, we get a1 plus a, sorry, a1 plus, let me write it this way. a1 times v1 plus a2 times v2 plus all the way an times vn. I just grabbed all the a terms. We get that plus all the b terms. All the b terms I'll do in this color. All the b terms are like that. So plus b1 times v1 plus b2 times v2 all the way to plus bn times vn. That's that guy right there. Is equivalent to this statement up here. I just regrouped everything, which is, of course, equivalent to that statement over there. But what's this equal to? This is equal to my vector, these columns are remember, the column for the matrix capital A. So this is equal to the matrix capital A times a1, a2, all the way down to an, which was our vector a. And what's this equal to? This is equal to plus these v1s, these are the columns for A. So it's equal to the matrix A times my vector B. v1, v2, all the way down to bn. This is my vector B. So we just saw that, we just showed you that if I add my two vectors, a and b, and then multiply it by the matrix, it's completely equivalent to multiplying each of the vectors times the matrix first and then adding them up. So we've satisfied, and this is for any m by n matrix. So we've now satisfied this first condition right there. And then what about the second condition? This one's even more straightforward to understand. c times a1, so let me write it this way. If I were to write, let me just, the vector A times, sorry, the matrix capital A times the vector lower case A, let me do it this way, because I want times the vector c lower case A. So I'm multiplying my vector times the scalar first is equal to, I can write my big matrix A, I've already labeled its columns, it's v1, v2, all the way to vn, that's my matrix A. And then what does ca look like? ca, you just multiply that scalar times each of the terms of A. So it's ca1, ca2, all the way down to can. And what does this equal? We know this. We've seen this show multiple times before right there. So it just equals, this equals, I'll write a little bit lower, it equals ca1 times this column vector, times v1, plus ca2 times v2, times this guy, all the way to plus can times vn, times vn, times the vector vn. And if you just factor this c out, once again, scalar multiplication times vectors exhibits the distributive property. I believe I've done a video on that, but it's very easy to prove. So this will be equal to c times, I'll just stay in one color right now, a1v1 plus a2v2 plus all the way to anvn. And what is this thing equal to? Well that's just our matrix A times our vector, or our matrix uppercase A, maybe I'm overloading the letter A. The matrix uppercase A times my vector lowercase A, times my lowercase A, right? Where the lowercase A is just this thing right here, a1, a2, and so forth. This thing up here was the same thing as that. So I just showed you that if I take my matrix and multiply it times some vector that was multiplied by a scalar first, that's equivalent to first multiplying the matrix times the vector, and then multiplying by the scalar. So we've shown you that matrix times vector products satisfy this condition of linear transformations and this condition. So the big takeaway right here is matrix multiplication. And this is an important takeaway. Matrix multiplication, or matrix products with vectors, is always a linear transformation. And this is a bit of a side note. In the next video I'm going to show you that any linear transformation, this is incredibly powerful, can be represented by a matrix product, or any transformation on any vector can be equivalently, I guess, written as a product of that vector with a matrix. As huge repercussions, and just as a side note, kind of tying this back to your everyday life, you probably have your Xbox or your Sony PlayStation, and you have these 3D graphic programs where you're running around and shooting at things. And the way that the software renders those programs, where you can see things from every different angle, you have a cube, and then if you kind of move this way a little bit, the cube will look more like this, and it gets rotated, and you move up and down, these are all transformations of matrices, and we'll do this in more detail, or these are all transformations of vectors, or the positions of vectors, and I'll do that in a lot more detail. And all of that is really just matrix multiplications. So all of these things that you're doing in your fancy 3D games on your Xbox or your PlayStation, they're all just matrix multiplications, and I'm going to prove that to you in the next video. And so when you have these graphics cards or these graphics engines, all they are, and this is kind of a, you know, we're jumping away from the theoretical, but all these graphics processors are, are hardwired matrix multipliers. If I have just a generalized, some type of CPU, I have to in software write how to multiply matrices, but if I'm making an Xbox or something, and 99% of what I'm doing is just rotating these abstract objects and displaying them in transformed ways, I should have a dedicated piece of hardware, a chip, that all it does, it's hardwired into it, is multiplying matrices, and that's what those graphics processors or graphics engines really are.