{"domain":{"id":"computational-linguistics","name":"Computational Linguistics","parent_id":"linguistics","level":"sub","region":{"x_min":0.619481,"x_max":0.652126,"y_min":0.612341,"y_max":0.648389},"grid_size":70},"questions":[{"id":"3936e301f81be9a4","question_text":"Which 1950 paper, proposing a test of machine intelligence, is considered a foundational milestone for natural language processing?","options":{"A":"John McCarthy's 'Programs with Common Sense,' which proposed the Advice Taker system","B":"Claude Shannon's 'A Mathematical Theory of Communication,' which proposed information entropy","C":"Alan Turing's 'Computing Machinery and Intelligence,' which proposed the Turing test","D":"Noam Chomsky's 'Syntactic Structures,' which proposed transformational generative grammar"},"correct_answer":"C","difficulty":1,"source_article":"Natural language processing","domain_ids":["computational-linguistics"],"concepts_tested":["natural language processing"],"x":0.747891,"y":0.728606},{"id":"d2c693b7b67d6296","question_text":"What was the first public demonstration of a machine translation system, held in January 1954 at IBM headquarters in New York?","options":{"A":"The MIT Lincoln experiment, which translated Japanese sentences into English","B":"The RAND Corporation experiment, which translated Chinese sentences into English","C":"The Bell Labs experiment, which translated French sentences into German","D":"The Georgetown-IBM experiment, which translated Russian sentences into English"},"correct_answer":"D","difficulty":1,"source_article":"Machine translation","domain_ids":["computational-linguistics"],"concepts_tested":["machine translation"],"x":0.752816,"y":0.729967},{"id":"5602d0caa10008af","question_text":"How does a basic spell checker identify misspelled words in a document?","options":{"A":"By comparing each word against the other words in the same document","B":"By comparing each word against a stored dictionary of correctly spelled words","C":"By sending each word to an online server for real-time verification","D":"By analyzing the grammatical structure of each sentence for syntactic errors"},"correct_answer":"B","difficulty":1,"source_article":"Spell checker","domain_ids":["computational-linguistics"],"concepts_tested":["spell checker"],"x":0.767817,"y":0.728301},{"id":"3267bb19c33e5e7b","question_text":"ELIZA, created in 1966 by Joseph Weizenbaum at MIT, is historically significant in computational linguistics for what reason?","options":{"A":"It was the first speech recognition system capable of understanding continuous human speech","B":"It was one of the first chatbots, simulating a Rogerian psychotherapist using pattern matching","C":"It was the first program to accurately translate between English and Russian using rule-based methods","D":"It was the first system to generate grammatically correct English text from structured data"},"correct_answer":"B","difficulty":1,"source_article":"Chatbot","domain_ids":["computational-linguistics"],"concepts_tested":["chatbot"],"x":0.836897,"y":0.80021},{"id":"1eebbce82660c016","question_text":"Speech recognition technology is a subfield of computational linguistics primarily concerned with which conversion task?","options":{"A":"Translating speech in one language directly into speech in another language","B":"Translating spoken language into text or other machine-readable formats","C":"Converting written text into audible spoken language output","D":"Identifying which individual person is speaking from a voice sample"},"correct_answer":"B","difficulty":1,"source_article":"Speech recognition","domain_ids":["computational-linguistics"],"concepts_tested":["speech recognition"],"x":0.8485,"y":0.78078},{"id":"f465312bc72596bd","question_text":"A text-to-speech system typically has two main components. What does the front-end component do?","options":{"A":"It adjusts the volume and playback speed based on the listener's preferences","B":"It normalizes raw text and assigns phonetic transcriptions to each word","C":"It selects the most natural-sounding voice from a database of recorded speakers","D":"It generates audio waveforms from acoustic parameters and applies voice filtering"},"correct_answer":"B","difficulty":1,"source_article":"Speech synthesis","domain_ids":["computational-linguistics"],"concepts_tested":["text-to-speech"],"x":0.769516,"y":0.730132},{"id":"cd7c1191e6fd5083","question_text":"Which innovation, introduced by Google in 1998, revolutionized web search engines by using hyperlink structure to rank pages?","options":{"A":"The Boolean retrieval model, which matched pages using logical keyword operators","B":"The TF-IDF algorithm, which weighted terms by their frequency across documents","C":"The PageRank algorithm, which assessed page importance from link structure","D":"The latent semantic indexing method, which grouped pages by topic similarity"},"correct_answer":"C","difficulty":1,"source_article":"Web search engine","domain_ids":["computational-linguistics"],"concepts_tested":["search engine"],"x":0.782916,"y":0.747842},{"id":"480ba4f26a61290d","question_text":"Autocomplete technology was originally invented in the 1950s to solve an input efficiency problem in which writing system?","options":{"A":"The Arabic writing system, due to its context-dependent letter shapes","B":"The Chinese writing system, due to its thousands of logographic characters","C":"The Korean writing system, due to its complex syllable block compositions","D":"The Japanese writing system, due to its three different alphabetic scripts"},"correct_answer":"B","difficulty":1,"source_article":"Autocomplete","domain_ids":["computational-linguistics"],"concepts_tested":["autocomplete"],"x":0.797622,"y":0.77211},{"id":"5a0167b32a5092c7","question_text":"Sentiment analysis, also known as opinion mining, primarily aims to identify what kind of information from text?","options":{"A":"Affective states and subjective opinions such as positive, negative, or neutral attitudes","B":"Named entities and relationships such as people, organizations, or geographic places","C":"Factual claims and objective statements such as dates, locations, or numerical figures","D":"Syntactic structures and grammatical patterns such as verb tenses or clause types"},"correct_answer":"A","difficulty":1,"source_article":"Sentiment analysis","domain_ids":["computational-linguistics"],"concepts_tested":["sentiment analysis"],"x":0.794668,"y":0.747798},{"id":"52365a44ffa8d3f6","question_text":"Naive Bayes spam filtering, a widely used email spam detection technique, classifies messages based on what principle?","options":{"A":"The geographic origin of the sending mail server's IP address","B":"The time of day the email was sent relative to the recipient's timezone","C":"The probability that particular words appear in spam versus legitimate email","D":"The total number of recipients listed in the email header fields"},"correct_answer":"C","difficulty":1,"source_article":"Email filtering","domain_ids":["computational-linguistics"],"concepts_tested":["spam filter"],"x":0.7564,"y":0.714642},{"id":"e6019cd25667d8cc","question_text":"What is optical character recognition (OCR)?","options":{"A":"The electronic method of converting machine-encoded text files into printed physical documents using laser technology.","B":"The automated process of translating spoken words captured by a microphone into editable digital text documents.","C":"The electronic or mechanical conversion of images of typed, handwritten, or printed text into machine-encoded text.","D":"The computational technique of generating realistic handwritten text images from typed digital character input."},"correct_answer":"C","difficulty":1,"source_article":"Optical character recognition","domain_ids":["computational-linguistics"],"concepts_tested":["optical character recognition"],"x":0.765842,"y":0.718167},{"id":"136ac93eb3fddcba","question_text":"What is question answering (QA) in computer science?","options":{"A":"A discipline focused on building systems that automatically detect grammatical errors in written natural language text.","B":"A discipline focused on building systems that automatically classify documents into predefined topical category labels.","C":"A discipline focused on building systems that automatically generate exam questions from structured database records.","D":"A discipline focused on building systems that automatically answer questions posed by humans in natural language."},"correct_answer":"D","difficulty":1,"source_article":"Question answering","domain_ids":["computational-linguistics"],"concepts_tested":["question answering"],"x":0.794096,"y":0.748273},{"id":"050da0c1ff85b9a7","question_text":"What are the two general approaches to automatic text summarization?","options":{"A":"Classification, which sorts documents by topic, and regression, which ranks sentences by their predicted importance scores.","B":"Compression, which shortens individual sentences, and expansion, which adds contextual background information to summaries.","C":"Tokenization, which splits text into word units, and clustering, which groups related paragraphs into thematic sections.","D":"Extraction, which selects existing sentences, and abstraction, which generates new sentences conveying key information."},"correct_answer":"D","difficulty":1,"source_article":"Automatic summarization","domain_ids":["computational-linguistics"],"concepts_tested":["text summarization"],"x":0.759705,"y":0.725563},{"id":"222a848b58a2fea3","question_text":"In natural language processing, what is tokenization?","options":{"A":"The process of converting written text into phonetic transcriptions suitable for speech synthesis applications.","B":"The process of replacing sensitive words in text with anonymous placeholder labels to protect user privacy.","C":"The process of splitting raw text into meaningful units such as words, subwords, or symbols called tokens.","D":"The process of assigning numerical weights to words based on their frequency of occurrence across document collections."},"correct_answer":"C","difficulty":2,"source_article":"Lexical analysis","domain_ids":["computational-linguistics"],"concepts_tested":["tokenization"],"x":0.764037,"y":0.730573},{"id":"a3e46857d27a764b","question_text":"In part-of-speech tagging, what does a tagger assign to each word in a text?","options":{"A":"A label indicating its grammatical category, such as noun, verb, adjective, or adverb.","B":"A label indicating its semantic role, such as agent, patient, instrument, or beneficiary.","C":"A label indicating its named entity type, such as person, organization, location, or date.","D":"A label indicating its sentiment polarity, such as positive, negative, neutral, or mixed."},"correct_answer":"A","difficulty":2,"source_article":"Part-of-speech tagging","domain_ids":["computational-linguistics"],"concepts_tested":["part-of-speech tagging"],"x":0.749057,"y":0.736668},{"id":"9bdcc6da5e858810","question_text":"What does named entity recognition (NER) seek to do with unstructured text?","options":{"A":"Identify and correct misspelled words by comparing them against predefined dictionaries of standard language usage.","B":"Extract and normalize temporal expressions by converting them into standardized date and time format representations.","C":"Detect and resolve pronoun references by linking them to their corresponding antecedent noun phrases in text.","D":"Locate and classify named entities into predefined categories such as persons, organizations, and locations."},"correct_answer":"D","difficulty":2,"source_article":"Named-entity recognition","domain_ids":["computational-linguistics"],"concepts_tested":["named entity recognition"],"x":0.764863,"y":0.73562},{"id":"17f01c7d0d59c64c","question_text":"In computational linguistics, what does parsing a sentence produce?","options":{"A":"A vector embedding representing the semantic meaning of the sentence as coordinates in high-dimensional space.","B":"A parse tree showing the syntactic structure and grammatical relations among the sentence's constituents.","C":"A sentiment score indicating the overall positive or negative emotional tone expressed by the sentence.","D":"A frequency table listing the occurrence count of each unique word appearing within the analyzed sentence."},"correct_answer":"B","difficulty":2,"source_article":"Parsing","domain_ids":["computational-linguistics"],"concepts_tested":["parsing"],"x":0.746733,"y":0.73365},{"id":"b267412d553338bf","question_text":"In linguistics and NLP, what is a corpus?","options":{"A":"A standardized annotation scheme used for labeling syntactic dependencies between words in parsed sentences.","B":"A large, structured collection of texts used for statistical analysis, language research, and training models.","C":"A curated dictionary of morphological rules used for reducing inflected word forms to their base lemmas.","D":"A specialized algorithm used for automatically generating grammatically correct sentences from semantic representations."},"correct_answer":"B","difficulty":2,"source_article":"Text corpus","domain_ids":["computational-linguistics"],"concepts_tested":["corpus"],"x":0.7648,"y":0.735281},{"id":"dce0d5e5c715f415","question_text":"What is an n-gram in computational linguistics?","options":{"A":"A ranked list of n candidate translations generated during the decoding phase of machine translation systems.","B":"A fixed-length binary encoding of n characters used to compress text for efficient storage in databases.","C":"A weighted graph of n interconnected words capturing long-range semantic dependencies across entire document collections.","D":"A contiguous sequence of n items from a given text, used in language modeling and text analysis."},"correct_answer":"D","difficulty":2,"source_article":"N-gram","domain_ids":["computational-linguistics"],"concepts_tested":["n-gram"],"x":0.744449,"y":0.720807},{"id":"79d64d818c92b372","question_text":"What key limitation of the bag-of-words model makes it unable to distinguish between 'man bites dog' and 'dog bites man'?","options":{"A":"It discards word order, representing text as an unordered collection of words regardless of sequence.","B":"It discards word meaning, representing text as a collection of character counts regardless of vocabulary.","C":"It discards word frequency, representing text as a binary set of present or absent unique terms.","D":"It discards word morphology, representing text as a collection of root stems regardless of inflection."},"correct_answer":"A","difficulty":2,"source_article":"Bag-of-words model","domain_ids":["computational-linguistics"],"concepts_tested":["bag of words"],"x":0.744374,"y":0.714688},{"id":"cba9bad8752716a4","question_text":"In TF-IDF weighting, what does the inverse document frequency (IDF) component measure about a term?","options":{"A":"How rare or informative the term is across all documents in the corpus","B":"The position of the term's first occurrence in each document","C":"How frequently the term appears within a single document","D":"The total number of documents that contain the term"},"correct_answer":"A","difficulty":2,"source_article":"Tf\u2013idf","domain_ids":["computational-linguistics"],"concepts_tested":["TF-IDF"],"x":0.749687,"y":0.714871},{"id":"a58216ed577ddd41","question_text":"In NLP preprocessing, stop words are typically removed before analysis. What kind of words are they, and why are they filtered out?","options":{"A":"Common function words like \"the\" and \"is\" that carry little semantic meaning","B":"Punctuation marks and special characters that interrupt token sequences","C":"Misspelled words automatically detected and removed by the spell checker","D":"Rare technical terms that appear too infrequently to be statistically useful"},"correct_answer":"A","difficulty":2,"source_article":"Stop word","domain_ids":["computational-linguistics"],"concepts_tested":["stop words"],"x":0.767538,"y":0.733075},{"id":"4f4c215d1da84cca","question_text":"What key property distinguishes stemming from lemmatization when reducing words to a base form in text preprocessing?","options":{"A":"Stemming only works on verbs while lemmatization handles all parts of speech","B":"Stemming may produce non-word stems, while lemmatization always returns valid dictionary words","C":"Stemming adds prefixes to words while lemmatization removes suffixes","D":"Stemming requires a part-of-speech tagger but lemmatization does not"},"correct_answer":"B","difficulty":2,"source_article":"Stemming","domain_ids":["computational-linguistics"],"concepts_tested":["stemming"],"x":0.774427,"y":0.734006},{"id":"db85a2964d0cb8f8","question_text":"In dependency grammar, what grammatical element is taken as the structural root of a clause from which all other words are connected via directed links?","options":{"A":"The first word appearing in the sentence","B":"The most frequently occurring word in the clause","C":"The finite verb of the clause","D":"The subject noun phrase of the clause"},"correct_answer":"C","difficulty":2,"source_article":"Dependency grammar","domain_ids":["computational-linguistics"],"concepts_tested":["dependency parsing"],"x":0.75064,"y":0.739374},{"id":"13d56ce01615707a","question_text":"Unlike one-hot encoding, word embeddings use dense real-valued vectors. How do they represent semantic similarity between words?","options":{"A":"Each word is assigned a unique integer index in a fixed vocabulary table","B":"Similar words share the same one-hot encoded binary vector representation","C":"Words with similar meanings are mapped to nearby vectors measured by cosine similarity","D":"Words are grouped into discrete semantic clusters using k-means classification"},"correct_answer":"C","difficulty":2,"source_article":"Word embedding","domain_ids":["computational-linguistics"],"concepts_tested":["word embedding"],"x":0.748027,"y":0.716438},{"id":"35b7a7640e3361be","question_text":"What does a statistical language model fundamentally assign to any given sequence of words?","options":{"A":"A semantic category label drawn from a predefined ontology","B":"A probability indicating how likely that sequence is in the language","C":"A sentiment polarity value ranging from negative to positive","D":"A grammatical correctness score based on hand-written syntactic rules"},"correct_answer":"B","difficulty":2,"source_article":"Language model","domain_ids":["computational-linguistics"],"concepts_tested":["language model"],"x":0.760746,"y":0.726612},{"id":"f294877ec193c29b","question_text":"The CYK parsing algorithm uses bottom-up dynamic programming to parse context-free grammars. What specific normal form must the grammar be in?","options":{"A":"Backus-Naur form","B":"Kuroda normal form","C":"Greibach normal form","D":"Chomsky normal form"},"correct_answer":"D","difficulty":3,"source_article":"CYK algorithm","domain_ids":["computational-linguistics"],"concepts_tested":["CYK algorithm"],"x":0.747796,"y":0.734122},{"id":"8bf78e8d875fd650","question_text":"What algorithm is commonly used to train a hidden Markov model's parameters, and of what general method is it a special case?","options":{"A":"The forward-backward algorithm, a special case of gradient descent","B":"The Baum-Welch algorithm, a special case of expectation-maximization","C":"The inside-outside algorithm, a special case of belief propagation","D":"The Viterbi algorithm, a special case of dynamic programming"},"correct_answer":"B","difficulty":3,"source_article":"Hidden Markov model","domain_ids":["computational-linguistics"],"concepts_tested":["hidden Markov model"],"x":0.761679,"y":0.723906},{"id":"dd9c4fcbf4b3be6c","question_text":"In language model evaluation, perplexity is mathematically defined as the exponentiation of what information-theoretic quantity?","options":{"A":"The mutual information between input and output","B":"The variance of the probability distribution","C":"The Kullback-Leibler divergence from the uniform distribution","D":"The entropy of the probability distribution"},"correct_answer":"D","difficulty":3,"source_article":"Perplexity","domain_ids":["computational-linguistics"],"concepts_tested":["perplexity"],"x":0.795901,"y":0.750796},{"id":"92ef99fabdf88c04","question_text":"The attention mechanism was introduced in 2014 by Bahdanau et al. to address limitations of which neural architecture for machine translation?","options":{"A":"Recurrent neural networks with the encoder-decoder framework","B":"Convolutional neural networks with fixed-size pooling layers","C":"Feedforward neural networks with backpropagation training","D":"Generative adversarial networks with discriminator feedback"},"correct_answer":"A","difficulty":3,"source_article":"Attention (machine learning)","domain_ids":["computational-linguistics"],"concepts_tested":["attention mechanism"],"x":0.743905,"y":0.712668},{"id":"4355ba41966eb08c","question_text":"In sequence decoding, what is beam search and how does it differ from greedy search?","options":{"A":"An exhaustive search algorithm that evaluates every possible candidate sequence at each decoding step, guaranteeing the globally optimal output sequence is found.","B":"A pruning search algorithm that eliminates candidate sequences falling below a fixed probability threshold at each decoding step, reducing the overall search space.","C":"A heuristic search algorithm that keeps the top-k most probable candidate sequences at each decoding step, rather than selecting only the single best candidate.","D":"A randomized search algorithm that samples candidate sequences proportionally to their probability at each decoding step, introducing controlled stochastic variation."},"correct_answer":"C","difficulty":3,"source_article":"Beam search","domain_ids":["computational-linguistics"],"concepts_tested":["beam search"],"x":0.789619,"y":0.74717},{"id":"8ae7685f1220af00","question_text":"What are the two neural network training architectures used by word2vec to learn word embeddings?","options":{"A":"Continuous bag of words (CBOW), which predicts a target word from context words, and skip-gram, which predicts context words from a target word.","B":"Long short-term memory network, which predicts a target word from gated memory cells, and temporal convolution, which predicts context words from dilated filter windows.","C":"Generative adversarial network, which predicts a target word from a noise vector, and variational autoencoder, which predicts context words from a latent distribution.","D":"Recurrent encoder network, which predicts a target word from sequential hidden states, and convolutional decoder, which predicts context words from pooled feature maps."},"correct_answer":"A","difficulty":3,"source_article":"Word2vec","domain_ids":["computational-linguistics"],"concepts_tested":["word2vec"],"x":0.747385,"y":0.714968},{"id":"fafaa7877353e303","question_text":"What primary advantage do conditional random fields have over hidden Markov models for sequence labeling tasks in NLP?","options":{"A":"CRFs are discriminative models that do not require the strict independence assumptions of generative HMMs, allowing them to incorporate arbitrary overlapping features.","B":"CRFs are generative models that explicitly model the joint distribution of inputs and labels, allowing them to generate new synthetic training examples.","C":"CRFs are unsupervised models that do not require any labeled training data during parameter estimation, allowing them to leverage large unlabeled corpora.","D":"CRFs are recurrent models that maintain persistent hidden state vectors across sequence boundaries, allowing them to capture long-range document-level dependencies."},"correct_answer":"A","difficulty":3,"source_article":"Conditional random field","domain_ids":["computational-linguistics"],"concepts_tested":["conditional random field"],"x":0.749649,"y":0.718766},{"id":"7aaa4ef42d8ddeb3","question_text":"What does semantic role labeling identify in a sentence, and what is an example of the roles it assigns?","options":{"A":"It identifies the discourse coherence structure, assigning relations such as cause, contrast, and elaboration to clauses within the sentence.","B":"It identifies the predicate-argument structure, assigning roles such as agent, theme, and recipient to phrases related to the verb.","C":"It identifies the morphological inflection structure, assigning features such as tense, aspect, and number to individual word forms in context.","D":"It identifies the syntactic constituency structure, assigning labels such as noun phrase, verb phrase, and prepositional phrase to word groups."},"correct_answer":"B","difficulty":3,"source_article":"Semantic role labeling","domain_ids":["computational-linguistics"],"concepts_tested":["semantic role labeling"],"x":0.759538,"y":0.738336},{"id":"8706183ad2f0f202","question_text":"What is coreference resolution in natural language processing?","options":{"A":"The task of identifying when different paragraphs in a text convey contradictory factual claims that require manual verification and correction.","B":"The task of identifying when different expressions in a text, such as pronouns and noun phrases, refer to the same real-world entity.","C":"The task of identifying when different sentences in a text share similar syntactic parse tree structures despite using entirely different vocabulary.","D":"The task of identifying when different documents in a corpus address the same general topic despite originating from unrelated independent sources."},"correct_answer":"B","difficulty":3,"source_article":"Coreference","domain_ids":["computational-linguistics"],"concepts_tested":["coreference resolution"],"x":0.767071,"y":0.740672},{"id":"8ee216439205048a","question_text":"What is the distributional hypothesis underlying distributional semantics?","options":{"A":"Words that were coined during similar historical periods tend to have similar meanings, so meaning can be inferred from etymological dating records.","B":"Words that occur in similar linguistic contexts tend to have similar meanings, so meaning can be inferred from co-occurrence patterns in corpora.","C":"Words that share similar phonological structures tend to have similar meanings, so meaning can be inferred from pronunciation patterns across languages.","D":"Words that appear with similar frequency in corpora tend to have similar meanings, so meaning can be inferred from raw token counts alone."},"correct_answer":"B","difficulty":3,"source_article":"Distributional semantics","domain_ids":["computational-linguistics"],"concepts_tested":["distributional semantics"],"x":0.770308,"y":0.731404},{"id":"c5d2065c4e03f7f2","question_text":"In the encoder-decoder (seq2seq) architecture, what are the respective roles of the encoder and decoder components?","options":{"A":"The encoder compresses the input sequence into a binary hash code, and the decoder retrieves the nearest matching output sequence from a database.","B":"The encoder processes the input sequence into a fixed context representation, and the decoder generates the output sequence from that representation.","C":"The encoder classifies the input sequence into a discrete category label, and the decoder expands that label into a full output sequence template.","D":"The encoder segments the input sequence into independent clause boundaries, and the decoder reorders those clauses into a grammatically correct output sequence."},"correct_answer":"B","difficulty":3,"source_article":"Seq2seq","domain_ids":["computational-linguistics"],"concepts_tested":["encoder-decoder architecture"],"x":0.747552,"y":0.710055},{"id":"203cd78a79cadb3a","question_text":"How does byte pair encoding (BPE) construct its subword vocabulary for use in language model tokenization?","options":{"A":"It starts with individual characters and iteratively merges the most frequent adjacent token pair into a new token until reaching the desired vocabulary size.","B":"It starts with individual characters and iteratively removes the least frequent single character token from the vocabulary until the desired vocabulary size is reached.","C":"It starts with complete words and iteratively splits the least frequent word into its component syllables until the desired vocabulary size is reached.","D":"It starts with complete sentences and iteratively extracts the most frequent recurring phrase as a single token until the desired vocabulary size is reached."},"correct_answer":"A","difficulty":3,"source_article":"Byte pair encoding","domain_ids":["computational-linguistics"],"concepts_tested":["byte pair encoding"],"x":0.755632,"y":0.722014},{"id":"e37bc70df9d36639","question_text":"In combinatory categorial grammar (CCG), what do the forward slash and backslash notations in a syntactic category like S\\NP or NP/N indicate?","options":{"A":"They indicate movement types: a forward slash means the constituent moved rightward from its base position, and a backslash means it moved leftward.","B":"They indicate functor types: a forward slash means the argument appears to the right, and a backslash means the argument appears to the left.","C":"They indicate dependency types: a forward slash means the head word governs a rightward dependent, and a backslash means it governs a leftward dependent.","D":"They indicate agreement types: a forward slash means the category agrees in number with the right neighbor, and a backslash means agreement with the left."},"correct_answer":"B","difficulty":4,"source_article":"Combinatory categorial grammar","domain_ids":["computational-linguistics"],"concepts_tested":["combinatory categorial grammar"],"x":0.7471,"y":0.731989},{"id":"e33206f03759022d","question_text":"What kind of grammar formalism is head-driven phrase structure grammar (HPSG), and how does it represent linguistic information?","options":{"A":"A stochastic-based, highly probabilistic formalism that represents linguistic information using weighted production rules organized in conditional probability tables and parse forests.","B":"A transformation-based, highly derivational formalism that represents linguistic information using ordered rewrite rules organized in sequential transformation cycles.","C":"A dependency-based, highly relational formalism that represents linguistic information using directed arc labels organized in head-dependent tree graphs and valency frames.","D":"A constraint-based, highly lexicalized formalism that represents linguistic information using typed feature structures organized in attribute-value matrices and a type hierarchy."},"correct_answer":"D","difficulty":4,"source_article":"Head-driven phrase structure grammar","domain_ids":["computational-linguistics"],"concepts_tested":["head-driven phrase structure grammar"],"x":0.750321,"y":0.732954},{"id":"9456cf7a9d8d1f33","question_text":"The minimum description length (MDL) principle, introduced by Jorma Rissanen in 1978, selects models by finding the shortest description of the data. How does MDL relate to Occam's razor, and how does it differ from the Bayesian minimum message length (MML) principle introduced by Wallace and Boulton in 1968?","options":{"A":"MDL formalizes Occam's razor by treating the best model as the one with the fewest parameters regardless of fit. Unlike MML, which is a Bayesian framework averaging over all possible data, MDL is rooted in information theory and evaluates models solely on observed data without requiring prior distributions.","B":"MDL formalizes Occam's razor by treating the best model as the one permitting greatest data compression. Unlike MML, which is a frequentist framework averaging over all possible data, MDL is rooted in information theory and evaluates models solely on observed data without requiring prior distributions.","C":"MDL formalizes Occam's razor by treating the best model as the one permitting greatest data compression. Unlike MML, which is a Bayesian framework averaging over all possible data, MDL is rooted in Kolmogorov complexity and requires computing the exact shortest program for observed data.","D":"MDL formalizes Occam's razor by treating the best model as the one permitting greatest data compression. Unlike MML, which is a Bayesian framework averaging over all possible data, MDL is rooted in information theory and evaluates models solely on observed data without requiring prior distributions."},"correct_answer":"D","difficulty":4,"source_article":"Minimum description length","domain_ids":["computational-linguistics"],"concepts_tested":["minimum description length","Occam's razor","model selection"],"x":0.775825,"y":0.74125},{"id":"9e22d0a3d7db1b9a","question_text":"The expectation-maximization (EM) algorithm iteratively estimates parameters in statistical models with latent variables. What are the two steps in each iteration, and what are two prominent NLP applications of EM that specialize it to sequential and hierarchical structures respectively?","options":{"A":"The E-step computes the maximum a posteriori parameters, and the M-step finds the expected latent variable assignments given those parameters. The Baum-Welch algorithm applies EM to hidden Markov models, and the inside-outside algorithm applies EM to probabilistic context-free grammars.","B":"The E-step computes the expected log-likelihood using current parameter estimates, and the M-step finds parameters that maximize that expected log-likelihood. The Viterbi algorithm applies EM to hidden Markov models, and the inside-outside algorithm applies EM to probabilistic context-free grammars.","C":"The E-step computes the expected log-likelihood using current parameter estimates, and the M-step finds parameters that maximize that expected log-likelihood. The Baum-Welch algorithm applies EM to hidden Markov models, and the inside-outside algorithm applies EM to probabilistic context-free grammars.","D":"The E-step computes the expected log-likelihood using current parameter estimates, and the M-step finds parameters that maximize that expected log-likelihood. The Baum-Welch algorithm applies EM to hidden Markov models, and the CYK algorithm applies EM to probabilistic context-free grammars."},"correct_answer":"C","difficulty":4,"source_article":"Expectation\u2013maximization algorithm","domain_ids":["computational-linguistics"],"concepts_tested":["expectation-maximization algorithm","Baum-Welch algorithm","inside-outside algorithm"],"x":0.762242,"y":0.727245},{"id":"9df9c05feeaa44eb","question_text":"The inside-outside algorithm, introduced by James K. Baker in 1979, re-estimates production probabilities in probabilistic context-free grammars. What earlier algorithm for hidden Markov models does it generalize, and what role do the inside and outside probabilities each play in the computation?","options":{"A":"It generalizes the forward-backward algorithm for HMMs. The inside probability is the probability that a nonterminal generates a given substring, while the outside probability is the total probability of all parse trees that do not include that nonterminal.","B":"It generalizes the Viterbi algorithm for HMMs. The inside probability is the probability that a nonterminal generates a given substring, while the outside probability is the probability of the rest of the sentence being generated outside that nonterminal's span.","C":"It generalizes the forward-backward algorithm for HMMs. The inside probability is the probability of the most likely parse tree rooted at a nonterminal, while the outside probability is the probability of the rest of the sentence being generated outside that nonterminal's span.","D":"It generalizes the forward-backward algorithm for HMMs. The inside probability is the probability that a nonterminal generates a given substring, while the outside probability is the probability of the rest of the sentence being generated outside that nonterminal's span."},"correct_answer":"D","difficulty":4,"source_article":"Inside\u2013outside algorithm","domain_ids":["computational-linguistics"],"concepts_tested":["inside-outside algorithm","forward-backward algorithm","probabilistic context-free grammar"],"x":0.738812,"y":0.719592},{"id":"02e8b97f1385a451","question_text":"Cross-lingual transfer learning leverages models trained on high-resource languages to improve NLP performance on low-resource languages. What type of shared representation enables this transfer, and how do multilingual models like mBERT achieve cross-lingual understanding without explicit parallel data during pre-training?","options":{"A":"Cross-lingual word embeddings map words from different languages into a shared vector space. Multilingual BERT achieves cross-lingual transfer by pre-training exclusively on parallel sentence pairs from many languages with a shared WordPiece vocabulary, causing direct translation equivalents to align representations across languages.","B":"Cross-lingual word embeddings map words from different languages into a shared vector space. Multilingual BERT achieves cross-lingual transfer by pre-training on concatenated monolingual corpora from many languages with language-specific vocabularies, causing similar sentence structures to align representations across languages.","C":"Cross-lingual parse trees map syntactic structures from different languages into a shared tree space. Multilingual BERT achieves cross-lingual transfer by pre-training on concatenated monolingual corpora from many languages with a shared WordPiece vocabulary, causing overlapping subwords and similar contexts to align representations across languages.","D":"Cross-lingual word embeddings map words from different languages into a shared vector space. Multilingual BERT achieves cross-lingual transfer by pre-training on concatenated monolingual corpora from many languages with a shared WordPiece vocabulary, causing overlapping subwords and similar contexts to align representations across languages."},"correct_answer":"D","difficulty":4,"source_article":"Transfer learning","domain_ids":["computational-linguistics"],"concepts_tested":["cross-lingual transfer","multilingual embeddings","multilingual BERT"],"x":0.764958,"y":0.72878},{"id":"d483cdb6edcadf4c","question_text":"Tree-adjoining grammar (TAG), defined by Aravind Joshi, uses trees rather than symbols as the elementary unit of rewriting. What are the two fundamental operations in TAG, and why is TAG described as mildly context-sensitive rather than fully context-sensitive?","options":{"A":"The two operations are substitution, which replaces a frontier node with an initial tree, and adjunction, which inserts an auxiliary tree at a node whose label matches the auxiliary tree's root and foot nodes. TAG is mildly context-sensitive because it generates exactly the same languages as context-free grammars while remaining efficiently parsable in polynomial time.","B":"The two operations are substitution, which replaces a frontier node with an initial tree, and adjunction, which inserts an auxiliary tree at a node whose label matches the auxiliary tree's root and foot nodes. TAG is mildly context-sensitive because it generates some but not all context-sensitive languages while remaining efficiently parsable in polynomial time.","C":"The two operations are substitution, which replaces a frontier node with an initial tree, and adjunction, which inserts an auxiliary tree at a node whose label matches the auxiliary tree's root and foot nodes. TAG is mildly context-sensitive because it generates all context-sensitive languages but restricts derivation depth to remain efficiently parsable in polynomial time.","D":"The two operations are substitution, which replaces a frontier node with an initial tree, and concatenation, which joins two initial trees at their matching labeled leaf nodes. TAG is mildly context-sensitive because it generates some context-sensitive languages but not all of them while remaining efficiently parsable in worst-case polynomial time."},"correct_answer":"B","difficulty":4,"source_article":"Tree-adjoining grammar","domain_ids":["computational-linguistics"],"concepts_tested":["tree-adjoining grammar","substitution and adjunction","mildly context-sensitive"],"x":0.743564,"y":0.730405},{"id":"60746d9d0e72dfdb","question_text":"The maximum entropy (MaxEnt) classifier is a log-linear model widely used in NLP for tasks such as text classification and sequence labeling. How does the maximum entropy principle determine the model's probability distribution, and how does the MaxEnt classifier relate to multinomial logistic regression?","options":{"A":"The maximum entropy principle selects the probability distribution with the lowest entropy among all distributions that satisfy the observed feature constraints from training data. The MaxEnt classifier is mathematically equivalent to multinomial logistic regression, both using the softmax function to produce conditional class probabilities from weighted feature sums.","B":"The maximum entropy principle selects the probability distribution with the highest entropy among all distributions that satisfy the observed feature constraints from training data. The MaxEnt classifier is mathematically equivalent to multinomial logistic regression, both using the sigmoid function to produce joint class probabilities from weighted feature sums.","C":"The maximum entropy principle selects the probability distribution with the highest entropy among all distributions that satisfy the observed feature constraints from training data. The MaxEnt classifier is mathematically equivalent to multinomial logistic regression, both using the softmax function to produce conditional class probabilities from weighted feature sums.","D":"The maximum entropy principle selects the probability distribution with the highest entropy among all distributions that satisfy the observed feature constraints from training data. The MaxEnt classifier is mathematically equivalent to naive Bayes classification, both using the softmax function to produce conditional class probabilities from weighted feature sums."},"correct_answer":"C","difficulty":4,"source_article":"Logistic regression","domain_ids":["computational-linguistics"],"concepts_tested":["maximum entropy model","log-linear model","multinomial logistic regression"],"x":0.774494,"y":0.733493},{"id":"1735ec093d699ff8","question_text":"Linear programming relaxation has been applied to dependency parsing by relaxing integer constraints on edge selection variables to continuous values. What does this relaxation provide in the context of structured prediction for parsing, and how is the fractional solution typically converted back into a valid parse tree?","options":{"A":"The LP relaxation provides a lower bound on the optimal integer solution's objective value, enabling exact inference over the exponentially large space of possible parse trees. The fractional solution is typically rounded or decoded using methods such as projecting onto the nearest valid tree structure.","B":"The LP relaxation provides an upper bound on the optimal integer solution's objective value, enabling efficient approximate inference over the exponentially large space of possible parse trees. The fractional solution is typically rounded or decoded using methods such as projecting onto the nearest valid tree structure.","C":"The LP relaxation provides a lower bound on the optimal integer solution's objective value, enabling efficient approximate inference over the exponentially large space of possible parse trees. The fractional solution is typically converted by selecting all edges with probability above a fixed threshold of 0.5.","D":"The LP relaxation provides a lower bound on the optimal integer solution's objective value, enabling efficient approximate inference over the exponentially large space of possible parse trees. The fractional solution is typically rounded or decoded using methods such as projecting onto the nearest valid tree structure."},"correct_answer":"D","difficulty":4,"source_article":"LP relaxation","domain_ids":["computational-linguistics"],"concepts_tested":["linear programming relaxation for parsing","structured prediction","approximate inference"],"x":0.748382,"y":0.73427},{"id":"e4c32b653c68dcb6","question_text":"The Earley parser, introduced by Jay Earley in his 1968 dissertation, is a top-down chart parser using dynamic programming. What three operations does the algorithm perform at each position in the input, and what are its time complexities for general, unambiguous, and deterministic context-free grammars?","options":{"A":"The three operations are prediction (expanding nonterminals by adding new items from matching rules), shifting (advancing items over matching terminal symbols), and reduction (combining completed items into higher-level nonterminals). It runs in O(n^3) for general, O(n^2) for unambiguous, and O(n) for deterministic context-free grammars.","B":"The three operations are prediction (expanding nonterminals by adding new items from matching rules), scanning (advancing items over matching terminal symbols), and completion (advancing items when a predicted nonterminal is fully recognized). It runs in O(n^3) for general, O(n^2) for unambiguous, and O(n) for deterministic context-free grammars.","C":"The three operations are prediction (expanding nonterminals by adding new items from matching rules), scanning (advancing items over matching terminal symbols), and completion (advancing items when a predicted nonterminal is fully recognized). It runs in O(n^3) for general, O(n^2) for unambiguous, and O(n log n) for deterministic context-free grammars.","D":"The three operations are prediction (expanding nonterminals by adding new items from matching rules), scanning (advancing items over matching terminal symbols), and completion (advancing items when a predicted nonterminal is fully recognized). It runs in O(n^2) for general, O(n log n) for unambiguous, and O(n) for deterministic context-free grammars."},"correct_answer":"B","difficulty":4,"source_article":"Earley parser","domain_ids":["computational-linguistics"],"concepts_tested":["Earley parser","chart parsing","parsing complexity"],"x":0.764579,"y":0.745437},{"id":"914707d00b4e6115","question_text":"Turkish is a highly agglutinative language where a single noun root can produce over 20,000 valid word forms through productive inflectional and derivational suffixation. What computational formalism is predominantly used for Turkish morphological analysis, and why is it particularly well-suited to agglutinative morphology?","options":{"A":"Context-free grammars (CFGs) are predominantly used, typically implemented as recursive descent parsers. CFGs are well-suited because they can compactly encode the nested patterns of sequential suffix attachment and phonological alternation rules characteristic of agglutinative languages, enabling efficient bidirectional mapping between surface forms and morphological analyses.","B":"Finite-state transducers (FSTs) are predominantly used, typically implemented as two-level morphology systems. FSTs are well-suited because they can enumerate all possible word forms in a lookup table, storing each inflected surface form and its morphological analysis as a dictionary entry for agglutinative languages.","C":"Finite-state transducers (FSTs) are predominantly used, typically implemented as two-level morphology systems. FSTs are well-suited because they can handle the context-sensitive rewrite rules needed for non-concatenative morphological processes characteristic of agglutinative languages, enabling efficient bidirectional mapping between surface forms and morphological analyses.","D":"Finite-state transducers (FSTs) are predominantly used, typically implemented as two-level morphology systems. FSTs are well-suited because they can compactly encode the regular patterns of sequential suffix attachment and phonological alternation rules characteristic of agglutinative languages, enabling efficient bidirectional mapping between surface forms and morphological analyses."},"correct_answer":"D","difficulty":4,"source_article":"Morphological analysis (linguistics)","domain_ids":["computational-linguistics"],"concepts_tested":["Turkish morphological analysis","finite-state transducers","agglutinative morphology"],"x":0.772572,"y":0.747326},{"id":"2b7fdccd69a59211","question_text":"Abstract Meaning Representation (AMR) represents sentence meaning as rooted, labeled, directed acyclic graphs. What key linguistic phenomena does AMR capture in its graph structure, and what design principle distinguishes AMR from syntactic parse trees?","options":{"A":"AMR captures predicate-argument structure, coreference, named entities, negation, and modality as labeled nodes and edges. Unlike syntactic parse trees, AMR assigns a unique canonical graph representation to each individual word in isolation rather than representing whole sentence meanings regardless of wording.","B":"AMR captures predicate-argument structure, coreference, named entities, negation, and modality as labeled nodes and edges. Unlike syntactic parse trees, AMR abstracts away from surface morphological and syntactic variation so that sentences with the same meaning receive the same graph representation regardless of wording.","C":"AMR captures predicate-argument structure, coreference, named entities, negation, and modality as labeled nodes and edges. Unlike syntactic parse trees, AMR preserves word order and inflectional morphology so that sentences with different surface forms receive distinct graph representations reflecting their wording.","D":"AMR captures only predicate-argument structure and named entities as labeled nodes and edges, excluding coreference and modality. Unlike syntactic parse trees, AMR abstracts away from surface morphological and syntactic variation so that sentences with the same meaning receive the same graph representation regardless of wording."},"correct_answer":"B","difficulty":4,"source_article":"Abstract Meaning Representation","domain_ids":["computational-linguistics"],"concepts_tested":["Abstract Meaning Representation","semantic graph","syntax-semantics abstraction"],"x":0.748467,"y":0.732527}],"labels":[],"articles":[]}