[
  {
    "id": "cc4c89d7dfd359c6",
    "question_text": "The derivative of a function at a point represents what geometric property of the function's graph?",
    "options": {
      "A": "The slope of the tangent line to the curve at that point — equivalently, the instantaneous rate of change of the function",
      "B": "The area enclosed between the curve and the horizontal axis from zero up to that point, calculated by summing infinitely thin rectangular strips",
      "C": "The y-intercept of the function's graph, where it crosses the vertical axis",
      "D": "The perpendicular distance measured from the curve to the horizontal axis at that specific x-coordinate"
    },
    "correct_answer": "A",
    "difficulty": 1,
    "x": 0.711114,
    "y": 0.820535,
    "z": 0.81866,
    "source_article": "derivative definition",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "derivative definition",
      "tangent line",
      "instantaneous rate of change"
    ]
  },
  {
    "id": "0a4e658a2e39f810",
    "question_text": "The Fundamental Theorem of Calculus connects differentiation and integration. What does it state?",
    "options": {
      "A": "Differentiation and integration are entirely unrelated operations that were developed independently by different mathematicians and happen to share notation but have no underlying mathematical connection, which is why they are taught as separate subjects",
      "B": "If F is an antiderivative of a continuous function f on [a,b], then ∫ₐᵇ f(x)dx = F(b) - F(a). Equivalently, d/dx ∫ₐˣ f(t)dt = f(x). This establishes that differentiation and integration are inverse operations",
      "C": "The definite integral of any continuous function over any finite closed interval is always exactly zero, because the positive areas above the horizontal axis always perfectly cancel the negative areas below it by a symmetry principle that applies to all continuous functions without exception",
      "D": "Only polynomial functions can be integrated; transcendental functions like exponentials, trigonometric functions, and logarithms have no antiderivatives"
    },
    "correct_answer": "B",
    "difficulty": 1,
    "x": 0.665678,
    "y": 0.847136,
    "z": 0.174508,
    "source_article": "Fundamental Theorem of Calculus",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Fundamental Theorem of Calculus",
      "antiderivative",
      "differentiation-integration duality"
    ]
  },
  {
    "id": "9e24ceeb0866d0e4",
    "question_text": "A continuous function f on a closed interval [a,b] is guaranteed to attain its maximum and minimum values somewhere on that interval. What theorem establishes this, and why are the hypotheses necessary?",
    "options": {
      "A": "The Mean Value Theorem guarantees that every function on any interval attains its extreme values, regardless of whether the function is continuous or the interval is closed",
      "B": "No theorem guarantees this property — continuous functions on closed intervals can grow without bound or oscillate indefinitely without attaining a maximum or minimum value",
      "C": "The Extreme Value Theorem: a continuous function on a closed, bounded interval [a,b] must attain a global maximum and minimum. Continuity is necessary because a discontinuous function can 'jump over' its extremum. The closed interval is necessary because on an open interval (a,b), the function can approach but never reach its supremum (e.g., f(x) = x on (0,1) has sup = 1 but never attains it)",
      "D": "This property holds only for functions that are differentiable on the entire interval, not for functions that are merely continuous, because finding extrema requires setting the derivative equal to zero"
    },
    "correct_answer": "C",
    "difficulty": 1,
    "x": 0.646301,
    "y": 0.76,
    "z": 0.304907,
    "source_article": "Extreme Value Theorem",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Extreme Value Theorem",
      "continuity",
      "closed intervals",
      "compactness"
    ]
  },
  {
    "id": "c304497f71820446",
    "question_text": "L'Hôpital's rule provides a method for evaluating indeterminate forms. Under what conditions does it apply, and what does it state?",
    "options": {
      "A": "L'Hôpital's rule applies to every limit calculation regardless of whether the form is indeterminate, because differentiating the numerator and denominator separately always simplifies the expression and produces the correct limiting value for any rational function",
      "B": "L'Hôpital's rule states that the limit of a product of two functions always equals the product of their individual limits, which is really the standard algebraic limit law rephrased with derivative notation rather than a genuinely new technique for resolving indeterminate forms",
      "C": "L'Hôpital's rule is a technique for solving differential equations by converting them into algebraic equations through a process of successive differentiation, and it has no application to evaluating limits of ratios or resolving indeterminate forms",
      "D": "If lim[x→c] f(x)/g(x) produces an indeterminate form 0/0 or ∞/∞, and if lim[x→c] f'(x)/g'(x) exists (or is ±∞), then lim[x→c] f(x)/g(x) = lim[x→c] f'(x)/g'(x). Both f and g must be differentiable near c with g'(x) ≠ 0 near c"
    },
    "correct_answer": "D",
    "difficulty": 1,
    "x": 0.658651,
    "y": 0.922342,
    "z": 0.688375,
    "source_article": "L'Hôpital's rule",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "L'Hôpital's rule",
      "indeterminate forms",
      "limit evaluation"
    ]
  },
  {
    "id": "ed628e9a7044e053",
    "question_text": "Integration by parts is derived from which differentiation rule, and when is it the natural technique to apply?",
    "options": {
      "A": "It is derived from the product rule for differentiation: ∫u·dv = u·v - ∫v·du. It is natural when the integrand is a product of two functions where one becomes simpler upon differentiation (u) and the other is easily integrated (dv) — such as ∫x·eˣ dx or ∫x·sin(x) dx, where differentiating x simplifies it to 1",
      "B": "Integration by parts is derived from the chain rule for composite functions and applies specifically when the integrand has the form f(g(x))·g'(x), allowing substitution of the inner function",
      "C": "Integration by parts is derived from the quotient rule for differentiation and is specifically designed for integrands that are ratios of two functions, converting the ratio into a more tractable form",
      "D": "Integration by parts works exclusively for polynomial integrands of degree three or higher, because the method requires at least three successive differentiations to reduce the integrand to a constant. For polynomials of degree two or lower, basic antidifferentiation rules suffice and the by-parts technique produces circular results that fail to simplify the integral"
    },
    "correct_answer": "A",
    "difficulty": 2,
    "x": 0.703149,
    "y": 0.969663,
    "z": 0.284066,
    "source_article": "integration by parts",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "integration by parts",
      "product rule connection",
      "technique selection"
    ]
  },
  {
    "id": "a64294a9bd3ea863",
    "question_text": "A Taylor series represents a function as an infinite sum of polynomial terms. Under what conditions does a Taylor series converge to the original function, and when can it fail?",
    "options": {
      "A": "Taylor series always converge to the original function for every smooth function at every point on the real line, because having infinitely many continuous derivatives at every point guarantees that the polynomial approximation captures the function's behavior exactly everywhere. The notion of a finite radius of convergence applies only to Laurent series in complex analysis, not to ordinary real-valued Taylor expansions",
      "B": "A Taylor series ∑(f⁽ⁿ⁾(a)/n!)·(x-a)ⁿ converges to f(x) within its radius of convergence, provided the remainder term Rₙ(x) → 0 as n → ∞. It can fail for functions that are smooth but not analytic: the classic example is f(x) = e^(-1/x²) (with f(0) = 0), whose Taylor series at 0 is identically zero (all derivatives at 0 vanish) even though the function is nonzero for x ≠ 0",
      "C": "Taylor series can only represent polynomial functions, because the sum of polynomial terms (each involving a power of x) is itself always a polynomial. Transcendental functions like eˣ, sin(x), and ln(x) fundamentally cannot be represented as sums of polynomial terms because they contain essential non-algebraic behavior that no finite or infinite sum of powers of x can ever faithfully capture",
      "D": "Taylor series converge only at the single expansion point x = a and diverge at every other point, making them useless for approximation"
    },
    "correct_answer": "B",
    "difficulty": 2,
    "x": 0.631256,
    "y": 0.883609,
    "z": 0.804806,
    "source_article": "Taylor series",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Taylor series",
      "radius of convergence",
      "smooth vs analytic",
      "remainder theorem"
    ]
  },
  {
    "id": "7cf9d0b498b776c4",
    "question_text": "The epsilon-delta definition of a limit is the rigorous foundation of calculus. What does 'lim[x→c] f(x) = L' mean precisely?",
    "options": {
      "A": "It means the function value at the exact point c equals L, which requires f to be defined at c and have that specific value there",
      "B": "It means f(x) approaches L as x grows toward positive or negative infinity, measuring the end behavior of the function at extreme values",
      "C": "For every ε > 0, there exists a δ > 0 such that for all x satisfying 0 < |x - c| < δ, we have |f(x) - L| < ε. This formalizes the intuition that f(x) can be made arbitrarily close to L by taking x sufficiently close to (but not equal to) c",
      "D": "It means f(x) = L for every x in some open interval containing c, so the function is constant on a neighborhood of that point"
    },
    "correct_answer": "C",
    "difficulty": 2,
    "x": 0.642682,
    "y": 0.841476,
    "z": 0.694866,
    "source_article": "epsilon-delta definition",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "epsilon-delta definition",
      "limits",
      "mathematical rigor",
      "Weierstrass"
    ]
  },
  {
    "id": "d65daebc6d7eceb5",
    "question_text": "Improper integrals extend integration to unbounded intervals or unbounded integrands. What determines whether an improper integral converges or diverges?",
    "options": {
      "A": "All improper integrals diverge to infinity because you cannot meaningfully sum an infinite number of areas. The notation ∫₁^∞ f(x)dx is a formal symbol representing a divergent process, and any finite value assigned to it is a convention rather than a genuine mathematical result. This is why improper integrals are excluded from the Riemann theory of integration and can only be handled using regularization schemes",
      "B": "An improper integral converges whenever the integrand f(x) is positive on the entire interval of integration, because positive functions accumulate area monotonically and the resulting limit of the partial integrals always exists and is finite. Divergence can occur only when the integrand oscillates between positive and negative values, creating destructive cancellation that can produce an undefined oscillating limit",
      "C": "An improper integral converges if and only if the integrand is continuous on the entire interval of integration including the point at infinity. Continuity ensures that the function does not blow up or oscillate wildly, which in turn guarantees that the area under the curve remains finite. Discontinuous integrands always produce divergent improper integrals regardless of how they behave at infinity",
      "D": "An improper integral ∫₁^∞ f(x)dx converges if the limit lim[b→∞] ∫₁ᵇ f(x)dx exists and is finite. Convergence depends on how fast the integrand decays: ∫₁^∞ 1/xᵖ dx converges if and only if p > 1 (the p-test). The comparison test, limit comparison test, and absolute convergence provide further tools for determining convergence without computing the integral explicitly"
    },
    "correct_answer": "D",
    "difficulty": 2,
    "x": 0.639153,
    "y": 0.955955,
    "z": 0.583521,
    "source_article": "improper integrals",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "improper integrals",
      "convergence tests",
      "p-test",
      "comparison test"
    ]
  },
  {
    "id": "c16a301784ef4ba5",
    "question_text": "The Mean Value Theorem (MVT) states that for a function continuous on [a,b] and differentiable on (a,b), there exists a point c in (a,b) where f'(c) = (f(b)-f(a))/(b-a). Why is this theorem so important, beyond its geometric interpretation?",
    "options": {
      "A": "The MVT is not important in itself but serves as a fundamental bridge between local information (derivative) and global behavior (function values). It is the key tool proving that: (1) if f'(x) = 0 on an interval then f is constant; (2) if f'(x) > 0 then f is increasing; (3) Taylor's theorem (via the mean value form of the remainder); (4) the uniqueness of antiderivatives up to constants. Essentially, it justifies using derivative information to draw conclusions about function behavior",
      "B": "The MVT has purely geometric significance as a statement about secant and tangent lines, with no deeper theoretical implications for analysis, differential equations, or the foundations of calculus",
      "C": "The MVT is important exclusively for physics applications such as calculating average velocity and has no relevance to pure mathematics, real analysis, or theoretical calculus",
      "D": "The MVT is important primarily because it provides a practical formula for computing derivatives numerically by evaluating the difference quotient [f(b)-f(a)]/(b-a) at the guaranteed interior point c. Without the MVT, there would be no way to approximate derivatives from function values, which is why the theorem is the foundation of all numerical differentiation algorithms. The MVT also establishes that every continuous function is differentiable at at least one interior point, providing a partial converse to the theorem that differentiable functions are continuous"
    },
    "correct_answer": "A",
    "difficulty": 3,
    "x": 0.658153,
    "y": 0.763654,
    "z": 0.298059,
    "source_article": "Mean Value Theorem",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Mean Value Theorem",
      "theoretical applications",
      "local-to-global reasoning"
    ]
  },
  {
    "id": "b3ccd071bbf6ab70",
    "question_text": "Multivariable calculus extends differentiation to functions of several variables. The gradient ∇f gives the direction of steepest ascent. What deeper geometric meaning does the gradient have, and how does it relate to level curves?",
    "options": {
      "A": "The gradient vector is always parallel to (tangent to) the level curves of the function at each point, running along the contour of constant function value rather than crossing it perpendicularly. This tangential alignment is why gradient descent algorithms follow level curves rather than crossing them directly, tracing out paths that remain on surfaces of constant function value while slowly spiraling inward toward local minima",
      "B": "The gradient ∇f at a point is perpendicular (normal) to the level set {x : f(x) = c} passing through that point, pointing in the direction of greatest rate of increase of f. Its magnitude |∇f| equals the maximum directional derivative. This perpendicularity follows from the chain rule: if r(t) is a curve on the level set, then f(r(t)) = c implies ∇f · r'(t) = 0, so ∇f is orthogonal to all tangent vectors of the level set",
      "C": "The gradient vector exists only for linear functions of several variables, because nonlinear functions have curved level sets whose local geometry varies from point to point in a way that prevents the definition of a single consistent directional vector. For nonlinear functions, the directional derivative depends on the chosen direction in a nonlinear way that cannot be captured by any single vector, which is why the gradient is restricted to the linear case and more general tools like the Hessian matrix are needed for nonlinear analysis",
      "D": "The gradient always points toward the origin regardless of the function, because it measures the direction from the current point back to the center of the coordinate system"
    },
    "correct_answer": "B",
    "difficulty": 3,
    "x": 0.742113,
    "y": 0.770258,
    "z": 0.77957,
    "source_article": "gradient",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "gradient",
      "level sets",
      "directional derivative",
      "multivariable calculus"
    ]
  },
  {
    "id": "6eb2fc97512e44b2",
    "question_text": "The divergence theorem (Gauss's theorem) and Stokes' theorem generalize the Fundamental Theorem of Calculus to higher dimensions. What is the unifying principle?",
    "options": {
      "A": "These theorems apply only to two-dimensional planar surfaces embedded in three-dimensional space and have no generalization to higher-dimensional manifolds or abstract differential forms",
      "B": "There is no mathematical connection between the divergence theorem, Stokes' theorem, and the Fundamental Theorem of Calculus — they address entirely different phenomena and were developed independently",
      "C": "The unifying principle is that the integral of a 'derivative' (divergence, curl) over a region equals the integral of the 'original quantity' (flux, circulation) over the boundary. The generalized Stokes' theorem ∫_Ω dω = ∫_∂Ω ω unifies all these results: FTC (0-forms on intervals), Green's theorem (1-forms on planar regions), classical Stokes' (1-forms on surfaces), and divergence theorem (2-forms on volumes) are all special cases of integrating an exterior derivative over a manifold and equating it to integrating the form over the boundary",
      "D": "These theorems apply exclusively to conservative vector fields (those with zero curl), because only conservative fields have well-defined potential functions that allow the boundary integral to be evaluated"
    },
    "correct_answer": "C",
    "difficulty": 3,
    "x": 0.703339,
    "y": 0.85989,
    "z": 0.0,
    "source_article": "generalized Stokes theorem",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "generalized Stokes theorem",
      "divergence theorem",
      "exterior derivative",
      "differential forms"
    ]
  },
  {
    "id": "a34be071b062056d",
    "question_text": "Uniform convergence of a sequence of functions fₙ → f is a stronger condition than pointwise convergence. Why is uniform convergence important, and what can fail with merely pointwise convergence?",
    "options": {
      "A": "Uniform convergence and pointwise convergence are two different names for the identical mathematical concept — both require that fₙ(x) → f(x) for each x in the domain, and the distinction drawn in textbooks is a purely pedagogical simplification with no actual mathematical content. The qualifier 'uniform' was introduced by Weierstrass purely for rhetorical emphasis but does not impose any additional condition beyond what pointwise convergence already demands. Any sequence that converges pointwise automatically converges uniformly",
      "B": "Pointwise convergence is always sufficient for interchanging limits with both integration and differentiation, because the dominated convergence theorem guarantees that limₙ ∫fₙ = ∫lim fₙ whenever fₙ → f pointwise. The concept of uniform convergence was introduced historically by Weierstrass before Lebesgue's measure-theoretic framework made it obsolete, and in modern Lebesgue integration theory, pointwise convergence combined with a dominating function provides strictly stronger interchange theorems than uniform convergence can ever provide",
      "C": "Uniform convergence is a concept that matters only for sequences of polynomial functions, because only polynomials have the algebraic structure needed to make the 'same N works for all x' condition meaningful. For sequences of transcendental, piecewise-defined, or distribution-valued functions, the rate of convergence inherently varies with x in ways that cannot be captured by any single uniform bound, which is why functional analysis replaces uniform convergence with weaker topologies like weak convergence and convergence in distribution for non-polynomial function spaces",
      "D": "Uniform convergence means |fₙ(x) - f(x)| < ε for ALL x simultaneously (the same N works for every x), unlike pointwise convergence where N can depend on x. This is critical because: (1) the pointwise limit of continuous functions can be discontinuous, but the uniform limit of continuous functions is continuous; (2) ∫ lim fₙ = lim ∫ fₙ is guaranteed under uniform convergence but can fail under pointwise convergence; (3) term-by-term differentiation of series requires uniform convergence of the derivative series"
    },
    "correct_answer": "D",
    "difficulty": 3,
    "x": 0.61,
    "y": 0.898964,
    "z": 0.431683,
    "source_article": "uniform convergence",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "uniform convergence",
      "pointwise convergence",
      "interchange of limits",
      "continuity preservation"
    ]
  },
  {
    "id": "34b596bd33dea75e",
    "question_text": "The method of Lagrange multipliers finds extrema of a function subject to constraints. What is the geometric insight behind the condition ∇f = λ∇g?",
    "options": {
      "A": "At a constrained extremum, the gradient of the objective function f must be parallel to the gradient of the constraint function g — meaning the level curve of f is tangent to the constraint surface g(x) = c. If they were not parallel, moving along the constraint in the direction of the component of ∇f tangent to the constraint surface would increase (or decrease) f, contradicting the assumption of an extremum",
      "B": "The condition ∇f = λ∇g means that f and g are the same function up to a constant multiple, which is why constrained optimization reduces to unconstrained optimization",
      "C": "Lagrange multipliers can only be applied when the constraint function defines a straight line or hyperplane in the domain, because curved constraints require nonlinear optimization techniques",
      "D": "The condition ∇f = λ∇g simply means that the objective function f equals zero at the constrained extremum, because the gradient being proportional to ∇g forces f to vanish at points where the constraint is active. The multiplier λ measures how far from zero f would be without the constraint, providing a sensitivity analysis of the optimum with respect to constraint relaxation. This zero-value interpretation is why Lagrange multipliers are used in complementarity problems where the objective function is set equal to zero at equilibrium"
    },
    "correct_answer": "A",
    "difficulty": 4,
    "x": 0.71882,
    "y": 0.795481,
    "z": 0.82227,
    "source_article": "Lagrange multipliers",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Lagrange multipliers",
      "constrained optimization",
      "gradient parallelism",
      "tangency condition"
    ]
  },
  {
    "id": "73d774fc6b9fd77d",
    "question_text": "The Jacobian matrix generalizes the derivative to multivariable functions. What role does the Jacobian determinant play in integration, and why?",
    "options": {
      "A": "The Jacobian determinant plays no role whatsoever in integration or change of variables — it is used exclusively in the study of dynamical systems to measure the local stability of fixed points and periodic orbits. Its appearance in multivariable calculus textbooks alongside coordinate transformations is a historical artifact from the era before the development of differential forms, which provide a coordinate-free theory of integration that makes the Jacobian entirely unnecessary. Modern treatments of integration on manifolds never mention the Jacobian",
      "B": "When performing a change of variables in a multiple integral (e.g., from Cartesian to polar coordinates), the Jacobian determinant |∂(x,y)/∂(r,θ)| = r gives the local scaling factor for area elements: dA = dx·dy becomes r·dr·dθ. Geometrically, the absolute value of the Jacobian determinant measures how much the transformation locally stretches or compresses volume — it is the ratio of infinitesimal volumes in the new coordinates to the old",
      "C": "The Jacobian determinant is always exactly equal to one for every coordinate transformation, because coordinate transformations are defined as volume-preserving diffeomorphisms. If a transformation changed volumes, it would distort the metric of the space and therefore would not qualify as a valid coordinate change under the standard definition. This unit-Jacobian property is what makes change of variables in integration straightforward — the integrand transforms but the volume element dx dy remains unchanged",
      "D": "The Jacobian matrix is a concept from linear algebra with no connection to calculus or integration, used for classifying square matrices by their rank"
    },
    "correct_answer": "B",
    "difficulty": 4,
    "x": 0.776713,
    "y": 0.890394,
    "z": 0.227442,
    "source_article": "Jacobian matrix",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Jacobian matrix",
      "change of variables",
      "area/volume scaling",
      "coordinate transformations"
    ]
  },
  {
    "id": "0831a0de9a242171",
    "question_text": "Green's functions are a powerful technique for solving linear differential equations. What is the conceptual idea, and why is it named after a 'function'?",
    "options": {
      "A": "Green's functions are simply antiderivatives expressed using a different name and notation. Finding the Green's function for a differential equation is identical to finding the antiderivative of the source term, and the convolution integral that produces the solution is just another way of writing the indefinite integral. The distinction between Green's functions and antiderivatives was introduced by physicists who wanted domain-specific terminology but were describing the same mathematical operation that calculus students learn as integration",
      "B": "Green's functions are a technique for solving algebraic equations (polynomial equations in one variable), not differential equations. George Green developed the method as an alternative to the quadratic formula for second-degree polynomials, and it was later extended to cubic and quartic equations. The method has no application to differential equations because it requires the unknown to appear only as a polynomial power, not as a derivative",
      "C": "A Green's function G(x,s) for a linear differential operator L is the response of the system to a point source (delta function) at position s: L·G(x,s) = δ(x-s). Once known, the solution to L·u(x) = f(x) for any source f is obtained by superposition: u(x) = ∫G(x,s)·f(s)ds. This works because linearity allows decomposing any source into a superposition of point sources, and the total response is the superposition of individual responses",
      "D": "Green's functions require the differential equation to be nonlinear, because linear equations can always be solved by direct integration or separation of variables. The superposition principle that underlies the Green's function method — decomposing an arbitrary source into delta functions — works only when the operator is nonlinear, since nonlinearity allows the individual responses to be combined multiplicatively rather than additively"
    },
    "correct_answer": "C",
    "difficulty": 4,
    "x": 0.710093,
    "y": 0.914077,
    "z": 0.296325,
    "source_article": "Green's functions",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Green's functions",
      "linear superposition",
      "delta function",
      "integral representation"
    ]
  },
  {
    "id": "bc1879038efc7576",
    "question_text": "The Lebesgue integral extends the Riemann integral to handle a broader class of functions. What is the key conceptual difference in how Lebesgue integration works?",
    "options": {
      "A": "The Lebesgue integral uses fundamentally the same domain-partitioning approach as the Riemann integral but with infinitely finer subdivisions of the x-axis, achieving convergence through brute-force refinement rather than any conceptual innovation",
      "B": "The Lebesgue integral is a specialized tool that works only for continuous functions and cannot handle the discontinuous or pathological functions that motivated its development",
      "C": "The Lebesgue integral is unable to handle functions with any discontinuities, because the measure-theoretic machinery breaks down at points where the function jumps",
      "D": "The Riemann integral partitions the domain (x-axis) into subintervals and sums f(x)·Δx. The Lebesgue integral instead partitions the range (y-axis): for each value y, it measures the 'size' (Lebesgue measure) of the set {x : f(x) ≈ y}, then sums y · μ({x : f(x) ≈ y}). This range-based approach handles functions with complicated discontinuity sets (like the Dirichlet function, which equals 1 on rationals and 0 on irrationals) that the Riemann integral cannot, and provides powerful convergence theorems (dominated convergence, monotone convergence)"
    },
    "correct_answer": "D",
    "difficulty": 4,
    "x": 0.657903,
    "y": 0.952594,
    "z": 0.220812,
    "source_article": "Lebesgue integration",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Lebesgue integration",
      "measure theory",
      "Riemann vs Lebesgue",
      "convergence theorems"
    ]
  },
  {
    "id": "11649a8d091d84bc",
    "question_text": "The calculus of variations finds functions that optimize functionals (functions of functions). The Euler-Lagrange equation is the fundamental result. What is the key idea, and how does it connect to physics?",
    "options": {
      "A": "The Euler-Lagrange equation is used to find the function y(x) that makes a functional J[y] = ∫L(x, y, y')dx stationary. Setting the first variation δJ = 0 yields ∂L/∂y - d/dx(∂L/∂y') = 0. This connects to physics through the principle of least action: physical trajectories minimize the action S = ∫L dt, so the Euler-Lagrange equations for the Lagrangian L = T - V yield Newton's equations of motion — providing a variational foundation for all of classical mechanics",
      "B": "The Euler-Lagrange equation is a standard ordinary differential equation for finding explicit formulas for derivatives, unrelated to optimization of functionals or variational principles",
      "C": "The calculus of variations applies exclusively to the problem of finding shortest paths (geodesics) between two points and has no application in physics, mechanics, optics, or any other scientific domain",
      "D": "The Euler-Lagrange equation can only be applied to functionals whose integrand is a quadratic function of y and y', because the variational derivative requires a second-degree polynomial structure to produce a solvable differential equation"
    },
    "correct_answer": "A",
    "difficulty": 5,
    "x": 0.711687,
    "y": 0.826207,
    "z": 0.478078,
    "source_article": "calculus of variations",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "calculus of variations",
      "Euler-Lagrange equation",
      "principle of least action",
      "Lagrangian mechanics"
    ]
  },
  {
    "id": "12ad9f5bc7865117",
    "question_text": "Fourier series decompose periodic functions into sums of sines and cosines. What is the deep mathematical reason this works, and what does convergence mean for discontinuous functions?",
    "options": {
      "A": "Fourier series work because sines and cosines are the only bounded periodic functions that exist on the interval [0, 2π], and since they span the full space of all bounded periodic functions by the Riesz representation theorem, any other bounded periodic function can necessarily be expressed as their linear combination. This spanning property is a consequence of the fact that sin(nx) and cos(nx) are the only solutions to the second-order boundary-value problem that remain bounded, making them the natural and inevitable basis for all periodic phenomena",
      "B": "The sines and cosines {1, cos(nx), sin(nx)} form a complete orthogonal basis for the Hilbert space L²([0,2π]) with the inner product ⟨f,g⟩ = ∫f·g dx. Completeness means any square-integrable function can be approximated arbitrarily well in the L² norm. For discontinuous functions, the Fourier series converges in L² but exhibits the Gibbs phenomenon at jump discontinuities: the partial sums overshoot by ~9% near the jump, and this overshoot does not disappear as more terms are added — it merely becomes narrower",
      "C": "Fourier series converge uniformly to the original function for every square-integrable function without any exception, including functions with jump discontinuities and functions that are unbounded near isolated singular points, because the orthogonality of the trigonometric basis functions ensures that the partial sums never overshoot or oscillate near points of discontinuity. The Gibbs phenomenon, which appears to show persistent overshooting near jumps, is actually a computational artifact of truncating the series at finite N rather than a genuine mathematical phenomenon of the full infinite series",
      "D": "Fourier series are purely an empirical approximation technique with no rigorous theoretical foundation in functional analysis or harmonic analysis"
    },
    "correct_answer": "B",
    "difficulty": 5,
    "x": 0.645746,
    "y": 0.900423,
    "z": 0.377101,
    "source_article": "Fourier series",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Fourier series",
      "orthogonal basis",
      "L2 convergence",
      "Gibbs phenomenon",
      "Hilbert space"
    ]
  },
  {
    "id": "7430af504b5bbfb0",
    "question_text": "The implicit function theorem guarantees that under certain conditions, a relation F(x,y) = 0 locally defines y as a function of x. What is the precise condition, and why is this theorem fundamental to differential geometry and analysis?",
    "options": {
      "A": "The implicit function theorem applies only to relations F(x,y) = 0 where F is a polynomial in both x and y, because the proof uses algebraic elimination theory to solve for y explicitly",
      "B": "The implicit function theorem applies only when the relation F(x,y) = 0 is linear in y, taking the form F(x,y) = a(x)y + b(x) = 0, so that y can be solved as y = -b(x)/a(x) by elementary algebra",
      "C": "If F(a,b) = 0 and ∂F/∂y(a,b) ≠ 0 (with F continuously differentiable), then near (a,b), the equation F(x,y) = 0 implicitly defines y as a differentiable function y(x). This is fundamental because: it guarantees smooth manifolds can be locally parameterized as graphs (essential for differential geometry), it underpins the inverse function theorem (the IFT is a corollary of the implicit function theorem), and it provides the existence of solution curves for differential equations via the transformation to fixed-point problems",
      "D": "The implicit function theorem guarantees the existence of a global function y(x) defined for all real values of x, not merely a local function near a single point. The condition ∂F/∂y ≠ 0 at one point propagates along the solution curve by continuity, ensuring that the implicit function extends uniquely to the entire real line. This global existence result is the primary value of the theorem, because local implicit functions near a single point can always be constructed by Newton's method without any theorem. The common textbook statement emphasizing locality is a weakened version that obscures the theorem's true power"
    },
    "correct_answer": "C",
    "difficulty": 5,
    "x": 0.695083,
    "y": 0.796464,
    "z": 0.350336,
    "source_article": "implicit function theorem",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "implicit function theorem",
      "non-degeneracy condition",
      "differential geometry",
      "manifolds"
    ]
  },
  {
    "id": "0562db5389774709",
    "question_text": "The Weierstrass approximation theorem states that every continuous function on a closed interval can be uniformly approximated by polynomials. Why is this result surprising, and what is its deeper significance in functional analysis?",
    "options": {
      "A": "The Weierstrass approximation theorem is trivially obvious because every continuous function on a closed interval is itself a polynomial, making the statement that polynomials can approximate continuous functions a mere tautology. The proof that continuous functions on closed bounded intervals are necessarily polynomials follows from the intermediate value theorem combined with the extreme value theorem, which together force continuous functions to have the algebraic structure of polynomials by constraining their zero sets and extreme values. The theorem is included in analysis textbooks purely for historical completeness, as Weierstrass himself acknowledged it was a rather trivial observation about objects that were already known to be polynomial in nature",
      "B": "The Weierstrass approximation theorem applies only to infinitely differentiable (smooth) functions, not to all continuous functions. The proof constructs the approximating polynomials by truncating the Taylor series of the target function, which requires the function to possess derivatives of all orders at every point. For merely continuous functions that are not differentiable — such as the absolute value function |x| or the Weierstrass nowhere-differentiable function — no sequence of polynomials can converge uniformly, because the non-differentiable points create obstructions that smooth polynomials cannot approximate within any finite uniform error tolerance",
      "C": "The theorem has no theoretical significance beyond the practical technique of polynomial interpolation, which was already well-known to Newton and Lagrange centuries before Weierstrass. The existence of Lagrange interpolating polynomials passing through finitely many data points trivially implies that polynomials can approximate any continuous function to arbitrary accuracy, making Weierstrass's contribution entirely redundant. The distinction between interpolation at finitely many points and uniform approximation over an entire continuous interval is a pedantic technicality with no mathematical or practical consequence, since increasing the number of interpolation points always produces uniform convergence by the equidistribution theorem",
      "D": "The surprise is that polynomials — which are smooth, analytic, and algebraic — can uniformly approximate continuous functions that may be nowhere differentiable, fractal-like, or have infinitely many oscillations. The deeper significance is that polynomials are dense in C([a,b]) with the supremum norm. This is a special case of the Stone-Weierstrass theorem, which characterizes which subalgebras of C(X) are dense: any subalgebra that separates points and contains constants is uniformly dense. This underpins approximation theory, numerical analysis (justifying polynomial-based methods), and the theory of function spaces"
    },
    "correct_answer": "D",
    "difficulty": 5,
    "x": 0.644853,
    "y": 0.835048,
    "z": 0.143792,
    "source_article": "Weierstrass approximation",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Weierstrass approximation",
      "Stone-Weierstrass theorem",
      "density in function spaces",
      "uniform approximation"
    ]
  },
  {
    "id": "88d279bee6be55cf",
    "question_text": "The chain rule is a fundamental differentiation technique for composite functions. If y = f(g(x)), what is dy/dx and why does this rule fail to apply when the outer or inner function is not differentiable?",
    "options": {
      "A": "dy/dx = f'(g(x)) · g'(x) — the derivative of the composite is the derivative of the outer function evaluated at the inner function, multiplied by the derivative of the inner function, reflecting that small changes in x are first scaled by g'(x) and then by f' at the resulting point, which requires both f and g to be differentiable at the relevant points for the product to exist",
      "B": "The chain rule for composite functions states that dy/dx = f'(x) · g'(x), which means you simply multiply the derivatives of the two component functions evaluated independently at x without any composition — there is no nesting, no evaluation of the outer function at the inner function's value, and the chain of dependencies between the functions plays absolutely no role in the differentiation process whatsoever",
      "C": "The chain rule is a specialized differentiation technique that applies exclusively to polynomial functions of the form f(g(x)) where both f and g are polynomials of degree at most five, and it fundamentally cannot be used with any trigonometric, exponential, logarithmic, hyperbolic, or inverse trigonometric composite function regardless of whether those component functions are differentiable at the point in question",
      "D": "The derivative of a composite function y = f(g(x)) is computed by adding rather than multiplying the component derivatives: dy/dx = f'(g(x)) + g'(x), because the total rate of change of a composition is the sum of the individual rates of change of the outer and inner functions, analogous to how the derivative of a sum is the sum of the derivatives by the linearity of differentiation"
    },
    "correct_answer": "A",
    "difficulty": 1,
    "x": 0.697675,
    "y": 0.884767,
    "z": 0.542291,
    "source_article": "chain rule",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "chain rule",
      "composite functions",
      "differentiability"
    ]
  },
  {
    "id": "790782b1ffaedc96",
    "question_text": "The Intermediate Value Theorem (IVT) is a foundational result about continuous functions. What does it guarantee, and why does it require continuity on a closed interval?",
    "options": {
      "A": "The IVT guarantees that a continuous function achieves both its maximum and minimum values on any interval",
      "B": "If f is continuous on [a,b] and N is any value between f(a) and f(b), then there exists some c in (a,b) with f(c) = N — continuity ensures the function cannot 'jump over' intermediate values, meaning the graph must cross every horizontal line between f(a) and f(b), which fails for discontinuous functions that can skip values",
      "C": "The Intermediate Value Theorem applies universally to all functions defined on a closed interval whether they are continuous, discontinuous, or even undefined at certain points within the interval — every function must take on all intermediate values between its endpoint values as an inherent algebraic property of functions themselves, completely independent of any topological condition such as continuity",
      "D": "The IVT states that f(a) must equal f(b) for any continuous function on [a,b]"
    },
    "correct_answer": "B",
    "difficulty": 1,
    "x": 0.648759,
    "y": 0.79135,
    "z": 0.182739,
    "source_article": "Intermediate Value Theorem",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Intermediate Value Theorem",
      "continuity",
      "existence of roots"
    ]
  },
  {
    "id": "e56485cfada7687a",
    "question_text": "Rolle's theorem is a special case of the Mean Value Theorem. What are its hypotheses and conclusion, and what geometric interpretation does it have?",
    "options": {
      "A": "Rolle's theorem states that any differentiable function defined on any interval must have exactly one critical point on that interval — there cannot be zero, two, or more points where the derivative equals zero, regardless of the function's values at the endpoints, whether the function is continuous on the closed interval, or what behavior the function exhibits within the interior of the interval in question",
      "B": "Rolle's theorem applies to absolutely any function defined on any interval regardless of whether the function is continuous on the closed interval, differentiable on the open interval, or satisfies any particular condition relating the endpoint values — no hypotheses whatsoever are needed, and the conclusion that some interior point has a zero derivative holds universally for all functions on all intervals without exception",
      "C": "If f is continuous on [a,b], differentiable on (a,b), and f(a) = f(b), then there exists at least one c in (a,b) where f'(c) = 0 — geometrically, if a continuous differentiable curve starts and ends at the same height, it must have at least one horizontal tangent line somewhere between the endpoints, which is proved by the Extreme Value Theorem forcing a maximum or minimum in the interior",
      "D": "Rolle's theorem guarantees that f'(c) = (f(b)-f(a))/(b-a) for some c, even when f(a) ≠ f(b)"
    },
    "correct_answer": "C",
    "difficulty": 1,
    "x": 0.677161,
    "y": 0.786541,
    "z": 0.321156,
    "source_article": "Rolle's theorem",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Rolle's theorem",
      "critical points",
      "horizontal tangent"
    ]
  },
  {
    "id": "27d208fea6f27200",
    "question_text": "Convergence tests for infinite series are essential for determining whether a sum Σaₙ converges. What is the ratio test, and what are its limitations?",
    "options": {
      "A": "The ratio test examines only whether the individual terms aₙ of a series approach zero as n grows large and it provides absolutely no information about the rate at which consecutive terms decrease relative to one another — this is fundamentally incorrect because the ratio test specifically computes the limit of |aₙ₊₁/aₙ| to determine convergence behavior based on the asymptotic geometric decay rate",
      "B": "The ratio test always gives a completely definitive and conclusive answer for any infinite series to which it is applied, and there are absolutely no cases in which the ratio test yields an inconclusive result — this is false because when the limit L equals exactly 1, the ratio test is genuinely inconclusive and provides no information about whether the series converges or diverges",
      "C": "The ratio test applies only to alternating series where consecutive terms switch sign and cannot be used for series whose terms are all positive or all nonnegative — this is incorrect because the ratio test works for any series with nonzero terms by examining |aₙ₊₁/aₙ|, using absolute values to handle arbitrary sign patterns including both positive and alternating series",
      "D": "The ratio test computes L = lim|aₙ₊₁/aₙ|: if L < 1 the series converges absolutely, if L > 1 it diverges, and if L = 1 the test is inconclusive — it excels for series involving factorials and exponentials where the ratio simplifies, but fails (gives L = 1) for polynomial-dominated series like Σ1/nᵖ, requiring other tests such as the integral or comparison test"
    },
    "correct_answer": "D",
    "difficulty": 2,
    "x": 0.610445,
    "y": 0.948956,
    "z": 0.844969,
    "source_article": "ratio test",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "ratio test",
      "series convergence",
      "inconclusive cases"
    ]
  },
  {
    "id": "02bcfad1cf3d5d9f",
    "question_text": "The radius of convergence R of a power series Σaₙ(x-c)ⁿ determines the interval on which the series converges. How is R computed, and what happens at the boundary points x = c ± R?",
    "options": {
      "A": "The radius of convergence is R = lim|aₙ/aₙ₊₁| (or 1/lim sup|aₙ|^(1/n) via the root test) — the series converges absolutely for |x-c| < R and diverges for |x-c| > R, but convergence at the endpoints x = c ± R must be checked separately because the ratio/root tests are inconclusive there, leading to half-open, open, or closed intervals depending on endpoint behavior",
      "B": "A power series always converges at every single point within its radius of convergence including both endpoints of the convergence interval without exception, meaning that convergence at the boundary points is automatically guaranteed by convergence within the interior — this is incorrect because convergence at the endpoints requires separate analysis using tests like Abel's theorem or Dirichlet's test",
      "C": "The radius of convergence of a power series depends solely on the choice of center point c and is completely independent of the coefficients aₙ appearing in the series — this is backwards because the radius is determined entirely by the coefficients via R = 1/lim sup|aₙ|^(1/n) (the Cauchy-Hadamard formula), and the center c affects only where the interval is located, not its width",
      "D": "Power series either converge everywhere or nowhere, with no intermediate radius"
    },
    "correct_answer": "A",
    "difficulty": 2,
    "x": 0.630059,
    "y": 0.942176,
    "z": 1.0,
    "source_article": "radius of convergence",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "radius of convergence",
      "power series",
      "endpoint behavior"
    ]
  },
  {
    "id": "0a9abc5af209535f",
    "question_text": "Integration using partial fractions decomposes rational functions into simpler terms. When does this method apply, and what form do the partial fraction terms take for repeated linear factors?",
    "options": {
      "A": "The method of partial fraction decomposition is a technique that works only for polynomial (non-rational) functions where the numerator has degree greater than the denominator, and it fundamentally cannot handle any proper rational function where the numerator's degree is strictly less than the denominator's degree — this reverses the actual requirement, since partial fractions are specifically designed for proper rational functions with deg(numerator) < deg(denominator)",
      "B": "For a rational function P(x)/Q(x) where deg(P) < deg(Q), partial fraction decomposition expresses it as a sum of simpler fractions — for a repeated linear factor (x-a)ᵏ in Q(x), the decomposition includes terms A₁/(x-a) + A₂/(x-a)² + ... + Aₖ/(x-a)ᵏ, one for each power up to the multiplicity, because the fundamental theorem of algebra guarantees this decomposition exists and the resulting terms can each be integrated using elementary antiderivatives",
      "C": "Repeated factors require only a single term A/(x-a) regardless of the multiplicity of the root",
      "D": "Partial fractions can only be applied when Q(x) has no real roots"
    },
    "correct_answer": "B",
    "difficulty": 2,
    "x": 0.699297,
    "y": 0.99,
    "z": 0.513955,
    "source_article": "partial fractions",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "partial fractions",
      "repeated factors",
      "rational function integration"
    ]
  },
  {
    "id": "72667a0940e78e77",
    "question_text": "A function can be continuous at a point but not differentiable there. What is the classic example demonstrating this, and what does it reveal about the relationship between continuity and differentiability?",
    "options": {
      "A": "Every continuous function defined on any interval whatsoever is automatically and necessarily differentiable at every single point of that interval with absolutely no exceptions — the conditions of continuity and differentiability are logically equivalent and interchangeable, meaning that establishing one immediately and without further verification establishes the other, making the distinction between them entirely unnecessary in all of mathematical analysis",
      "B": "Differentiability at a point is a strictly weaker condition than continuity at that point, requiring fewer properties and imposing fewer constraints on the function's behavior near that point, meaning that any function that is differentiable at a given point need not be continuous there — in other words, a function can possess a well-defined finite derivative at a point while simultaneously exhibiting a jump or removable discontinuity at that very same point",
      "C": "f(x) = |x| is continuous everywhere but not differentiable at x = 0 because the left and right derivatives differ (-1 vs +1) — this shows differentiability is strictly stronger than continuity (differentiable implies continuous, but not conversely), and the Weierstrass function demonstrates the extreme case: a function continuous everywhere but differentiable nowhere, proving that non-differentiability can be the rule rather than the exception",
      "D": "The only functions that are continuous at a point but not differentiable there are functions that have been artificially and deliberately constructed by mathematicians purely as pathological counterexamples — no naturally arising function in any real-world application of physics, engineering, economics, or applied science can ever be continuous at a point without also being automatically differentiable there, and the Weierstrass nowhere-differentiable function is merely a theoretical curiosity"
    },
    "correct_answer": "C",
    "difficulty": 2,
    "x": 0.664532,
    "y": 0.844638,
    "z": 0.466068,
    "source_article": "continuity vs differentiability",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "continuity vs differentiability",
      "absolute value",
      "Weierstrass function"
    ]
  },
  {
    "id": "bd65611795fe91cc",
    "question_text": "Riemann sums approximate definite integrals by summing areas of rectangles. How does the choice of partition and sample points affect the approximation, and when does the limit of Riemann sums exist?",
    "options": {
      "A": "Riemann sums can only be computed using a perfectly uniform partition where every subinterval has exactly the same width Δx = (b-a)/n, and any attempt to use a non-uniform partition where the subinterval widths vary from one to the next will immediately and necessarily cause the sum to diverge or produce a meaningless result — this is incorrect because Riemann sums work with arbitrary partitions and convergence depends only on the mesh approaching zero",
      "B": "Riemann sums converge to the correct value of the integral only when left endpoints are used as the sample points within each subinterval, and using right endpoints, midpoints, or any other choice of sample point within each subinterval will always cause the sum to diverge or converge to an entirely incorrect value — this is false because the Riemann integral definition explicitly allows any arbitrary sample point within each subinterval",
      "C": "The Riemann integral exists and is well-defined for every bounded function on a closed interval regardless of how many discontinuities the function possesses, including functions with uncountably many discontinuities — this contradicts Lebesgue's criterion, which states that a bounded function is Riemann integrable if and only if its set of discontinuities has Lebesgue measure zero, ruling out functions discontinuous on sets of positive measure",
      "D": "The Riemann integral ∫ₐᵇf(x)dx exists as the limit of Riemann sums Σf(xᵢ*)Δxᵢ as the mesh (maximum partition width) approaches zero, provided this limit is the same regardless of how partitions and sample points are chosen — this limit exists for all continuous functions and more generally for bounded functions with at most countably many discontinuities (Lebesgue's criterion: Riemann integrable iff discontinuities have measure zero)"
    },
    "correct_answer": "D",
    "difficulty": 3,
    "x": 0.654957,
    "y": 0.97003,
    "z": 0.408462,
    "source_article": "Riemann sums",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Riemann sums",
      "Riemann integrability",
      "Lebesgue criterion"
    ]
  },
  {
    "id": "41186d5f260774e5",
    "question_text": "The directional derivative generalizes partial derivatives to arbitrary directions. How is the directional derivative of f at point p in direction u computed, and what direction maximizes it?",
    "options": {
      "A": "The directional derivative equals the dot product of the gradient and the unit direction vector: Dᵤf(p) = ∇f(p) · û — this is maximized when û points in the direction of ∇f(p) (the gradient direction), with maximum value |∇f(p)|, and is zero when û is perpendicular to the gradient, confirming that the gradient points in the direction of steepest ascent and its magnitude gives the maximum rate of change",
      "B": "The directional derivative of a scalar function at a point is always equal to the magnitude of the gradient vector at that point regardless of which direction is chosen — the direction vector u plays no role whatsoever in determining the rate of change, and the directional derivative is the same constant value in every direction, contradicting the fundamental fact that rates of change vary with direction and the gradient gives the maximum",
      "C": "The directional derivative can only be computed in the x and y coordinate directions (the standard basis directions i and j) and fundamentally cannot be defined or evaluated in any arbitrary direction such as diagonal, oblique, or non-axis-aligned directions — this is incorrect because the directional derivative D_u f = ∇f · u is defined for any unit vector u pointing in any direction, not just coordinate axis directions",
      "D": "The direction of steepest ascent is always perpendicular to the gradient vector"
    },
    "correct_answer": "A",
    "difficulty": 3,
    "x": 0.742567,
    "y": 0.83769,
    "z": 0.769954,
    "source_article": "directional derivative",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "directional derivative",
      "gradient",
      "steepest ascent"
    ]
  },
  {
    "id": "0f1542fd66010841",
    "question_text": "Line integrals extend integration from intervals to curves. What is the difference between a scalar line integral and a vector line integral, and what does the vector line integral compute physically?",
    "options": {
      "A": "A scalar line integral ∫_C f ds that integrates a scalar-valued function f with respect to arc length along a curve C, and a vector line integral ∫_C F · dr that integrates a vector field F along C, are entirely identical computations that always produce exactly the same numerical result — there is no mathematical or conceptual distinction between the two types of line integrals, and they measure the same quantity in every situation without exception",
      "B": "A scalar line integral ∫_C f ds integrates a scalar function along a curve weighted by arc length (computing total 'mass' if f is density), while a vector line integral ∫_C F·dr computes the work done by the vector field F along the path C — the vector integral depends on the path's direction (reversing the path negates the integral) and equals zero around closed paths only if F is conservative (F = ∇φ for some potential function φ)",
      "C": "Line integrals can only be computed along perfectly straight line segments connecting two points in Euclidean space and fundamentally cannot be defined, evaluated, or computed along any curved path, smooth arc, or arbitrary parametric trajectory — the concept of integrating a function or vector field along a path intrinsically requires that the path be a straight line with constant direction",
      "D": "Vector line integrals are always path-independent regardless of which vector field F is being integrated — the value of ∫_C F · dr depends only on the endpoints of the curve C and never on the specific path taken between those endpoints, whether the field is conservative or not, whether the domain is simply connected or not, and whether curl F vanishes or not, making the particular choice of path entirely irrelevant in all cases whatsoever"
    },
    "correct_answer": "B",
    "difficulty": 3,
    "x": 0.748812,
    "y": 0.936148,
    "z": 0.308286,
    "source_article": "line integrals",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "line integrals",
      "work",
      "conservative fields"
    ]
  },
  {
    "id": "472a7adcc4f7776f",
    "question_text": "Conservative vector fields are those that can be written as the gradient of a scalar potential: F = ∇φ. What are the equivalent characterizations of a conservative field in a simply connected domain?",
    "options": {
      "A": "A field is conservative only if its divergence is zero, with curl being irrelevant",
      "B": "Every vector field is conservative regardless of its curl or the domain topology",
      "C": "A vector field in a simply connected domain is conservative if and only if its curl is zero (∇×F = 0), equivalently if its line integral is path-independent (depends only on endpoints), equivalently if its integral around every closed curve is zero — all four statements (gradient field, zero curl, path-independence, zero circulation) are equivalent in simply connected domains, but crucially, zero curl alone is insufficient in domains with holes",
      "D": "Conservative vector fields are a concept that exists exclusively in one-dimensional settings and has absolutely no meaningful generalization to two-dimensional or three-dimensional Euclidean spaces — the notion of a potential function, path-independence of line integrals, and the gradient relationship F = ∇φ are all confined to the real number line and cannot be extended to R² or R³ in any mathematically rigorous or practically useful way whatsoever"
    },
    "correct_answer": "C",
    "difficulty": 3,
    "x": 0.749665,
    "y": 0.869968,
    "z": 0.55497,
    "source_article": "conservative fields",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "conservative fields",
      "curl-free",
      "path independence",
      "simply connected"
    ]
  },
  {
    "id": "5bf8acf55e97ad1e",
    "question_text": "Separable differential equations have the form dy/dx = g(x)h(y). What is the solution method, and when does this method fail or produce extraneous solutions?",
    "options": {
      "A": "The method of separation of variables never misses solutions and always produces the complete general solution set for any separable ODE — there are no singular solutions, equilibrium solutions, or constant solutions that can be lost during the algebraic process of dividing both sides by g(y), because division by a function that might be zero at certain y values has absolutely no effect on the solution set of the original differential equation",
      "B": "Separable ordinary differential equations of the form dy/dx = f(x)g(y) can only be solved using numerical methods such as Euler's method or Runge-Kutta, and they have absolutely no analytical solution technique available — it is impossible to separate the variables, integrate both sides independently, and obtain an explicit or implicit closed-form solution, because the product structure f(x)g(y) prevents any algebraic manipulation",
      "C": "All first-order differential equations are separable and can be solved by this method",
      "D": "Separate variables as dy/h(y) = g(x)dx, then integrate both sides: ∫dy/h(y) = ∫g(x)dx + C — this produces the general solution, but the method fails at points where h(y) = 0 (division by zero), which correspond to equilibrium solutions y = y₀ that may be missed by the separation process and must be checked separately, potentially creating singular solutions not captured by the general solution family"
    },
    "correct_answer": "D",
    "difficulty": 3,
    "x": 0.720277,
    "y": 0.9693,
    "z": 0.601934,
    "source_article": "separable ODEs",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "separable ODEs",
      "equilibrium solutions",
      "singular solutions"
    ]
  },
  {
    "id": "f76d5e3da09f0953",
    "question_text": "The Hessian matrix of a multivariable function f contains all second-order partial derivatives. What role does the Hessian play in classifying critical points?",
    "options": {
      "A": "At a critical point where ∇f = 0, the Hessian H determines the nature of the critical point: if H is positive definite (all eigenvalues positive), it is a local minimum; if negative definite (all eigenvalues negative), a local maximum; if indefinite (eigenvalues of mixed sign), a saddle point — for functions of two variables, this reduces to checking the determinant D = fₓₓfᵧᵧ - (fₓᵧ)² and the sign of fₓₓ",
      "B": "The Hessian matrix only exists for linear functions and is undefined for nonlinear functions",
      "C": "The Hessian determines whether a function is continuous but provides no information about extrema",
      "D": "A positive determinant of the Hessian always indicates a saddle point rather than an extremum"
    },
    "correct_answer": "A",
    "difficulty": 4,
    "x": 0.727531,
    "y": 0.806226,
    "z": 0.644412,
    "source_article": "Hessian matrix",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Hessian matrix",
      "second derivative test",
      "critical point classification"
    ]
  },
  {
    "id": "66244124f9372920",
    "question_text": "The change of variables theorem for multiple integrals generalizes u-substitution. What role does the Jacobian determinant play, and why must it appear as an absolute value?",
    "options": {
      "A": "The Jacobian determinant is irrelevant to changes of variables and can always be set to 1",
      "B": "When transforming ∫∫f(x,y)dA via the substitution x = x(u,v), y = y(u,v), the area element transforms as dA = |∂(x,y)/∂(u,v)|dudv — the Jacobian determinant |J| measures how the transformation locally scales areas, and the absolute value is needed because the determinant can be negative (when the transformation reverses orientation), but area/volume elements are always positive",
      "C": "The Jacobian must always be positive and the absolute value is never needed",
      "D": "Changes of variables in multiple integrals work only for purely linear transformations such as rotation, scaling, and shearing, and they categorically cannot be applied to any nonlinear coordinate change such as polar coordinates (x = r cos θ, y = r sin θ), spherical coordinates, cylindrical coordinates, or any other curvilinear coordinate system — the Jacobian determinant formula is only valid when the transformation matrix has constant entries"
    },
    "correct_answer": "B",
    "difficulty": 4,
    "x": 0.754926,
    "y": 0.882002,
    "z": 0.248622,
    "source_article": "change of variables",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "change of variables",
      "Jacobian determinant",
      "area scaling"
    ]
  },
  {
    "id": "c19ae21e2884817e",
    "question_text": "The Picard-Lindelöf theorem (existence and uniqueness theorem) guarantees solutions to initial value problems y' = f(t,y), y(t₀) = y₀. What conditions does it require, and what does it NOT guarantee?",
    "options": {
      "A": "The Picard-Lindelöf theorem requires absolutely no conditions on the function f(t,y) and unconditionally guarantees the existence and uniqueness of solutions for all differential equations y' = f(t,y) on the entire real line — there is no need for continuity of f, no Lipschitz condition on the second variable, and no restriction to a local neighborhood, because solutions always exist globally for all time without any possibility of finite-time blowup",
      "B": "The theorem only requires f to be continuous and always guarantees uniqueness of solutions",
      "C": "If f(t,y) is continuous in t and Lipschitz continuous in y in a neighborhood of (t₀,y₀), then there exists a unique local solution — continuity of f alone (Peano's theorem) guarantees existence but not uniqueness, and the Lipschitz condition prevents the branching behavior seen in equations like y' = y^(2/3) where y(0) = 0, which has both y = 0 and y = (t/3)³ as solutions",
      "D": "Solutions guaranteed by the Picard-Lindelöf theorem always exist for all t ∈ (-∞, ∞) on the entire real line with absolutely no possibility of finite-time blowup, singularity formation, or loss of existence beyond a finite interval — this contradicts the local nature of the theorem, which only guarantees existence on a neighborhood of the initial point, and solutions like y' = y², y(0) = 1 blow up at t = 1"
    },
    "correct_answer": "C",
    "difficulty": 4,
    "x": 0.678452,
    "y": 0.857283,
    "z": 0.468306,
    "source_article": "Picard-Lindelöf theorem",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Picard-Lindelöf theorem",
      "Lipschitz condition",
      "uniqueness"
    ]
  },
  {
    "id": "e6f1e5d1490cad54",
    "question_text": "The Laplace transform converts differential equations into algebraic equations. What is the Laplace transform of f(t), and why is it particularly useful for solving linear ODEs with constant coefficients and initial conditions?",
    "options": {
      "A": "The Laplace transform does not incorporate initial conditions and they must be applied separately after solving",
      "B": "The Laplace transform is identical to the Fourier transform with no differences in definition or properties",
      "C": "The Laplace transform can only be applied to polynomial functions of the variable t and it fundamentally fails to handle exponential functions, trigonometric functions, step functions, delta functions, or any other non-polynomial function — this is entirely incorrect because the Laplace transform L{f(t)} = ∫₀^∞ e^(-st) f(t) dt works for any function of exponential order, including exponentials, sines, cosines, and piecewise functions",
      "D": "The Laplace transform F(s) = ∫₀^∞ e^(-st)f(t)dt converts time-domain functions to the s-domain — its power lies in the derivative property: L{f'(t)} = sF(s) - f(0), which transforms differentiation into multiplication by s and automatically incorporates initial conditions, turning an ODE into an algebraic equation in s that can be solved and then inverse-transformed back to the time domain"
    },
    "correct_answer": "D",
    "difficulty": 4,
    "x": 0.710103,
    "y": 0.927556,
    "z": 0.550006,
    "source_article": "Laplace transform",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Laplace transform",
      "derivative property",
      "initial conditions"
    ]
  },
  {
    "id": "e433c2978df44022",
    "question_text": "Arc length of a curve y = f(x) from x = a to x = b is given by an integral formula. What is this formula, and why does it involve a square root that makes many arc length integrals analytically intractable?",
    "options": {
      "A": "Arc length = ∫ₐᵇ √(1 + (f'(x))²) dx — this formula arises from the Pythagorean theorem applied to infinitesimal line segments (ds² = dx² + dy²), and the square root of 1 + (f')² makes the integrand algebraically complex for most functions, since even simple curves like y = x² lead to integrands involving √(1 + 4x²) that require hyperbolic substitutions or elliptic integrals, making closed-form arc lengths rare",
      "B": "Arc length is simply b - a regardless of the curve's shape",
      "C": "Arc length equals the definite integral of f(x) itself: ∫ₐᵇ f(x)dx",
      "D": "The formula for arc length involves no square root and is always a simple polynomial integral that can be evaluated in closed form — the true arc length formula L = ∫_a^b √(1 + (dy/dx)²) dx involves an integrand under a radical that typically produces integrals with no elementary antiderivative, which is why most arc length computations require numerical methods or special functions such as elliptic integrals to evaluate"
    },
    "correct_answer": "A",
    "difficulty": 2,
    "x": 0.721169,
    "y": 0.924281,
    "z": 0.533602,
    "source_article": "arc length",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "arc length",
      "Pythagorean theorem",
      "intractable integrals"
    ]
  },
  {
    "id": "179a11f7b0a47276",
    "question_text": "Double integrals extend integration to two dimensions. How does Fubini's theorem simplify the computation of ∫∫_R f(x,y)dA, and when does it fail?",
    "options": {
      "A": "Fubini's theorem states that double integrals cannot be reduced to iterated single integrals under any conditions",
      "B": "Fubini's theorem states that a double integral over a rectangle R = [a,b]×[c,d] can be computed as iterated integrals in either order: ∫ₐᵇ∫_c^d f(x,y)dydx = ∫_c^d∫ₐᵇ f(x,y)dxdy, provided f is integrable (continuous is sufficient) — the theorem fails when f is not absolutely integrable over R, where the two iterated integrals may exist but give different values, demonstrating that order of integration matters for conditionally convergent cases",
      "C": "The order of integration never matters, even for functions that are not absolutely integrable",
      "D": "Fubini's theorem applies only to functions of one variable and has no multivariate version"
    },
    "correct_answer": "B",
    "difficulty": 3,
    "x": 0.711316,
    "y": 0.918,
    "z": 0.028881,
    "source_article": "Fubini's theorem",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Fubini's theorem",
      "iterated integrals",
      "order of integration"
    ]
  },
  {
    "id": "99f9805483a0a150",
    "question_text": "The residue theorem from complex analysis provides a powerful method for evaluating real definite integrals. How can ∫₋∞^∞ f(x)dx be computed using contour integration?",
    "options": {
      "A": "Contour integration in complex analysis cannot be used to evaluate any real-valued integral whatsoever and is useful only for computing integrals of complex-valued functions along curves in the complex plane — this is fundamentally incorrect because one of the most powerful applications of contour integration is evaluating difficult real integrals by extending the integrand to the complex plane and closing the contour with semicircles or other paths",
      "B": "The residue theorem requires the function f(z) to have no poles anywhere in the entire complex plane in order for it to be applicable, meaning that only entire functions (functions holomorphic everywhere) can be handled by residue calculus — this reverses the actual requirement, since the residue theorem specifically deals with functions that do have isolated singularities and computes integrals by summing residues at those poles",
      "C": "Close the real line integral with a semicircular contour in the upper (or lower) half-plane — if f(z) decays sufficiently fast on the semicircle (Jordan's lemma), the contour integral equals the real integral, and by the residue theorem, the contour integral equals 2πi times the sum of residues of f(z) at poles enclosed by the contour, converting a difficult real integration into residue computation",
      "D": "Contour integration gives exactly the same numerical result regardless of which contour is chosen to enclose the singularities, so the specific choice of contour path is completely arbitrary and has no effect on the computation — while the integral around a closed contour enclosing the same poles is indeed determined by those poles, the practical choice of contour matters greatly for evaluability, and different contours enclosing different poles yield different results"
    },
    "correct_answer": "C",
    "difficulty": 5,
    "x": 0.688196,
    "y": 0.953748,
    "z": 0.357103,
    "source_article": "residue theorem",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "residue theorem",
      "contour integration",
      "real integrals"
    ]
  },
  {
    "id": "020f1b9b02afc58b",
    "question_text": "Parametric curves describe paths via x = x(t), y = y(t). How are the tangent vector, speed, and curvature computed for a parametric curve, and what does curvature measure geometrically?",
    "options": {
      "A": "Curvature depends on the speed of parameterization and changes if the curve is reparameterized",
      "B": "Curvature can only be defined for circles, not for arbitrary curves",
      "C": "The tangent vector to a parametric curve is always (1, 0) regardless of the parameterization",
      "D": "The tangent vector is (x'(t), y'(t)), speed is |(x'(t), y'(t))| = √(x'²+y'²), and curvature κ = |x'y''-y'x''|/(x'²+y'²)^(3/2) measures how rapidly the curve is turning per unit arc length — a straight line has κ = 0 everywhere, a circle of radius r has constant κ = 1/r, and high curvature indicates tight bending, making curvature intrinsic to the curve's geometry regardless of how fast the parameter t traverses it"
    },
    "correct_answer": "D",
    "difficulty": 1,
    "x": 0.756282,
    "y": 0.850973,
    "z": 0.829438,
    "source_article": "parametric curves",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "parametric curves",
      "curvature",
      "tangent vector"
    ]
  },
  {
    "id": "3db1f8c534d16391",
    "question_text": "The method of characteristics solves first-order partial differential equations by reducing them to ODEs along characteristic curves. What is the geometric idea behind this method?",
    "options": {
      "A": "A first-order PDE aᵤuₓ + bᵤuₜ = c defines a vector field (a,b,c) in the (x,t,u) space — characteristic curves are integral curves of this vector field along which the PDE reduces to an ODE du/dt = c(x,t,u), meaning the solution surface u(x,t) is generated by 'sweeping' initial data along these curves, and the solution is determined wherever characteristics cover the domain without crossing",
      "B": "The method of characteristics is applicable only to linear partial differential equations with constant coefficients and completely breaks down when applied to variable-coefficient linear PDEs, semilinear PDEs, quasilinear PDEs, or fully nonlinear first-order PDEs — this is incorrect because the method works for general quasilinear first-order PDEs of the form a(x,y,u)uₓ + b(x,y,u)uᵧ = c(x,y,u), including nonlinear cases",
      "C": "Characteristic curves of a first-order PDE are always perfectly straight lines in the (x,y) plane regardless of whether the PDE is linear, semilinear, quasilinear, or fully nonlinear — this is only true for linear constant-coefficient equations; for quasilinear equations the characteristics are generally curved because the coefficients depend on the solution u itself, making the characteristic equations a coupled nonlinear ODE system",
      "D": "The method of characteristics transforms a partial differential equation into a system of algebraic equations rather than ordinary differential equations, allowing the PDE to be solved by simple algebra without any calculus — this is false because characteristics reduce the PDE to a system of ODEs along characteristic curves, and those ODEs must then be integrated using standard ODE techniques to find the solution"
    },
    "correct_answer": "A",
    "difficulty": 5,
    "x": 0.730342,
    "y": 0.91764,
    "z": 0.754013,
    "source_article": "method of characteristics",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "method of characteristics",
      "first-order PDEs",
      "integral curves"
    ]
  },
  {
    "id": "2f4d182cf78df184",
    "question_text": "Exact differential equations have the form M(x,y)dx + N(x,y)dy = 0 where ∂M/∂y = ∂N/∂x. What does the exactness condition mean geometrically, and how is the solution found?",
    "options": {
      "A": "The exactness condition ∂M/∂y = ∂N/∂x for a differential equation M dx + N dy = 0 has absolutely no geometric or conceptual interpretation and is purely an arbitrary algebraic criterion with no connection to the existence of a potential function, no relationship to conservative vector fields, and no meaning in terms of path-independence of line integrals — this is incorrect because exactness means (M,N) is a conservative field with a potential function",
      "B": "Exactness means M dx + N dy is the total differential of some function F(x,y), so ∂F/∂x = M and ∂F/∂y = N — geometrically, (M,N) is a conservative vector field with potential F, and solutions are the level curves F(x,y) = C, found by integrating M with respect to x (incorporating a function of y), then determining the unknown function by comparing ∂F/∂y with N",
      "C": "All first-order ordinary differential equations written in the form M(x,y) dx + N(x,y) dy = 0 are automatically exact without any condition whatsoever needing to hold between the partial derivatives of M and N — this is false because exactness requires the specific compatibility condition ∂M/∂y = ∂N/∂x, and when this condition fails, an integrating factor must be found to convert the equation to exact form before solving",
      "D": "Exact differential equations of the form M dx + N dy = 0 satisfying ∂M/∂y = ∂N/∂x cannot be solved analytically by any known mathematical technique and always require numerical approximation methods such as Euler's method or Runge-Kutta to obtain even approximate solution values — this is entirely wrong because the exactness condition guarantees a potential function φ exists with dφ = M dx + N dy, yielding the implicit solution φ(x,y) = C"
    },
    "correct_answer": "B",
    "difficulty": 4,
    "x": 0.732815,
    "y": 0.879123,
    "z": 0.707262,
    "source_article": "exact differential equations",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "exact differential equations",
      "total differential",
      "potential function"
    ]
  },
  {
    "id": "5cca24fb975a125d",
    "question_text": "Asymptotic analysis describes the behavior of functions as arguments grow large. What is the difference between f = O(g) (big-O) and f ~ g (asymptotic equivalence), and why is this distinction important in analyzing series and integrals?",
    "options": {
      "A": "Big-O notation applies only to computer science algorithms and has no role in mathematical analysis",
      "B": "Big-O and asymptotic equivalence are identical notations with the same meaning",
      "C": "f = O(g) as x → ∞ means |f(x)| ≤ C|g(x)| for some constant C and sufficiently large x (f grows no faster than g), while f ~ g means f(x)/g(x) → 1 (f and g have the same leading behavior) — asymptotic equivalence is much stronger than big-O, and the distinction matters because Stirling's formula n! ~ √(2πn)(n/e)ⁿ gives a precise approximation while n! = O(nⁿ) is a loose bound, determining whether you can substitute one expression for another in limit computations",
      "D": "f ~ g means f(x) = g(x) exactly for all x, not just asymptotically"
    },
    "correct_answer": "C",
    "difficulty": 5,
    "x": 0.660695,
    "y": 0.917096,
    "z": 0.736671,
    "source_article": "big-O notation",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "big-O notation",
      "asymptotic equivalence",
      "Stirling's formula"
    ]
  },
  {
    "id": "e8790578b7fb6a9c",
    "question_text": "Green's theorem relates a line integral around a simple closed curve to a double integral over the enclosed region. What does it state, and how is it a special case of the generalized Stokes' theorem?",
    "options": {
      "A": "Green's theorem is a completely independent and isolated result in vector calculus that has absolutely no mathematical or conceptual connection to the classical Stokes' theorem, the divergence theorem, or the generalized Stokes' theorem on differential forms — it cannot be derived from, or viewed as a special case of, any of these more general theorems, and exists entirely on its own with no relationship whatsoever to the broader framework of exterior calculus on smooth manifolds",
      "B": "Green's theorem states that every line integral around a closed curve equals zero",
      "C": "Green's theorem applies only and exclusively to perfectly rectangular regions in the plane whose sides are aligned with the coordinate axes, and it categorically cannot be extended, generalized, or applied to any region with curved boundaries, irregular shapes, circular domains, annular regions, or any other non-rectangular simply connected domain in the two-dimensional Euclidean plane — the theorem fundamentally requires straight horizontal and vertical boundary segments only",
      "D": "Green's theorem: ∮_C (P dx + Q dy) = ∫∫_D (∂Q/∂x - ∂P/∂y) dA where C bounds D with positive (counterclockwise) orientation — it equates circulation around the boundary to the flux of the curl through the region, and is the two-dimensional case of the generalized Stokes' theorem (∫_∂Ω ω = ∫_Ω dω), which unifies Green's, classical Stokes', and the divergence theorem as instances of integrating a differential form over a boundary versus its exterior derivative over the interior"
    },
    "correct_answer": "D",
    "difficulty": 4,
    "x": 0.731816,
    "y": 0.90581,
    "z": 0.147886,
    "source_article": "Green's theorem",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Green's theorem",
      "generalized Stokes' theorem",
      "differential forms"
    ]
  },
  {
    "id": "8e44e05bc07b6730",
    "question_text": "The Cauchy-Schwarz inequality in the context of integrals states (∫fg)² ≤ (∫f²)(∫g²). Why is this inequality fundamental to analysis, and when does equality hold?",
    "options": {
      "A": "(∫ₐᵇ f(x)g(x)dx)² ≤ (∫ₐᵇ f(x)²dx)(∫ₐᵇ g(x)²dx) with equality if and only if f and g are linearly dependent (f = λg a.e.) — it is fundamental because it defines the inner product space structure of L² functions, proves the triangle inequality for the L² norm, underpins Fourier analysis (Bessel's inequality, Parseval's theorem), and constrains covariances in probability theory",
      "B": "Equality always holds for any choice of f and g with no conditions required",
      "C": "The inequality applies only to polynomial functions and fails for transcendental functions",
      "D": "The integral Cauchy-Schwarz inequality is a trivial result with no significant applications in analysis"
    },
    "correct_answer": "A",
    "difficulty": 5,
    "x": 0.673234,
    "y": 0.908025,
    "z": 0.253552,
    "source_article": "Cauchy-Schwarz inequality",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Cauchy-Schwarz inequality",
      "L² space",
      "linear dependence"
    ]
  },
  {
    "id": "bde08c594f8587c2",
    "question_text": "Polar coordinates (r,θ) are natural for problems with circular symmetry. How does the area element transform from Cartesian to polar coordinates, and why does the factor 'r' appear?",
    "options": {
      "A": "The area element in polar coordinates is simply dA = dr dθ with no additional factor needed",
      "B": "The area element in polar coordinates is dA = r dr dθ — the factor r appears because the Jacobian of the transformation x = r cos θ, y = r sin θ has determinant |∂(x,y)/∂(r,θ)| = r, reflecting that infinitesimal angular wedges at larger radii sweep out more area, so a small change dθ at radius r corresponds to arc length r dθ rather than just dθ",
      "C": "The factor r appears only as an approximation and is not mathematically exact",
      "D": "Polar coordinates (r, θ) cannot be used for integration in two dimensions because the coordinate transformation from Cartesian to polar is not invertible at the origin and the area element r dr dθ is not a valid replacement for dx dy — this is entirely incorrect because polar coordinates are perfectly valid for integration, the factor r in the area element correctly accounts for the Jacobian of the transformation, and the non-invertibility at r = 0 is a set of measure zero"
    },
    "correct_answer": "B",
    "difficulty": 1,
    "x": 0.774629,
    "y": 0.92941,
    "z": 0.493117,
    "source_article": "polar coordinates",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "polar coordinates",
      "Jacobian",
      "area element"
    ]
  },
  {
    "id": "8d916f778a7c9e7b",
    "question_text": "The Weierstrass M-test provides a sufficient condition for uniform convergence of function series. What does it state, and why is uniform convergence important for interchanging limits with integration and differentiation?",
    "options": {
      "A": "Pointwise convergence of a sequence of functions fₙ → f is entirely sufficient for interchanging limits with integrals, meaning lim ∫fₙ = ∫ lim fₙ holds whenever fₙ converges pointwise — no stronger condition such as uniform convergence or domination by an integrable function is ever needed, and the interchange is always valid regardless of how wildly the functions oscillate or concentrate their mass",
      "B": "The Weierstrass M-test requires no bound on the individual terms and works for any series",
      "C": "If |fₙ(x)| ≤ Mₙ for all x in the domain and ΣMₙ converges, then Σfₙ(x) converges uniformly — uniform convergence is crucial because it permits term-by-term integration (∫Σfₙ = Σ∫fₙ) and, with additional conditions, term-by-term differentiation, while pointwise convergence alone does not justify these interchanges, as shown by examples where Σ∫fₙ ≠ ∫Σfₙ under merely pointwise convergence",
      "D": "The M-test applies only to power series and cannot be used for general function series"
    },
    "correct_answer": "C",
    "difficulty": 5,
    "x": 0.614867,
    "y": 0.890386,
    "z": 0.454019,
    "source_article": "Weierstrass M-test",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "Weierstrass M-test",
      "uniform convergence",
      "term-by-term integration"
    ]
  },
  {
    "id": "75f5137ed0575b89",
    "question_text": "Integration in polar, cylindrical, and spherical coordinates requires understanding how the volume element changes. What is the volume element in spherical coordinates (ρ, θ, φ), and how is it derived?",
    "options": {
      "A": "The volume element is dV = ρ dρ dθ dφ, with only a single factor of ρ rather than ρ²",
      "B": "The volume element in spherical coordinates is simply dV = dρ dθ dφ with no additional scaling factors",
      "C": "Spherical coordinates use a Jacobian of 1, identical to Cartesian coordinates",
      "D": "The volume element is dV = ρ² sin φ dρ dθ dφ (where ρ is radial distance, θ is azimuthal angle, φ is polar angle from the z-axis) — this is derived from the Jacobian determinant |∂(x,y,z)/∂(ρ,θ,φ)| = ρ² sin φ, which accounts for both the radial scaling (ρ²) from spherical shells growing as ρ² and the latitude compression (sin φ) from meridional circles shrinking toward the poles"
    },
    "correct_answer": "D",
    "difficulty": 2,
    "x": 0.79,
    "y": 0.932388,
    "z": 0.378487,
    "source_article": "spherical coordinates",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "spherical coordinates",
      "volume element",
      "Jacobian"
    ]
  },
  {
    "id": "e847183c30321b81",
    "question_text": "The concept of uniform continuity is stronger than pointwise continuity. What distinguishes them, and why is this distinction critical for integration theory?",
    "options": {
      "A": "A function f is uniformly continuous on S if for every ε > 0 there exists δ > 0 such that |f(x)-f(y)| < ε whenever |x-y| < δ for ALL x,y in S — crucially, the same δ works for every point in S, unlike pointwise continuity where δ may depend on the point, and the Heine-Cantor theorem proves that every continuous function on a closed bounded interval is automatically uniformly continuous, which is essential for proving Riemann integrability",
      "B": "Uniform continuity applies only to differentiable functions and is undefined for merely continuous functions",
      "C": "Uniform and pointwise continuity are identical concepts with no mathematical difference",
      "D": "Uniform continuity is a weaker condition than pointwise continuity and is easier to satisfy"
    },
    "correct_answer": "A",
    "difficulty": 5,
    "x": 0.643935,
    "y": 0.908928,
    "z": 0.160452,
    "source_article": "uniform continuity",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "uniform continuity",
      "Heine-Cantor theorem",
      "Riemann integrability"
    ]
  },
  {
    "id": "4615e75a6a34909a",
    "question_text": "The substitution method (u-substitution) is the reverse of the chain rule for integration. When applying u = g(x) to evaluate ∫f(g(x))g'(x)dx, what determines the success of a substitution, and what common mistake leads to incorrect results?",
    "options": {
      "A": "U-substitution works for every integral regardless of whether the derivative g'(x) appears in the integrand",
      "B": "A successful u-substitution requires that the integrand contain both a composite function f(g(x)) and the derivative g'(x) as a factor (up to a constant multiple) — the substitution u = g(x) transforms the integral to ∫f(u)du, and the common error is failing to transform ALL instances of x, including the differential, or neglecting to substitute back to x when computing definite integrals with the original limits",
      "C": "U-substitution applies only to polynomial integrands and cannot be used with trigonometric or exponential functions",
      "D": "The substitution changes only the integrand but not the differential or the limits of integration"
    },
    "correct_answer": "B",
    "difficulty": 1,
    "x": 0.700956,
    "y": 0.971909,
    "z": 0.453548,
    "source_article": "u-substitution",
    "domain_ids": [
      "calculus",
      "mathematics"
    ],
    "concepts_tested": [
      "u-substitution",
      "chain rule reversal",
      "integration technique"
    ]
  }
]