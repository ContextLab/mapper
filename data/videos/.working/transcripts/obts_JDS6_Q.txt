I will now show you my preferred way of finding an inverse of a 3 by 3 matrix. And I actually think it's a lot more fun, and you're less likely to make careless mistakes. But if I remember correctly from algebra 2, they didn't teach it this way in algebra 2, and that's why I taught the other way initially. But let's go through this, and in a future video, I will teach you why it works, because that's always important. But in linear algebra, this is one of the few subjects where I think it's very important to learn how to do the operations first, and then later we'll learn the why. Because the how is very mechanical, and it really just involves some basic arithmetic for the most part. But the why tends to be quite deep. So I'll leave that to later videos. And you can often think about the depth of things when you have confidence that you at least understand the hows. So anyway, let's go back to our original matrix. And what was that original matrix that I did in the last video? It was 1, 0, 1, 0, 2, 1, 1, 1, 1. And we wanted to find the inverse of this matrix. So what we're going to do is called Gauss-Jordan elimination, to find the inverse of the matrix. And the way you do it, and it might seem a little bit like magic, it might seem a little bit like voodoo, but I think if you'll see in future videos, it makes a lot of sense. What we do is we augment this matrix. And what does augment mean? It means you just add something to it. So I draw a dividing line. Some people don't. So if I put a dividing line here, and what do I put on the other side of the dividing line? I put the identity matrix of the same size. So this is 3 by 3, so I put a 3 by 3 identity matrix. So that's 1, 0, 0, 0, 1, 0, 0, 0, 1. So what are we going to do? What I'm going to do is perform a series of elementary row operations, and I'm about to tell you what are valid elementary row operations on this matrix. But whatever I do to any of these rows here, I have to do to the corresponding rows here. And my goal is essentially to perform a bunch of operations on the left-hand side, and of course the same operations will be applied to the right-hand side, so that I eventually end up with the identity matrix on the left-hand side. And then when I have the identity matrix on the left-hand side, what I have left on the right-hand side will be the inverse of this original matrix. And when you put this in, when this becomes an identity matrix, that's actually called reduced row echelon form. And I'll talk more about that. There's a lot of names and labels in linear algebra, but fairly simple concepts. But anyway, let's get started, and this should become a little clearer. At least the process will become clearer, maybe not why it works. So first of all, I said I'm going to perform a bunch of operations here. What are legitimate operations? They're called elementary row operations. So there's a couple things I can do. I can replace any row with that row multiplied by some number, so I can do that. I can swap any two rows. And of course, if I swap, say, the first row and the second row, I'd have to do it here as well. And I can add or subtract one row from another row. And when I do that, so for example, I could take this row and replace it with this row added to this row. And you'll see what I mean in a second. And if you combine it, you could say, well, I'm going to multiply this row times negative 1 and add it to this row and replace this row with that. So if you start to feel like this is something like what you learned when you learned system of equations or solving systems of linear equations, that's no coincidence, because matrices are actually a very good way to represent that, and I will show you that soon. But anyway, let's do some elementary row operations to get this left-hand side into reduced row echelon form, which is really just a fancy way of saying, let's turn it into the identity matrix. So let's see what we want to do. We want to have 1's all across here. So let's see how we can do this efficiently. Let me draw the matrix again. I want to get a 0 here. That would be convenient. So I'm going to keep the top two rows the same, 1, 0, 1. I have my dividing line, 1, 0, 0. I didn't do anything there. I'm not doing anything to the second row. 0, 2, 1. 0, 2, 1. 0, 1, 0. And what I'm going to do, I'm going to replace this row. And just so you know my motivation, my goal is to get a 0 here. So I'm a little bit closer to having the identity matrix here. So how do I get a 0 here? Well, what I could do is I can replace this row with this row minus this row. So I can replace the third row with the third row minus the first row. So what's the third row minus the first row? 1 minus 1 is 0. 1 minus 0 is 1. 1 minus 1 is 0. Well, I did it on the left-hand side, so I have to do it on the right-hand side. I have to replace this with this minus this. So 0 minus 1 is minus 1. 0 minus 0 is 0. And 1 minus 0 is 1. Fair enough. Now what can I do? Well, this row right here, this third row, it has 0, 1, 0. It looks a lot like what I want for my second row in the identity matrix. So why don't I just swap these two rows? Why don't I just swap the first and second rows? So let's do that. I'm going to swap the first and second rows. So the first row stays the same. 1, 0, 1. And then on the other side, it stays the same as well. And I am swapping the second and third rows. So now my second row is now 0, 1, 0. And I have to swap it on the right-hand side. So it's minus 1, 0, 1. I'm just swapping these two. So then my third row now becomes what the second row was here, 0, 2, 1, and 0, 1, 0. Fair enough. Now what do I want to do? Well, it would be nice if I had a 0 right here. That would get me that much closer to the identity matrix. So how could I get a 0 here? Well, what if I subtracted 2 times row 2 from row 1, right, because this would be 1 times 2 is 2. And if I subtracted that from this, I'll get a 0 here. So let's do that. So the first row has been very lucky. It hasn't had to do anything. It's just sitting there. 1, 0, 1. 1, 0, 0. And the second row is not changing for now. Minus 1, 0, 1. Now what did I say I was going to do? I'm going to subtract 2 times row 2 from row 3. So this is 0 minus 2 times 0 is 0. 2 minus 2 times 1, well, that's 0. 1 minus 2 times 0 is 1. 0 minus 2 times negative 1 is, so 0, let's remember, 0 minus 2 times negative 1. So that's 0 minus negative 2. So that's positive 2. 1 minus 2 times 0, well, that's just still 1. 0 minus 2 times 1. So that's minus 2. Have I done that right? I just want to make sure. 0 minus 2 times, right, 2 times minus 1 is minus 2. And I'm subtracting it, so it's plus. So I'm close. This almost looks like the identity matrix, or reduced row echelon form, except for this 1 right here. So I'm finally going to have to touch the top row. And what can I do? Well, how about I replace the top row with the top row minus the bottom row, right? Because if I subtract this from that, this will get a 0 there. So let's do that. So I'm replacing the top row with the top row minus the third row. So 1 minus 0 is 1. 0 minus 0 is 0. 1 minus 1 is 0. That was our whole goal. And then 1 minus 2 is negative 1. 0 minus 1 is negative 1. 0 minus negative 2. Well, that's positive 2. And then the other rows stay the same, right? 0, 1, 0. Minus 1, 0, 1. And then 0, 0, 1, 2, 1, negative 2. And there you have it. We have performed a series of operations on the left-hand side, and we performed the same operations on the right-hand side. This became the identity matrix, or reduced row echelon form. And we did this using Gauss-Jordan elimination. And what is this? Well, this is the inverse of this original matrix. This times this will equal the identity matrix. So if this is A inverse. I'm sorry, if this is A, then this is A inverse. And that's all you have to do. And as you can see, this took me half the amount of time. It required a lot less hairy mathematics than when I did it using the adjoint and the cofactors and the determinant. And if you think about it, I'll give you a little hint of why this worked. Every one of these operations I did on the left-hand side, every one of those operations, you could kind of view them as multiplying. To get from here to here, I multiplied. You can kind of say that there's a matrix that if I multiplied by that matrix, it would have performed this operation. And then I would have had to multiply by another matrix to do this operation. So essentially what we did is we multiplied by a series of matrices to get here. And if you multiplied all of those, what we call elimination matrices together, you essentially multiplied this times the inverse. So what am I saying? So if we have a, to go from here to here, we had to multiply a times the elimination matrix. And this might be completely confusing for you, so ignore it if it is, but it might be insightful. So what did we eliminate in this? We eliminated 3, 1. We multiplied by the elimination matrix 3, 1 to get here. And then to go from here to here, we multiplied by some matrix, and I'll tell you more. I'll show you how we can construct these elimination matrices. We multiplied by elimination matrix. Well actually we had a row swap here. I don't know what you want to call that. We could call that the swap matrix. We swapped row 2 for 3. And then here we multiplied by elimination matrix. What did we do? We eliminated this. So this was row 3, column 2, 3, 2. And then finally, to get here, we had to multiply by elimination matrix. We had to eliminate this right here. So we eliminated row 1, column 3. So elimination rate row 1, column 3. And I want you to know right now that it's not important what these matrices are. I'll show you how we can construct these matrices. But I just want you to have kind of a leap of faith that each of these operations could have been done by multiplying by some matrix. But what we do know is by multiplying by all of these matrices, we essentially got the identity matrix back here. So the combination of all of these matrices when you multiply by each other, this must be the inverse matrix. If I were to multiply each of these elimination and row swap matrices, this must be the inverse matrix of A. Because when you multiply all of them times A, you get the inverse. Well, what happened? If these matrices are collectively the inverse matrix, if I do them, if I multiply the identity matrix times them, the elimination matrix, this one times that equals that, this one times that equals that, this one times that equals that, and so forth, I'm essentially multiplying, when you combine all of these, A inverse times the identity matrix. So if you think about it, just very big picture. And I don't want to confuse you. It's good enough at this point if you just understood what I did. But what I'm doing from all of these steps, I'm essentially multiplying both sides of this augmented matrix, you could call it, by A inverse. So I multiplied this by A inverse to get to the identity matrix. But then of course, if I multiply the inverse matrix times the identity matrix, I'll get the inverse matrix. But anyway, I don't want to confuse you. Hopefully that gave you a little intuition. I'll do this later with some more concrete examples. But hopefully you see that this is a lot less hairy than the way we did it with the adjoint and the cofactors and the minor matrices and the determinants, et cetera. Anyway, I'll see you in the next video.