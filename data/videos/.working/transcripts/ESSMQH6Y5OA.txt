Say I've got some matrix A. It's an n by k matrix. Let's say it's not just any n by k matrix. This matrix A has a bunch of columns that are all linearly independent. So A1, A2, all the way through A k are linearly independent. They are linearly independent columns. Let me write that down here. So A1, A2, all the column vectors of A all the way through A k are linearly independent. Now what does that mean? That means that the only solution to x1 times A1 plus x2 times A2, that's a 1, plus all the way to xk times A k, that the only solution to this is all of these x's have to be 0. So all xi's must be equal to 0. That's what linear independence implies. Or another way to write it is all the solutions to this equation, x1, x2, all the way down, xk equaling the 0 vector, that all of the solutions to this are all of these entries have to be equal to 0. This is just another way of writing this right there. We've seen it multiple times. The 0 vector right there. So if all of these have to be 0, that's like saying that the only solution to Ax is equal to 0 is x is equal to the 0 vector. Or another way to say it, this is all coming out of the fact that this guy's columns are linearly independent. So linear independence of columns, based on that, we can say since the only solution to Ax is equal to 0 is x is equal to the 0 vector, we know that the null space of A must be equal to the 0 vector. Or it's a set with just the 0 vector in it. And that is all a bit of review. Now, n by k, we don't know its dimensions. It may or may not be a square matrix. So we don't know necessarily whether it's invertible and all of that. But maybe we can construct an invertible matrix with it. So if we take A, let's study A transpose times A. A transpose times A. A is an n by k matrix. A transpose will be a k by n matrix. So A transpose A is going to be a k by k matrix. So it's a square matrix. So that's a nice place to start for an invertible matrix. So let's see if it is actually invertible. We don't know anything about A. All we know is it's columns are linearly independent. Let's see if A transpose A is invertible. And essentially, to show that it's invertible, if we could show that all of its columns are linearly independent, then we'll know it's invertible. If we have any, and I'll get back to this at the end of the video, but if you have a square matrix with linearly independent columns, remember, the linearly independent columns are associated with pivot columns when you put them in reduced row echelon form. So if you have a square matrix, then you're going to have exactly, so if it's a k by k matrix, that means you're going to have k, that means that the reduced row echelon form of matrix will have k pivot columns, and b at k by k, and be a square k by k matrix. And there's only one k by k matrix with k pivot columns, and that's the identity matrix. And that is the identity matrix. The k by k identity matrix. And if when you reduce something to reduced row echelon form, and you get the identity matrix, that means that your matrix is invertible. I could have probably left that to the end of the video, but I just want to show you, if we can show that we already know that this guy is square, that A transpose A is a square matrix, if we can show that given that A has linearly independent columns, that A transpose times A also has linearly independent columns, then given that the columns are linearly independent, and it's a square matrix, that tells us that when we put into reduced row echelon form, we'll get the identity matrix, and that tells us that this thing would be invertible. So let's see if we can prove that all of this guy's columns are linearly independent. So let's say I have some vector v. Let's say my vector v is a member of the null space of A transpose A. That means that if I take A transpose A times my vector v, I'm going to get the zero vector. Fair enough. Now, what happens if I multiply both sides of this equation times the transpose of this guy? So I'll get v transpose. Actually, let me just do it right here. If I multiply v transpose on this side and v transpose on this side, you could view this as a matrix vector product. Or in general, if you take a row vector times a column vector, it's essentially their dot product. So this right hand side of the equation, you dot anything with the zero vector, that is just going to be the zero vector. Now, what is the left hand side of this going to be? We've seen this before. If you have the transpose of, we could view this as, even though it's a transpose of a vector, you could view it as a row vector, but you could also view it as a matrix. A column vector is a, let's say v is a member, v is a k by 1 matrix, v transpose would be a 1 by k matrix. Anyway, we've seen this before, that that is equal to the reverse product, the transpose of the reverse product. Or if we take the product of two things and transpose it, that's the same thing as the reverse product of the transposes of either of those two matrices. So given that, we can replace this right here with a times a vector v transpose, and we're multiplying this vector times a v times this vector right here, and that is going to be equal to the zero vector. Now, what is this? If I'm taking some vectors transpose, and let's say this is a vector, remember, even though I have a matrix vector product right here, when I multiply a matrix times this vector, it will result in another vector. So this is a vector, and this is a vector right here. And if I take some vector and I multiply its transpose times that vector, we've seen this before, that is the same thing as y dot y. These two statements are identical. So this thing right here is the same thing as a v dot a v. This is the same thing as a v dot a v. And so what does the right hand side equal? The right hand side is going to be equal to zero. Actually, let me just make a correction up here. When I take v transpose times the zero vector, v transpose is going to have k elements, and then the zero vector is also going to have k elements. And when I take this product, that's like dotting it. You're taking the dot product of v and zeros. So this is the dot product of v with the zero vector, which is equal to zero, the scalar zero. So this right here is the scalar zero. I want to make sure I clarified that. It wouldn't have made sense otherwise. So the right hand side, when I multiply the zero vector times the transpose of v, it gets just the number zero, no vector zero there. So this a v dot a v is going to be equal to zero, or we could say that the magnitude of a or the length of a v squared is equal to zero. Or that tells us that a v has to be equal to zero. The only vector whose length is zero is a zero vector. So a v, let me switch colors. I'm using that a little bit too much. So we know that a v must be equal to zero, to the zero vector. This must be equal to the zero vector, since its length is zero. Now, we started off with saying v is a member of the null space, is a member of the null space of a transpose a. v could be any member of the null space of a transpose a. But then from that assumption, it turns out that v also has to be a member of the null space of a, that a v is equal to zero. Let's write that down. If v is a member of the null space of a transpose a, then v is a member of the null space of a. Now, our null space of a, because a's columns are linearly independent, it only contains one vector. It only contains the zero vector. So if this guy is a member of the null space of a transpose a, and he has to member a member of the null space of a, there's only one thing he can be. There's only one entry there. So then v has to be equal to the zero vector. Or another way to say that is any v that's in the null space of a transpose a has to be the zero vector. Or the null space of a transpose a is equal to the null space of a, which is equal to just the zero vector sitting there. Now, what does that do for us? That tells us that the only solution to a transpose a times some vector x equal to zero, this says that the only solution is the zero vector is x is equal to the zero vector. Because the null space of a transpose a is the same as the null space of a, and that just has the zero vector in it. The null space is just the solutions to this. So if the only solutions to the null space is this, that means that the columns of a are linearly independent. You could essentially write all of the linear combinations of the columns by the weights of the entries of x. We actually did that at the beginning. It's the same argument we used up here. So if all of their columns are linearly independent, and I said it over here, a transpose a has linearly independent columns, and it's a square matrix, and it's a square matrix, that was kind of from the definition of it. So we now know that a transpose a, if I were to put it, let me do it this way. That tells me that the reduced row echelon form of a transpose a is going to be equal to the k by k identity matrix, which tells me that a transpose a is invertible. Which is a pretty neat result. I started with a matrix that has linearly independent columns, so it wasn't just any matrix, it wasn't just any run of the mill matrix. It did have linearly independent columns, but it might have weird dimensions. It's not necessarily a square matrix. But I could construct a square matrix, a transpose a with it, and we now know that it also has linearly independent columns, it's a square matrix, and therefore it is invertible.