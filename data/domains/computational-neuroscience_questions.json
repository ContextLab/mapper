[
  {
    "id": "5306efd34bc615f2",
    "question_text": "The Hodgkin-Huxley model describes how action potentials are generated in neurons. What is the core mathematical framework?",
    "options": {
      "A": "A system of coupled ordinary differential equations describing membrane voltage changes driven by voltage-dependent sodium and potassium conductances, each governed by gating variables (m, h, n) that follow first-order kinetics — capturing the positive feedback of Na+ channel activation and the delayed negative feedback of K+ channel activation and Na+ inactivation",
      "B": "A single algebraic equation relating the membrane voltage linearly to the total injected current, analogous to a simple application of Ohm's law with a fixed conductance and no time-varying components",
      "C": "A stochastic model where action potentials occur randomly at a fixed probability per unit time, independent of the membrane voltage and any input currents",
      "D": "A digital binary model where each neuron exists in either an 'on' or 'off' state with no continuous voltage dynamics, threshold behavior, or temporal integration of synaptic inputs, such that the transition between states is instantaneous and governed by a simple logical gate function applied to the sum of binary inputs from presynaptic neurons with no biophysical mechanism underlying the switch"
    },
    "correct_answer": "A",
    "difficulty": 1,
    "x": 0.41,
    "y": 0.73386,
    "z": 0.651763,
    "source_article": "Hodgkin-Huxley model",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "Hodgkin-Huxley model",
      "action potential generation",
      "gating variables",
      "ion conductances"
    ]
  },
  {
    "id": "128e9cb0d9f527e9",
    "question_text": "In neural network models, what does a 'firing rate' model assume, and when is this simplification appropriate?",
    "options": {
      "A": "Firing rate models assume neurons communicate exclusively through electrical gap junction synapses and never use chemical synaptic transmission involving neurotransmitter release, receptor binding, and postsynaptic potential generation — this restriction fundamentally limits their applicability to only those neural circuits where gap junctions predominate, such as the retina and inferior olive",
      "B": "A firing rate model represents each neuron's output as a continuous variable (average spike rate) rather than individual spikes, assuming the relevant information is carried by rate, not precise spike timing. This is appropriate when the population is large and temporal dynamics of interest are slower than individual inter-spike intervals",
      "C": "Firing rate models simulate every individual ion channel in every neuron with full Hodgkin-Huxley dynamics including voltage-dependent gating kinetics for sodium, potassium, calcium, and leak conductances, combined with detailed dendritic morphology and cable equation computations for signal propagation along the neuronal arbor",
      "D": "Firing rate models assume neurons never generate action potentials at all and communicate exclusively through continuously graded subthreshold membrane voltage changes that propagate passively through electrical gap junctions and dendritic cable properties without any threshold-crossing regenerative spike events, synaptic neurotransmitter release, or any form of all-or-nothing digital signaling"
    },
    "correct_answer": "B",
    "difficulty": 1,
    "x": 0.457171,
    "y": 0.729493,
    "z": 0.840231,
    "source_article": "rate coding",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "rate coding",
      "firing rate models",
      "neural network abstraction levels"
    ]
  },
  {
    "id": "d841da73d1d1e985",
    "question_text": "Hebbian learning is often summarized as 'neurons that fire together, wire together.' What is the formal mathematical expression, and what fundamental problem does it have?",
    "options": {
      "A": "Hebbian learning states that synaptic connections between two neurons weaken when those neurons fire synchronously and strengthen when they fire asynchronously",
      "B": "Hebbian learning requires a global error signal broadcast to every synapse in the network, explicitly telling each connection how to change its weight in order to reduce the network's output error",
      "C": "The formal rule is Δw = η·x·y (weight change proportional to the product of pre- and post-synaptic activity). It captures associative learning but has a critical instability problem: weights grow without bound because correlated activity always increases weights, creating a positive feedback loop",
      "D": "Hebbian learning applies exclusively to inhibitory GABAergic synapses and has no effect on excitatory glutamatergic connections"
    },
    "correct_answer": "C",
    "difficulty": 1,
    "x": 0.497589,
    "y": 0.816212,
    "z": 0.320625,
    "source_article": "Hebbian learning",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "Hebbian learning",
      "synaptic plasticity rules",
      "weight instability"
    ]
  },
  {
    "id": "05e8095c8868c6e7",
    "question_text": "The leaky integrate-and-fire (LIF) neuron model is widely used in computational neuroscience. What simplification does it make compared to Hodgkin-Huxley?",
    "options": {
      "A": "The leaky integrate-and-fire model is actually more complex than Hodgkin-Huxley, incorporating additional ion channel types including calcium-activated potassium channels, hyperpolarization-activated cation channels, and persistent sodium conductances that produce a richer repertoire of firing patterns including bursting, adaptation, and plateau potentials not captured by the four-variable Hodgkin-Huxley formalism",
      "B": "LIF models do not produce any output spikes at all and compute only continuously varying subthreshold membrane potential trajectories without any threshold mechanism or reset rule, making them fundamentally incapable of generating the discrete all-or-nothing spike events that are necessary for communicating information to downstream neurons through conventional chemical synaptic transmission or for driving spike-timing-dependent plasticity",
      "C": "The leaky integrate-and-fire model and the Hodgkin-Huxley model are mathematically identical formulations that produce exactly the same voltage trajectories, spike shapes, firing rates, and dynamical behaviors for all possible input currents — the LIF designation is simply an alternative name for the Hodgkin-Huxley equations used in certain computational neuroscience textbooks",
      "D": "LIF replaces the detailed voltage-dependent conductance dynamics with a single linear differential equation for membrane voltage (τ·dV/dt = -(V-Vrest) + R·I) plus a threshold-and-reset rule: when V reaches threshold, a spike is recorded and V is reset. This sacrifices biophysical accuracy of spike shape for computational efficiency while preserving input-output relationships"
    },
    "correct_answer": "D",
    "difficulty": 1,
    "x": 0.452986,
    "y": 0.721006,
    "z": 0.88693,
    "source_article": "integrate-and-fire model",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "integrate-and-fire model",
      "model simplification",
      "membrane time constant",
      "threshold dynamics"
    ]
  },
  {
    "id": "27089c805f5d72e6",
    "question_text": "Spike-timing-dependent plasticity (STDP) extends Hebbian learning by incorporating temporal order. What is the key asymmetry, and why is it computationally significant?",
    "options": {
      "A": "If the presynaptic spike precedes the postsynaptic spike (pre-before-post), the synapse strengthens (LTP); if post-before-pre, it weakens (LTD). The magnitude depends on the time difference, decaying exponentially. This temporal asymmetry allows synapses to detect and reinforce causal relationships — inputs that consistently drive the postsynaptic cell are strengthened, while those that arrive too late are weakened",
      "B": "STDP strengthens all synaptic connections uniformly regardless of the relative timing of presynaptic and postsynaptic spikes",
      "C": "STDP weakens synaptic connections specifically when presynaptic spikes precede postsynaptic spikes within a narrow time window of approximately twenty milliseconds, and strengthens connections when the temporal order is reversed",
      "D": "STDP operates on a timescale of several hours to days, requiring sustained correlated activity over many thousands of spike pairings before any detectable change in synaptic efficacy occurs, making it far too slow to contribute to the fast synaptic modifications required for real-time perceptual learning, motor adaptation, or rapid associative memory formation during single behavioral episodes lasting seconds to minutes"
    },
    "correct_answer": "A",
    "difficulty": 2,
    "x": 0.507218,
    "y": 0.786101,
    "z": 0.382262,
    "source_article": "STDP",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "STDP",
      "temporal coding",
      "causal learning",
      "Bi and Poo 1998"
    ]
  },
  {
    "id": "555a5901c191af6c",
    "question_text": "Attractor networks are used to model associative memory. In a Hopfield network, how are memories stored and retrieved?",
    "options": {
      "A": "Memories in a Hopfield network are stored in a conventional digital lookup table data structure and retrieved by exact binary address matching, with each memory assigned a unique numerical index that must be specified precisely to retrieve the corresponding stored pattern without error",
      "B": "Memories are stored as stable fixed-point attractors of the network dynamics, encoded in the symmetric weight matrix via a Hebbian-like outer product rule (w_ij = Σ ξᵢᵘ·ξⱼᵘ). Retrieval is content-addressable: presenting a partial or noisy cue initializes the network near the correct attractor, and the dynamics converge to the stored pattern, implementing pattern completion",
      "C": "Memories in a Hopfield network are stored in individual neurons using a grandmother cell coding scheme, with each memory represented by the activation of exactly one dedicated neuron, meaning the network's storage capacity is limited to the total number of neurons and each memory occupies a single cell rather than being distributed across the synaptic weight matrix that connects all neurons in the network",
      "D": "Memories in a Hopfield network are stored only temporarily and decay within milliseconds after presentation because the network dynamics are purely dissipative with no stable attractor states, meaning there is no mechanism for sustained maintenance of stored patterns once the external input is removed and the network relaxes back to a homogeneous resting state through exponential decay of all neural activity"
    },
    "correct_answer": "B",
    "difficulty": 2,
    "x": 0.484839,
    "y": 0.780853,
    "z": 0.170867,
    "source_article": "Hopfield network",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "Hopfield network",
      "attractor dynamics",
      "associative memory",
      "content-addressable memory"
    ]
  },
  {
    "id": "988ec96d5d7f2aac",
    "question_text": "The Wilson-Cowan model describes the dynamics of interacting populations of excitatory and inhibitory neurons. What key emergent behavior does it produce?",
    "options": {
      "A": "The Wilson-Cowan model produces only stable, unchanging activity levels that converge to a single fixed point regardless of the parameter values chosen for excitatory and inhibitory coupling strengths",
      "B": "The Wilson-Cowan equations can only model a single isolated neuron and cannot represent the dynamics of interacting excitatory and inhibitory populations at the mesoscopic level",
      "C": "Through the interaction of excitatory and inhibitory populations (E-I dynamics), the Wilson-Cowan equations produce oscillations, bistability, and hysteresis — the system can exhibit limit cycles (sustained oscillations) when the E-I balance and coupling strengths fall in specific parameter ranges, providing a mean-field explanation for neural rhythms like gamma oscillations",
      "D": "The Wilson-Cowan model produces purely random, unpredictable activity patterns that have no structured temporal organization, oscillatory components, or systematic dependence on the model's coupling parameters"
    },
    "correct_answer": "C",
    "difficulty": 2,
    "x": 0.428413,
    "y": 0.753715,
    "z": 0.442181,
    "source_article": "Wilson-Cowan model",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "Wilson-Cowan model",
      "E-I balance",
      "neural oscillations",
      "mean-field models",
      "bifurcation analysis"
    ]
  },
  {
    "id": "4c33a02eea282343",
    "question_text": "Bayesian inference provides a framework for understanding how the brain combines prior knowledge with sensory evidence. In the context of perception, what does Bayes' theorem tell us?",
    "options": {
      "A": "The brain always trusts raw sensory input exactly as it is received from the peripheral sensory organs, performing no inferential processing, probabilistic weighting, or integration with prior expectations — perception is simply a faithful, veridical readout of the physical stimulus properties transduced by receptors without any top-down modulation or contextual influence from higher cortical areas",
      "B": "Bayesian perception means the brain hallucinates all perceptual experiences entirely from internally generated predictions without utilizing any bottom-up sensory data whatsoever, such that the content of conscious perception is completely independent of the actual physical stimuli present in the external environment and is determined solely by the brain's generative model",
      "C": "Prior beliefs about the statistical structure of the environment are genetically hardwired during embryonic brain development and remain permanently fixed throughout the organism's entire lifetime, with absolutely no capacity for updating or revision based on accumulated sensory experience, statistical learning, or environmental feedback at any point during postnatal development or adulthood",
      "D": "The posterior probability of a perceptual interpretation equals the likelihood of the sensory data given that interpretation, multiplied by the prior probability, normalized by the total evidence. The brain's percept is a weighted combination of prior expectations and sensory likelihood, with the weighting determined by their relative precision (inverse variance)"
    },
    "correct_answer": "D",
    "difficulty": 2,
    "x": 0.576477,
    "y": 0.823,
    "z": 0.676973,
    "source_article": "Bayesian inference",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "Bayesian inference",
      "prior and likelihood",
      "perceptual inference",
      "precision weighting"
    ]
  },
  {
    "id": "7baabbe660777226",
    "question_text": "Reinforcement learning (RL) models have been remarkably successful in explaining dopaminergic neuron activity. What specific signal do dopamine neurons encode, and how was this discovered?",
    "options": {
      "A": "Dopamine neurons encode the temporal difference (TD) reward prediction error: the difference between received reward (plus the discounted value of the new state) and the expected value of the previous state. Schultz, Dayan & Montague (1997) showed that midbrain dopamine neurons fire exactly as predicted by TD learning — responding to unexpected rewards, ceasing to respond to fully predicted rewards, and showing a depression (pause) when expected rewards are omitted",
      "B": "Dopamine neurons encode the absolute magnitude of reward received at any moment, firing at a rate directly proportional to reward size regardless of prior expectations or prediction history",
      "C": "Dopamine neurons exclusively encode motor commands for approaching and reaching toward rewarding stimuli in the environment, with no involvement in reward evaluation or learning signals",
      "D": "Dopamine neurons fire in a completely random and stochastic pattern that has no systematic relationship to reward delivery, reward omission, reward prediction, or any other behaviorally relevant event occurring during operant conditioning, classical conditioning, or instrumental learning tasks — the observed firing rate fluctuations in midbrain dopaminergic neurons are entirely attributable to the intrinsic biophysical noise generated by stochastic ion channel gating"
    },
    "correct_answer": "A",
    "difficulty": 3,
    "x": 0.522143,
    "y": 0.774365,
    "z": 0.666156,
    "source_article": "reward prediction error",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "reward prediction error",
      "temporal difference learning",
      "dopamine",
      "Schultz et al. 1997"
    ]
  },
  {
    "id": "74d7f0187525b4ae",
    "question_text": "Neural population coding represents information through patterns of activity across many neurons. What advantage does population coding have over single-neuron coding, and how is information typically decoded?",
    "options": {
      "A": "Population coding is strictly less accurate than single-neuron coding because adding more neurons to a population inevitably introduces additional uncorrelated noise that degrades signal quality",
      "B": "Population coding allows higher-dimensional representations, noise averaging, and graceful degradation. Information is decoded using methods like population vector averaging (Georgopoulos et al. 1986), maximum likelihood estimation, or Bayesian decoding. The Fisher information in a population scales with the number of neurons, allowing population accuracy to exceed any individual neuron's precision",
      "C": "Population coding means that all neurons within a population fire at exactly the same average rate and carry precisely identical redundant copies of the same information about the stimulus, providing no additional representational capacity beyond what a single neuron could achieve alone — the only benefit is reliability through exact redundancy, with no capacity for representing higher-dimensional stimulus features through diverse heterogeneous tuning curves",
      "D": "Population coding operates exclusively within primary sensory cortical areas such as V1, A1, and S1, and fundamentally cannot represent motor commands, decision variables, cognitive states, spatial location maps, reward values, or any other type of neural information processed by prefrontal, premotor, posterior parietal, hippocampal, or any other non-primary-sensory cortical or subcortical brain region"
    },
    "correct_answer": "B",
    "difficulty": 3,
    "x": 0.538378,
    "y": 0.759136,
    "z": 0.882073,
    "source_article": "population coding",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "population coding",
      "population vector",
      "Fisher information",
      "neural decoding"
    ]
  },
  {
    "id": "6a88e4db10fde067",
    "question_text": "The balanced excitation-inhibition (E-I balance) regime is a fundamental operating principle of cortical circuits. What computational properties does this regime confer?",
    "options": {
      "A": "Excitatory-inhibitory balance means that excitatory and inhibitory currents are precisely equal in magnitude at all times, producing a net synaptic current of exactly zero, so that neurons in balanced networks never fire any action potentials under any stimulus conditions",
      "B": "E-I balance is purely an artifact of in vitro brain slice preparations created by the artificial perfusion medium and does not occur in intact cortical circuits in vivo during normal behavioral states",
      "C": "In the balanced regime, strong excitatory and inhibitory currents nearly cancel, leaving neurons in a fluctuation-driven state where spiking is driven by momentary imbalances. This produces irregular, Poisson-like firing (matching cortical recordings), high sensitivity to small input changes (operating near threshold), rapid response times, and enables the network to represent a wide dynamic range. Disrupted E-I balance is implicated in epilepsy (excess excitation) and potentially autism and schizophrenia",
      "D": "E-I balance means the brain uses only excitatory glutamatergic neurons throughout all cortical layers and that GABAergic inhibitory interneurons are unnecessary for any aspect of cortical computation, rhythm generation, or gain control"
    },
    "correct_answer": "C",
    "difficulty": 3,
    "x": 0.496752,
    "y": 0.745956,
    "z": 0.527228,
    "source_article": "E-I balance",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "E-I balance",
      "fluctuation-driven firing",
      "cortical dynamics",
      "irregular spiking"
    ]
  },
  {
    "id": "ce7246a0a77d0d66",
    "question_text": "Information theory quantifies how much information neural responses carry about stimuli. What does 'mutual information' between a stimulus and a neural response measure, and why is it preferred over simpler metrics like correlation?",
    "options": {
      "A": "Mutual information between neural responses and stimulus features is always exactly zero regardless of the neural code used, because the noise inherent in biological neural systems is too large to permit any reliable transmission of stimulus-related information from the periphery to central processing areas — the observed correlations between stimuli and neural activity in electrophysiology experiments are statistical artifacts of insufficient trial-count sampling and multiple-comparison problems",
      "B": "Mutual information measures only linear statistical relationships between stimulus variables and neural response variables and is mathematically equivalent to computing the Pearson correlation coefficient between stimulus and response distributions, providing no additional information about nonlinear dependencies, higher-order statistical structure, or complex coding schemes beyond what a simple correlation analysis would reveal",
      "C": "Mutual information is a purely abstract mathematical tool developed for engineering telecommunications systems that has no relevance or applicability to the analysis of actual neural coding in biological nervous systems, because the assumptions underlying Shannon's information theory — discrete symbol alphabets, stationary ergodic processes, and memoryless channels — are fundamentally violated by biological neural systems",
      "D": "Mutual information I(S;R) quantifies the reduction in uncertainty about the stimulus S given the neural response R (or equivalently, about R given S). Unlike correlation, it captures all statistical dependencies — linear and nonlinear — between stimulus and response, making it a model-free measure of neural coding efficiency. It is measured in bits and bounded by the entropy of both the stimulus and response"
    },
    "correct_answer": "D",
    "difficulty": 3,
    "x": 0.524752,
    "y": 0.774086,
    "z": 0.822292,
    "source_article": "information theory",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "information theory",
      "mutual information",
      "neural coding",
      "Shannon entropy"
    ]
  },
  {
    "id": "8d49a441a38bd741",
    "question_text": "The drift-diffusion model (DDM) is the standard computational model for two-alternative forced choice decisions. What is the underlying mechanism, and what behavioral phenomena does it explain?",
    "options": {
      "A": "The DDM proposes that the brain accumulates noisy sensory evidence over time as a stochastic process (random walk with drift) between two decision boundaries. The drift rate reflects evidence quality, boundaries reflect the speed-accuracy tradeoff, and the non-decision time captures sensory/motor delays. This single mechanism simultaneously explains choice accuracy, mean reaction times, reaction time distributions, speed-accuracy tradeoffs, and the neural ramping activity observed in LIP and FEF during decision-making",
      "B": "The drift-diffusion model states that perceptual decisions are made instantaneously at the moment of stimulus onset, with no temporal accumulation of noisy sensory evidence occurring between stimulus presentation and motor response",
      "C": "The drift-diffusion model describes only the motor response selection stage of decision-making and has no application to the perceptual evidence evaluation process that precedes response execution",
      "D": "The drift-diffusion model assumes that all decisions are fundamentally random and statistically independent of stimulus quality, discriminability, or signal-to-noise ratio, predicting flat psychometric functions at chance performance regardless of stimulus strength, and identical reaction time distributions across all experimental conditions — no parameter of the model including drift rate, boundary separation, or non-decision time is in any way influenced by or correlated with the physical properties of the stimulus being discriminated"
    },
    "correct_answer": "A",
    "difficulty": 4,
    "x": 0.505782,
    "y": 0.782843,
    "z": 0.540331,
    "source_article": "drift-diffusion model",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "drift-diffusion model",
      "evidence accumulation",
      "speed-accuracy tradeoff",
      "decision neuroscience"
    ]
  },
  {
    "id": "135458b6faeea7f9",
    "question_text": "Recurrent neural networks (RNNs) trained on cognitive tasks have emerged as models of neural computation. What key insight has this approach provided about how the brain might implement complex cognitive functions?",
    "options": {
      "A": "Trained RNNs reveal that cognitive functions require millions of completely separate and specialized neural modules, with exactly one dedicated module per cognitive task and no shared computational resources or representations between different task demands",
      "B": "Trained RNNs reveal that complex cognitive functions emerge from low-dimensional dynamical structures in high-dimensional neural state spaces: fixed points, limit cycles, saddle points, and slow manifolds organize the network's computation. For example, a context-dependent decision task produces saddle-point dynamics where context inputs select which attractor basin the trajectory enters — the same network elements implement different computations through dynamical reconfiguration rather than separate circuits",
      "C": "RNNs trained on neuroscience tasks conclusively prove that recurrent connections between neurons are entirely unnecessary for any form of temporal computation, working memory maintenance, sequence generation, or context-dependent processing — feedforward architectures with no feedback loops are fully sufficient to replicate every dynamical pattern observed in neural recordings from prefrontal cortex, motor cortex, and hippocampus during cognitive tasks requiring temporal integration over hundreds of milliseconds to seconds",
      "D": "RNNs trained on cognitive neuroscience tasks universally produce completely random, unstructured, and uninterpretable dynamics in their hidden state spaces that bear no resemblance to neural population recordings, cannot be analyzed using dynamical systems methods such as fixed-point analysis or dimensionality reduction, and provide no computational or scientific insight into how biological recurrent circuits implement cognitive functions — the trained networks are opaque black boxes with no explanatory power for neural computation"
    },
    "correct_answer": "B",
    "difficulty": 4,
    "x": 0.525682,
    "y": 0.725679,
    "z": 0.318506,
    "source_article": "recurrent neural networks",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "recurrent neural networks",
      "dynamical systems",
      "neural state space",
      "computational models of cognition"
    ]
  },
  {
    "id": "2c82616be7f1186c",
    "question_text": "Normative models of neural coding ask: what is the optimal code given constraints? Efficient coding (Barlow, 1961) proposes that sensory neurons maximize information transmission. What is the key prediction, and how has it been validated?",
    "options": {
      "A": "Efficient coding predicts that all neurons throughout every level of the sensory processing hierarchy should have identical tuning curves with the same preferred stimulus value, bandwidth, dynamic range, and response gain — there should be no diversity in neural selectivity because maximal information transmission requires all neurons to redundantly encode the single most probable stimulus feature in the natural environment, concentrating all coding resources on the modal stimulus value at the expense of representing any other features",
      "B": "Efficient coding has absolutely no experimental support whatsoever and remains a purely theoretical framework with no validated predictions about actual neural response properties — no published study has ever successfully demonstrated that retinal ganglion cell contrast sensitivity, V1 receptive field structure, auditory nerve fiber frequency tuning curves, or any other measured neural response property matches the statistical predictions derived from information-theoretic optimization of coding efficiency for natural stimulus ensembles in any sensory modality",
      "C": "Efficient coding predicts that neural response distributions should be adapted to the statistics of natural stimuli — specifically, neurons should allocate more coding capacity to frequently occurring stimulus features. This has been validated by showing that retinal ganglion cell contrast sensitivity matches the contrast distribution in natural scenes, that V1 receptive fields resemble Gabor-like basis functions optimal for representing natural images, and that adaptation adjusts neural coding to current stimulus statistics",
      "D": "Efficient coding applies exclusively to artificial neural networks and deep learning architectures used in machine learning applications and has absolutely no relevance to biological neural systems, because the metabolic and biophysical constraints of real biological neurons — including finite maximum firing rates, substantial metabolic costs of spike generation and synaptic transmission, stochastic synaptic vesicle release unreliability, and complex dendritic integration nonlinearities — fundamentally preclude the optimization of information transmission that efficient coding assumes"
    },
    "correct_answer": "C",
    "difficulty": 4,
    "x": 0.551484,
    "y": 0.761817,
    "z": 1.0,
    "source_article": "efficient coding",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "efficient coding",
      "Barlow's hypothesis",
      "natural scene statistics",
      "optimal neural coding"
    ]
  },
  {
    "id": "23f2ec54222bacd7",
    "question_text": "The Neural Engineering Framework (NEF, Eliasmith & Anderson 2003) provides a systematic method for building biologically plausible neural models. What are its three core principles?",
    "options": {
      "A": "The Neural Engineering Framework uses only rate coding and fundamentally cannot represent any temporal dynamics, transient responses, or time-varying computations in spiking neural networks",
      "B": "The Neural Engineering Framework requires all neurons in a population to be identical with the same tuning curve parameters, firing thresholds, maximum rates, and background activity levels, with no heterogeneity in neural response properties",
      "C": "The Neural Engineering Framework is identical to deep learning and trains all synaptic weights using error backpropagation through time, with no biological plausibility constraints on the learning rule or network architecture",
      "D": "The NEF is based on: (1) representation — variables are encoded in population activity via tuning curves and decoded by weighted linear combinations; (2) transformation — connections implement mathematical functions by combining decoders and encoders into weight matrices; (3) dynamics — recurrent connections implement dynamical systems by choosing weights that approximate desired differential equations. This framework built Spaun, the largest functional brain model (2.5 million neurons performing multiple cognitive tasks)"
    },
    "correct_answer": "D",
    "difficulty": 4,
    "x": 0.504859,
    "y": 0.781647,
    "z": 0.88804,
    "source_article": "Neural Engineering Framework",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "Neural Engineering Framework",
      "Spaun",
      "population coding",
      "neural representation",
      "biological plausibility"
    ]
  },
  {
    "id": "a675680b40bd8f93",
    "question_text": "Reservoir computing (echo state networks / liquid state machines) uses a different approach to computation than traditional trained networks. What is the fundamental principle, and why is it relevant to neuroscience?",
    "options": {
      "A": "A large recurrent network (the 'reservoir') with fixed random weights projects input into a high-dimensional nonlinear dynamical space; only the output readout weights are trained. This is relevant to neuroscience because it suggests that the brain's recurrent circuits may not need precise weight tuning — instead, random but appropriately structured connectivity may provide a rich dynamical substrate from which downstream areas can learn to read out task-relevant information through simple linear combinations",
      "B": "Reservoir computing uses only feedforward connections with no recurrent connectivity between neurons within the reservoir layer, meaning it cannot perform any temporal computation or maintain any memory of past inputs",
      "C": "Reservoir computing has absolutely no connection to neuroscience and is a purely engineering approach designed for time-series prediction in industrial applications with no implications for understanding biological neural computation",
      "D": "Reservoir computing trains every single synaptic weight in the entire network including all recurrent connections within the reservoir using standard error backpropagation through time with gradient descent optimization"
    },
    "correct_answer": "A",
    "difficulty": 5,
    "x": 0.537483,
    "y": 0.735958,
    "z": 0.370027,
    "source_article": "reservoir computing",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "reservoir computing",
      "echo state networks",
      "liquid state machines",
      "random recurrent networks"
    ]
  },
  {
    "id": "dc0bfde83c605c59",
    "question_text": "Neural field models describe the spatiotemporal dynamics of cortical activity as continuous fields. What key phenomenon do they explain, and what is the mathematical framework?",
    "options": {
      "A": "The Amari equation with Mexican hat local excitation and surround lateral inhibition",
      "B": "Neural field models use purely algebraic equations with no spatial component",
      "C": "Only the dynamics of single neurons in complete isolation",
      "D": "Neural field models describe cortical dynamics using continuous spatiotemporal activity fields based on integro-differential equations that produce traveling waves, pattern formation, and persistent activity bumps"
    },
    "correct_answer": "B",
    "difficulty": 5,
    "x": 0.45431,
    "y": 0.740585,
    "z": 0.401368,
    "source_article": "neural field theory",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "neural field theory",
      "Amari equation",
      "pattern formation",
      "traveling waves",
      "bump attractors"
    ]
  },
  {
    "id": "e1ccf29f4f402e3d",
    "question_text": "The credit assignment problem is a central challenge in biological learning: how does a synapse deep in a network 'know' how to change to improve behavior? How do biological circuits potentially solve this, and why is backpropagation considered biologically implausible?",
    "options": {
      "A": "Backpropagation is perfectly biologically plausible as the brain's algorithm",
      "B": "The brain relies entirely on pre-programmed genetic wiring with no learning",
      "C": "Synapses change randomly with no computational principle governing plasticity",
      "D": "Backpropagation is biologically implausible because it requires symmetric forward and backward weights, separate computational phases, and exact derivatives — biological alternatives include feedback alignment with random backward weights, predictive coding implementing equivalent local computations through prediction errors, equilibrium propagation, and three-factor Hebbian rules modulated by neuromodulatory signals encoding global reward or error information"
    },
    "correct_answer": "C",
    "difficulty": 5,
    "x": 0.556257,
    "y": 0.807383,
    "z": 0.560426,
    "source_article": "credit assignment",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "credit assignment",
      "backpropagation",
      "biological plausibility",
      "feedback alignment",
      "predictive coding"
    ]
  },
  {
    "id": "9989d34b114b8c8d",
    "question_text": "Dimensionality reduction techniques (PCA, GPFA, LFADS) applied to neural population recordings have revealed that high-dimensional neural activity often lies on low-dimensional manifolds. What does this mean for neural computation, and what is the 'communication through coherence' (CTC) hypothesis in this context?",
    "options": {
      "A": "Low-dimensional manifolds discovered through dimensionality reduction of neural population recordings mean that the vast majority of individual neurons recorded in these populations are completely inactive and functionally unnecessary, contributing no information to the population code — the low dimensionality directly implies that only a small handful of neurons are actually firing while all others remain silent, and the manifold structure is simply an artifact of recording from a small active subset embedded within a much larger population of non-functional neurons that could be removed without any impact on the circuit's computational output or behavioral performance in the task being studied",
      "B": "Dimensionality reduction techniques applied to neural population recordings are purely mathematical artifacts of the specific analysis methods employed and have no meaningful biological interpretation — the apparently low-dimensional manifold structure reflects nothing more than the mathematical properties of principal component analysis, Gaussian process factor analysis, or other linear and nonlinear dimensionality reduction algorithms when applied to any high-dimensional dataset with temporal correlations, and would emerge identically if applied to recordings of random noise generated by neural tissue in the complete absence of any structured stimulus input or behavioral task engagement",
      "C": "Low-dimensional manifolds indicate that neural populations in cortical circuits are constrained to a small number of specific covariation modes reflecting task-relevant computational structure, but the communication through coherence hypothesis proposed by Pascal Fries is a completely unrelated theoretical framework that has no conceptual or mathematical connection to dimensionality reduction, manifold analysis, or the geometry of neural population dynamics — CTC concerns oscillatory phase relationships between local field potentials in different brain areas and operates at an entirely different spatial and temporal scale than the single-neuron population activity patterns captured by manifold analysis methods",
      "D": "Neural activity lying on low-dimensional manifolds means that despite thousands of neurons, the population dynamics evolve along a small number of collective modes (dimensions), reflecting the computational structure of the task. The CTC hypothesis (Fries 2005, 2015) proposes that communication between brain areas is gated by oscillatory coherence: when sender and receiver populations oscillate in phase, spikes from the sender arrive during the receiver's excitable phase, maximizing impact. In the manifold framework, inter-area communication may require alignment of the sending population's manifold with the receiving population's input dimensions"
    },
    "correct_answer": "D",
    "difficulty": 5,
    "x": 0.539212,
    "y": 0.718568,
    "z": 0.356274,
    "source_article": "neural manifolds",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "neural manifolds",
      "dimensionality reduction",
      "communication through coherence",
      "population dynamics",
      "inter-area communication"
    ]
  },
  {
    "id": "f764a7b094840838",
    "question_text": "The FitzHugh-Nagumo model is a simplified two-variable reduction of the Hodgkin-Huxley equations. What key dynamical insight does this reduction preserve that makes it useful for understanding neural excitability?",
    "options": {
      "A": "It preserves the nullcline structure and excitable dynamics — a fast voltage-like variable interacts with a slow recovery variable, creating a phase plane where small perturbations decay but suprathreshold perturbations trigger large excursions (action potentials), capturing the essential threshold and refractory behavior without modeling individual ion channels",
      "B": "The FitzHugh-Nagumo reduction eliminates all nonlinearity from the original Hodgkin-Huxley equations in order to produce a purely linear dynamical system that can be solved analytically using standard matrix exponential techniques, completely removing the threshold behavior, excitability, and refractory period that characterize real neuronal action potential generation and propagation along the axonal membrane",
      "C": "The FitzHugh-Nagumo model retains only the sodium channel activation dynamics from the original Hodgkin-Huxley equations while completely ignoring all potassium channel dynamics, the recovery variable, membrane capacitance, leak conductance, and every other biophysical mechanism involved in action potential repolarization and the return of the neuronal membrane to its resting potential after depolarization",
      "D": "The FitzHugh-Nagumo model replaces the continuous voltage dynamics of the Hodgkin-Huxley equations with a discrete binary on-off representation that has absolutely no threshold behavior, no graded response to input amplitude, no refractory period, and no interaction between fast excitatory and slow recovery variables, producing a model that is qualitatively identical to a simple Boolean logic gate"
    },
    "correct_answer": "A",
    "difficulty": 1,
    "x": 0.440173,
    "y": 0.711661,
    "z": 0.708814,
    "source_article": "FitzHugh-Nagumo model",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "FitzHugh-Nagumo model",
      "phase plane analysis",
      "neural excitability"
    ]
  },
  {
    "id": "6cdcf32a6c9b0e3b",
    "question_text": "Cable theory models how electrical signals propagate along dendrites and axons. What fundamental parameter determines how far a subthreshold voltage signal can spread along a passive (non-spiking) neural process?",
    "options": {
      "A": "The signal propagation distance depends on the total number of synapses rather than any biophysical membrane property",
      "B": "The length constant (lambda), defined as the square root of the ratio of membrane resistance to axial resistance — it determines the exponential distance over which voltage decays to 1/e of its original value, such that high membrane resistance and low axial resistance allow signals to spread further along the cable",
      "C": "The signal spreads infinitely far with no attenuation in all passive neural processes regardless of their biophysical properties",
      "D": "The conduction velocity of actively propagating action potentials is the only biophysical parameter that has any relevance whatsoever to the propagation or spread of electrical signals in any neural compartment, whether passive dendritic process or active axonal membrane — subthreshold passive signal spread, cable properties, membrane resistance, and axial resistance play absolutely no role in determining how far signals travel"
    },
    "correct_answer": "B",
    "difficulty": 1,
    "x": 0.499065,
    "y": 0.744655,
    "z": 0.623951,
    "source_article": "cable theory",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "cable theory",
      "length constant",
      "passive signal propagation"
    ]
  },
  {
    "id": "810f1674722aa276",
    "question_text": "Winner-take-all (WTA) circuits are fundamental computational motifs in neural networks. What computation does a WTA circuit perform, and what biophysical mechanism typically implements it?",
    "options": {
      "A": "Winner-take-all circuits simply sum all inputs equally with no competition, no suppression of losing units, and no amplification of winners, using only excitatory feedforward connections between all neurons in the network — there is absolutely no lateral or recurrent inhibition, no shared inhibitory interneuron pool, and no competitive dynamics that could implement any form of selection or argmax computation",
      "B": "Winner-take-all circuits perform exclusively temporal integration over very long timescales of hours to days, relying solely on long-term synaptic plasticity mechanisms such as long-term potentiation and long-term depression rather than fast recurrent inhibitory dynamics — they have no role in rapid real-time competitive neural selection or moment-to-moment decision-making computations operating on millisecond timescales",
      "C": "WTA circuits select the most strongly activated neuron while suppressing all others — implemented through lateral (recurrent) inhibition where neurons compete via shared inhibitory interneurons, producing a competitive dynamics that amplifies the strongest input and suppresses weaker alternatives, effectively computing an argmax function",
      "D": "WTA circuits transmit signals without any transformation, functioning as passive relay stations with no computational role"
    },
    "correct_answer": "C",
    "difficulty": 1,
    "x": 0.54349,
    "y": 0.738542,
    "z": 0.430036,
    "source_article": "winner-take-all circuits",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "winner-take-all circuits",
      "lateral inhibition",
      "competitive dynamics"
    ]
  },
  {
    "id": "6a90c38824962d0f",
    "question_text": "Gain modulation is a fundamental neural computation where one input multiplicatively scales the response to another input. What is the computational significance of gain modulation for neural processing?",
    "options": {
      "A": "Divisive normalization multiplies all neural responses by a single fixed constant multiplicative factor that never changes regardless of the overall level of network activity, the statistics of the input population, the number of active neurons in the normalization pool, or any other contextual variable — it applies the same unchanging scaling to every neuron's response under every possible stimulus and network condition without exception",
      "B": "Divisive normalization is found only in the retina as a mechanism for light adaptation and has never been observed, measured, or even hypothesized to occur in any cortical area including primary visual cortex V1, motion-processing area MT, prefrontal cortex, or any other brain region involved in attention, multisensory integration, decision-making, value computation, or any higher cognitive function beyond early retinal processing",
      "C": "Divisive normalization doubles every individual neuron's firing rate uniformly and identically across the entire neural population without any competitive interaction, suppression, or relative scaling between neurons — it functions as a simple multiplicative amplifier that increases all responses by the same factor regardless of relative activation levels, producing no winner-take-all competition and no contrast enhancement",
      "D": "Gain modulation enables flexible coordinate transformations and context-dependent processing — by multiplicatively scaling a neuron's response to its primary input based on a modulatory signal (e.g., eye position modulating visual responses), it allows the same neural circuit to implement different input-output mappings depending on context, which is essential for sensorimotor transformations and attention"
    },
    "correct_answer": "D",
    "difficulty": 2,
    "x": 0.548373,
    "y": 0.700819,
    "z": 0.545313,
    "source_article": "gain modulation",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "gain modulation",
      "coordinate transformations",
      "multiplicative interactions"
    ]
  },
  {
    "id": "a1655c77091cf113",
    "question_text": "Divisive normalization has been called a 'canonical neural computation' because it appears across many brain areas and modalities. What does this operation compute, and why is it considered canonical?",
    "options": {
      "A": "Divisive normalization computes the response of a neuron by dividing its excitatory drive by a factor that includes the summed activity of a pool of neighboring neurons — this creates competition, contrast enhancement, and invariance to overall input magnitude, and it appears in retinal adaptation, V1 orientation tuning, attention, multisensory integration, and decision-making, suggesting it is a universal building block of cortical computation",
      "B": "Spike-frequency adaptation implements a low-pass temporal filter that emphasizes sustained steady-state activity levels over transient changes, meaning that adapting synapses respond most strongly to constant unchanging inputs maintained for long periods and progressively attenuate their responses to rapid fluctuations, novel onsets, or any form of temporal change in the presynaptic firing rate — this prevents change detection and novelty sensitivity",
      "C": "It is found only in the retina and has never been observed in any cortical area or higher cognitive process",
      "D": "Spike-frequency adaptation affects only inhibitory GABAergic synapses in the cortex and never occurs at any excitatory glutamatergic connection anywhere in the central nervous system — the phenomenon is restricted entirely to inhibitory interneurons and their output synapses, with pyramidal cells and all other excitatory neuron types being completely immune to any form of activity-dependent reduction in firing rate during sustained depolarization"
    },
    "correct_answer": "A",
    "difficulty": 2,
    "x": 0.5419,
    "y": 0.699157,
    "z": 0.545153,
    "source_article": "divisive normalization",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "divisive normalization",
      "canonical computations",
      "contrast gain control"
    ]
  },
  {
    "id": "e8f26ca8bc308495",
    "question_text": "The Izhikevich neuron model achieves a different design tradeoff than either Hodgkin-Huxley or integrate-and-fire models. What is this tradeoff, and what makes the model distinctive?",
    "options": {
      "A": "It is less biophysically realistic than integrate-and-fire models and more computationally expensive than Hodgkin-Huxley",
      "B": "The Izhikevich model uses just two differential equations with a reset rule, yet by varying only four parameters it can reproduce over 20 distinct firing patterns observed in real cortical neurons (regular spiking, fast spiking, bursting, chattering, etc.) — achieving near-Hodgkin-Huxley biological realism at near-integrate-and-fire computational cost",
      "C": "It can only reproduce one firing pattern (regular spiking) and requires hundreds of parameters to do so",
      "D": "Shallow processing of surface-level perceptual features such as letter font, case, and visual appearance always produces substantially better and more durable long-term memory than deep semantic processing involving meaningful elaboration, self-referential encoding, or relational processing, because perceptual surface features create more distinctive, more accessible, and more interference-resistant memory traces at every retention interval tested in every experimental paradigm"
    },
    "correct_answer": "B",
    "difficulty": 2,
    "x": 0.466903,
    "y": 0.719221,
    "z": 0.980736,
    "source_article": "Izhikevich model",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "Izhikevich model",
      "neuron model tradeoffs",
      "firing patterns"
    ]
  },
  {
    "id": "ada7e58467781fbb",
    "question_text": "Synfire chains are a theoretical construct in computational neuroscience. What are they, and what computational problem do they address regarding reliable signal propagation?",
    "options": {
      "A": "Synfire chains refer to the physical molecular chains of synaptic adhesion proteins, scaffolding molecules, and neurotransmitter receptor complexes that form the physical structural connections between presynaptic axon terminals and postsynaptic dendritic spines — they are a biochemical concept from molecular neuroscience describing the protein interaction cascades within individual synaptic junctions rather than any circuit-level computational mechanism",
      "B": "Synfire chains describe the strictly sequential one-to-one activation of individual single neurons arranged in a simple linear chain with absolutely no population structure, no synchronized group activity, no convergent or divergent connectivity, and no mechanism for noise tolerance — each neuron in the chain connects to exactly one downstream neuron, making the chain completely unreliable because any single synaptic failure terminates propagation",
      "C": "Synfire chains are feed-forward chains of neuronal groups where synchronous volleys propagate stably from one group to the next — they solve the problem of reliable signal transmission in noisy neural circuits by showing that precisely timed synchronous activity in groups of ~100 neurons can propagate without degrading, unlike single-neuron signals which are unreliable due to synaptic failure and noise",
      "D": "Synfire chains are a specialized type of purely inhibitory neural circuit whose sole function is to suppress and prevent all neural activity propagation throughout the cortex — they function as a global braking mechanism that intercepts and blocks excitatory signal transmission between brain regions, ensuring that no coordinated sequential activity can ever propagate through cortical networks under any stimulus or behavioral condition"
    },
    "correct_answer": "C",
    "difficulty": 3,
    "x": 0.533578,
    "y": 0.750691,
    "z": 0.459104,
    "source_article": "synfire chains",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "synfire chains",
      "reliable signal propagation",
      "synchronous volleys"
    ]
  },
  {
    "id": "ccf864ccc326d308",
    "question_text": "Mean-field theory is a widely used approximation in computational neuroscience for analyzing large neural networks. What does it approximate, and when does it break down?",
    "options": {
      "A": "Mean-field theory tracks every individual action potential in every single neuron simultaneously across the entire neural network with absolutely no approximation, no averaging, no information loss, no dimensionality reduction, and no simplifying assumptions about connectivity structure, correlation patterns, or the statistical properties of synaptic inputs — it provides a complete microscopic description of every spike time in every neuron at all moments without any computational shortcuts",
      "B": "Mean-field theory applies only to neural networks containing exactly two neurons connected by a single synapse, and it fundamentally cannot scale to larger populations of three or more interacting neurons because the mathematical framework breaks down completely when more than one pairwise interaction exists — the theory was specifically derived for the simplest possible two-neuron circuit and all attempts to generalize it to networks of any larger size have consistently failed",
      "C": "Mean-field theory assumes that all neurons in the entire network fire at exactly the same identical rate at all times with absolutely no fluctuations, no variability, no heterogeneity between individual neurons, no temporal dynamics, and no dependence on stimulus conditions or network state — every neuron is treated as producing the exact same number of spikes per second regardless of its inputs, connectivity, intrinsic properties, cell type, or any other distinguishing factor",
      "D": "Mean-field theory tracks only the average firing rate of neural populations rather than individual spikes, treating each neuron as responding to the population-averaged input — it provides accurate predictions for network states (fixed points, oscillations, chaos) when neurons are sparsely and randomly connected, but breaks down when correlations between neurons are strong, such as during synchronous oscillations, highly structured connectivity, or near critical transitions"
    },
    "correct_answer": "D",
    "difficulty": 3,
    "x": 0.508662,
    "y": 0.745081,
    "z": 0.622078,
    "source_article": "mean-field theory",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "mean-field theory",
      "population dynamics",
      "neural network analysis"
    ]
  },
  {
    "id": "e4628f4d3fc8e2c4",
    "question_text": "The concept of 'neural manifolds' has transformed how researchers interpret neural population recordings. What does it mean for neural activity to lie on a low-dimensional manifold, and why is this significant?",
    "options": {
      "A": "Although a population of N neurons could theoretically occupy an N-dimensional state space, the coordinated activity patterns actually trace out a much lower-dimensional surface (manifold) — this means the population's activity is constrained by underlying dynamical structure, and the manifold's geometry reveals the computational variables the circuit manipulates, separating signal from noise",
      "B": "Although a population of N neurons could theoretically occupy an N-dimensional state space, every neuron in a real population operates completely independently with zero correlations, zero shared variability, and zero coordinated activity patterns, meaning that the full N-dimensional state space is always utilized uniformly without any low-dimensional structure, manifold geometry, or dynamical constraints reducing the effective dimensionality of the neural representation",
      "C": "Neural manifolds refer exclusively to the physical macroscopic folds, sulci, and gyri of the cortical tissue surface that give the cerebral cortex its characteristic wrinkled appearance, and they have absolutely no conceptual or mathematical relationship to neural activity patterns, population dynamics, dimensionality reduction, latent variables, or any other aspect of computational neuroscience analysis of multi-neuron recordings from electrophysiology or imaging",
      "D": "Neural activity always occupies exactly one dimension regardless of population size or task complexity"
    },
    "correct_answer": "A",
    "difficulty": 3,
    "x": 0.541434,
    "y": 0.691585,
    "z": 0.420323,
    "source_article": "neural manifolds",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "neural manifolds",
      "dimensionality reduction",
      "population geometry"
    ]
  },
  {
    "id": "b2cd3627a270c33f",
    "question_text": "Stochastic resonance is a counterintuitive phenomenon relevant to neural signal detection. What does it describe, and what is the condition for it to occur?",
    "options": {
      "A": "Stochastic resonance means that adding any amount of random noise to a signal always degrades signal detection performance monotonically in every type of system, whether linear or nonlinear, threshold-based or continuous, biological or artificial — there is never any noise level at which detection improves, and the relationship between noise intensity and signal detection accuracy is always strictly decreasing without any peak or optimum",
      "B": "Adding an optimal amount of noise to a subthreshold signal can paradoxically improve detection performance in a nonlinear threshold system — at the right noise level, random fluctuations occasionally push the weak signal above threshold, producing responses correlated with the signal, but too much noise overwhelms the signal again, creating an inverted-U relationship between noise intensity and detection accuracy",
      "C": "Adding an optimal amount of noise to a subthreshold signal can paradoxically improve detection performance in a nonlinear threshold system — at the right noise level, random fluctuations occasionally push the weak signal above threshold, producing responses correlated with the signal, but too much noise overwhelms the signal, creating an inverted-U relationship between noise and accuracy",
      "D": "Stochastic resonance is a phenomenon that occurs exclusively and only in artificial electronic circuits, digital signal processing systems, and engineered communication channels, and it has never been observed, measured, modeled, or even theoretically proposed to occur in any biological neural system, sensory receptor, mechanoreceptor, electroreceptor, or any other living organism at any level of neural processing from peripheral sensory transduction to central cortical computation"
    },
    "correct_answer": "B",
    "difficulty": 3,
    "x": 0.525547,
    "y": 0.734282,
    "z": 0.365824,
    "source_article": "stochastic resonance",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "stochastic resonance",
      "noise-enhanced detection",
      "nonlinear threshold systems"
    ]
  },
  {
    "id": "303c32819ad4cd2d",
    "question_text": "Predictive coding is a computational framework proposing that cortical circuits implement hierarchical Bayesian inference. What specific computation does each level of the cortical hierarchy perform in this framework?",
    "options": {
      "A": "Divisive normalization computes the response of a neuron by dividing its excitatory input drive by a normalization factor that includes the summed activity of a pool of neighboring neurons, creating competition, contrast enhancement, and invariance to overall input magnitude — it appears in retinal light adaptation, V1 orientation tuning surround suppression, MT motion processing, attentional modulation, multisensory integration, value-based decision making, and olfactory processing, suggesting it is a universal canonical building block",
      "B": "Each level passively relays all incoming signals to the next level without any comparison, subtraction, or prediction",
      "C": "Each cortical level generates top-down predictions of activity at the level below, and only the prediction error (the difference between predicted and actual input) is propagated upward — this means feedforward signals carry surprise (unexplained input), feedback signals carry predictions, and the entire hierarchy works to minimize total prediction error, with different cortical layers (superficial vs deep) carrying error vs prediction signals respectively",
      "D": "All levels of the hierarchy perform identical computations with no specialization for prediction versus error signaling"
    },
    "correct_answer": "C",
    "difficulty": 4,
    "x": 0.586738,
    "y": 0.787166,
    "z": 0.742916,
    "source_article": "predictive coding",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "predictive coding",
      "hierarchical inference",
      "prediction error minimization"
    ]
  },
  {
    "id": "ad18b7aec374c24a",
    "question_text": "Balanced amplification is a dynamical regime in recurrent neural networks that differs from classical attractor dynamics. What distinguishes it, and what computational advantage does it provide?",
    "options": {
      "A": "The theory of efficient coding predicts that early sensory neurons should develop identical receptive fields that all respond to exactly the same visual features with the same orientation preference, spatial frequency tuning, phase sensitivity, and receptive field location — maximizing redundancy across the neural population by ensuring every neuron in primary visual cortex carries exactly the same information about the visual input, with no diversity or complementarity between neural responses whatsoever",
      "B": "Efficient coding theory predicts that neural representations should maximize the total amount of redundancy and statistical correlation in their population responses to natural stimuli — rather than removing correlations, the optimal neural code should ensure that every neuron's response is perfectly predictable from every other neuron's response, creating a maximally correlated and completely redundant representation where no individual neuron contributes any unique information",
      "C": "Balanced amplification reduces all signals equally regardless of their pattern or direction in neural state space",
      "D": "It describes a regime where recurrent networks strongly amplify inputs even though the network has no stable attractor states — excitatory and inhibitory populations are balanced such that the network is globally stable (activity doesn't diverge) yet locally unstable along specific directions, transiently amplifying input patterns along these directions before activity decays, enabling strong but temporary signal amplification without committing to fixed-point attractors"
    },
    "correct_answer": "D",
    "difficulty": 4,
    "x": 0.511724,
    "y": 0.73789,
    "z": 0.185422,
    "source_article": "balanced amplification",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "balanced amplification",
      "non-normal dynamics",
      "transient amplification"
    ]
  },
  {
    "id": "c64fad94fdbba1b2",
    "question_text": "The free energy principle (Friston) provides a unifying theoretical framework for brain function. What does it propose, and how does it relate to variational inference?",
    "options": {
      "A": "The free energy principle proposes that biological systems minimize variational free energy — an upper bound on surprise (negative log-evidence) — by updating either their internal models (perception) or acting on the world to change sensory inputs (action), implementing approximate Bayesian inference through variational methods, where the brain's generative model and recognition density jointly minimize the divergence between predicted and observed sensory states",
      "B": "The free energy principle states that the brain maximizes surprise and prediction error to maintain alertness and arousal",
      "C": "Free energy refers to thermodynamic energy and the principle only applies to metabolism with no cognitive implications",
      "D": "The principle applies only to motor control and has no relevance to perception, learning, or inference"
    },
    "correct_answer": "A",
    "difficulty": 4,
    "x": 0.572106,
    "y": 0.787735,
    "z": 0.606173,
    "source_article": "free energy principle",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "free energy principle",
      "variational inference",
      "active inference"
    ]
  },
  {
    "id": "2a9d9434645ef3a2",
    "question_text": "Spike-frequency adaptation is a common property of cortical neurons where firing rate decreases during sustained input. What computational function does this serve at the network level?",
    "options": {
      "A": "The BCM theory introduces a sliding modification threshold for synaptic plasticity — potentiation occurs when postsynaptic activity exceeds this threshold and depression occurs below it, but crucially the threshold itself slides as a super-linear function of recent average postsynaptic activity, rising when the neuron is highly active and falling when inactive, creating automatic homeostatic stabilization that prevents runaway excitation while preserving input selectivity and competitive learning between synapses",
      "B": "Spike-frequency adaptation implements a high-pass temporal filter that emphasizes changes in input over steady-state levels — at the network level, this enables novelty detection, sensory habituation, switching between multistable percepts, slow oscillatory dynamics, and improved coding efficiency by removing redundant constant signals and allocating dynamic range to informative fluctuations",
      "C": "Adaptation causes neurons to fire increasingly faster over time, amplifying all sustained signals without limit",
      "D": "Adaptation affects only motor neurons and plays no role in sensory or cortical processing"
    },
    "correct_answer": "B",
    "difficulty": 2,
    "x": 0.54598,
    "y": 0.677,
    "z": 0.604342,
    "source_article": "spike-frequency adaptation",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "spike-frequency adaptation",
      "temporal filtering",
      "efficient coding"
    ]
  },
  {
    "id": "bd0fcd8f76ad4900",
    "question_text": "The theory of efficient coding (Barlow, 1961) proposes that neural representations are optimized for the statistics of natural stimuli. What specific prediction does this make about early sensory processing?",
    "options": {
      "A": "Line attractors and point attractors are mathematically and dynamically identical objects with absolutely no functional difference in their stability properties, dimensionality, response to perturbations, or computational capabilities — a line attractor is simply another name for a point attractor and both terms describe exactly the same type of stable fixed point in neural circuit dynamics where the system converges to a single unique equilibrium state regardless of perturbation direction",
      "B": "A line attractor is a single isolated unstable equilibrium point in the phase space of a dynamical system that actively repels all nearby trajectories in every direction simultaneously — any perturbation, no matter how small or in which direction it occurs, causes the system state to diverge exponentially away from this point, making it impossible for the network to sustain any form of persistent activity, graded memory, or continuous integration of inputs",
      "C": "Early sensory neurons should develop receptive fields that are statistically independent (factorial code) when exposed to natural stimuli — and indeed, applying independent component analysis or sparse coding algorithms to natural images produces Gabor-like filters resembling V1 simple cell receptive fields, demonstrating that V1 achieves an efficient representation by removing the redundant correlations present in natural visual input",
      "D": "The theory applies only to artificial neural networks and makes no testable predictions about biological visual systems"
    },
    "correct_answer": "C",
    "difficulty": 3,
    "x": 0.573624,
    "y": 0.743961,
    "z": 0.968027,
    "source_article": "efficient coding",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "efficient coding",
      "sparse coding",
      "V1 receptive fields"
    ]
  },
  {
    "id": "f5d83f6fa336e231",
    "question_text": "Line attractor dynamics have been proposed as the neural mechanism underlying persistent activity in working memory and evidence accumulation. What distinguishes a line attractor from a point attractor?",
    "options": {
      "A": "Line attractors and point attractors are identical dynamical objects with no functional difference",
      "B": "The free energy principle proposes that biological systems minimize variational free energy — an upper bound on surprise computed as the negative log-evidence — by updating either their internal generative models through perceptual inference or by acting on the world to change sensory inputs through active inference, implementing approximate Bayesian inference through variational methods where the brain's hierarchical generative model and recognition density jointly minimize the divergence between predicted and observed states",
      "C": "A line attractor is a single unstable point that repels all trajectories in all directions",
      "D": "A point attractor pulls the system to a single stable state, while a line attractor has a continuous family of marginally stable states along one dimension — perturbations perpendicular to the line are corrected (stability), but perturbations along the line persist indefinitely (marginal stability), allowing the network to maintain any value along this continuum, which is ideal for integrating inputs over time or sustaining graded persistent activity"
    },
    "correct_answer": "D",
    "difficulty": 4,
    "x": 0.517035,
    "y": 0.724864,
    "z": 0.0,
    "source_article": "line attractors",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "line attractors",
      "marginal stability",
      "persistent activity"
    ]
  },
  {
    "id": "d74c4edb91eb84c1",
    "question_text": "Compartmental models subdivide neurons into multiple isopotential segments connected by axial resistance. What critical biological phenomenon do compartmental models capture that single-compartment (point neuron) models miss?",
    "options": {
      "A": "Compartmental models capture the spatiotemporal processing of dendritic inputs — including location-dependent synaptic integration, dendritic spikes in distal compartments, nonlinear interactions between inputs on the same versus different branches, and the attenuation and delay of signals propagating from dendrites to soma, which dramatically affects how individual neurons compute functions of their inputs",
      "B": "Compartmental models are simpler and faster to simulate than point neuron models",
      "C": "Single-compartment models already capture all spatial effects of dendritic processing with perfect accuracy",
      "D": "Compartmental models are used exclusively for modeling axonal conduction velocity and action potential propagation speed along myelinated and unmyelinated axons, and they have absolutely no relevance whatsoever to dendritic computation, synaptic integration, local dendritic nonlinearities, branch-specific processing, back-propagating action potentials in dendrites, or any other aspect of how the dendritic tree transforms distributed synaptic inputs into the somatic output of a neuron"
    },
    "correct_answer": "A",
    "difficulty": 2,
    "x": 0.504115,
    "y": 0.69441,
    "z": 0.765778,
    "source_article": "compartmental modeling",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "compartmental modeling",
      "dendritic computation",
      "spatial integration"
    ]
  },
  {
    "id": "9f63e330507e3c9d",
    "question_text": "Neural oscillations at different frequency bands (theta, gamma, etc.) are hypothesized to play specific computational roles. What is the 'communication through coherence' (CTC) hypothesis proposed by Fries?",
    "options": {
      "A": "CTC states that neurons communicate only through chemical neurotransmitters and oscillations play no role in information routing",
      "B": "CTC proposes that neural communication between brain regions is gated by the phase relationship of their oscillations — when two regions oscillate coherently (phase-locked), their excitability windows align, allowing spikes from the sending region to arrive during the receiving region's excitable phase, creating an effective communication channel, while regions oscillating incoherently fail to communicate because spikes arrive during inhibitory phases",
      "C": "The hypothesis proposes that all brain regions are always perfectly coherent with each other at all frequencies simultaneously",
      "D": "CTC suggests that neural oscillations are epiphenomenal artifacts of volume conduction and have no functional role"
    },
    "correct_answer": "B",
    "difficulty": 3,
    "x": 0.552905,
    "y": 0.699598,
    "z": 0.256466,
    "source_article": "communication through coherence",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "communication through coherence",
      "neural oscillations",
      "phase relationships"
    ]
  },
  {
    "id": "bca4c2ce5a002556",
    "question_text": "Hebbian plasticity alone is unstable because it creates a positive feedback loop (strong synapses get stronger). What mechanisms stabilize Hebbian learning, and what is the BCM theory's specific contribution?",
    "options": {
      "A": "Transcranial magnetic stimulation offers absolutely no methodological advantage whatsoever over traditional permanent lesion studies from neuropsychological patients or correlational functional magnetic resonance imaging activation mapping, providing precisely identical information about brain-behavior causal relationships with exactly the same temporal resolution, spatial specificity, reversibility, and ability to study normal healthy participants — all three methods are completely interchangeable",
      "B": "Transcranial magnetic stimulation can only stimulate and affect deep subcortical brain structures located far beneath the cortical surface, such as the hypothalamus, ventral tegmental area, substantia nigra, basal ganglia nuclei, cerebellar deep nuclei, and brainstem motor nuclei, while being physically and electromagnetically completely unable to induce any electrical current or magnetic effect in any region of the superficial cerebral cortex at any stimulation intensity",
      "C": "The BCM (Bienenstock-Cooper-Munro) theory introduces a sliding modification threshold — synaptic potentiation occurs when postsynaptic activity exceeds this threshold and depression occurs below it, but crucially, the threshold itself slides as a function of recent average activity, rising when the neuron is very active and falling when inactive, automatically preventing runaway excitation while maintaining selectivity for input features",
      "D": "Transcranial magnetic stimulation is used exclusively and only for clinical therapeutic purposes such as treating major depressive disorder, chronic neuropathic pain, obsessive-compulsive disorder, and post-traumatic stress disorder, and it has absolutely no established application whatsoever in basic cognitive neuroscience research for studying normal brain function, testing computational models, or investigating causal brain-behavior relationships in healthy participants"
    },
    "correct_answer": "C",
    "difficulty": 4,
    "x": 0.512612,
    "y": 0.785186,
    "z": 0.1239,
    "source_article": "BCM theory",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "BCM theory",
      "synaptic homeostasis",
      "sliding threshold"
    ]
  },
  {
    "id": "d36122a7c6d2b5b8",
    "question_text": "The dynamical systems approach to motor control proposes that movements are generated by the autonomous dynamics of neural circuits rather than explicit trajectory planning. What is the key evidence from neural recordings that supports this view?",
    "options": {
      "A": "Neural activity during movement is entirely random with no temporal structure or low-dimensional organization",
      "B": "Motor cortex neurons directly and exclusively encode movement velocity throughout the entire duration of a movement",
      "C": "All motor commands originate from the spinal cord with no contribution from cortical dynamics",
      "D": "Neural activity during movement preparation and execution lies on low-dimensional dynamical trajectories that are well-described by autonomous dynamical systems (e.g., rotational dynamics in motor cortex during reaching), and the initial state of these dynamics during preparation determines the subsequent movement — suggesting motor cortex acts as a dynamical engine whose state evolution generates movement commands, rather than explicitly encoding kinematic variables"
    },
    "correct_answer": "D",
    "difficulty": 5,
    "x": 0.545213,
    "y": 0.699356,
    "z": 0.368211,
    "source_article": "dynamical systems motor control",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "dynamical systems motor control",
      "rotational dynamics",
      "neural trajectories"
    ]
  },
  {
    "id": "3289ac0b81426138",
    "question_text": "Criticality in neural networks refers to the hypothesis that the brain operates near a phase transition. What computational advantages does operating at criticality provide?",
    "options": {
      "A": "At criticality, neural networks exhibit maximal dynamic range (sensitivity to inputs spanning many orders of magnitude), optimal information transmission and storage, maximal correlation length (enabling long-range coordination), and power-law distributions of avalanche sizes — this poised state between order (where activity dies out) and chaos (where it explodes) maximizes computational flexibility",
      "B": "Criticality ensures that all neurons in the entire network fire at exactly the same constant rate with absolutely zero trial-to-trial variability, zero fluctuations in population activity, zero correlation between neurons, and zero sensitivity to input magnitude — the critical state produces perfectly uniform and predictable neural responses that show no power-law distributions, no avalanches, and no long-range spatial or temporal correlations of any kind",
      "C": "Operating at criticality provides no computational benefit whatsoever to neural information processing and instead represents a purely pathological state equivalent to epileptic seizure activity — networks at criticality show runaway excitation, uncontrolled synchronous discharges, and complete loss of information processing capacity, which is why the healthy brain actively avoids the critical regime through strong homeostatic inhibitory mechanisms at all times",
      "D": "The concept of criticality and phase transitions applies only to temperature-dependent physical systems in condensed matter physics and statistical mechanics, such as ferromagnetic materials and liquid-gas transitions, and it has never been observed, measured, proposed, modeled, or even theoretically hypothesized to occur in any biological neural system, neuronal culture, brain slice preparation, or intact brain recording at any spatial or temporal scale"
    },
    "correct_answer": "A",
    "difficulty": 5,
    "x": 0.55443,
    "y": 0.718017,
    "z": 0.374205,
    "source_article": "criticality",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "criticality",
      "neural avalanches",
      "phase transitions"
    ]
  },
  {
    "id": "741aff44800bc457",
    "question_text": "Variational autoencoders (VAEs) and related deep generative models have been proposed as models of cortical processing. What aspect of cortical computation do they potentially capture that discriminative models (standard classifiers) do not?",
    "options": {
      "A": "Discriminative classifier models such as support vector machines, logistic regression, and feedforward deep networks already capture every single aspect of cortical processing including top-down prediction, generative imagination, dream synthesis, mental imagery, missing data completion, and probabilistic inference about hidden causes — adding explicit generative modeling capacity provides absolutely no additional explanatory power for any cortical computation",
      "B": "VAEs model how the brain might learn an internal generative model of sensory input — by learning to both generate (decode) and infer (encode) latent representations, they capture the cortex's hypothesized ability to perform top-down prediction and imagination, fill in missing information, and perform inference about hidden causes of sensory data, which a purely feedforward discriminative model cannot do",
      "C": "Deep generative models including variational autoencoders, generative adversarial networks, normalizing flows, and diffusion models apply only and exclusively to natural language processing tasks such as text generation, machine translation, and dialogue systems, and they have absolutely no relevance whatsoever to modeling sensory cortical computation, visual processing, auditory processing, or any non-linguistic perceptual or cognitive function",
      "D": "Variational autoencoders are used exclusively for commercial image generation applications in computer graphics, advertising, entertainment, and artistic production, and they have never been applied to, proposed for, or even considered as computational models of cortical processing by any neuroscientist, computational modeler, or theoretician working on understanding how biological neural circuits represent, process, or generate internal models of sensory information"
    },
    "correct_answer": "B",
    "difficulty": 5,
    "x": 0.576065,
    "y": 0.714752,
    "z": 0.622113,
    "source_article": "generative models",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "generative models",
      "variational autoencoders",
      "cortical computation"
    ]
  },
  {
    "id": "799fb63d84c3deaf",
    "question_text": "The 'binding problem' in computational neuroscience asks how distributed neural representations are integrated into coherent percepts. What is the temporal binding hypothesis, and what is its main challenge?",
    "options": {
      "A": "The binding problem has been definitively solved and is no longer an active research question in neuroscience",
      "B": "All binding is achieved by single dedicated neurons that represent each possible feature combination individually",
      "C": "The temporal binding hypothesis proposes that features belonging to the same object are bound together by synchronous gamma-band (~40 Hz) oscillations — neurons representing different features of a unified percept fire in temporal synchrony, distinguishing them from neurons representing features of different objects that fire asynchronously, but this hypothesis faces challenges from studies showing that gamma synchrony can occur without binding and binding can occur without measurable gamma synchrony",
      "D": "Temporal binding proposes that object features are bound by their spatial proximity in cortex with no role for timing"
    },
    "correct_answer": "C",
    "difficulty": 5,
    "x": 0.560575,
    "y": 0.744742,
    "z": 0.371165,
    "source_article": "binding problem",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "binding problem",
      "temporal synchrony",
      "gamma oscillations"
    ]
  },
  {
    "id": "faeb0c129dcfb486",
    "question_text": "The Boltzmann machine is a stochastic recurrent neural network that learns probability distributions. What is the key difference between restricted Boltzmann machines (RBMs) and general Boltzmann machines in terms of learning tractability?",
    "options": {
      "A": "The temporal binding hypothesis proposes that features belonging to the same perceptual object are bound together through spatial proximity and shared anatomical location within the cortical sheet, with no role whatsoever for temporal synchrony, oscillatory phase relationships, gamma-band activity, or any other timing-based mechanism — binding is achieved entirely through the physical adjacency of neurons in topographic cortical maps, and neurons representing different features of the same object are always anatomically co-located",
      "B": "All perceptual binding throughout the entire visual system is achieved exclusively by single dedicated grandmother cells that each represent one specific combination of features, with individual neurons allocated to every possible conjunction of color, shape, texture, motion, depth, and spatial location — there are no distributed representations, no population codes, no temporal synchrony mechanisms, and no dynamical binding processes of any kind operating anywhere in the visual hierarchy",
      "C": "General Boltzmann machines have no connections at all, while RBMs are fully connected",
      "D": "General Boltzmann machines have connections between all neurons including within-layer connections, making the 'negative phase' of learning (sampling from the model distribution) intractable — RBMs restrict connections to between visible and hidden layers only (no within-layer connections), creating a bipartite graph where the hidden units become conditionally independent given the visible units, enabling efficient block Gibbs sampling and the contrastive divergence learning algorithm"
    },
    "correct_answer": "D",
    "difficulty": 5,
    "x": 0.548908,
    "y": 0.783902,
    "z": 0.443772,
    "source_article": "Boltzmann machines",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "Boltzmann machines",
      "restricted Boltzmann machines",
      "contrastive divergence"
    ]
  },
  {
    "id": "ded77c40c5ab0bbd",
    "question_text": "Optogenetics has transformed computational neuroscience by enabling causal manipulation of specific neuron types. What methodological advantage does this provide over electrical stimulation for testing computational models?",
    "options": {
      "A": "Optogenetics activates or silences genetically defined cell types (e.g., only parvalbumin-positive interneurons) with millisecond precision — unlike electrical stimulation which indiscriminately activates all nearby neurons and passing fibers, optogenetics enables selective manipulation of specific components of a circuit, directly testing computational models that make predictions about the distinct functional roles of different cell types within the same brain region",
      "B": "Optogenetics provides no advantage over electrical stimulation for testing any computational model",
      "C": "Electrical stimulation is more cell-type-specific than optogenetics and can target individual neuron classes",
      "D": "Optogenetics can only be used in invertebrates and has never been applied to mammalian neural circuits"
    },
    "correct_answer": "A",
    "difficulty": 1,
    "x": 0.555072,
    "y": 0.681911,
    "z": 0.779147,
    "source_article": "optogenetics",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "optogenetics",
      "cell-type specificity",
      "causal manipulation"
    ]
  },
  {
    "id": "db7ca79077f32e16",
    "question_text": "The concept of 'neural noise' is not merely a nuisance for the brain — it may serve computational functions. What is one proposed beneficial role of neural variability?",
    "options": {
      "A": "Optogenetics provides absolutely no methodological advantage over traditional electrical microstimulation for testing any computational model of neural circuit function — both methods offer identical cell-type specificity, identical spatial resolution, identical temporal precision, and identical ability to selectively target genetically defined populations of excitatory or inhibitory neurons, making optogenetics a completely redundant and unnecessary addition to the neuroscientist's methodological toolkit",
      "B": "Neural variability can implement probabilistic sampling from probability distributions — if neural activity fluctuations correspond to samples from the brain's internal posterior distribution over possible states of the world, then trial-to-trial variability becomes a feature rather than a bug, enabling approximate Bayesian inference through Monte Carlo-like sampling of the space of possible interpretations of ambiguous sensory input",
      "C": "Noise is beneficial only in artificial neural networks and never in biological neural systems",
      "D": "Neural noise makes all neural responses perfectly identical across trials with no variability whatsoever"
    },
    "correct_answer": "B",
    "difficulty": 4,
    "x": 0.59,
    "y": 0.709061,
    "z": 0.598428,
    "source_article": "neural variability",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "neural variability",
      "sampling hypothesis",
      "probabilistic computation"
    ]
  },
  {
    "id": "0312086de78170f9",
    "question_text": "Calcium imaging (e.g., two-photon, miniscopes) allows recording from thousands of neurons simultaneously but with different properties than electrophysiology. What computational consideration is critical when analyzing calcium imaging data?",
    "options": {
      "A": "Calcium imaging directly measures action potentials with the same temporal resolution as electrophysiology",
      "B": "Neural variability across repeated presentations of identical stimuli serves absolutely no computational function whatsoever and exclusively degrades the accuracy, reliability, and information content of every neural response in every brain region under every experimental condition — trial-to-trial fluctuations are purely the result of thermodynamic molecular noise that the brain has failed to eliminate, and they have no relationship to probabilistic inference, sampling, exploration, or any other beneficial computational process",
      "C": "Calcium signals are a slow, nonlinear convolution of underlying spike trains — the calcium indicator acts as a temporal low-pass filter with rise times of ~50-200ms and decay times of 0.5-2 seconds, meaning rapid spike timing information is lost, overlapping spikes are temporally blurred, and spike inference algorithms with deconvolution are required to estimate the underlying neural activity, introducing algorithm-dependent biases that must be carefully characterized",
      "D": "Calcium imaging has better temporal resolution than electrophysiology and captures faster dynamics"
    },
    "correct_answer": "C",
    "difficulty": 2,
    "x": 0.543015,
    "y": 0.682392,
    "z": 0.883192,
    "source_article": "calcium imaging",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "calcium imaging",
      "spike inference",
      "temporal resolution"
    ]
  },
  {
    "id": "5facc633ab37c327",
    "question_text": "The 'ring model' of head direction and orientation tuning uses a continuous attractor on a circular topology. What fundamental property makes ring attractors suitable for representing circular variables?",
    "options": {
      "A": "Ring attractors have a single stable state and cannot represent continuous variables",
      "B": "Ring attractors are used only for modeling linear (non-circular) variables like temperature",
      "C": "Ring attractors require external input at every moment and cannot sustain activity in the absence of sensory input",
      "D": "The ring attractor has a continuous manifold of stable states arranged in a circle — the network can sustain a localized bump of activity at any angular position, and this bump persists and can be smoothly updated by velocity inputs, naturally wrapping around the circular topology so that 0° and 360° are identical states, matching the periodic nature of head direction and orientation"
    },
    "correct_answer": "D",
    "difficulty": 1,
    "x": 0.516962,
    "y": 0.693822,
    "z": 0.086281,
    "source_article": "ring attractors",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "ring attractors",
      "continuous attractors",
      "head direction system"
    ]
  },
  {
    "id": "835f50516dcf1640",
    "question_text": "Short-term synaptic plasticity (facilitation and depression) operates on timescales of milliseconds to seconds, unlike long-term plasticity (LTP/LTD). What computational function does short-term synaptic depression serve in neural circuits?",
    "options": {
      "A": "Short-term synaptic depression implements temporal filtering that emphasizes transient changes in presynaptic firing rate over sustained rates — depressing synapses respond strongly to the onset of activity but attenuate during sustained firing, effectively computing a temporal derivative of the presynaptic rate, which enables change detection, adaptation to input statistics, and gain control that normalizes neural responses to ambient activity levels",
      "B": "Short-term depression serves no computational function and is simply a failure of synaptic transmission",
      "C": "Short-term depression permanently eliminates all synaptic connections after a single use",
      "D": "It only affects inhibitory synapses and never occurs at excitatory connections"
    },
    "correct_answer": "A",
    "difficulty": 1,
    "x": 0.530259,
    "y": 0.719959,
    "z": 0.367307,
    "source_article": "short-term synaptic plasticity",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "short-term synaptic plasticity",
      "temporal filtering",
      "gain control"
    ]
  },
  {
    "id": "6765a3008447f519",
    "question_text": "The concept of 'degeneracy' in neural systems (Edelman & Gally, 2001) describes how structurally different neural configurations can produce functionally equivalent outputs. How does this differ from redundancy, and why is it computationally significant?",
    "options": {
      "A": "Degeneracy and redundancy are identical concepts with no theoretical distinction between them",
      "B": "Redundancy uses duplicate copies of the same component (identical elements producing the same output), while degeneracy uses structurally different elements that produce the same output under one condition but can respond differently under other conditions — this makes degenerate systems more robust (multiple independent failure modes) and more flexible (the different components can be recombined for novel functions), contributing to both resilience and evolvability",
      "C": "Degeneracy means the neural system has no backup mechanisms and any damage causes complete functional loss",
      "D": "Degeneracy occurs only in artificial neural networks and has never been observed in biological neural systems"
    },
    "correct_answer": "B",
    "difficulty": 5,
    "x": 0.550225,
    "y": 0.730799,
    "z": 0.685378,
    "source_article": "degeneracy",
    "domain_ids": [
      "computational-neuroscience",
      "neuroscience"
    ],
    "concepts_tested": [
      "degeneracy",
      "redundancy",
      "neural robustness"
    ]
  }
]