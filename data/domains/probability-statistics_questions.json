[
  {
    "id": "80eee6d580e3899c",
    "question_text": "The law of large numbers states that the sample mean converges to the expected value as the sample size increases. What is the key distinction between the weak and strong forms?",
    "options": {
      "A": "The weak law (Khintchine) says the sample mean converges in probability: P(|X̄ₙ - μ| > ε) → 0 for all ε > 0. The strong law (Kolmogorov) says it converges almost surely: P(X̄ₙ → μ) = 1. The strong law is strictly stronger — it guarantees that the sequence of sample means actually converges along almost every sample path, not just that deviations become unlikely",
      "B": "The weak law says convergence holds only for normally distributed random variables, while the strong law extends this guarantee to any distribution with finite mean, including heavy-tailed distributions like the Cauchy",
      "C": "The weak law guarantees convergence of the sample median rather than the sample mean, while the strong law extends the guarantee to all order statistics including quantiles and the mode",
      "D": "The weak law requires the random variables to be identically distributed but allows dependence between observations, while the strong law removes the identical distribution requirement but demands strict independence. The strong law also requires the existence of all moments up to fourth order, whereas the weak law only needs the variance to exist, making the strong law more restrictive in its moment conditions but less restrictive in its distributional assumptions"
    },
    "correct_answer": "A",
    "difficulty": 1,
    "x": 0.924362,
    "y": 0.698227,
    "z": 0.549425,
    "source_article": "law of large numbers",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "law of large numbers",
      "convergence in probability",
      "almost sure convergence"
    ]
  },
  {
    "id": "36f3d399aca16d31",
    "question_text": "Bayes' theorem provides a way to update probabilities when new evidence is observed. What is the conceptual framework?",
    "options": {
      "A": "Bayes' theorem states that the posterior probability equals the prior probability divided by the marginal likelihood, without any contribution from the likelihood function. This means that prior beliefs alone determine the posterior, and observed data plays no role in updating the probability of a hypothesis. The formula is simply P(H|E) = P(H)/P(E), which shows that evidence affects only the normalization constant",
      "B": "P(H|E) = P(E|H)·P(H)/P(E). The posterior probability of hypothesis H given evidence E equals the likelihood of E given H, times the prior probability of H, divided by the marginal probability of E. This formalizes how rational belief updating should work: the prior P(H) represents beliefs before seeing data, and the posterior P(H|E) represents updated beliefs after seeing data",
      "C": "Bayes' theorem requires that the prior distribution be a conjugate prior from the exponential family, because only conjugate priors produce analytically tractable posterior distributions. Without conjugacy, the posterior cannot be computed, which is why non-conjugate priors are mathematically invalid in Bayesian inference and all practical applications must use conjugate priors such as the beta-binomial or normal-normal pairs",
      "D": "Bayes' theorem is only valid for continuous random variables with smooth density functions and cannot be applied to discrete events or categorical outcomes"
    },
    "correct_answer": "B",
    "difficulty": 1,
    "x": 0.81,
    "y": 0.620102,
    "z": 0.863977,
    "source_article": "Bayes' theorem",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Bayes' theorem",
      "prior and posterior",
      "likelihood",
      "belief updating"
    ]
  },
  {
    "id": "9db4732d314280f9",
    "question_text": "The Central Limit Theorem (CLT) states that the sum of many independent random variables is approximately normally distributed, regardless of the underlying distribution. What conditions are required?",
    "options": {
      "A": "The CLT requires the underlying population distribution to itself be approximately normal, because the proof relies on the symmetry properties of the Gaussian density function when summing independent copies of a symmetric distribution",
      "B": "The CLT applies only when the random variables are drawn from distributions with finite support, such as the uniform or triangular distribution, because unbounded distributions can produce sums that grow without limit",
      "C": "The classic Lindeberg-Lévy CLT requires: (1) independent and identically distributed random variables, (2) finite mean μ and finite variance σ². Then (X̄ₙ - μ)/(σ/√n) →d N(0,1). The Lindeberg-Feller CLT generalizes to non-identically distributed variables, requiring that no single variable dominates the sum. The CLT explains why so many natural phenomena are approximately normal: height, measurement errors, test scores — they are sums of many small independent effects",
      "D": "The CLT states that the sum of any number of random variables, even as few as two or three, is exactly normally distributed regardless of sample size, which is why normal approximations are always valid in practice"
    },
    "correct_answer": "C",
    "difficulty": 1,
    "x": 0.962312,
    "y": 0.658407,
    "z": 0.586951,
    "source_article": "Central Limit Theorem",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Central Limit Theorem",
      "convergence in distribution",
      "normal approximation",
      "Lindeberg condition"
    ]
  },
  {
    "id": "7fce1201a3477eab",
    "question_text": "A p-value is the probability of observing data as extreme as (or more extreme than) the observed data, assuming the null hypothesis is true. What is the most common misinterpretation of p-values?",
    "options": {
      "A": "The most common misinterpretation is that p-values represent the probability that the observed effect is due to random chance alone. In reality, the p-value measures the proportion of the sample that deviates from the null hypothesis prediction, which means that a smaller p-value indicates a larger fraction of observations are inconsistent with randomness. This fraction-based interpretation leads researchers to treat the p-value as a direct measure of how reproducible a finding will be across future experiments, when it is actually the percentage of discordant data points",
      "B": "The most common misinterpretation is confusing statistical significance with practical significance — a small p-value means the effect is large enough to matter in real-world applications. Researchers frequently assume that if p < 0.05, the treatment or intervention has a meaningful practical effect size, when in fact with large enough sample sizes even trivially small effects that have no real-world importance produce extremely small p-values. The p-value only measures whether an effect is nonzero, not whether the magnitude of that effect has any practical consequence for decision-making",
      "C": "The most common misinterpretation is that the p-value represents the probability that the experimental results were produced by a flaw in the study design such as selection bias, measurement error, or confounding variables rather than by a genuine effect. Under this incorrect reading, a low p-value supposedly rules out all methodological threats to validity. In truth, the p-value assumes the statistical model is correct and cannot detect violations of its own assumptions, so it provides no information about whether the data were collected or analyzed properly",
      "D": "The most common misinterpretation is believing that the p-value is P(H₀ is true | data) — the probability the null hypothesis is true given the data. It is actually P(data this extreme | H₀ is true) — the probability of the data given the null. These are not the same (inverting the conditional). A small p-value means the data are unlikely under H₀, not that H₀ is unlikely to be true. Determining P(H₀|data) requires Bayes' theorem and a prior probability for H₀"
    },
    "correct_answer": "D",
    "difficulty": 1,
    "x": 0.820179,
    "y": 0.602522,
    "z": 0.526549,
    "source_article": "p-value interpretation",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "p-value interpretation",
      "conditional probability inversion",
      "null hypothesis testing"
    ]
  },
  {
    "id": "9deba9af08c9fcbc",
    "question_text": "Maximum likelihood estimation (MLE) finds parameter values that maximize the probability of the observed data. What are MLE's key asymptotic properties, and when does it fail?",
    "options": {
      "A": "Under regularity conditions (smooth likelihood, parameter in the interior of the parameter space, identifiability), the MLE is: (1) consistent (converges to true parameter), (2) asymptotically normal (√n(θ̂-θ₀) →d N(0, I(θ₀)⁻¹)), and (3) asymptotically efficient (achieves the Cramér-Rao lower bound). It fails when: the likelihood is multimodal (convergence to local maxima), the parameter is on the boundary (regularity violated), or the model is misspecified",
      "B": "MLE is consistent and asymptotically efficient, but its primary advantage is that it always produces unbiased estimates in finite samples for any parametric model, regardless of the regularity conditions or sample size",
      "C": "Under regularity conditions, the MLE is asymptotically normally distributed but converges at the rate n^{-1/3} rather than the standard n^{-1/2} rate, which means it requires substantially larger sample sizes than method-of-moments estimators to achieve comparable precision for most parametric families",
      "D": "The MLE minimizes the Kullback-Leibler divergence from the empirical distribution to the parametric model, but this property holds only when the true data-generating process belongs to the specified parametric family. When the model is misspecified, the MLE converges to the parameter value that minimizes the KL divergence, but the resulting estimator is neither consistent nor asymptotically normal. Furthermore, the Fisher information matrix becomes singular under misspecification, invalidating the standard confidence intervals and hypothesis tests derived from MLE theory"
    },
    "correct_answer": "A",
    "difficulty": 2,
    "x": 0.892712,
    "y": 0.738294,
    "z": 0.27113,
    "source_article": "maximum likelihood estimation",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "maximum likelihood estimation",
      "asymptotic normality",
      "Cramér-Rao bound",
      "Fisher information"
    ]
  },
  {
    "id": "481ec7b815736507",
    "question_text": "Confidence intervals are often misunderstood. What does a 95% confidence interval actually mean in the frequentist framework?",
    "options": {
      "A": "A 95% confidence interval means there is a 95% probability that the true population parameter falls within this specific computed interval. Once the interval is calculated from the data, the parameter is either inside it or not, but we can assign a 95% posterior probability to it being inside because the interval was constructed using a procedure that accounts for sampling variability. This probabilistic interpretation is valid because the endpoints of the interval are random variables whose distribution encompasses the true parameter value 95% of the time",
      "B": "If we repeated the experiment many times and computed a 95% CI each time, 95% of those intervals would contain the true parameter value. Any particular interval either contains the true parameter or does not — the probability statement is about the procedure, not about any single interval. This frequentist interpretation contrasts with Bayesian credible intervals, where P(θ ∈ CI | data) = 0.95 is a valid statement because θ is treated as a random variable with a posterior distribution",
      "C": "A 95% confidence interval captures 95% of the individual data points in the sample, serving as a range within which most future observations from the same population would fall. This makes the CI essentially identical to a prediction interval, where a wider interval indicates greater dispersion of individual values around the central estimate. The 95% level specifically means that only 5% of observations are expected to lie outside the computed bounds, providing a direct summary of the spread and variability present in the observed data",
      "D": "Confidence intervals and Bayesian credible intervals always produce identical numerical results and differ only in philosophical interpretation"
    },
    "correct_answer": "B",
    "difficulty": 2,
    "x": 0.823595,
    "y": 0.63431,
    "z": 0.838174,
    "source_article": "confidence intervals",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "confidence intervals",
      "frequentist interpretation",
      "coverage probability",
      "Bayesian credible intervals"
    ]
  },
  {
    "id": "dbca1939562f6e0f",
    "question_text": "The bias-variance tradeoff is fundamental to statistical learning. What does it mean, and how does it affect model selection?",
    "options": {
      "A": "Bias and variance can both be minimized simultaneously by using ensemble methods such as bagging or boosting, which eliminate the tradeoff entirely by averaging over multiple models that each independently reduce one component of the error",
      "B": "The bias-variance tradeoff applies only to linear regression and its direct extensions such as ridge and lasso regression, not to nonparametric methods like decision trees or neural networks which can reduce bias and variance independently through architectural choices",
      "C": "For any estimator, the expected prediction error decomposes as: E[(Y - f̂(x))²] = Bias²(f̂(x)) + Var(f̂(x)) + irreducible noise (σ²). Simple models have high bias (underfitting — systematic error) but low variance (stable predictions). Complex models have low bias but high variance (overfitting — sensitive to training data). The optimal model minimizes total error by balancing these two sources. Regularization (L1/L2 penalties) explicitly introduces bias to reduce variance",
      "D": "Only variance matters for prediction accuracy because bias can always be eliminated through cross-validation, which identifies and removes systematic errors from any model without affecting its variance"
    },
    "correct_answer": "C",
    "difficulty": 2,
    "x": 0.818663,
    "y": 0.63795,
    "z": 0.0,
    "source_article": "bias-variance tradeoff",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "bias-variance tradeoff",
      "overfitting",
      "underfitting",
      "regularization",
      "model complexity"
    ]
  },
  {
    "id": "f5f25f5a5bb6adc8",
    "question_text": "Markov chains are stochastic processes where the future state depends only on the present, not the past (the Markov property). Under what conditions does a Markov chain converge to a stationary distribution?",
    "options": {
      "A": "All finite-state Markov chains converge to a unique stationary distribution regardless of their structure, because the finite state space guarantees that the chain will eventually visit every state. Periodicity and reducibility are irrelevant to convergence — these properties only affect the speed of convergence, not whether convergence occurs. Even chains with absorbing states or disconnected components will converge, since the transition matrix raised to a sufficiently large power always approaches a matrix with identical rows representing the stationary distribution",
      "B": "A Markov chain converges to a stationary distribution if and only if all transition probabilities are equal, creating a doubly stochastic matrix where each row and column sums to one. This uniformity condition ensures that no state is preferred over any other in the long run, so the chain settles into a uniform distribution over all states. Non-uniform transition matrices cannot have stationary distributions because the asymmetry in transition rates creates persistent oscillations that prevent the chain from reaching equilibrium",
      "C": "Markov chains require continuous state spaces to possess stationary distributions because discrete state spaces create inherent periodicity that prevents convergence. In continuous state spaces, the chain can move infinitesimally between states, smoothing out the periodic behavior that discrete jumps create. This is analogous to how differential equations have smooth solutions while difference equations can oscillate — the continuous structure provides a damping mechanism that drives the distribution toward stationarity",
      "D": "A finite-state Markov chain converges to a unique stationary distribution π if it is: (1) irreducible (every state is reachable from every other state) and (2) aperiodic (the chain does not cycle deterministically with a fixed period). The stationary distribution satisfies π = πP (left eigenvector of the transition matrix with eigenvalue 1). The convergence rate depends on the spectral gap (1 - |λ₂|), where λ₂ is the second-largest eigenvalue magnitude"
    },
    "correct_answer": "D",
    "difficulty": 2,
    "x": 0.931604,
    "y": 0.683799,
    "z": 1.0,
    "source_article": "Markov chains",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Markov chains",
      "stationary distribution",
      "irreducibility",
      "aperiodicity",
      "convergence"
    ]
  },
  {
    "id": "83675bbb724387be",
    "question_text": "The bootstrap is a resampling method for estimating the sampling distribution of a statistic. What is its key insight, and when does it fail?",
    "options": {
      "A": "The bootstrap's key insight is treating the empirical distribution F̂ₙ as a proxy for the true distribution F: just as the statistic T(F̂ₙ) estimates the parameter T(F), the distribution of T(F̂*ₙ) (where F̂*ₙ is drawn with replacement from the sample) approximates the sampling distribution of T(F̂ₙ). This works because F̂ₙ → F by Glivenko-Cantelli, so bootstrapped distributions inherit this consistency. It fails for: non-smooth statistics (sample maximum), heavy-tailed distributions where CLT convergence is slow, and small samples where F̂ₙ poorly represents F",
      "B": "The bootstrap requires specifying the true population distribution before resampling can begin, because the resampling scheme must match the parametric family from which the data were generated in order to produce valid distributional approximations",
      "C": "The bootstrap works only for the sample mean and cannot be applied to other statistics such as the median, correlation coefficient, or regression coefficients because the convergence theory underpinning the method is specific to linear functions of the data",
      "D": "The bootstrap always gives exact confidence intervals and standard errors regardless of sample size, because drawing thousands of resamples effectively creates an infinite dataset that eliminates all sampling variability. The law of large numbers applied to the resampled datasets guarantees that the bootstrap distribution converges to the exact sampling distribution even with as few as five or ten original observations, making it superior to every asymptotic approximation including those based on the central limit theorem, which require much larger sample sizes to achieve the same level of accuracy"
    },
    "correct_answer": "A",
    "difficulty": 3,
    "x": 0.881549,
    "y": 0.735377,
    "z": 0.67636,
    "source_article": "bootstrap",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "bootstrap",
      "resampling",
      "empirical distribution",
      "sampling distribution estimation"
    ]
  },
  {
    "id": "e83db117c69f707c",
    "question_text": "The Kolmogorov-Smirnov test compares a sample distribution to a reference distribution (or two samples). What makes it distribution-free, and what is its main limitation?",
    "options": {
      "A": "The KS test is not distribution-free and requires the reference distribution to be normal, because the critical values of the KS statistic were originally derived by Kolmogorov under the assumption of Gaussian data. When testing against a non-normal reference distribution, the critical values are invalid and the test produces systematically inflated Type I error rates. This fundamental limitation is why the KS test is considered essentially redundant with the Shapiro-Wilk normality test — both assess normality, but the Shapiro-Wilk test is strictly more powerful because it exploits the specific structure of the normal distribution rather than comparing generic cumulative distribution functions through a supremum distance metric",
      "B": "The KS test statistic Dₙ = sup|F̂ₙ(x) - F₀(x)| measures the maximum vertical distance between the empirical CDF and the reference CDF. It is distribution-free because, under H₀ (sample from F₀), the distribution of Dₙ depends only on n, not on F₀ — a consequence of the probability integral transform (if X ~ F, then F(X) ~ Uniform(0,1)). Its main limitation is low power against differences in the tails, since the supremum is most sensitive near the median where the CDF is steepest, and it has less power than parametric tests when distributional assumptions are met",
      "C": "The KS test compares only the means and variances of two distributions rather than their entire shapes, making it equivalent to a two-sample t-test with an added variance comparison. The test statistic is computed as the absolute difference in sample means normalized by the pooled standard deviation, and it rejects the null hypothesis when this standardized difference exceeds a critical value from the t-distribution. This is why the KS test has low power against alternatives that differ only in skewness, kurtosis, or tail behavior — it cannot detect distributional differences that leave the first two moments unchanged",
      "D": "The KS test works only for discrete distributions with finitely many possible values, since the supremum over a continuous domain is not computable"
    },
    "correct_answer": "B",
    "difficulty": 3,
    "x": 0.929774,
    "y": 0.693197,
    "z": 0.673998,
    "source_article": "Kolmogorov-Smirnov test",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Kolmogorov-Smirnov test",
      "distribution-free tests",
      "empirical CDF",
      "probability integral transform"
    ]
  },
  {
    "id": "214d068ae3fe192c",
    "question_text": "Simpson's paradox occurs when a trend present in several groups reverses when the groups are combined. What causes this, and what does it reveal about statistical reasoning?",
    "options": {
      "A": "Simpson's paradox is a data entry or aggregation error that produces contradictory results when subgroup data are incorrectly combined. It can always be avoided by careful data cleaning and correct computation of weighted averages across groups",
      "B": "Simpson's paradox occurs only when there are exactly two subgroups in the data and the subgroups are of very different sizes, creating a weighted averaging artifact that reverses the within-group trends in the combined table",
      "C": "Simpson's paradox is caused by a confounding variable that is unequally distributed across groups, creating a spurious reversal in the aggregate. The classic example: a treatment can have higher success rates in BOTH men and women separately, yet lower success rate overall, if men (who have lower baseline success) disproportionately receive the treatment. It reveals that aggregated statistics can be deeply misleading without accounting for confounders, and that causal reasoning (not just statistical association) is needed to determine the correct level of analysis",
      "D": "Simpson's paradox proves that aggregating data across groups is always statistically invalid, which is why all analyses should be conducted separately within each subgroup and combined results should never be reported or interpreted in scientific research"
    },
    "correct_answer": "C",
    "difficulty": 3,
    "x": 0.852677,
    "y": 0.629602,
    "z": 0.533231,
    "source_article": "Simpson's paradox",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Simpson's paradox",
      "confounding variables",
      "aggregation bias",
      "causal reasoning"
    ]
  },
  {
    "id": "1c58a1db0830d585",
    "question_text": "Bayesian statistics and frequentist statistics provide fundamentally different frameworks for inference. What is the core philosophical difference, and does it matter in practice?",
    "options": {
      "A": "There is no meaningful difference between Bayesian and frequentist statistics — both frameworks always produce identical point estimates, confidence/credible intervals, and hypothesis test conclusions when applied to the same data. The apparent differences are purely philosophical and have no practical consequences for data analysis. Any Bayesian analysis can be exactly replicated by a frequentist procedure through appropriate choice of test statistic and rejection region, and conversely any frequentist procedure corresponds to a Bayesian analysis with some implicit prior distribution. The distinction persists in textbooks only for historical reasons, as the mathematical equivalence between the two approaches was definitively proven by Lindley's theorem in the 1950s",
      "B": "Bayesian statistics was the dominant framework throughout the 19th century but has been conclusively shown to be logically incoherent by frequentist critiques, particularly the problem of prior sensitivity. Because different prior distributions lead to different posterior conclusions, Bayesian inference introduces unavoidable subjectivity that makes scientific results non-reproducible. The frequentist framework resolves this by basing all inference on the sampling distribution of test statistics, which depends only on the data and the assumed model. Modern computational advances such as MCMC have made Bayesian calculations feasible, but this computational convenience does not address the fundamental logical flaw of requiring prior specification that influences the final conclusions in ways that cannot be objectively validated",
      "C": "Frequentist statistics works optimally with small samples because its exact tests, such as Fisher's exact test and the exact binomial test, make no asymptotic approximations. Bayesian statistics works only with large samples because the posterior distribution converges to the true parameter value only as the sample size approaches infinity, via the Bernstein-von Mises theorem. With small samples, the Bayesian posterior is dominated by the prior and provides unreliable inferences. This complementarity means that researchers should use frequentist methods when data are scarce and switch to Bayesian methods only when hundreds or thousands of observations are available, which is the opposite of common practice in fields like clinical trials and psychometrics",
      "D": "The core difference is the interpretation of probability: frequentists define probability as long-run frequency (parameters are fixed but unknown, data are random), while Bayesians define probability as degree of belief (parameters have probability distributions reflecting uncertainty). In practice, this matters: Bayesian methods naturally incorporate prior information, provide posterior distributions (not just point estimates), and answer the question 'what do I believe about the parameter given the data?' while frequentist methods answer 'how would this procedure perform if repeated many times?' With informative priors and small samples, results can differ substantially; with vague priors and large samples, they typically agree"
    },
    "correct_answer": "D",
    "difficulty": 3,
    "x": 0.811349,
    "y": 0.649525,
    "z": 0.86399,
    "source_article": "Bayesian vs frequentist",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Bayesian vs frequentist",
      "philosophical foundations of statistics",
      "prior information",
      "posterior distribution"
    ]
  },
  {
    "id": "02c74de0ce78da2d",
    "question_text": "The Cramér-Rao lower bound establishes a fundamental limit on the precision of any unbiased estimator. What does it state, and what is its connection to the Fisher information?",
    "options": {
      "A": "For any unbiased estimator θ̂ of parameter θ, the variance satisfies Var(θ̂) ≥ 1/I(θ), where I(θ) = E[(∂log f(X;θ)/∂θ)²] is the Fisher information. This means no unbiased estimator can have variance below 1/I(θ). The Fisher information quantifies how much information about θ a single observation carries: high curvature of the log-likelihood (sharp peak) means high information, tight bound, and precise estimation. An estimator achieving this bound is called efficient; the MLE is asymptotically efficient",
      "B": "The Cramér-Rao bound states that all unbiased estimators of the same parameter necessarily have the same variance, because the constraint of unbiasedness uniquely determines the estimator's sampling distribution through the Rao-Blackwell theorem",
      "C": "Fisher information measures the curvature of the prior distribution in a Bayesian framework and is relevant only to Bayesian estimation, not to frequentist point estimation or hypothesis testing",
      "D": "The Cramér-Rao bound applies only to linear estimators (those expressible as weighted sums of the observations) and provides no information about the optimality of nonlinear estimators. Nonlinear estimators such as the maximum likelihood estimator can achieve arbitrarily lower variance than the Cramér-Rao bound by exploiting higher-order information in the likelihood function that the Fisher information matrix, being based only on second derivatives, cannot capture. This is why the MLE frequently outperforms the minimum-variance unbiased estimator in finite samples despite both achieving the bound asymptotically"
    },
    "correct_answer": "A",
    "difficulty": 4,
    "x": 0.877989,
    "y": 0.705328,
    "z": 0.067563,
    "source_article": "Cramér-Rao bound",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Cramér-Rao bound",
      "Fisher information",
      "efficiency",
      "minimum variance"
    ]
  },
  {
    "id": "0c75db233c060b84",
    "question_text": "Principal Component Analysis (PCA) finds the directions of maximum variance in multivariate data. What is the mathematical formulation, and what does PCA actually optimize?",
    "options": {
      "A": "PCA finds the directions in the data that minimize the total variance of the projected data, concentrating the remaining variance into as few dimensions as possible. The first principal component is the direction along which the data are most tightly clustered, and subsequent components capture progressively larger amounts of spread. This minimization is solved by finding the eigenvectors of the inverse covariance matrix Σ⁻¹, with the smallest eigenvalues corresponding to the most important components since they represent the directions of least dispersion. This is why PCA is particularly effective at identifying low-dimensional structures hidden in noisy high-dimensional data",
      "B": "PCA maximizes the variance of the projected data: the first principal component w₁ = argmax_{‖w‖=1} wᵀΣw, where Σ is the covariance matrix. This is solved by the eigenvector of Σ with the largest eigenvalue. Subsequent PCs are eigenvectors with decreasing eigenvalues, constrained to be orthogonal to previous PCs. Equivalently, PCA finds the rank-k approximation minimizing the Frobenius norm of the reconstruction error — connecting to the SVD (the PCs are the right singular vectors of the centered data matrix)",
      "C": "PCA requires the data to follow a multivariate normal distribution because the covariance matrix fully characterizes a Gaussian distribution but is insufficient for non-Gaussian data. For non-normal data, higher-order statistics such as skewness and kurtosis contain essential structural information that PCA cannot capture, which is why independent component analysis (ICA) was developed as a generalization that uses fourth-order cumulants. Applying PCA to non-normal data produces components that are mathematically valid eigenvectors but statistically meaningless because they fail to separate the true underlying sources of variation in the data",
      "D": "PCA is a clustering algorithm that partitions observations into groups based on their proximity in the space defined by the principal component scores"
    },
    "correct_answer": "B",
    "difficulty": 4,
    "x": 0.908116,
    "y": 0.652128,
    "z": 0.146786,
    "source_article": "PCA",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "PCA",
      "covariance matrix eigenvectors",
      "variance maximization",
      "SVD connection"
    ]
  },
  {
    "id": "8633a5526da2db41",
    "question_text": "The false discovery rate (FDR) addresses the multiple testing problem differently from the family-wise error rate (FWER). What is the key distinction, and why was FDR control a breakthrough?",
    "options": {
      "A": "FDR and FWER are identical concepts that use different notation but control exactly the same error rate. Both the Bonferroni correction and the Benjamini-Hochberg procedure divide the significance level α by the number of tests m, producing the same adjusted threshold for declaring significance. The only difference is computational: the Bonferroni method uses a simple division α/m while the BH procedure sorts p-values and compares each to a linearly increasing threshold, but these two approaches always agree on which null hypotheses to reject. Claims that FDR is less conservative than FWER are based on simulations that used incorrect implementations of the Bonferroni correction, and properly implemented Bonferroni controls are equally powerful as BH for any configuration of true and false null hypotheses",
      "B": "Multiple testing corrections are unnecessary in modern statistics because the original significance threshold of α = 0.05 already accounts for the possibility of conducting multiple tests. The 5% error rate is a per-experiment rate, not a per-test rate, meaning that as long as all tests are conducted within a single study, the overall false positive rate remains at 5% regardless of how many tests are performed. This is a consequence of the closure principle in hypothesis testing, which guarantees that the familywise error rate of a collection of tests never exceeds the nominal level of each individual test when the tests are performed on the same dataset. Corrections like Bonferroni and BH are therefore overly conservative and substantially reduce statistical power without providing any additional error control",
      "C": "FWER (Bonferroni) controls P(≥1 false positive among all tests), becoming extremely conservative as the number of tests grows (dividing α by m). FDR (Benjamini-Hochberg, 1995) instead controls E[false positives / total positives] — the expected proportion of false discoveries among rejected hypotheses. This is less conservative because it allows some false positives as long as they are a controlled fraction of discoveries. For genomics (testing 20,000+ genes), Bonferroni gives virtually no discoveries, while BH at FDR = 0.05 identifies hundreds of plausible candidates. FDR control was a breakthrough because it matched the scientific goal: controlling the quality of a list of discoveries, not eliminating all errors",
      "D": "FWER control is always preferable to FDR control in every application because controlling the probability of at least one false positive is the only scientifically meaningful error criterion. FDR control allows a specified proportion of rejections to be false positives, which means that published results using FDR corrections contain a known fraction of incorrect findings that have been deliberately accepted as false. This undermines scientific credibility because readers cannot determine which specific results are true and which are the accepted false positives. The Benjamini-Hochberg procedure gained popularity only because it produces more significant results, not because it provides better error control, making it a form of statistical p-hacking disguised as a principled correction"
    },
    "correct_answer": "C",
    "difficulty": 4,
    "x": 0.851089,
    "y": 0.66773,
    "z": 0.603245,
    "source_article": "false discovery rate",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "false discovery rate",
      "Benjamini-Hochberg",
      "multiple testing",
      "Bonferroni correction",
      "FWER"
    ]
  },
  {
    "id": "0186a7ddb034eb5e",
    "question_text": "Bayesian hierarchical models use partial pooling. What is its advantage over complete pooling or no pooling?",
    "options": {
      "A": "Complete pooling (ignoring group structure and fitting a single parameter for all groups) is always optimal because it uses all available data to estimate a single value, maximizing statistical power and minimizing standard errors compared to any approach that splits data into subgroups",
      "B": "No pooling (estimating each group independently with no information sharing) is always optimal because it respects the unique characteristics of each group and avoids the bias introduced by borrowing information from dissimilar groups",
      "C": "Hierarchical models are computationally identical to fitting separate independent models for each group and then averaging the resulting point estimates, requiring no iterative MCMC sampling or variational inference algorithms",
      "D": "Partial pooling estimates each group's parameter as a weighted average of its own data and the overall mean, shrinking small-sample groups toward the grand mean (borrowing strength). This adaptive compromise reduces MSE by trading small bias for large variance reduction — the James-Stein phenomenon showing sample means are inadmissible for 3+ groups"
    },
    "correct_answer": "D",
    "difficulty": 4,
    "x": 0.82131,
    "y": 0.697321,
    "z": 0.494507,
    "source_article": "hierarchical models",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "hierarchical models",
      "partial pooling",
      "shrinkage",
      "James-Stein"
    ]
  },
  {
    "id": "1c1c7fadc10f7b22",
    "question_text": "Concentration inequalities provide non-asymptotic bounds on how much a random variable deviates from its expectation. How do Markov, Chebyshev, Chernoff, and sub-Gaussian inequalities relate, and why are they important for modern statistics and machine learning?",
    "options": {
      "A": "Concentration inequalities form a hierarchy of increasingly sharp bounds: Markov (P(X ≥ t) ≤ E[X]/t, uses only the mean), Chebyshev (P(|X-μ| ≥ t) ≤ σ²/t², uses the variance), and Chernoff/sub-Gaussian bounds (P(|X̄-μ| ≥ t) ≤ 2exp(-nt²/2σ²), exponential decay using moment-generating functions). Sub-Gaussian concentration gives dimension-free, exponentially tight bounds that are essential for: PAC learning theory (bounding generalization error), compressed sensing (restricted isometry property), high-dimensional statistics (estimating covariance matrices with n < p), and bandit algorithms (confidence bounds for exploration)",
      "B": "Concentration inequalities are purely theoretical tools used in asymptotic probability proofs and have no practical applications in statistics, machine learning, or algorithm design because they produce bounds that are too loose to be informative in finite-sample settings",
      "C": "All concentration inequalities — Markov, Chebyshev, Chernoff, Hoeffding, and Bernstein — give identical numerical bounds for any given probability of deviation, differing only in the mathematical elegance of their proofs rather than in the tightness of their tail probability estimates",
      "D": "Concentration inequalities apply only to normally distributed random variables because their derivations rely on the moment-generating function of the Gaussian distribution, which has the unique property of being finite for all values of the parameter"
    },
    "correct_answer": "A",
    "difficulty": 5,
    "x": 0.929722,
    "y": 0.693127,
    "z": 0.506853,
    "source_article": "concentration inequalities",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "concentration inequalities",
      "sub-Gaussian",
      "Chernoff bound",
      "high-dimensional statistics",
      "PAC learning"
    ]
  },
  {
    "id": "e83eeb005d98a760",
    "question_text": "Causal inference from observational data is one of the most important problems in statistics. The Rubin causal model (potential outcomes framework) formalizes causation. What is the fundamental problem of causal inference?",
    "options": {
      "A": "Causal inference is fundamentally impossible from any type of data — whether observational or experimental — because David Hume's philosophical critique of causation demonstrated that we can only observe correlation, never causation. The Rubin causal model and the do-calculus of Pearl both attempt to define causal quantities mathematically, but these definitions are circular because they presuppose the existence of causal relationships in order to define them. Randomized controlled trials are better than observational studies for establishing association but still cannot establish causation, as the assignment mechanism eliminates confounding but does not create a causal link. The apparent success of causal inference in medicine and social science reflects consistent patterns of strong association rather than true identification of causal mechanisms, which remain forever beyond the reach of statistical methodology",
      "B": "The fundamental problem is that we can never observe both potential outcomes for the same unit: for individual i, we observe either Y_i(1) (outcome under treatment) or Y_i(0) (outcome under control), never both — the counterfactual is always missing. The individual causal effect Y_i(1) - Y_i(0) is therefore unobservable. We can only estimate average causal effects (ATE = E[Y(1) - Y(0)]) under assumptions: randomization (ensures independence of treatment and potential outcomes), SUTVA (no interference between units), and positivity (nonzero probability of each treatment for all covariate levels). When randomization fails, methods like propensity score matching, instrumental variables, and regression discontinuity attempt to recover causal effects from observational data under additional assumptions",
      "C": "The fundamental problem of causal inference is that observational studies always have insufficient sample sizes to detect true causal effects, because confounding variables introduce additional sources of variation that inflate the standard errors of treatment effect estimates. This means that no observational study can ever have enough statistical power to distinguish a genuine causal effect from a spurious correlation produced by unmeasured confounders, regardless of the study design or analytical method employed. Instrumental variables, regression discontinuity, and difference-in-differences all attempt to address this power problem by effectively increasing the usable sample size through quasi-experimental variation, but they remain fundamentally limited by the finite number of observations that are available in practice",
      "D": "The Rubin causal model applies only to randomized clinical trials in medicine and cannot be extended to observational studies in economics, sociology, or epidemiology because those fields lack controlled treatment assignment"
    },
    "correct_answer": "B",
    "difficulty": 5,
    "x": 0.857123,
    "y": 0.635947,
    "z": 0.535305,
    "source_article": "causal inference",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "causal inference",
      "potential outcomes",
      "fundamental problem",
      "counterfactuals",
      "observational studies"
    ]
  },
  {
    "id": "5dd14c354423bf3a",
    "question_text": "Random matrix theory (RMT) has become essential for understanding high-dimensional statistics. The Marchenko-Pastur law describes the limiting distribution of eigenvalues of sample covariance matrices when the dimension grows with the sample size. What does it predict?",
    "options": {
      "A": "The eigenvalues of the sample covariance matrix always equal the population covariance eigenvalues in expectation, with deviations decreasing as 1/√n, regardless of the ratio of dimensions to sample size",
      "B": "Random matrix theory applies exclusively to quantum physics and nuclear physics, where it was originally developed to model energy levels of heavy atomic nuclei, and has no relevance to multivariate statistics or machine learning",
      "C": "When p/n → γ ∈ (0,1] (dimension-to-sample-size ratio), the empirical spectral distribution of the sample covariance matrix (with population covariance = I) converges to the Marchenko-Pastur distribution supported on [(1-√γ)², (1+√γ)²]. This means even when the true covariance is identity (no structure), sample eigenvalues are spread over a wide interval, with the largest eigenvalue approaching (1+√γ)². This has profound implications: in high dimensions, PCA eigenvalues are biased upward, classical statistical tests break down, and regularization is essential. The Tracy-Widom distribution describes fluctuations of the largest eigenvalue, enabling detection of true signal eigenvalues emerging from the MP bulk",
      "D": "The Marchenko-Pastur law requires the data matrix entries to be exactly Gaussian (standard normal), because the derivation relies on the rotational invariance of the multivariate normal distribution. For non-Gaussian data, the spectral distribution of the sample covariance matrix does not converge to any deterministic limit, and the eigenvalues exhibit erratic behavior that depends on the specific distribution of the data. This Gaussianity requirement is the primary reason that random matrix theory has limited practical applicability in statistics — most real-world data are not normally distributed, so the Marchenko-Pastur distribution does not describe their sample covariance eigenvalue distributions, and corrections for non-normality require knowledge of the fourth cumulant of the data distribution"
    },
    "correct_answer": "C",
    "difficulty": 5,
    "x": 0.919982,
    "y": 0.699965,
    "z": 0.350736,
    "source_article": "random matrix theory",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "random matrix theory",
      "Marchenko-Pastur law",
      "high-dimensional statistics",
      "sample covariance eigenvalues"
    ]
  },
  {
    "id": "5ce3738c59764eab",
    "question_text": "Stochastic processes model systems evolving randomly over time. Brownian motion (Wiener process) is the fundamental continuous-time stochastic process. What are its defining properties, and why is it central to so many fields?",
    "options": {
      "A": "Brownian motion has discontinuous sample paths that contain random jumps of finite size at random times, making it a special case of a compound Poisson process. The jumps occur at a rate proportional to the diffusion coefficient, and between jumps the process remains constant. The apparent continuity of Brownian motion observed in physical diffusion experiments is an illusion created by the high frequency of small jumps, which makes the path look continuous at macroscopic scales. Mathematically, the sample paths are càdlàg (right-continuous with left limits) rather than continuous, which is why stochastic integrals with respect to Brownian motion must be defined using the Itô convention that accounts for the jumps at each infinitesimal time step",
      "B": "Brownian motion is a deterministic process whose sample paths are smooth, infinitely differentiable functions of time. The apparent randomness observed in diffusion phenomena is due to our inability to measure the initial conditions of all particles precisely — if we could specify the exact position and velocity of every molecule, the trajectory would be perfectly predictable from Newton's laws. This perspective, known as the Langevin interpretation, shows that Brownian motion is fundamentally a classical mechanical phenomenon dressed in probabilistic language for computational convenience. The Wiener measure on path space is therefore not a genuine probability measure but an approximation to the deterministic dynamics of the underlying molecular system",
      "C": "Brownian motion models only the physical motion of microscopic particles suspended in a fluid, as originally described by Robert Brown's observations of pollen grains in 1827. The mathematical formalization by Einstein and Wiener was specific to this physical context, and the process has no applications outside of fluid mechanics and thermodynamics. Claims that Brownian motion is relevant to financial mathematics, signal processing, or abstract probability theory are based on superficial analogies rather than rigorous mathematical connections. The Black-Scholes model, for instance, uses geometric Brownian motion as an empirical approximation to stock price dynamics, but the stochastic process underlying stock prices has fundamentally different properties including heavy tails and long-range dependence",
      "D": "Brownian motion is defined by: (1) B(0) = 0, (2) independent increments (B(t)-B(s) independent of B(u) for u ≤ s), (3) normally distributed increments (B(t)-B(s) ~ N(0, t-s)), and (4) continuous sample paths. Despite continuous paths, Brownian motion is almost surely nowhere differentiable — it is a fractal with Hausdorff dimension 2 (in the plane). It is central to: stochastic calculus (Itô calculus, fundamental to mathematical finance — Black-Scholes equation), probability theory (Donsker's theorem: it is the universal scaling limit of random walks), statistical physics (fluctuation-dissipation theorem), and pure mathematics (Wiener measure on path space)"
    },
    "correct_answer": "D",
    "difficulty": 5,
    "x": 0.946744,
    "y": 0.651005,
    "z": 0.966242,
    "source_article": "Brownian motion",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Brownian motion",
      "Wiener process",
      "stochastic calculus",
      "nowhere differentiability",
      "Donsker's theorem"
    ]
  },
  {
    "id": "23a0dd0f89924845",
    "question_text": "Independence of events A and B means P(A ∩ B) = P(A)P(B). Why does this differ from mutual exclusivity, and why can two events with nonzero probability be independent but not mutually exclusive?",
    "options": {
      "A": "Independence means knowing one event occurred gives no information about the other — P(A|B) = P(A) — while mutual exclusivity means A ∩ B = ∅ (they cannot co-occur); if A and B are mutually exclusive with P(A), P(B) > 0, then P(A|B) = 0 ≠ P(A), so they are dependent, making independence and mutual exclusivity incompatible for events with positive probability",
      "B": "Independence and mutual exclusivity are identical concepts expressed with different terminology, and the equation P(A ∩ B) = 0 that defines mutual exclusivity always implies the equation P(A ∩ B) = P(A)P(B) that defines independence — this is wrong because P(A)P(B) = 0 only if at least one event has probability zero, so for events with positive probability, mutual exclusivity and independence are fundamentally incompatible",
      "C": "Independent events must always have probability exactly 1/2 each, since independence requires perfect and complete symmetry between the two possible outcomes of each event — this is entirely false because independence is a relationship between the joint and marginal probabilities (P(A∩B) = P(A)P(B)) that holds for any arbitrary values of P(A) and P(B), not just 1/2",
      "D": "Two events can be simultaneously independent and mutually exclusive even when both events have strictly positive probability, meaning P(A∩B) = 0 and P(A∩B) = P(A)P(B) can hold together for P(A), P(B) > 0 — this is a direct mathematical contradiction because these two equations combined together give P(A)P(B) = 0, which requires at least one probability to be zero"
    },
    "correct_answer": "A",
    "difficulty": 1,
    "x": 0.915837,
    "y": 0.51,
    "z": 0.647829,
    "source_article": "independence",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "independence",
      "mutual exclusivity",
      "conditional probability",
      "incompatibility"
    ]
  },
  {
    "id": "66695d893423e4b9",
    "question_text": "The expected value E[X] of a random variable X is a measure of central tendency. Why is it a linear operator, and what does linearity of expectation imply for sums of dependent random variables?",
    "options": {
      "A": "Linearity of expectation holds only when the random variables being summed are independent of one another, and the identity E[X + Y] = E[X] + E[Y] breaks down completely whenever X and Y are correlated, dependent, or have any form of statistical association between them — this is entirely wrong because linearity of expectation requires absolutely no independence assumption whatsoever and holds universally",
      "B": "E[aX + bY] = aE[X] + bE[Y] always holds regardless of dependence between X and Y — this follows directly from the linearity of summation/integration and requires no independence assumption; it means E[X₁ + ... + Xₙ] = ΣE[Xᵢ] even for arbitrarily dependent variables, which is why expected value calculations for complex systems often decompose into simple indicator variable sums",
      "C": "The expected value is not actually linear because E[XY] = E[X]E[Y] holds for all random variables, making expectation multiplicative rather than additive",
      "D": "Linearity of expectation is an approximation that becomes exact only in the limit as the number of random variables grows to infinity"
    },
    "correct_answer": "B",
    "difficulty": 1,
    "x": 0.976006,
    "y": 0.546062,
    "z": 0.343578,
    "source_article": "linearity of expectation",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "linearity of expectation",
      "independence irrelevance",
      "indicator variables",
      "decomposition"
    ]
  },
  {
    "id": "cf2d4f8ccb454b1a",
    "question_text": "Variance Var(X) = E[(X - μ)²] measures the spread of a distribution. Why does Var(X + Y) = Var(X) + Var(Y) require independence, unlike the corresponding expectation identity?",
    "options": {
      "A": "Var(X + Y) = Var(X) + Var(Y) holds for all random variables regardless of their dependence structure, just like linearity of expectation, because variance is also a linear operator that distributes over sums — this is wrong because variance is not linear: the correct formula Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y) includes a covariance cross-term that vanishes only for uncorrelated variables",
      "B": "Variance is always negative for dependent random variables because the covariance term subtracts from the total variance, causing the overall variance of the sum to become a negative number — this is impossible since variance is defined as E[(X-μ)²], which is a non-negative quantity for any random variable regardless of its dependence on others",
      "C": "Var(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y), so the identity Var(X+Y) = Var(X) + Var(Y) holds only when Cov(X,Y) = 0 — independence implies zero covariance (though not conversely), and the cross-term 2Cov(X,Y) arises because squaring (X+Y-μ_X-μ_Y)² produces cross-products that don't cancel unless the variables are uncorrelated",
      "D": "Variance of a sum always equals the product of the individual variances: Var(X+Y) = Var(X) · Var(Y)"
    },
    "correct_answer": "C",
    "difficulty": 1,
    "x": 0.94811,
    "y": 0.519466,
    "z": 0.277661,
    "source_article": "variance of sums",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "variance of sums",
      "covariance",
      "independence vs uncorrelatedness",
      "cross-terms"
    ]
  },
  {
    "id": "742bac80bc235f7a",
    "question_text": "The normal distribution N(μ, σ²) is ubiquitous in statistics. What property makes the sum of independent normal random variables also normal, and why is this unusual among distributions?",
    "options": {
      "A": "The sum of independent random variables from any probability distribution is always exactly normally distributed, not just approximately — this is what the Central Limit Theorem states for finite sums of any size, even n = 2 or n = 3 — this is wrong because the CLT gives convergence in distribution as n → ∞, and finite sums are not exactly normal (e.g. sum of 2 uniforms is triangular, not normal)",
      "B": "The normal distribution is closed under summation only when the two normal random variables being summed have exactly equal means μ₁ = μ₂ and exactly equal variances σ₁² = σ₂², and the closure property fails entirely when the parameters differ in any way whatsoever — this is wrong because X + Y is normal with parameters μ₁+μ₂ and σ₁²+σ₂² regardless of equality",
      "C": "Normal random variables remain normally distributed under summation because the normal probability density function is a polynomial function, and polynomials are closed under pointwise addition — this is wrong on multiple levels: the normal density is not a polynomial (it involves exp(-x²/2)), and the distribution of a sum is determined by convolution, not pointwise addition of densities",
      "D": "If X ~ N(μ₁,σ₁²) and Y ~ N(μ₂,σ₂²) are independent, then X + Y ~ N(μ₁+μ₂, σ₁²+σ₂²) — this stability under convolution follows from the moment generating function M_{X+Y}(t) = M_X(t)M_Y(t), which remains of the normal MGF form; most distributions lack this closure property, making the normal distribution special and fundamental to statistical inference"
    },
    "correct_answer": "D",
    "difficulty": 1,
    "x": 0.99,
    "y": 0.611055,
    "z": 0.447223,
    "source_article": "normal distribution",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "normal distribution",
      "stability under convolution",
      "moment generating functions",
      "closure property"
    ]
  },
  {
    "id": "d61522dac5afdd34",
    "question_text": "The law of total probability states P(A) = Σ P(A|Bᵢ)P(Bᵢ) for a partition {Bᵢ}. Why does this decomposition require a partition, and how does it connect to Bayesian reasoning?",
    "options": {
      "A": "A partition {B₁,...,Bₙ} satisfies Bᵢ ∩ Bⱼ = ∅ for i ≠ j and ∪Bᵢ = Ω, ensuring every outcome belongs to exactly one Bᵢ — the total probability formula then decomposes P(A) by conditioning on each scenario Bᵢ, and inverting via Bayes' theorem gives P(Bᵢ|A) = P(A|Bᵢ)P(Bᵢ)/P(A), where the denominator P(A) comes from the total probability formula applied to the partition",
      "B": "The law of total probability works for any arbitrary collection of events, not just for partitions of the sample space Ω, and overlapping events cause absolutely no issues whatsoever because any repeated counting that occurs due to overlap automatically averages itself out in the final summation — this is wrong because overlapping events lead to systematic double-counting",
      "C": "The decomposition P(A) = Σ P(A|Bᵢ)P(Bᵢ) does not require the events Bᵢ to fully cover the entire sample space Ω because any outcomes that lie outside the union ∪Bᵢ contribute exactly zero probability to the event A and can be safely ignored — this is wrong because if ∪Bᵢ ≠ Ω, the formula misses the contribution from outcomes in Ω \\ ∪Bᵢ, systematically underestimating P(A)",
      "D": "The law of total probability and Bayes' theorem are unrelated results that cannot be derived from each other"
    },
    "correct_answer": "A",
    "difficulty": 1,
    "x": 0.868802,
    "y": 0.593564,
    "z": 0.769026,
    "source_article": "law of total probability",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "law of total probability",
      "partition requirement",
      "Bayes' theorem derivation"
    ]
  },
  {
    "id": "4481009128cee1e1",
    "question_text": "Type I and Type II errors represent the two ways hypothesis testing can fail. What does each represent, and why is there a fundamental tradeoff between them?",
    "options": {
      "A": "Type I error is failing to reject a false null hypothesis (which is actually the definition of a Type II error), and Type II error is rejecting a true null hypothesis (which is actually the definition of a Type I error) — these definitions are precisely the reverse of the standard and universally accepted convention used in hypothesis testing, where α = P(reject H₀ | H₀ true) defines the Type I error rate",
      "B": "Type I error (α) is rejecting a true null hypothesis (false positive), and Type II error (β) is failing to reject a false null hypothesis (false negative) — reducing α by demanding stronger evidence to reject H₀ increases β because the more conservative the test, the more likely it is to miss a real effect, creating a fundamental tradeoff that can only be ameliorated by increasing sample size",
      "C": "Type I and Type II errors can both be simultaneously reduced to zero by choosing the right significance level, without needing to increase sample size",
      "D": "The distinction between Type I and Type II errors exists only in frequentist statistics and has no Bayesian analog or interpretation"
    },
    "correct_answer": "B",
    "difficulty": 1,
    "x": 0.835032,
    "y": 0.614806,
    "z": 0.517561,
    "source_article": "Type I error",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Type I error",
      "Type II error",
      "power",
      "error tradeoff",
      "sample size"
    ]
  },
  {
    "id": "924826c2d8180bdb",
    "question_text": "The likelihood function L(θ|x) = f(x|θ) plays a central role in statistical inference. Why is it not a probability distribution over θ, and what principle does maximum likelihood estimation follow?",
    "options": {
      "A": "The likelihood function L(θ|x) is a proper normalized probability density function over the parameter θ that integrates to exactly 1 over all possible parameter values, just like any other probability density function in the frequentist or Bayesian framework — this is wrong because the likelihood is a function of θ for fixed data x and generally does not integrate to 1 over the parameter space, which is precisely why it is not a probability density over θ",
      "B": "Maximum likelihood estimation systematically chooses the parameter value θ that minimizes the likelihood function L(θ|x), finding the value of θ that makes the observed data as improbable as possible under the assumed statistical model — this reverses the fundamental optimization: MLE maximizes (not minimizes) the likelihood, seeking the parameter value under which the observed data would have been most probable to occur",
      "C": "The likelihood L(θ|x) = f(x|θ) views the density as a function of θ for fixed data x — it does not integrate to 1 over θ (it's not a density on parameter space), but MLE chooses θ̂ = argmax L(θ|x) to find the parameter making the observed data most probable; MLE is consistent, asymptotically normal, and achieves the Cramér-Rao bound asymptotically, making it asymptotically efficient",
      "D": "The likelihood function depends only on the total sample size n and not on the actual observed data values x₁, x₂, ..., xₙ, meaning two completely different datasets of the same size always produce identical likelihood functions — this is fundamentally wrong because the entire purpose of the likelihood function is to evaluate how probable the specific observed data values are under each candidate parameter value θ, making data-dependence its defining characteristic"
    },
    "correct_answer": "C",
    "difficulty": 2,
    "x": 0.914342,
    "y": 0.664057,
    "z": 0.399381,
    "source_article": "likelihood function",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "likelihood function",
      "MLE",
      "consistency",
      "asymptotic efficiency",
      "Cramér-Rao"
    ]
  },
  {
    "id": "7229260c3b256318",
    "question_text": "A sufficient statistic T(X) captures all information about a parameter θ that the data provides. What is the formal definition, and how does the factorization theorem identify sufficient statistics?",
    "options": {
      "A": "A sufficient statistic T(X) is any statistic whose numerical value uniquely and completely determines every individual data point in the original sample, meaning that the full dataset x₁, x₂, ..., xₙ can always be reconstructed from the value of T(X) alone — this confuses sufficiency with invertibility; a sufficient statistic only needs to make the conditional distribution of the full data independent of the parameter θ, and does not require full data recovery",
      "B": "Sufficiency requires the statistic T(X) to be a one-to-one bijective function of the entire data vector, which is precisely why only the order statistics (X₍₁₎, X₍₂₎, ..., X₍ₙ₎) can ever be sufficient for any statistical model — this is wrong because many statistics that drastically reduce the dimension of the data are sufficient: for example, the sample mean X̄ compresses n observations to a single number yet is sufficient for the mean of a normal distribution with known variance",
      "C": "The Fisher-Neyman factorization theorem states that T is sufficient if and only if the likelihood factors as L(θ|x) = g(T(x)) · h(x) where g depends only on T and h depends only on x, but this fundamental theorem applies only to discrete probability distributions and fails entirely for all continuous probability distributions — this is wrong because the factorization theorem holds equally for both discrete and continuous distributions",
      "D": "T(X) is sufficient for θ if the conditional distribution of X given T(X) = t does not depend on θ — intuitively, once T is known, the remaining randomness in X carries no further information about θ; the Fisher-Neyman factorization theorem states T is sufficient iff f(x|θ) = g(T(x), θ) · h(x), where the θ-dependence enters only through T(x), making sufficient statistics easy to identify from the likelihood's algebraic form"
    },
    "correct_answer": "D",
    "difficulty": 2,
    "x": 0.909177,
    "y": 0.63074,
    "z": 0.545784,
    "source_article": "sufficient statistics",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "sufficient statistics",
      "factorization theorem",
      "data reduction",
      "conditional independence"
    ]
  },
  {
    "id": "d28a3c02c2083c75",
    "question_text": "The exponential family of distributions includes normal, Poisson, binomial, and many others. What structural property defines this family, and why does membership in the exponential family guarantee the existence of sufficient statistics of fixed dimension?",
    "options": {
      "A": "A distribution belongs to the exponential family if its density can be written as f(x|θ) = h(x) exp(η(θ)ᵀT(x) - A(θ)), where T(x) is the sufficient statistic vector and A(θ) is the log-partition function — the key property is that T(x) has fixed dimension regardless of sample size n, because for n iid observations the joint density factors with sufficient statistic ΣT(xᵢ), achieving maximal data compression while preserving all information about θ",
      "B": "The exponential family consists exclusively and solely of distributions whose probability density functions explicitly contain the exponential function e^x as a literal factor appearing in the formula, and any distribution without a visible exponential in its density — such as the binomial, negative binomial, geometric, Poisson, or beta distribution — is automatically excluded from the family — this confuses the structural name 'exponential family' with the presence of a literal exponential function in the density formula",
      "C": "Every probability distribution automatically belongs to the exponential family because any probability density function can be trivially rewritten in the canonical exponential form by simply taking the logarithm of the density and then exponentiating — this is wrong because specific structural requirements like the density factoring as h(x)exp(η(θ)ᵀT(x) - A(θ)) with natural parameter η and sufficient statistic T of fixed dimension are not met by many distributions, including the uniform distribution on [0,θ]",
      "D": "The exponential family has no special statistical properties and membership in it provides no computational or theoretical advantages for inference"
    },
    "correct_answer": "A",
    "difficulty": 2,
    "x": 0.933372,
    "y": 0.649823,
    "z": 0.619788,
    "source_article": "exponential family",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "exponential family",
      "natural parameters",
      "sufficient statistics",
      "log-partition function"
    ]
  },
  {
    "id": "ae6dee8fdda27b9c",
    "question_text": "The Neyman-Pearson lemma provides the most powerful test for simple hypotheses. What does 'most powerful' mean, and why does the optimal test have a likelihood ratio form?",
    "options": {
      "A": "The Neyman-Pearson lemma states that the most powerful test for simple hypotheses always rejects H₀ when the sample mean exceeds a fixed threshold, regardless of the specific distributions involved under H₀ and H₁ — this is wrong because the optimal test statistic is the likelihood ratio, not the sample mean, and the rejection region depends on the specific parametric forms",
      "B": "A test is most powerful at level α if it maximizes power (1-β = P(reject H₀ | H₁ true)) among all tests with Type I error rate ≤ α — the lemma proves this optimal test rejects when L(θ₁|x)/L(θ₀|x) > k, because the likelihood ratio concentrates evidence about which hypothesis generated the data, and the threshold k is set to achieve the desired α level",
      "C": "The most powerful test in the Neyman-Pearson framework minimizes the sample size n required to achieve a given power level rather than maximizing the rejection probability (power) for a fixed sample size and significance level, and the optimal test has absolutely no connection to the likelihood ratio of the two simple hypotheses — this fundamentally inverts the optimization target and ignores the LR structure",
      "D": "The Neyman-Pearson lemma applies only to testing normal means and cannot handle Poisson, exponential, or any other parametric family"
    },
    "correct_answer": "B",
    "difficulty": 2,
    "x": 0.904953,
    "y": 0.664135,
    "z": 0.330667,
    "source_article": "Neyman-Pearson lemma",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Neyman-Pearson lemma",
      "most powerful test",
      "likelihood ratio",
      "power optimization"
    ]
  },
  {
    "id": "832c266a9343f5ea",
    "question_text": "The EM algorithm finds maximum likelihood estimates when data is incomplete or has latent variables. What are the E-step and M-step, and why does the algorithm guarantee non-decreasing likelihood?",
    "options": {
      "A": "The E-step maximizes the likelihood and the M-step computes expectations — the names are reversed in most textbooks due to a historical error",
      "B": "The EM algorithm can decrease the observed-data log-likelihood at each iteration step and provides absolutely no convergence guarantees whatsoever, making it an unreliable optimization method with no theoretical foundation — this is fundamentally wrong because the EM algorithm provably produces a non-decreasing sequence of observed-data log-likelihood values at every single iteration, with the monotonicity guarantee following from Jensen's inequality applied to the expectation step",
      "C": "The E-step computes the expected value of the complete-data log-likelihood Q(θ|θ_old) = E[log f(x,z|θ) | x, θ_old] with respect to the latent variables z given current parameters, and the M-step maximizes Q over θ — the likelihood is non-decreasing because maximizing Q guarantees the observed likelihood increases or stays constant (via Jensen's inequality applied to the log), and the algorithm converges to a local maximum or saddle point",
      "D": "The EM algorithm requires the complete-data likelihood to be convex in θ, and fails entirely when the likelihood has multiple local maxima"
    },
    "correct_answer": "C",
    "difficulty": 2,
    "x": 0.904416,
    "y": 0.701508,
    "z": 0.291798,
    "source_article": "EM algorithm",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "EM algorithm",
      "E-step",
      "M-step",
      "likelihood monotonicity",
      "latent variables"
    ]
  },
  {
    "id": "a8b5f258bddb2044",
    "question_text": "Conditional expectation E[Y|X] is a random variable (function of X), not a number. Why is this distinction important, and what optimality property does it possess?",
    "options": {
      "A": "The conditional expectation E[Y|X] is a fixed numerical constant (not a random variable that depends on X) whose value always equals the unconditional expectation E[Y] regardless of what value of X is observed — this is wrong because E[Y|X] is a measurable function of the random variable X that takes different values for different realizations of X, capturing how the optimal prediction of Y changes as we learn X, and equals E[Y] only when X and Y are independent",
      "B": "Conditional expectation has absolutely no optimality property and is merely one of many equally valid and equally effective methods for predicting the random variable Y from the random variable X, with no special theoretical status distinguishing it from other predictors — this is wrong because E[Y|X] is the unique minimizer of E[(Y-g(X))²] over all measurable functions g, making it the optimal L² predictor with the smallest possible mean squared error",
      "C": "E[Y|X] minimizes E[(Y - g(X))⁴] over all functions g, optimizing the fourth moment rather than the second moment of the prediction error",
      "D": "E[Y|X] is the function of X that minimizes the mean squared prediction error E[(Y - g(X))²] over all measurable functions g(X) — it is the optimal L² predictor, and treating it as a random variable is essential because its value changes with X, capturing how the best prediction of Y varies as we learn different values of X; the tower property E[E[Y|X]] = E[Y] then relates conditional and unconditional expectations"
    },
    "correct_answer": "D",
    "difficulty": 2,
    "x": 0.935503,
    "y": 0.586625,
    "z": 0.426691,
    "source_article": "conditional expectation",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "conditional expectation",
      "optimal prediction",
      "L² minimization",
      "tower property"
    ]
  },
  {
    "id": "b5f840056d6db96d",
    "question_text": "The delta method approximates the distribution of g(X̄) when X̄ is asymptotically normal. What is the formula, and why does it fail when g'(μ) = 0?",
    "options": {
      "A": "If √n(X̄ - μ) →_d N(0,σ²) and g is differentiable with g'(μ) ≠ 0, then √n(g(X̄) - g(μ)) →_d N(0, σ²[g'(μ)]²) — this follows from a first-order Taylor expansion g(X̄) ≈ g(μ) + g'(μ)(X̄ - μ); when g'(μ) = 0, the first-order term vanishes and one needs the second-order delta method: n(g(X̄) - g(μ)) →_d (g''(μ)σ²/2)χ₁², producing a chi-squared rather than normal limit",
      "B": "The delta method gives an exact (not approximate) distribution for g(X̄) that holds for every sample size n, not just asymptotically",
      "C": "The delta method produces a normal approximation regardless of whether g'(μ) is zero or nonzero, because the second derivative never appears in the asymptotic expansion",
      "D": "The delta method applies only to linear functions g(x) = ax + b and provides no information about nonlinear transformations"
    },
    "correct_answer": "A",
    "difficulty": 3,
    "x": 0.968234,
    "y": 0.69027,
    "z": 0.478968,
    "source_article": "delta method",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "delta method",
      "Taylor expansion",
      "second-order delta method",
      "chi-squared limit"
    ]
  },
  {
    "id": "89fb30695059e6d0",
    "question_text": "The Rao-Blackwell theorem improves estimators by conditioning on sufficient statistics. What is the improvement, and why can't it make a bad estimator worse?",
    "options": {
      "A": "The Rao-Blackwell theorem states that conditioning an unbiased estimator on a sufficient statistic always strictly increases the variance of the resulting estimator, making the conditioned version uniformly worse than the original — this is the exact opposite of the truth: Rao-Blackwellization provably reduces (or maintains) variance via the law of total variance decomposition",
      "B": "If δ(X) is an unbiased estimator of θ and T is sufficient, then δ*(T) = E[δ(X)|T] is also unbiased with Var(δ*) ≤ Var(δ) — the variance reduction follows from the law of total variance: Var(δ) = E[Var(δ|T)] + Var(E[δ|T]) = E[Var(δ|T)] + Var(δ*), so Var(δ*) = Var(δ) - E[Var(δ|T)] ≤ Var(δ), with equality only when δ is already a function of T",
      "C": "Rao-Blackwellization preserves unbiasedness but provides no guarantee about variance and can arbitrarily increase the MSE of the estimator",
      "D": "The theorem requires T to be both sufficient and complete, and fails entirely when T is sufficient but not complete"
    },
    "correct_answer": "B",
    "difficulty": 3,
    "x": 0.907683,
    "y": 0.647962,
    "z": 0.221997,
    "source_article": "Rao-Blackwell theorem",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Rao-Blackwell theorem",
      "variance reduction",
      "law of total variance",
      "sufficiency conditioning"
    ]
  },
  {
    "id": "41bd612b73e4bfe8",
    "question_text": "MCMC methods (like Metropolis-Hastings) sample from distributions that are difficult to sample from directly. What ensures the chain converges to the target distribution, and what is the burn-in period?",
    "options": {
      "A": "MCMC Markov chains always converge to their stationary target distribution after exactly 100 iterations regardless of the specific target distribution, the dimension of the sample space, the choice of proposal distribution, the starting point, or any other aspect of the chain's overall design and configuration — this is wrong because convergence speed depends critically on the chain's mixing properties, the target's geometry, and can require millions of iterations",
      "B": "The Metropolis-Hastings acceptance ratio must always equal 1 (accept every proposal) for the chain to converge to the target distribution",
      "C": "A Markov chain converges to its stationary distribution π if it is irreducible (can reach any state from any state) and aperiodic (doesn't cycle) — Metropolis-Hastings constructs a chain satisfying detailed balance π(x)q(y|x)α(x,y) = π(y)q(x|y)α(y,x) with acceptance probability α ensuring π is stationary; the burn-in period discards initial samples before the chain has converged, since early samples reflect the arbitrary starting distribution rather than π",
      "D": "MCMC sampling methods such as Metropolis-Hastings and Gibbs sampling require the target probability distribution to have a fully known and completely computed normalizing constant, making them entirely useless for Bayesian posterior distributions where the marginal likelihood (the normalizing constant) is typically unknown and computationally intractable — this is the exact opposite of MCMC's key strength, which is that it requires only the unnormalized density"
    },
    "correct_answer": "C",
    "difficulty": 3,
    "x": 0.931715,
    "y": 0.70864,
    "z": 0.785104,
    "source_article": "MCMC",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "MCMC",
      "Metropolis-Hastings",
      "detailed balance",
      "burn-in",
      "ergodic theorem"
    ]
  },
  {
    "id": "c98c7f00d8f80306",
    "question_text": "The multivariate normal distribution N(μ, Σ) generalizes the univariate normal to d dimensions. What properties make it central to multivariate statistics, and how do conditional distributions relate to regression?",
    "options": {
      "A": "In the multivariate normal, all marginal distributions are uniform rather than normal, because projecting a bell curve onto a lower dimension always flattens it into a uniform shape",
      "B": "AIC and BIC always use identical penalty terms of exactly 2k (where k is the number of model parameters) and they always select exactly the same model in every single application, making the distinction between them purely a matter of historical notational convention with absolutely no statistical substance — this is wrong because BIC uses k·log(n), which exceeds 2k for n ≥ 8, so BIC penalizes complexity more heavily for moderate to large samples",
      "C": "Conditional distributions of a multivariate normal given some components are not normal and require non-parametric methods to characterize",
      "D": "All marginals and conditionals of a multivariate normal are themselves normal — if (X,Y)ᵀ ~ N(μ, Σ), then Y|X=x ~ N(μ_Y + Σ_YX Σ_XX⁻¹(x-μ_X), Σ_YY - Σ_YX Σ_XX⁻¹ Σ_XY), which is exactly the linear regression of Y on X with Gaussian errors; this closure under marginalization and conditioning is why the multivariate normal underlies linear regression, discriminant analysis, and Gaussian processes"
    },
    "correct_answer": "D",
    "difficulty": 3,
    "x": 0.968644,
    "y": 0.600756,
    "z": 0.322903,
    "source_article": "multivariate normal",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "multivariate normal",
      "conditional distributions",
      "linear regression connection",
      "closure properties"
    ]
  },
  {
    "id": "be57833c22b3327d",
    "question_text": "Cross-validation estimates prediction error by repeatedly holding out subsets of data. Why does leave-one-out (LOO) cross-validation have high variance despite being approximately unbiased?",
    "options": {
      "A": "LOO trains on n-1 of n observations in each fold, so the n training sets overlap almost completely — this high correlation between fold-specific error estimates inflates the variance of their average, because averaging highly correlated quantities reduces variance much less than averaging independent ones; k-fold CV with smaller k (like 5 or 10) uses less-overlapping training sets, reducing correlation at the cost of slightly more bias",
      "B": "Leave-one-out cross-validation has low variance and high bias because it uses only n-1 of n observations for training in each fold, which is far too few data points to build a reliable model, leading to systematic underestimation of model performance — this reverses the actual properties: LOO has low bias (because the training set is nearly the full dataset) but high variance (because the n fold-specific error estimates are highly correlated due to near-complete overlap of their training sets)",
      "C": "LOO has high variance because it uses only a single observation for testing in each fold, and any individual test observation is an unreliable estimate of performance",
      "D": "LOO and k-fold cross-validation have identical bias and variance properties regardless of k, so there is no statistical reason to prefer one over the other"
    },
    "correct_answer": "A",
    "difficulty": 3,
    "x": 0.851909,
    "y": 0.624107,
    "z": 0.05423,
    "source_article": "cross-validation",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "cross-validation",
      "bias-variance tradeoff",
      "LOO",
      "correlated fold estimates"
    ]
  },
  {
    "id": "74da1dd9bc16e0ae",
    "question_text": "The Bayesian information criterion (BIC) and Akaike information criterion (AIC) both balance model fit against complexity. What is the key difference in their penalty terms, and what does each optimize?",
    "options": {
      "A": "AIC and BIC use identical penalty terms and always select the same model, making the distinction between them purely notational",
      "B": "AIC penalizes by 2k (where k = number of parameters) and approximates the expected Kullback-Leibler divergence from the true model, selecting for prediction; BIC penalizes by k·log(n) and approximates the log Bayes factor, selecting for the true model — BIC's penalty grows with n, so it favors simpler models as data grows and is consistent (selects the true model as n → ∞), while AIC tends to overfit but gives better finite-sample predictions",
      "C": "BIC always selects simpler models than AIC because it has a smaller penalty term, making it more prone to underfitting",
      "D": "AIC is a Bayesian criterion and BIC is a frequentist criterion, and they cannot be used in the same inferential framework"
    },
    "correct_answer": "B",
    "difficulty": 3,
    "x": 0.832031,
    "y": 0.64651,
    "z": 0.370613,
    "source_article": "AIC",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "AIC",
      "BIC",
      "model selection",
      "KL divergence",
      "Bayes factor approximation"
    ]
  },
  {
    "id": "d40232a6803bd0b8",
    "question_text": "The Stein paradox shows that the sample mean is inadmissible as an estimator of a multivariate normal mean in dimension d ≥ 3. What does 'inadmissible' mean here, and why is the James-Stein estimator better?",
    "options": {
      "A": "The sample mean is inadmissible as an estimator of a multivariate normal mean vector in all dimensions d ≥ 1, including d = 1 and d = 2, because the James-Stein shrinkage estimator uniformly dominates it with strictly smaller risk for every parameter value in every dimension — this is specifically wrong because the Stein paradox requires d ≥ 3: in d = 1 and d = 2, the sample mean is admissible and no dominating estimator of any kind exists",
      "B": "Inadmissibility in the Stein paradox means the estimator has infinite variance that makes it completely and practically useless, and the James-Stein estimator reduces the variance to exactly zero by shrinking all estimates completely and without remainder to the origin — this is doubly wrong: inadmissibility means another estimator has uniformly smaller risk for all parameter values simultaneously, and the James-Stein estimator still has strictly positive variance",
      "C": "An estimator is inadmissible under squared error loss if another estimator has strictly smaller risk E[‖θ̂ - θ‖²] for all θ — the James-Stein estimator (1 - (d-2)/‖X‖²)X shrinks the sample mean X toward zero, and for d ≥ 3 it uniformly dominates X; the paradox is that shrinking unrelated estimates toward a common point (even an arbitrary one) reduces total risk, which fails in d ≤ 2 where the geometry prevents simultaneous improvement",
      "D": "The James-Stein estimator improves on the sample mean only for a single specific and isolated value of the true parameter vector θ and is strictly worse than the sample mean for all other possible parameter values, making the improvement completely and practically irrelevant for real applications — this is wrong because the defining feature of the Stein inadmissibility result is that the James-Stein estimator has smaller risk than the sample mean for ALL θ simultaneously"
    },
    "correct_answer": "C",
    "difficulty": 4,
    "x": 0.889617,
    "y": 0.683251,
    "z": 0.17995,
    "source_article": "Stein paradox",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Stein paradox",
      "James-Stein estimator",
      "inadmissibility",
      "shrinkage",
      "dimension effect"
    ]
  },
  {
    "id": "a7acfdf79fc32048",
    "question_text": "The minimax estimator minimizes the maximum risk over all possible parameter values. Why does the minimax approach differ from Bayesian estimation, and when do the two coincide?",
    "options": {
      "A": "Minimax and Bayesian estimators are always identical because minimizing maximum risk is equivalent to minimizing average risk under any prior distribution",
      "B": "Minimax estimators cannot be computed for any realistic statistical problem and exist only as theoretical curiosities in textbooks",
      "C": "The minimax approach always produces the same estimator as maximum likelihood, making the two frameworks indistinguishable",
      "D": "Minimax minimizes sup_θ R(θ, δ) (worst-case risk), while Bayes minimizes ∫R(θ, δ)π(θ)dθ (average risk under prior π) — by the minimax theorem, the two coincide when the least favorable prior π* exists: the Bayes estimator under π* is minimax, and the minimax risk equals the Bayes risk under π*; this connects game-theoretic and Bayesian perspectives, with the least favorable prior representing nature's adversarial choice of θ"
    },
    "correct_answer": "D",
    "difficulty": 4,
    "x": 0.823749,
    "y": 0.689724,
    "z": 0.419412,
    "source_article": "minimax estimation",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "minimax estimation",
      "Bayes risk",
      "least favorable prior",
      "minimax theorem"
    ]
  },
  {
    "id": "e1dc4f89df795ca8",
    "question_text": "Empirical process theory studies the behavior of √n(F̂ₙ - F) as a stochastic process indexed by t. What does the Donsker theorem say, and why is it a functional extension of the CLT?",
    "options": {
      "A": "The Donsker theorem states that √n(F̂ₙ(t) - F(t)) converges in distribution to a Brownian bridge B(F(t)) as a process — this is a functional CLT because while the ordinary CLT gives convergence of √n(X̄ - μ) to a normal random variable (a single number), Donsker gives convergence of the entire empirical process to a Gaussian process, enabling simultaneous inference over all thresholds t and providing the theoretical basis for Kolmogorov-Smirnov tests",
      "B": "The Donsker theorem applies only to the empirical cumulative distribution function F̂ₙ evaluated at a single fixed threshold point t and provides absolutely no information about the joint behavior of the entire empirical process across multiple thresholds or as a stochastic process indexed by all of ℝ — this defeats the entire purpose of the theorem, which establishes weak convergence of the complete empirical process as a random element in a function space to a Brownian bridge",
      "C": "Empirical process convergence as described by the Donsker invariance principle requires the underlying population distribution F to be specifically Gaussian (normally distributed), and the theorem fails completely and without remedy for any non-Gaussian population distribution — this is wrong because the Donsker theorem holds for any distribution F with finite second moment, making the result essentially distribution-free and applicable to arbitrary continuous distributions",
      "D": "The Donsker theorem is a purely finite-sample result that holds exactly and non-asymptotically for every fixed sample size n and is not an asymptotic convergence statement about limiting behavior as n → ∞ — this is fundamentally wrong because Donsker's invariance principle is by its very nature an asymptotic result about the weak convergence of the rescaled empirical process √n(F̂ₙ - F) to a Brownian bridge B(F(t)) in the Skorokhod space D[0,1] as n grows without bound"
    },
    "correct_answer": "A",
    "difficulty": 4,
    "x": 0.951177,
    "y": 0.678728,
    "z": 0.792083,
    "source_article": "Donsker theorem",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Donsker theorem",
      "empirical processes",
      "Brownian bridge",
      "functional CLT",
      "Kolmogorov-Smirnov"
    ]
  },
  {
    "id": "1f116cedb5d20325",
    "question_text": "Exchangeability means the joint distribution of (X₁,...,Xₙ) is invariant under permutation. How does de Finetti's theorem relate exchangeability to conditional independence, and why is it foundational to Bayesian statistics?",
    "options": {
      "A": "Exchangeability is a strictly stronger condition than the assumption that random variables are independent and identically distributed (iid), meaning that exchangeability applies to fewer sequences and de Finetti's theorem is a restrictive result rather than a generalization — this reverses the logical relationship: every iid sequence is automatically exchangeable (permutation-invariant), but many exchangeable sequences are not iid, so exchangeability is the weaker assumption",
      "B": "De Finetti's theorem states that an infinite exchangeable sequence is a mixture of iid sequences: the joint distribution can be written as ∫ Πf(xᵢ|θ) dμ(θ) for some mixing measure μ — this is foundational because it justifies the Bayesian framework: if you believe observations are exchangeable, there must exist a prior μ on some parameter θ such that the data are conditionally iid given θ, deriving the prior-likelihood-posterior structure from a symmetry assumption",
      "C": "De Finetti's representation theorem shows that exchangeable sequences of random variables are always fully and unconditionally independent of one another (not merely conditionally independent given some latent mixing parameter), completely eliminating the need for any probabilistic model, parameter space, prior distribution, or Bayesian framework — this critically confuses conditional and unconditional independence; exchangeable sequences are typically marginally dependent but become conditionally independent given the latent parameter",
      "D": "De Finetti's representation theorem applies only to binary (Bernoulli 0/1) random variables taking values in the set {0,1} and fundamentally cannot be extended or generalized to handle continuous real-valued random variables, count data, categorical data with more than two categories, or any other type of random variable beyond the binary case — this is wrong because de Finetti's theorem has been generalized to exchangeable sequences of random variables taking values in arbitrary standard Borel measurable spaces, covering all practical cases"
    },
    "correct_answer": "B",
    "difficulty": 4,
    "x": 0.87381,
    "y": 0.592692,
    "z": 0.805514,
    "source_article": "exchangeability",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "exchangeability",
      "de Finetti's theorem",
      "conditional independence",
      "Bayesian justification"
    ]
  },
  {
    "id": "007e3f46ee4bf9f0",
    "question_text": "The semiparametric efficiency bound limits how well a finite-dimensional parameter can be estimated in the presence of an infinite-dimensional nuisance parameter. What is the role of the efficient influence function?",
    "options": {
      "A": "Semiparametric efficiency bounds are always strictly weaker than parametric Cramér-Rao bounds, meaning you can estimate parameters more precisely in semiparametric models than in fully parametric ones",
      "B": "The efficient influence function is identical to the score function and provides no additional information beyond what the parametric likelihood already gives",
      "C": "The efficient influence function ψ(X;θ) characterizes the semiparametric efficiency bound: any regular estimator θ̂ₙ satisfies Var(√n θ̂ₙ) ≥ E[ψ²]⁻¹ (the semiparametric Cramér-Rao bound), and an estimator achieving this bound is semiparametrically efficient — computing ψ requires projecting the parametric score onto the tangent space orthogonal to the nuisance tangent space, which is why the bound depends on the specific nuisance model even though the parameter of interest is the same",
      "D": "Semiparametric models have no well-defined efficiency bounds because infinite-dimensional nuisance parameters make the Fisher information matrix infinite and non-invertible"
    },
    "correct_answer": "C",
    "difficulty": 4,
    "x": 0.882932,
    "y": 0.692208,
    "z": 0.276218,
    "source_article": "semiparametric efficiency",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "semiparametric efficiency",
      "influence function",
      "tangent space",
      "nuisance parameters"
    ]
  },
  {
    "id": "6e5758ad576ec117",
    "question_text": "Gibbs sampling is a special case of MCMC that updates one variable at a time from its full conditional. Under what conditions does it converge, and what is its main computational advantage?",
    "options": {
      "A": "Gibbs sampling requires all full conditional distributions to be normal, and it fails completely when any conditional is non-normal",
      "B": "The covariance matrix Σ of a multivariate normal distribution N(μ, Σ) is allowed to have negative entries on its main diagonal because variances of individual components in a multivariate setting can become negative when the corresponding variables are negatively correlated with other variables in the system — this is entirely wrong because the diagonal entries of Σ are the marginal variances σᵢ² = Var(Xᵢ), which are always non-negative by definition as expectations of non-negative random variables, and furthermore the entire matrix Σ must be positive semidefinite",
      "C": "Conditional distributions of components of a multivariate normal random vector given the values of other components are not themselves normally distributed and require non-parametric kernel density estimation methods or simulation-based approaches to characterize — this is fundamentally wrong because one of the most important and useful properties of the multivariate normal distribution is that all conditional distributions are again exactly normal, with explicit closed-form formulas for the conditional mean vector and conditional covariance matrix",
      "D": "Gibbs sampling updates each variable from its full conditional p(xᵢ|x₋ᵢ) in turn, and converges to the joint target distribution if the chain is irreducible and aperiodic — its main advantage is that it requires no tuning of proposal distributions (unlike Metropolis-Hastings) because the full conditionals serve as perfect proposals with acceptance probability 1; however, it can mix slowly when variables are strongly correlated, since component-wise updates struggle to explore the joint distribution efficiently"
    },
    "correct_answer": "D",
    "difficulty": 4,
    "x": 0.897345,
    "y": 0.70791,
    "z": 0.772149,
    "source_article": "Gibbs sampling",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Gibbs sampling",
      "full conditionals",
      "mixing",
      "acceptance probability",
      "correlation slowdown"
    ]
  },
  {
    "id": "2c854710a225d4b9",
    "question_text": "Le Cam's local asymptotic normality (LAN) framework characterizes the fundamental limits of statistical estimation. What does LAN mean, and how does it connect to the convolution theorem?",
    "options": {
      "A": "LAN states that for 'nice' parametric models, the log-likelihood ratio log(Lₙ(θ₀ + h/√n)/Lₙ(θ₀)) converges to h·Δ - h²I/2 where Δ ~ N(0,I) and I is the Fisher information — this means the experiment locally 'looks like' observing a normal shift, and the convolution theorem (Hájek-Le Cam) proves that any regular estimator's limit distribution is the sum of a N(0,I⁻¹) component (achievable by MLE) and an independent noise term, establishing I⁻¹ as the asymptotic efficiency bound",
      "B": "LAN applies only to exponential family models and provides no information about the asymptotic behavior of likelihood ratios for non-exponential-family distributions",
      "C": "The convolution theorem states that all estimators achieve the Cramér-Rao bound exactly in finite samples, making asymptotic analysis unnecessary",
      "D": "LAN is a property of the sample mean specifically and does not generalize to likelihood-based inference or arbitrary regular estimators"
    },
    "correct_answer": "A",
    "difficulty": 5,
    "x": 0.928783,
    "y": 0.737021,
    "z": 0.26598,
    "source_article": "local asymptotic normality",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "local asymptotic normality",
      "Le Cam theory",
      "convolution theorem",
      "asymptotic efficiency"
    ]
  },
  {
    "id": "cb98e2684ad15325",
    "question_text": "The Dirichlet process (DP) is a distribution over probability measures used as a nonparametric Bayesian prior. What are its key properties, and how does the stick-breaking construction characterize it?",
    "options": {
      "A": "The Dirichlet process DP(α, G₀) always produces continuous probability measures with smooth probability density functions, making it suitable only for modeling continuous distributions with well-behaved smooth densities and fundamentally incapable of modeling discrete distributions or distributions with point masses — this is the exact opposite of the truth: draws from a Dirichlet process are almost surely discrete (consisting of countably many atoms with random weights), which is why DP mixture models are used to create continuous distributions",
      "B": "A DP(α, G₀) is characterized by the property that for any finite partition (A₁,...,Aₖ), the vector (P(A₁),...,P(Aₖ)) ~ Dir(αG₀(A₁),...,αG₀(Aₖ)) — the stick-breaking construction represents draws as P = Σ wₖ δ_{θₖ} where θₖ ~ G₀ and weights wₖ = βₖ Π_{j<k}(1-βⱼ) with βⱼ ~ Beta(1,α), revealing that DP draws are discrete with probability 1 (countably many atoms), with α controlling the concentration: small α gives few large atoms, large α approaches G₀",
      "C": "The concentration parameter α has no effect on the Dirichlet process — changing α leaves the distribution over probability measures completely unchanged",
      "D": "The Dirichlet process is a parametric model with finitely many parameters and provides no more flexibility than a finite mixture model with a fixed number of components"
    },
    "correct_answer": "B",
    "difficulty": 5,
    "x": 0.919394,
    "y": 0.689831,
    "z": 0.88573,
    "source_article": "Dirichlet process",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Dirichlet process",
      "stick-breaking construction",
      "nonparametric Bayes",
      "concentration parameter"
    ]
  },
  {
    "id": "f5516b652d1a4ee6",
    "question_text": "High-dimensional statistics studies settings where the dimension p may exceed the sample size n. Why do classical methods fail in this regime, and what structural assumptions enable consistent estimation?",
    "options": {
      "A": "Classical statistical methods such as ordinary least squares, principal component analysis, and the standard sample covariance matrix work perfectly in all high-dimensional settings where p > n, because the law of large numbers and central limit theorem have no dependence whatsoever on the relationship between the dimension p and the sample size n — this is wrong because the sample covariance is singular when p > n, OLS is underdetermined, and classical asymptotics break down",
      "B": "When p > n, the only valid approach is to collect more data until n > p, because no statistical method can produce meaningful results in the p > n regime",
      "C": "When p > n, the sample covariance matrix is singular (rank ≤ n), OLS has infinitely many solutions, and classical asymptotics (which fix p and let n → ∞) become misleading — consistent estimation requires structural assumptions like sparsity (most coefficients are zero, enabling LASSO with ‖β‖₁ ≤ s requiring n ~ s·log(p) samples), low-rank structure (enabling nuclear norm minimization), or manifold assumptions that effectively reduce the intrinsic dimension",
      "D": "High-dimensional statistics is purely theoretical and has no practical applications in genomics, signal processing, or machine learning"
    },
    "correct_answer": "C",
    "difficulty": 5,
    "x": 0.903764,
    "y": 0.697364,
    "z": 0.193994,
    "source_article": "high-dimensional statistics",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "high-dimensional statistics",
      "sparsity",
      "LASSO",
      "sample covariance singularity",
      "structural assumptions"
    ]
  },
  {
    "id": "ee1358c4eede2765",
    "question_text": "Stein's method is a technique for proving distributional convergence by characterizing distributions through differential operators. What is the Stein characterization of the normal distribution, and why is it powerful for proving CLT-type results?",
    "options": {
      "A": "Stein's method characterizes the normal distribution using its moment generating function and is simply an alternative notation for the standard characteristic function proof of the CLT",
      "B": "Stein's method works only for the normal distribution and cannot be extended to Poisson, exponential, or any other distributional approximation",
      "C": "Stein's method requires the random variables being summed to be independent and identically distributed, providing no advantage over classical CLT proofs for dependent or non-identically distributed sequences",
      "D": "Z ~ N(0,1) iff E[f'(Z) - Zf(Z)] = 0 for all smooth bounded f — Stein's method bounds |E[h(W)] - E[h(Z)]| for a target variable W by solving the 'Stein equation' f' - xf = h - E[h(Z)] and bounding E[f'(W) - Wf(W)], which depends on the specific dependence structure of W; its power lies in handling dependent, non-identically distributed summands with explicit error bounds, enabling Berry-Esseen-type rates even under complex dependence"
    },
    "correct_answer": "D",
    "difficulty": 5,
    "x": 0.951207,
    "y": 0.74,
    "z": 0.621137,
    "source_article": "Stein's method",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Stein's method",
      "Stein characterization",
      "distributional approximation",
      "Berry-Esseen bounds",
      "dependence"
    ]
  },
  {
    "id": "0f8907df4a20a8ac",
    "question_text": "Optimal transport theory provides a metric (Wasserstein distance) between probability distributions. Why is it preferred over KL divergence in some machine learning applications, and what is its connection to the Monge-Kantorovich problem?",
    "options": {
      "A": "The Wasserstein distance W_p(μ,ν) = (inf_{γ ∈ Γ(μ,ν)} ∫ d(x,y)^p dγ)^(1/p) metrizes weak convergence for compactly supported measures and is finite even when μ and ν have non-overlapping supports (where KL divergence is infinite) — this makes it ideal for GANs (Wasserstein GAN) where generator and data distributions may have disjoint supports during training; the Monge problem seeks an optimal transport map T pushing μ to ν, while Kantorovich relaxes this to a coupling γ ∈ Γ(μ,ν), and the dual formulation gives W₁ = sup_{‖f‖_L ≤ 1} E_μ[f] - E_ν[f]",
      "B": "Wasserstein distance is always larger than KL divergence and provides strictly worse bounds in every statistical application compared to total variation or Hellinger distance",
      "C": "The Monge-Kantorovich problem has no connection to probability theory and concerns only physical transportation of goods between factories and warehouses",
      "D": "Wasserstein distance can only be computed between distributions on the real line ℝ and has no generalization to ℝᵈ or abstract metric spaces"
    },
    "correct_answer": "A",
    "difficulty": 5,
    "x": 0.936058,
    "y": 0.663329,
    "z": 0.500256,
    "source_article": "Wasserstein distance",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "Wasserstein distance",
      "optimal transport",
      "Monge-Kantorovich",
      "WGAN",
      "Kantorovich duality"
    ]
  },
  {
    "id": "739ed8abf0b42378",
    "question_text": "Conformal prediction provides distribution-free prediction intervals with guaranteed finite-sample coverage. What is the key insight, and why does it require only exchangeability rather than any parametric assumption?",
    "options": {
      "A": "Conformal prediction requires the data to follow a specific parametric distribution (typically Gaussian) and provides no coverage guarantees for non-parametric settings",
      "B": "Given exchangeable data (Z₁,...,Zₙ,Zₙ₊₁), conformal prediction constructs a prediction set C(Zₙ₊₁) by including all labels y for which the nonconformity score of (Xₙ₊₁, y) is not among the most extreme — exchangeability ensures each Zᵢ is equally likely to have the largest score, giving P(Yₙ₊₁ ∈ C) ≥ 1-α with exactly ⌈(1-α)(n+1)⌉/(n+1) coverage; this works for ANY model (neural nets, random forests), requires NO distributional assumptions beyond exchangeability, and provides valid coverage in finite samples rather than only asymptotically",
      "C": "Conformal prediction produces only point predictions and cannot construct prediction intervals or sets of any kind",
      "D": "The coverage guarantee of conformal prediction holds only asymptotically as n → ∞ and provides no finite-sample validity, making it equivalent to bootstrap prediction intervals"
    },
    "correct_answer": "B",
    "difficulty": 5,
    "x": 0.871722,
    "y": 0.706048,
    "z": 0.685627,
    "source_article": "conformal prediction",
    "domain_ids": [
      "probability-statistics",
      "mathematics"
    ],
    "concepts_tested": [
      "conformal prediction",
      "exchangeability",
      "distribution-free coverage",
      "nonconformity scores"
    ]
  }
]